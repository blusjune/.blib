INSERT INTO `radiohead_text` VALUES (279,'== _BPT-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== _BPT-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== _BPT-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n[[Bnote patidea 2013-003]]\n\n\n== _BPT-004 ==\n\n=== Event-based I/O Prediction (for Proactive Data Placement) ===\n\n=== 요약 (4) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n=== 기술 상세 (4) ===\n\n* I/O 발생 메커니즘의 모델링\n: I/O prediction을 위해, type-I event (indirect I/O trigger)와 type-II event (direct I/O trigger)를 관찰 및 분석함. Type-I event의 예로서, user의 log-in, 기 log-in되어 있던 user session의 상태 변경 (idle-to-busy), system service에 대한 invocation from local or remote node 등이 포함될 수 있으며, type-II event의 예로서, user application 의 실행, system service의 실행 등이 포함될 수 있음. Type-I event는 type-II event의 발생에 영향을 미치고, type-II event는 I/O request event (data access)의 발생에 영향을 미치므로, \'type-I event\' -> \'type-II event\' -> \'I/O request event\' 까지 이어지는 인과사슬 (cause-effect chain) 기반으로 I/O 발생 메커니즘을 모델링하였음.\n\n\n* Event 정보 수집: type-I event 정보 및 type-II event 정보 정의 및 수집 방법.\n\n\n* I/O 패턴 학습 (Offline Learning): 수집된 Event 정보를 기반으로 I/O 패턴을 learning하는 방법\n\n\n* Run-time I/O 예측 (Online Matching): 시스템 모니터링에서 얻어지는 최근 및 현재의 event들과 learning으로 얻어진 I/O 패턴 DB에 기반하여 Run-time으로 미래 I/O 예측\n\n\n* 시스템 구조: 정보 수집 및 패턴 학습 모듈의 시스템 내 위치 (I/O 패턴 DB 등의 위치 및 실시간 예측 모듈과의 관계)\n\n\n* 순서도: 수집부터 학습까지 전체적인 task의 흐름\n\n\n* 예상 효과: 성능적 측면에서 사용자가 체감하는 lower latency, higher throughput 효과, 이미 발생해버린 congestion time을 피해 미리 적절한 곳에 data를 배치시킴으로써 얻어지는 bottleneck prevention 효과.\n\n=== Data Collection (4) ===\n\n* 역할: I/O 패턴 학습/발굴에 필요한 정보 수집\n\n* 수집 정보 종류\n: 전조 기반 예측에 필요한 간접 I/O 유발자, 직접 I/O 유발자, I/O 발생 이벤트 등 I/O 예측에 도움이 될 수 있는 다양한 정보를 수집. 다음 정보들이 포함될 수 있음.\n:- indirect I/O trigger trace: {(user-id | user-type), status-type, before-status, after-status, event-time}\n:- direct I/O trigger trace: {(program-id | program-type), status-type, before-status, after-status, process-owner, event-time [, process-current-working-directory, files-opened]}\n:- I/O trace: {access-time, address-accessed, read/write-flag, io-size, file-path, process, co-accessed-addresses}\n\n\n* 수집 모듈의 위치\n: storage-server가 분리되어 있는 구조인 SAN/NAS 타입의 architecture인 경우에는 application이 구동되는 server-side에 정보 수집 모듈이 위치한다. storage-server가 결합되어 있는 converged-architecture 기반의 system인 경우에는 해당 system 내에 정보 수집 모듈이 위치한다.\n\n\n* 정보 수집 방식\n: 운영체체가 제공하는 시스템 상태 정보 제공 interface를 이용하거나, blktrace, lsof, debug file system 등 이용 (e.g., /sys/kernel/debug/ 등) 혹은 /proc file system 등을 이용 가능.\n\n== _BPT-005 ==\n\n=== Coaccess-based I/O Prediction (for Proactive Data Placement) ===\n\n* (영문) Coaccess-based I/O Prediction (for Proactive Data Placement)\n* (한글) Coaccess 기반의 I/O 예측 방법 및 시스템 구조\n\n=== 요약 (5) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 coaccess-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n\n=== 발명의 이용 분야 (5) ===\n\n데이터를 저장하고 처리하는 일반적인 컴퓨팅 시스템으로서,\n* 프론트엔드/백엔드 서버\n* 네트워크 스토리지 시스템\n* 컨버지드 서버-스토리지 시스템\n* 엔터프라이즈 혹은 데이터 센터 내의 IT 인프라\n등에서 활용 가능.\n\n\n=== 발명의 목적 (5) ===\n\n==== 기존 기술의 한계점 ====\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n\n==== Intelligent I/O Prediction의 필요성 ====\n\n\n\n\n=== 기술 상세 (5) ===\n\n* Data의 coaccess 특성에 기반한 proactive data placement 기법\n: 소정의 time window 내에 같이 access되는 확률이 높은 data 그룹 (coaccess group)이 있을 때, 그 그룹 내의 일부 data가 access되면 나머지 data들도 미리 fast media에 가져다 놓음으로써, 성능 향상을 얻을 수 있게 하는 방법임.\n\n\n* 개본 개념 정의 (coaccess property, coaccessness, coaccess group)\n: 지정된 시간 동안의 measurement window 내에서, 어떤 data가 access될 때 함께 access되는 경향을 가지는 다른 data들이 존재할 때 그 data들 간에 coaccess property 혹은 coaccessness가 있다고 정의한다.  이때, coaccess property를 가지는 data 그룹을 coaccess group이라고 한다. frequent item set mining 관점에서 보면 기준이 되는 support 값 이상의 확률로 관측되는 transaction들을 coaccess group으로 정의할 수 있다.\n\n\n* 추가 개념 정의 (merged coaccess group)\n: 서로 다른 coaccess group들을 개별적으로 다루기보다 이들을 묶어서 하나의 coaccess group으로 만들어 다룸으로써 meta data management의 효율성 및 proactive data placement의 효과 측면에서 장점이 발생하는 경우가 있다. 이렇게 만들어지는 coaccess group을 merged coaccess group이라 정의한다. \n: 서로 다른 coaccess group CG_A와 coaccess group CG_B가 있는데, CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합) 시켜서 새롭게 얻어진 CG_C = {CG_A, CG_B}를 하나의 coaccess group으로 다룬다.\n: computing resource의 한계로 인한 현실적인 문제로 인해 모든 경우의 frequent item set을 발굴하기 어려운 경우가 있음 (Frequent item set mining 시 실제로 maximum size of transaction을 한계 값으로 지정). merged/unionized coaccess group은 이러한 한계적인 상황을 극복할 수 있는 방법으로서 제안되는 개념임. (실제로 관측되었던 실험 data 및 graph를 이용하여 실재 근거를 제시할 것)\n\n<!--\nmerged coaccess group 관련해서.\n\n: CG_A = {a1, a2, ..., aM |supp=s1}, CG_B = {b1, b2, ..., bN |supp=s1}, CG_C = {union of CG_A and CG_B |supp=s1} 이 존재하고 발생 시기가 일치하는 것을 확인하였다면, CG_A, CG_B, CG_C 를 모두 개별적인 coaccess group으로 관리할 것이 아니라, CG_C 만을 coaccess group으로 관리한다. (조건 i) supp(CG_A) == supp(CG_B) == supp(CG_C), (조건 ii) union of CG_A and CG_B is identical to CG_C, (조건 iii) occurrence_time(CG_A) == occurrence_time(CG_B) == occurrence_time(CG_C) 이 있다고 하자. 상기 조건 i, ii, iii이 모두 만족되는 경우에는 CG_C 만을 coaccess group으로 관리하고 지정함으로써, coaccess group의 meta data management overhead를 감소시키고 coaccess-based proactive data placement의 operation efficiency를 증대시킴. /* 여기서, (조건 i)과 (조건 ii)가 만족되면 굳이 (조건 iii)까지 확인할 필요는 없을 수도 있음. 확인 필요 */\n* GCD (Greatest Common Divisor) / LCM(Least Common Multiple) / largest coaccess group 개념과 관련.\n\n* 유사 coaccess group 간 merging?\n: 예를 들어 coaccess group CG_A = {\'a\', \'b\', \'c\', \'d\'} 와 coaccess group CG_B = {\'b\', \'c\', \'d\', \'e\'} 가 있다고 했을 때, \'b\', \'c\', \'d\' data 가 겹치기는 하지만, CG_A와 CG_B는 동일한 coaccess group으로 볼 수는 없다. 그러나 만약 CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 (기준이 되는 확률 값은 시스템과 workload의 상황에 따라서 적절히 조절 가능) CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합)화 하여 단일 coaccess group으로 다루는 방식. (말이 되는 소리인지 확인 필요)\n-->\n\n\n* frequent item set mining을 활용한 coaccess group 발굴 방법\n: Coaccess group 발굴을 위해 frequent item set mining을 이용할 수 있으며, 구체적인 방법은 다음과 같다.\n: (1) 시스템의 resource 상황에 따라 적절한 support 값을 설정한다. 예를 들어 fast media (cache 혹은 tier 1 storage)의 저장 가능 용량에 따라, 얼마나 높은 등급의 hot data까지 fast media에 placement 시킬 수 있을지가 결정될 수 있다. 즉 fast media의 용량이 매우 넉넉하여 10번 중 2번의 빈도로 (support=0.2) 관찰되는 coaccess group 까지 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있고, 혹은 fast media의 용량이 여유롭지 않아서 10번 중 5번 이상의 빈도로 (support=0.5) 관찰되는 coaccess group까지만 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있다. support 값은 상황에 따라 dynamic하게 설정될 수 있으며, 시스템 관리자 혹은 data scientist등 사람에 의해 수동적으로 설정될 수도 있지만 algorithm에 의해 automatic하게 설정되도록 구현될 수도 있다.\n: (2) coaccess group에 대한 단위 관측 시간이 되는 transaction measurement window (TMW) 크기를 설정한다. transaction measurement window의 크기에 따라 coaccess group에 포함될 수 있는 data의 갯수가 달라질 수 있다. transaction measurement window 크기가 너무 작으면 함께 access되는 것으로 판단될 대상이 되는 data 갯수 자체가 감소하게 되므로 coaccessness 분석이 어렵게 된다. 반대로, transaction measurement window 크기가 너무 크면 coaccess group의 종류는 감소하고 각 coaccess group의 크기는 커지는 경향이 생기는데, 이 경우의 문제점은 data placement 시 fast media를 효율적으로 사용하는 것이 어려워질 수 있다는 것이다.\n: (3) access되는 data address 정보를 기반으로 frequent item set mining을 시작한다 (이를 위해서는 support 값과 transaction measurement window size 값이 설정되어 있어야 함).  transaction measurement window 동안 access 되는 address들의 집합을 하나의 transaction (basket)으로 묶는다. 전체 관찰 기간 동안 N 번의 transaction measurement window가 있었다면, N 개 만큼의 transaction이 만들어진다. 이 때 N 개의 transaction을 통틀어 support 값 확률 이상으로 빈번하게 등장하는 item set이 있다면 그것을 coaccess group으로 지정하고 coaccess group management ID를 부여, 관리한다.\n: (4) frequent item set mining에서의 confidence 개념을 도입하여 precursor의 존재 감지 시, 미래에 access되게 될 나머지 data들을 신속하게  (일부 데이터가 access되고 나면 나머지 데이터들도 지정된 시간 내에 함께 access될 확률을 의미하게 됨)\n: (4) run-time 시, coaccess group의 빠른 속도로 찾아낼 수 있도록 하기 위해, coaccess group \n\n\n : coaccess group 발굴이 되었다면, 어떻게 저장하고, 어떻게 불러내서(매칭시켜서) proactive data placement에 활용할 수 있게 할 것인가?\n\n\n* O(1)으로, 매칭되는 패턴을 찾을 수 있게 하려면?\n\n\n* proactive data placement를 위해 \n\n\n* Coaccess group pattern matching 방법\n\n\n* Fig{association graph, temporal occurance graph}\n: Frequent item set mining 분석 결과로 얻어진 association graph 및 시간에 따른 frequent item 발생 추이 graph를 이용하여 coaccess pattern의 실재함을 설명\n\n\n* Coaccess property는 다음과 같은 방식으로 활용될 수 있음\n:- 특정 coaccess pattern이 특정 workload에 대한 precursor가 되는 경우가 있을 수 있음. 이를 이용하여 다가올 workload의 prediction에 활용 가능 (예를 들어, coaccess group A가 access된 후 T1 시간 경과 후에 address range B를 random read하는 workload가 발생하고, T2 시간 경과 후에 address range C를 sequential write하는 workload가 오는 경향이 있다는 것을 알고 있다면, 이 정보를 기반으로 proactive하게 address range B를 fast media에 placement 시키고, address range C에 대해서는 caching  \n:- coaccess group 내의 일부 data가 access되면 그 coaccess group 내의 다른 data들을 미리 fast media에 가져다 놓음으로써 성능 향상 가능\n\n<!--\n* (for event-based I/O prediction) coaccess group이 access되는 순서를 event-based I/O prediction에 이용할 수도 있음. 예를 들어, coaccess group A가 access되고 나서 T1 시간 내로 coaccess group B가 access될 확률이 80% 이상이라고 하고, coaccess group B가 access되고 나서 T2 시간 내로 coaccess group C와 coaccess group D가 access될 확률이 90% 이상이라고 한다면, 이러한 정보를 기반으로 ... 무엇을 할 수 있나?\n-->\n\n\n* 이를 위해서 기존의 storage system 구조에 (1) coaccess pattern analysis engine, (2) coacccess pattern aware data placement controller가 추가되는 것이 필요하다.\n\n\n* coaccess pattern analyzer의 역할: workload 별로(???) coaccess pattern list 구축\n\n\n* coaccess pattern analyzer의 구조 및 동작 방식:\n\n\n* coaccess pattern aware data placement controller의 역할:\n\n\n* coaccess pattern aware data placement controller의 구조 및 동작 방식: coaccess pattern 정보를 받아들이기 위한 interface, interface로 받아들인 coaccess pattern 정보의 queuing? data placement의 I/O 단위 (granularity), data placement의 주기 (static? or dynamic?) 등 coaccess pattern 정보를 data placement 시 어떤 식으로 활용하는지\n\n=== Coaccess-based I/O Prediction의 성능 향상 효과 Estimation ===\n\n* 시뮬레이션 기반의 성능 향상 효과 비교\n:- Fast media의 4KB random read latency는 96.47 usec (SSD 830 128GB 기준), slow media의 4KB random read latency는 1,884.03 usec (HDD 7,200rpm 2TB 기준)으로 설정 (iozone에 의한 실측값 사용)\n:- 실험 시작 전에는 4GB 크기의 target data가 slow media (HDD)에 저장되어 있음.\n:- 여기에 1,000,000 개의 4KB random read request로 구성되는 workload를 인가. (각 read request들은 서로 중복되는 data를 가리키지 않으며, 4GB 크기의 target data를 한 번씩 다 request하게 된다고 가정)\n:- Workload 패턴 학습 결과, 4GB의 전체 data 중, \'가\' type의 coaccess 특성을 가지는 10MB 크기의 data 그룹 (data set 당 2,500개의 4KB data 포함)이 200개가 있으며, \'나\' type의 coaccess 특성을 가지는 1MB 크기의 data 그룹 (data set 당 250개의 4KB data 포함)이 1,000개 있는 것을 발견한 것으로 가정.\n:- \'가\' type의 coaccess 패턴은 해당 data 그룹 내 100개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 80%인 것으로 정의.\n:- \'나\' type의 coaccess 패턴을 해당 data 그룹 내 50개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 100%인 것으로 정의.\n\n\n\n\n본 기법을 적용한 경우와 그렇지 않은 경우의 성능 차이를 IOPS로 비교해보면 다음과 같다. \n\n: 예를 들어 설명하면 다음과 같다.\n: data 1, data 2, ..., data 10000으로 구성된 data set이 있으며, 각 data들의 크기는 4KB이다. 학습을 통해서, (1) 이 중 어느 한 data가 access되었을 때, 1분 이내에 나머지 data들도 같이 access되게 될 확률은 55%, (2) 이 중 2개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 80%, (3) 이 중 3개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 95%라는 것을 알고 있다고 가정한다.\n: fast media의 크기는 4GB 이며, 기존에 \n\n이라는 것을 알고 있다면, data 1, 2, 3이 access되었을 때 data 4~128까지도 미리 proactive하게 fast tier에 배치시킴으로써, 그로 인한 성능 이득을 얻을 수 있게 된다. (!!!실제 실험 사례?!!!)\n\n=== Real Data Analysis (MSN File Server) (5) ===\n\n* Analysis Target Workload Specification\n:- MSN FileServer Tracelog (2008-03-10, 약 5GB 데이터)\n:- 21720977703 microseconds (약 6시간)\n:- 29345085 IO operations / 21720 sec => 1351 IOPS\n\n\n* 첫 16.5 minutes (990 seconds) 동안의 I/O에 대해 10 seconds 단위의 Measurement Window로 측정\n\n\n* Frequent Item Sets Mining (Eclat algorithm)\n: support=0.3, minlen=1, maxlen=15\n <pre>\n> fi_s3 <- eclat(ciop_data, parameter=list(supp=0.3, maxlen=15, minlen=1, tidLists=T))\n\nparameter specification:\n tidLists support minlen maxlen            target   ext\n     TRUE     0.3      1     15 frequent itemsets FALSE\n\nalgorithmic control:\n sparse sort verbose\n      7   -2    TRUE\n\neclat - find frequent item sets with the eclat algorithm\nversion 2.6 (2004.08.16)         (c) 2002-2004   Christian Borgelt\ncreate itemset ...\nset transactions ...[1146629 item(s), 100 transaction(s)] done [1.31s].\nsorting and recoding items ... [21 item(s)] done [0.12s].\ncreating bit matrix ... [21 row(s), 100 column(s)] done [0.00s].\nwriting  ... [28761 set(s)] done [0.01s].\nCreating S4 object  ... done [0.25s].\n\n\n\n\n> summary(fi_s3)\nset of 28761 itemsets\n\nmost frequent items:\n  22159360   22155264 1504849920 3170566144 4086710272    (Other)\n     14372      14370      14336      14336      14336     123094\n\nelement (itemset/transaction) length distribution:sizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13\n  21  168  696 1951 3933 5808 6336 5115 3025 1276  364   63    5\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n  1.000   6.000   7.000   6.775   8.000  13.000\n\nsummary of quality measures:\n    support\n Min.   :0.300\n 1st Qu.:0.330\n Median :0.340\n Mean   :0.388\n 3rd Qu.:0.430\n Max.   :0.920\n\nincludes transaction ID lists: TRUE\n\nmining info:\n      data ntransactions support\n ciop_data           100     0.3\n> dim(tidLists(fi_s3))\n[1] 28761   100\n\n</pre>\n\n\n=== Brief Description of the Drawings (5) ===\n\n# [Concept] Coaccess-based I/O Prediction Concepts\n# [Architecture] Storage System Architecture with Coaccess-based Proactive Data Placement\n# [Concept] Coaccessness Analysis using Frequent Itemset Mining\n# [Flowchart] Proactive Data Placement based on Coaccessness\n# [Auxiliary] Experiment Results 1\n# [Auxiliary] Experiment Results 2\n# [Auxiliary] Performance Gain Estimation\n# [Auxiliary] Comparison with prior arts\n\n== Memo for {_BPT-004, _BPT-005} ==\n\n\n=== Think outside the box // Questions ===\n\n* 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n\n* Tiering을 좀 더 다양한 각도에서 바라보자\n:- 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 것이 기존의 tiering 개념임.\n:- SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n:- SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n:- access pattern-aware optimal placement (APOP) - 이런 것은 어떨까?\n:- Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n\n* write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반의 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지?\n\n\n* 만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? SSD-internal memory buffer를 이용해서 SSD를 NVRAM처럼 쓰도록 하는 것이 실제로 말이 되는 이야기일까?\n\n\n\n\n\n=== 기존 기술의 한계점 (Intelligent I/O Prediction 기술의 필요성) ===\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache만으로는 성능 향상을 기대하기 어렵게 됨.\n\n <pre>\n실제 tracelog 사례를 보여주고 hot area가 변화하고 있음을 보일 것.\n</pre>\n\n* Data Placement (Tiering) 관점에서 본 EMC의 FAST, FAST-VP 기술의 한계점\n:- Coarse-grained Data Migration (768KB 까지 작게 나눌 수 있다고는 하지만, 실제로 그렇게 해서 성능이 얼마나 잘 나오는지는 확인 필요. 귀중한 자원인 DRAM을 관리를 위한 metadata로 가득 채우게 될 위험도 있음)\n:- Flexible하지 않은 Tiering 주기\n:- Busy-hour 지난 후 Idle-time에 Tiering 실시\n:- 낮은 지능의 미래 I/O 예측 능력\n:: data access count를 기반으로 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두도록 하는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n\n=== 본 발명의 차별화 포인트 ===\n\n* 본 기술의 특징 1: I/O를 triggering하는 전조 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Indicator는 Run-time에 저비용으로 감지될 수 있는 형태의 정보들로 구성\n:- 미래에 access될 data를 예측케 하는 Indicator간의 mapping은 Bayesian Inference 이용\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n=== 특허 검색/분석 결과 {storage I/O workload prediction} ===\n\n* 검색식: storage I/O workload prediction\n\n* http://portal.uspto.gov/pair/PublicPair 에서 특허 상태 확인 가능.\n\n* I/O prediction을 기반으로 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n:- 기존엔 proactive tiering이 없었나? -> IBM 특허가 나름 유사함. (US7930505, US7512766)\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n==== 1. 비교가 될만한 특허들 ====\n\n예측에 대한 언급이 되어 있는 스토리지 시스템 관련 특허들. 크게 두 가지로서, 그 중의 하나는 가상 volume에 대해 peak I/O가 발생하는 주기를 분석하여, peak I/O 발생시점이 닥치기 전에 미리 가상 volume의 data를 migration 시키는 방법에 대한 것이나, 어느 위치의 data가 언제 access될지에 대한 예측은 아님 (IBM 특허). 다른 하나는 I/O 예측을 기반으로 미래에 자주 access될만한 data를 미리 여러 노드에 분산시키는 방법에 대한 것이나, 예측 방법 자체에 대한 내용은 다루고 있지 않음 (HP 특허)\n\n\n; US7930505, US7512766 (IBM)\n\n* US7930505 (Application Number: US 12/053582), \"Controlling preemptive work balancing in data storage,\" IBM, 특허일 2011-04-19\n: storage network의 I/O 관찰을 통해 peak I/O activity의 repeating instance에 대한 signature를 VLU (virtual logical unit) 단위로 생성하고, 이에 기반하여 언제 heavy I/O가 도래할 것인지를 예측하여, heavy I/O가 오기 전에 미래 data를 faster media로 migration하도록 하는 tiering 기술에 대한 특허. (이미 I/O peak time이 온 후에 hot data를 옮기는 것은 오히려 시스템의 IO성능을 악화시킬 수도 있기 때문에 미리 migration을 하고자 하는 것임)\n: VLU 단위로, peak I/O activity가 어느 주기로 발생할지에 대한 예측임. 지능 수준과 Fine-granularity 측면에서 초보적인 수준임.\n\n* US7512766 (Application Number: US 11/213494), \"Controlling preemptive work balancing in data storage\", IBM, 특허일 2009-03-31\n: 위의 US7930505 특허와 동일 건으로 보임\n\n\n; US6912635 (HP)\n\n* US6912635 (Application Number: US 10/140968), \"Distributing workload evenly across storage media in a storage array\", HP, 특허일 2005-06-28\n: 미래에 자주 access될만한 data를 여러 arrayed storage device에 분산시키겠다는 기술. 그러나 I/O에 대한 예측 방법은 다루어지고 있지 않음.\n\n==== 2. 검색은 되었으나 관련은 없는 특허들 ====\n\n무언가를 예측한다는 측면, Storage System에 대한 기술이라는 측면에서는 관련이 있으나, 미래에 Access될 Data를 예측한다는 측면에서 제안 기술과 관련 없는 특허들\n\n\n; US7640231 (IBM)\n* US7640231 (Application Number: 11/280012), \"Approach based on self-evolving models for performance guarantees in a shared storage system\", IBM, 특허일 2009-12-29\n: network 기반 storage system에서, average latency 혹은 I/O request rate 등에 대한 주기적인 performance sampling을 기반으로, 주어진 objective function의 값을 최대화할 수 있도록 system resource를 제어할 수 있게 하는 기술.\n\n\n; US7774491, US7519725 (IBM)\n\n* US7774491 (Application Number: 12/359377), \"Utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2010-08-10\n: 복수의 클라이언트들을 대상으로 통계적인 성능 QoS를 보장하기 위한 informed throttling 방법\n: A system for utilizing informed throttling to guarantee quality of service to a plurality of clients\n\n* US7519725 (Application Number: US 10/444712), \"System and method for utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2009-04-14\n: resolve the problem of providing statistical performance guarantees for applications generating streams of read/write accesses (I/Os) on a shared, potentially distributed storage system of finite resources, by initiating throttling whenever an I/O stream is receiving insufficient resources\n\n\n; US7827283 (IBM)\n* US7827283 (Application Number: 10/367444), \"System for managing and controlling storage access requirements\", IBM, 2010-11-02\n: SAN 등의 환경에서, storage bandwidth를 관리하고 reserve하는 기술\n: A Resource Management and Reservation System (RMRS) for managing and reserving storage bandwidth, is a platform independent middleware layer that provides an interface to applications, their database management systems, or some other higher level data management systems like ADRS which does data management on behalf of the applications\n\n\n; US8433554, US8140319 (IBM)\n\n* US8433554, \"Predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2013-04-30\n: An approach for predicting performance and capacity of an information technology (IT) system before the IT system is built is described, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n* US8140319, \"Method and system for predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2012-03-20\n: A method and system for predicting performance and capacity of an information technology (IT) system before the IT system is built, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n\n\n; US8255915 (HP)\n* US8255915, \"Workload management for computer system with container hierarchy and workload-group policies\" HP, 특허일 2012-08-28\n\n\n; US7050956 (HP)\n* US7050956 (Application Number: US 09/843930), \"Method and apparatus for morphological modeling of complex systems to predict performance\", HP, 특허일 2006-05-23\n: I/O 성능 예측을 위한 complex storage system에서의 data flow 모델링 방법\n\n\n; US6718434 (HP)\n* US6718434 (Application Number: US 09/871043), \"Method and apparatus for assigning raid levels\", HP, 특허일 2004-04-06\n: Data를 disk array에 load하기 전에, workload 명세 및 device 명세에 기반하여, 적합한 RAID level을 판단하여 disk array에 할당 (RAID build)하는 방법\n\n\n; US8112586 (EMC)\n* US8112586 (Application Number: US 12/190071), \"Predicting and optimizing I/O performance characteristics in a multi-level caching system\", EMC, 특허일 2012-02-07\n: multi-level caching 시스템에서, cost대비 performance를 최적화할 수 있도록, workload에 따른 resource allocation design에 대한 recommendation 기술 (multi-level cache들 간의 workload propagation을 예측하는 것이 핵심)\n\n\n; US6629266 (IBM)\n* US6629266 (Application Number: US 09/442001), \"Method and system for transparent symptom-based selective software rejuvenation\", IBM, 특허일 2003-09-30\n: software가 언제 동작이 멈추는지를 예측함으로써, software의 dependability를 향상할 수 있게 하는 방법\n\n\n; US5088058 (Unisys)\n* US5088058 (Application Number: 07/237120), \"Apparatus and method for evaluating and predicting computer I/O performance using I/O workload snapshots for model input\", Unisys Corporation, 특허일 1992-02-11\n: I/O delay 등에 대한 확률적 모형을 기반으로 disk 시스템의 성능을 검증하고 예측하는 시뮬레이션 기반의 방법\n: A method of evaluating and predicting the performance of an I/O disk system comprised of one or more disk controllers and cooperating disk units using a simulation model containing a stact constructed such that delays are formulated as sets of stochastic processes\n\n=== Comparison of I/O Prediction Approaches ===\n\n미래에 access될 data의 위치와 시기를 예측하기 위한 방법으로 event-based I/O prediction, coaccess-based I/O prediction, periodicity-based I/O prediction 방식이 제안되며, 각각의 방식은 예측에 필요한 연산량 및 input data의 richness, 그리고 workload 변화에 대한 적응성 측면에서 장단점을 가지고 있음. 따라서, 상황에 따라서 적절한 방식을 이용하면 됨.\n\n{| style=\"border-collapse: collapse; border: 1px solid #000\" border=1\n| style=\"width: 25%\" | 비교 항목\n| style=\"width: 25%\" | periodicity-based I/O prediction\n| style=\"width: 25%\" | coaccess-based I/O prediction\n| style=\"width: 25%\" | event-based I/O prediction \n|-\n| context richness required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| computation overhead required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| adaptability to workload changes (lower is better)\n| high\n| high\n| high\n|-\n|}\n\n\n* 전조 기반 예측 vs. 주기성 기반 예측\n{| border=1\n| 비교 항목\n| 전조 기반 예측\n| 주기성 기반 예측\n|-\n| 기술 요약\n| 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식\n| address 별 access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식\n|-\n| 분석 오버헤드\n| High (학습/마이닝에 드는 비용임)\n| Low (주기성 파악 비용임)\n|-\n| Run-time 오버헤드\n| Mid (실시간 전조 모니터링 및 매칭 비용)\n| Low (주기성 계산)\n|-\n| Workload 변화 적응성\n| High\n| High (Workload 변경 사실 알림 hint 필요)\n|-\n| ...\n| ...\n| ...\n|-\n| Incrementality of Mining/Learning\n| ...\n| ...\n|}\n\n* 전조 기반 예측은, 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식임.  주기성 기반 예측은, data access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식임.\n\n\n=== Intelligent Data Placement ===\n\n* Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case\n: 여기에 HML이 사용될 수 있는 것이라면, 어떤 방식으로 어떤 이유로 그런 것일까?\n\n\n* I/O 예측 방식으로서 다음 방식이 제안되며, 각 방식에 따라서 prediction model, 수집되어야 하는 context data, I/O 패턴 DB에 저장되는 정보의 종류 및 형태가 달라질 수 있음. 예측에 이용할 수 있는 context 정보의 richness에 따라 적절한 방식을 사용할 수 있다. (서로 다른 발명이므로, 별도의 특허로 낸다)\n:- event-based I/O prediction\n:- coaccess-based I/O prediction\n:: 여기서, periodicity-based I/O prediction은 기존 특허에서 언급되었으므로, 본 발명에서는 크게 언급하지 않기로 한다.\n\n\n* I/O prediction 결과에 따라 적절한 위치에 data를 배치하게 되는데, 여기서 적절한 위치라 함은 SSD와 같은 fast tier가 될 수도 있겠으나, sequential read/write 같은 경우 striping된 HDD array가 될 수도 있고, 혹은 SAS HDD가 될 수도 있겠음. 또한 SSD 중에서도 SLC, MLC, TLC 로 구분되는 fine-grained fast tier로 구분될 수도 있음\n\n\n* 미래에 오게 될 I/O의 type도 알 수 있을까? 즉, {random/sequential}x{read/write} type 중 어느 것에 해당되는지를 안다면, 그리고 I/O의 intensiveness도 예상할 수 있다면 그에 맞춰서 placement 시 적절한 tier 혹은 적절한 node로 찾아가게 할 수 있을 것임.\n\n\n* context 정보는, 수집되는 위치에 따라, server-side에서 수집되는 context data와 storage-side에서 수집되는 context data로 분류될 수 있음. Event context data는 server-side에서 수집 가능하며, coaccess context data 및 periodicity context data는 storage-side에서 수집될 수 있음.\n\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n: 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n\n=== Design/Implementation Issues ===\n\n* workload type/class tagging 필요\n\n* cache와 tiering을 같이 쓸 때, DRAM cache가 아니라 Flash Cache의 경우 쓸데없이 모든 data가 Flash Cache에 다 들어가게 두기 보다는, tiering에서 cover하지 못하는 것들 위주로 caching하도록 하면 Flash Cache의 공간 낭비를 줄임으로써, 유효 cache 공간을 늘일 수 있게 되고, 그만큼의 성능 향상으로 연결될 수 있음.\n\n* DRAM 기반의 write cache를 적극 이용하여 마치 log-structured file system에서 제공하는 I/O merging and sequentialization 효과를 얻을 수 있음.\n\n* Tiering되는 data는 caching되는 data보다 상대적으로 coarse-grained하게 grouping하여 다룸으로써, 한꺼번에 많은 data를 move할 수 있도록 하고, meta data 관리에 필요한 overhead를 줄일 수 있게 한다. 당장 access되지 않더라도 일정 시간 내에 같이 access될 수 있는 data들은 함께 faster tier로 옮겨진다. 이때 필요한 정보가 coaccess 정보임.\n\n* Cache - Tiering 기법의 연동 방법: 예측이 실패했거나, target tier media의 가용 공간이 부족하여 tiering에서 놓치는 missing data가 있을 수 있음. 이러한 missing data들이 temporal locality가 있는 경우에는 당장은 faster tier에서 serve하지 못하더라도 cache에서 serve될 수 있는데, temporal locality가 적은 경우에는 cache만으로 cover되지 않는다. \n\n* Management table size optimization issue - 만약 모든 LBA 별로 event-based I/O prediction을 해야 한다면 현실적으로 사용 가능한 아키텍쳐가 아닐 수 있음. 만약 spatial locality가 있어서 LBA 1,000개 단위로 묶어서 대응할 수 있다면, 최소 1/1000 의 management table size 감소 효과를 얻을 수 있음. pattern mining 혹은 machine learning 실행 시에는 record 수가 감소함으로써 얻는 이득은 1,000배 이상이 될 수 있음. 관건은 얼마나 LBA 묶음을 크게 만들어서 예측에 사용할 수 있는지임.\n\n\n=== I/O Insight ===\n\n* ML 기반의 I/O Insight 발굴 (Data Placement를 위한 Macroscopic Guideline)\n: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:-> 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n\n* I/O Insight의 요건:\n:# indication, as simple as possible\n:# indication, as specific as possible\n:# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n:# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n\n* I/O Insight의 형태\n: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n=== X-Y Modeling ===\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n=== 예측 실패 대비 (별도 특허화 가능) ===\n\n* I/O 예측이 실패할 수 있으며, 이로 인한 성능 penalty를 최소화하기 위한 방법 필요.\n:- fast tier에 여유공간이 있다면 1타겟과 2타겟을 모두 올려놓아도 되지 않을까? 1타겟과 2타겟을 placement함으로써 예측 정확도를 95%까지 올릴 수 있다면, 꽤 쓸만할 수 있겠음. 그리고 1타겟과 2타겟의 모든 데이터를 다 fast tier에 올려두지 말고 일부만 fast tier에 올려두었다가, 둘 중에 하나의 타겟이 hit되었을 때, hit된 타겟의 나머지 data를 재빨리 fast tier에 올리도록 하는 incremental 방법을 사용할 수 있겠음.\n:- 그러나 이것 또한 예측 실패에 대한 완벽한 prevention이 될 수는 없다. 따라서 예측 실패가 발생하더라도 재빨리 그 성능 penalty를 mitigation할 수 있는 방법이 필요. \n:- cache를 이용하여 mitigation할 수 있을까?\n\n=== Data Read/Write 관련 Issues ===\n\n* read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n\n* 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n\n* write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간에, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면?\n\n* 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있음.\n\n* 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간이 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 scatter했던 I/O들을 나중에 다시 읽어들이려 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 원하는 고성능을 낼 수 있게 된다.\n\n* replica-based dynamic clustering\n: 위와 같은 write-and-then-read-again 상황에서, 해당 data를 저장했었던 HDD의 I/O busy/idle 여부에 상관 없이, 최고의 효과를 낼 수 있게 하기 위해, replica management를 적극적으로 활용하는 것은 어떨까? HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까?\n\n* data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 논의가 아님. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 논의임.\n\n* 그러나, 조심해야 할 것은 위의 논의는 상당히 case-dependency를 상당히 많이 탈 수도 있다는 것임. 책상 위의 아이디어로 끝날 수도 있기 때문에 조심스럽게 접근하는 것이 필요함.\n\n\n\n\n<br/>\n\n== _BPT-006 ==\n\n\n=== Data Lifetime (Valid-to-invalid) Interval 분석에 기반한 Data Placement (Tiering) ===\n\n\n* Data 별로, 혹은 LBA 별로, 혹은 Erase Block 별로? Data의 Valid to Invalid Time (Duration) 패턴을 분석할 수 있을까?\n: 이를 SSD 내에서 Low Overhead로 수행할 수 있나?\n: 혹은, 굳이 SSD 내에서 수행할 필요가 있나?\n: Host side에서 LBA 별로 분석 될 수 있다면, 이 정보를 기반으로 Tiering 시에 활용 할 수 있을 것임\n: 특정 Address에 대해서 W이 되고 나서, 다시 W이 되기까지의 시간을 측정하여 ARM 하면 될 듯. 그 두 W 간에 얼마나 많은 R들이 오는지에 대해서도 고려하여 placement 시 고려할 수 있을 것임.\n\nMeasurement Window 내에서 \'W 간의 Interval이 길다\'는 식으로 인식하는 것과, \'W의 횟수가 잦다\'고 인식하는 것 간에 어떤 차이가 있을 수 있을까?\n\n <pre>\n    01234567890123456789\na1: W-R-W-R-W-R-W-W-R--W : T20W06R04 : Short-Lifetime, Many-Reads : SLC\na2: W---W-R-W---W-W----W : T20W06R01 : Short-Lifetime, Few-Reads : SLC, DRAM?\na3: W-RRRR-W---RR-W-R-RW : T20W04R08 : Mid-Lifetime, Many-Reads : MLC\na4: W------W--R---W----W : T20W04R01 : Mid-Lifetime, Few-Reads : MLC\na5: W-R-R-R-RR-RRRR----R : T20W01R10 : Long-Lifetime, Many-Reads : MLC\na6: W------------R----R- : T20W01R02 : Long-Lifetime, Few-Reads : TLC\n</pre>\n\n=== Idle-time Prediction Based Free Space Management in SSD ===\n\n* Idle-time의 도래 시기와 지속 시간 패턴 분석에 기반한 SSD 내의 Free Space 확보 방법\n:- SSD 내에서 Low overhead (in computation, memory space)로 Idle-time 도래 시기 및 지속 시간 분석이 가능한가?\n:- 가능하다면, SSD 내에서 Garbage Collection 등으로 Free Space를 확보하는 데 있어서 어떤 장점을 가질 수 있는가?\n\n\n\n: 이를 기반으로 \n\n\n\n=== 요약 ===\n* \n\n\n=== 기술 상세 ===\n\n*\n\n\n\n\n\n== _BPT-007 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== _BPT-008 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== _BPT-009 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Ef?cient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== _BPT-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== _BPT-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== _BPT-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== _BPT-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== _BPT-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== _BPT-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== _BPT-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== _BPT-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n----\n=== (SmartSSD API) SSD-internal I/O pattern logging interface and mechanism ===\n\n\n==== Questions ====\n\n* SmartSSD에서 제공할 수 있는 logging 서비스로서 어떤 것들이 있을 수 있나?\n:- 어느? 정도의 free space를 필요로하는 critical (heavy) I/O가 어떤? 주기로 도래할 지 알 수 있을까?\n:- I/O prediction에 도움될 수 있는 정보를 SmartSSD가 기록했다가 필요한 때 (안전한 방법으로?) 줄 수 있을까?\n:- I/O prediction까지는 아니더라도 해당 SSD의 I/O wellness(어떻게 정의?)를 측정했다가 bottleneck을 회피하는 데 유용한 정보를 제공할 수 있을까?\n:- 어떤 SSD가 현재 주어지고 있는 I/O workload에 대해 list performance spec을 만족하는데 어려움을 겪고 있다는 것을 알려줄 수 있는 정보가 어떤 것이 있을까? (SSD 내 write buffer의 fullness 혹은 I/O queue의 유동성? amount of free space in over-provisioning area?) (-> I/O wellness 라는 metric을 이것과 연관시켜 정의할 수도 있으며, 결국 SLA 혹은 Performance QoS와 연관시킬 수 있음)\n:- 지금까지의 S.M.A.R.T.와 비교하였을 때, 어떤 차별점이 있는가? 기존의 S.M.A.R.T.가 제공하지 못하던 타입의 정보를 제공하고, 이로 인해 기존에는 불가능했던 새로운 알고리즘 구현 혹은 새로운 서비스 제공이 가능해짐을 보이면 좋겠음.\n\n\n\n:- workload의 history?\n:- 각 erase block이 erase되어야만 했던 주기?\n:- free space의 추이 (얼마나 많이 남아 돌던지)?\n:- I/O의 heaviness 패턴?\n:- SSD내 write buffer (혹은 write cache)의 utilization정보? (즉, write buffer가 넘칠 정도가 되어서 I/O wait을 해야만 했던 상황이 얼마나 자주 발생했는지, write buffer의 가득찬 정도를 백분율로 표현한다고 했을 때, 평균/표준편차 등으로 대표할 수 있는 normal distribution을 따르는지?\n:- 전체 sequential read/write \n\n\n==== 배경 ====\n\n* 각 SSD 모델별로, 또 동일한 모델의 SSD라 하더라도 어떤 workload에 노출되어왔는지에 따라, 현재 낼 수 있는 I/O performance가 동일하지 않을 수 있다 (!check! 관련 실험 결과 - SSD 830을 이용하여 같은 시간 동안 하나의 host에서 생성되는 workload들을 나누어 받았다 하더라도 내부적인 free block의 갯수 및 random write에 의한 block 내 파편화등의 상태가 상이하여 성능에 차이가 나는 것을 보여줄 것. uFLIP 혹은 filebench 활용 가능). 한 예로, SSD가 어느 호스트의 어느 file system에 매핑되었는지에 따라 그 SSD가 받는 workload의 특성이 다를 수 있다. file system mount point가 달라지거나, 기존과 다른 새로운 응용이 설치/서비스되는 상황이 되면 \n\n\n----\n=== I/O Prediction-based Proactive Data Placement ===\n\n* 미래에 access될 data를 예측하여 적소에 미리 가져다 놓는 기술로서, (1) I/O prediction을 위한 정보 수집 모듈, (2) 주어진 정보들에 기반하여 다가올 I/O를 예측하는 모듈, (3) data placement를 수행하는 모듈로 구성됨.\n\n\n* 본 기술의 특징 1: I/O를 triggering하는 indicator (전조) 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Run-time에 저비용으로 감지될 수 있는 Indicator 정보와, 이에 의해 access될 data 간의 mapping은 Bayesian Inference 이용\n\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n\n----\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n----\n\n=== Tiering Method Avoiding I/O Bottleneck ===\n\n* I/O Prediction에 의해 예측된 미래의 Hot Data를 Proactive하게 Placement한다고 할 때, 예상되는 access pattern으로 인해 발생 가능한 I/O Bottleneck을 최소화할 수 있도록 배치하는 방법\n\n* 예를 들어 미래의 hot data block 1 (200MB), hot data block 2 (100MB), hot data block 3 (400MB)이 1분 후에 access 될 것으로 예측되었다고 가정. data block 1에 대해서는 4KB 단위로 random read가 dominant한 access pattern이 예상되고, data block 2에 대해서는 1024KB 단위의 sequential read가 dominant한 access pattern이 예상되고, data block 3에 대해서는 4KB 단위의 random read와 512KB 단위의 sequential write이 각각 80:20의 비율로 mix된 형태의 access pattern이 예상된다고 했을 때, 각 data block을 어디에 어떤 형태로 placement시킬 것인가?\n\n: machine learning 혹은 pattern mining에 기반한 base workload pattern data를 기반으로 incremental하게 update될 수 있는 data access의 spatial locality와 periodicity\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n\n* 기존 기술과의 대비 (naive I/O prediction vs. full-fledged I/O prediction)\n: 기존 기술은 누적 hit count 정보에 의지하는 naive I/O prediction으로 볼 수 있음. 이러한 방식은 workload의 변화를 제대로 반영하지 못하기 때문에, static한 workload에 대해서는 잘 동작하지만, moving target처럼 dynamic한 workload에 대해서는 오히려 과거의 누적 hit count metric이 현재의 workload 패턴을 왜곡하는 문제점을 가지고 있음. (temporal locality와 spatial locality가 깨지는 순간이 여러번 존재할 수록 기존의 naive I/O prediction 방식으로 성능 향상은 어려워짐)\n\n\n\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== _BPT-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(280,'== _BPT-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== _BPT-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== _BPT-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n[[Bnote patidea 2013-003]]\n\n\n== _BPT-004 ==\n\n=== Event-based I/O Prediction (for Proactive Data Placement) ===\n\n=== 요약 (4) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n=== 기술 상세 (4) ===\n\n* I/O 발생 메커니즘의 모델링\n: I/O prediction을 위해, type-I event (indirect I/O trigger)와 type-II event (direct I/O trigger)를 관찰 및 분석함. Type-I event의 예로서, user의 log-in, 기 log-in되어 있던 user session의 상태 변경 (idle-to-busy), system service에 대한 invocation from local or remote node 등이 포함될 수 있으며, type-II event의 예로서, user application 의 실행, system service의 실행 등이 포함될 수 있음. Type-I event는 type-II event의 발생에 영향을 미치고, type-II event는 I/O request event (data access)의 발생에 영향을 미치므로, \'type-I event\' -> \'type-II event\' -> \'I/O request event\' 까지 이어지는 인과사슬 (cause-effect chain) 기반으로 I/O 발생 메커니즘을 모델링하였음.\n\n\n* Event 정보 수집: type-I event 정보 및 type-II event 정보 정의 및 수집 방법.\n\n\n* I/O 패턴 학습 (Offline Learning): 수집된 Event 정보를 기반으로 I/O 패턴을 learning하는 방법\n\n\n* Run-time I/O 예측 (Online Matching): 시스템 모니터링에서 얻어지는 최근 및 현재의 event들과 learning으로 얻어진 I/O 패턴 DB에 기반하여 Run-time으로 미래 I/O 예측\n\n\n* 시스템 구조: 정보 수집 및 패턴 학습 모듈의 시스템 내 위치 (I/O 패턴 DB 등의 위치 및 실시간 예측 모듈과의 관계)\n\n\n* 순서도: 수집부터 학습까지 전체적인 task의 흐름\n\n\n* 예상 효과: 성능적 측면에서 사용자가 체감하는 lower latency, higher throughput 효과, 이미 발생해버린 congestion time을 피해 미리 적절한 곳에 data를 배치시킴으로써 얻어지는 bottleneck prevention 효과.\n\n=== Data Collection (4) ===\n\n* 역할: I/O 패턴 학습/발굴에 필요한 정보 수집\n\n* 수집 정보 종류\n: 전조 기반 예측에 필요한 간접 I/O 유발자, 직접 I/O 유발자, I/O 발생 이벤트 등 I/O 예측에 도움이 될 수 있는 다양한 정보를 수집. 다음 정보들이 포함될 수 있음.\n:- indirect I/O trigger trace: {(user-id | user-type), status-type, before-status, after-status, event-time}\n:- direct I/O trigger trace: {(program-id | program-type), status-type, before-status, after-status, process-owner, event-time [, process-current-working-directory, files-opened]}\n:- I/O trace: {access-time, address-accessed, read/write-flag, io-size, file-path, process, co-accessed-addresses}\n\n\n* 수집 모듈의 위치\n: storage-server가 분리되어 있는 구조인 SAN/NAS 타입의 architecture인 경우에는 application이 구동되는 server-side에 정보 수집 모듈이 위치한다. storage-server가 결합되어 있는 converged-architecture 기반의 system인 경우에는 해당 system 내에 정보 수집 모듈이 위치한다.\n\n\n* 정보 수집 방식\n: 운영체체가 제공하는 시스템 상태 정보 제공 interface를 이용하거나, blktrace, lsof, debug file system 등 이용 (e.g., /sys/kernel/debug/ 등) 혹은 /proc file system 등을 이용 가능.\n\n== _BPT-005 ==\n\n=== Coaccess-based I/O Prediction (for Proactive Data Placement) ===\n\n* (영문) Coaccess-based I/O Prediction (for Proactive Data Placement)\n* (한글) Coaccess 기반의 I/O 예측 방법 및 시스템 구조\n\n=== 요약 (5) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 coaccess-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n\n=== 발명의 이용 분야 (5) ===\n\n데이터를 저장하고 처리하는 일반적인 컴퓨팅 시스템으로서,\n* 프론트엔드/백엔드 서버\n* 네트워크 스토리지 시스템\n* 컨버지드 서버-스토리지 시스템\n* 엔터프라이즈 혹은 데이터 센터 내의 IT 인프라\n등에서 활용 가능.\n\n\n=== 발명의 목적 (5) ===\n\n==== 기존 기술의 한계점 ====\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n\n==== Intelligent I/O Prediction의 필요성 ====\n\n\n\n\n=== 기술 상세 (5) ===\n\n* Data의 coaccess 특성에 기반한 proactive data placement 기법\n: 소정의 time window 내에 같이 access되는 확률이 높은 data 그룹 (coaccess group)이 있을 때, 그 그룹 내의 일부 data가 access되면 나머지 data들도 미리 fast media에 가져다 놓음으로써, 성능 향상을 얻을 수 있게 하는 방법임.\n\n\n* 개본 개념 정의 (coaccess property, coaccessness, coaccess group)\n: 지정된 시간 동안의 measurement window 내에서, 어떤 data가 access될 때 함께 access되는 경향을 가지는 다른 data들이 존재할 때 그 data들 간에 coaccess property 혹은 coaccessness가 있다고 정의한다.  이때, coaccess property를 가지는 data 그룹을 coaccess group이라고 한다. frequent item set mining 관점에서 보면 기준이 되는 support 값 이상의 확률로 관측되는 transaction들을 coaccess group으로 정의할 수 있다.\n\n\n* 추가 개념 정의 (merged coaccess group)\n: 서로 다른 coaccess group들을 개별적으로 다루기보다 이들을 묶어서 하나의 coaccess group으로 만들어 다룸으로써 meta data management의 효율성 및 proactive data placement의 효과 측면에서 장점이 발생하는 경우가 있다. 이렇게 만들어지는 coaccess group을 merged coaccess group이라 정의한다. \n: 서로 다른 coaccess group CG_A와 coaccess group CG_B가 있는데, CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합) 시켜서 새롭게 얻어진 CG_C = {CG_A, CG_B}를 하나의 coaccess group으로 다룬다.\n: computing resource의 한계로 인한 현실적인 문제로 인해 모든 경우의 frequent item set을 발굴하기 어려운 경우가 있음 (Frequent item set mining 시 실제로 maximum size of transaction을 한계 값으로 지정). merged/unionized coaccess group은 이러한 한계적인 상황을 극복할 수 있는 방법으로서 제안되는 개념임. (실제로 관측되었던 실험 data 및 graph를 이용하여 실재 근거를 제시할 것)\n\n<!--\nmerged coaccess group 관련해서.\n\n: CG_A = {a1, a2, ..., aM |supp=s1}, CG_B = {b1, b2, ..., bN |supp=s1}, CG_C = {union of CG_A and CG_B |supp=s1} 이 존재하고 발생 시기가 일치하는 것을 확인하였다면, CG_A, CG_B, CG_C 를 모두 개별적인 coaccess group으로 관리할 것이 아니라, CG_C 만을 coaccess group으로 관리한다. (조건 i) supp(CG_A) == supp(CG_B) == supp(CG_C), (조건 ii) union of CG_A and CG_B is identical to CG_C, (조건 iii) occurrence_time(CG_A) == occurrence_time(CG_B) == occurrence_time(CG_C) 이 있다고 하자. 상기 조건 i, ii, iii이 모두 만족되는 경우에는 CG_C 만을 coaccess group으로 관리하고 지정함으로써, coaccess group의 meta data management overhead를 감소시키고 coaccess-based proactive data placement의 operation efficiency를 증대시킴. /* 여기서, (조건 i)과 (조건 ii)가 만족되면 굳이 (조건 iii)까지 확인할 필요는 없을 수도 있음. 확인 필요 */\n* GCD (Greatest Common Divisor) / LCM(Least Common Multiple) / largest coaccess group 개념과 관련.\n\n* 유사 coaccess group 간 merging?\n: 예를 들어 coaccess group CG_A = {\'a\', \'b\', \'c\', \'d\'} 와 coaccess group CG_B = {\'b\', \'c\', \'d\', \'e\'} 가 있다고 했을 때, \'b\', \'c\', \'d\' data 가 겹치기는 하지만, CG_A와 CG_B는 동일한 coaccess group으로 볼 수는 없다. 그러나 만약 CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 (기준이 되는 확률 값은 시스템과 workload의 상황에 따라서 적절히 조절 가능) CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합)화 하여 단일 coaccess group으로 다루는 방식. (말이 되는 소리인지 확인 필요)\n-->\n\n\n* frequent item set mining을 활용한 coaccess group 발굴 방법\n: Coaccess group 발굴을 위해 frequent item set mining을 이용할 수 있으며, 구체적인 방법은 다음과 같다.\n: (1) 시스템의 resource 상황에 따라 적절한 support 값을 설정한다. 예를 들어 fast media (cache 혹은 tier 1 storage)의 저장 가능 용량에 따라, 얼마나 높은 등급의 hot data까지 fast media에 placement 시킬 수 있을지가 결정될 수 있다. 즉 fast media의 용량이 매우 넉넉하여 10번 중 2번의 빈도로 (support=0.2) 관찰되는 coaccess group 까지 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있고, 혹은 fast media의 용량이 여유롭지 않아서 10번 중 5번 이상의 빈도로 (support=0.5) 관찰되는 coaccess group까지만 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있다. support 값은 상황에 따라 dynamic하게 설정될 수 있으며, 시스템 관리자 혹은 data scientist등 사람에 의해 수동적으로 설정될 수도 있지만 algorithm에 의해 automatic하게 설정되도록 구현될 수도 있다.\n: (2) coaccess group에 대한 단위 관측 시간이 되는 transaction measurement window (TMW) 크기를 설정한다. transaction measurement window의 크기에 따라 coaccess group에 포함될 수 있는 data의 갯수가 달라질 수 있다. transaction measurement window 크기가 너무 작으면 함께 access되는 것으로 판단될 대상이 되는 data 갯수 자체가 감소하게 되므로 coaccessness 분석이 어렵게 된다. 반대로, transaction measurement window 크기가 너무 크면 coaccess group의 종류는 감소하고 각 coaccess group의 크기는 커지는 경향이 생기는데, 이 경우의 문제점은 data placement 시 fast media를 효율적으로 사용하는 것이 어려워질 수 있다는 것이다.\n: (3) access되는 data address 정보를 기반으로 frequent item set mining을 시작한다 (이를 위해서는 support 값과 transaction measurement window size 값이 설정되어 있어야 함).  transaction measurement window 동안 access 되는 address들의 집합을 하나의 transaction (basket)으로 묶는다. 전체 관찰 기간 동안 N 번의 transaction measurement window가 있었다면, N 개 만큼의 transaction이 만들어진다. 이 때 N 개의 transaction을 통틀어 support 값 확률 이상으로 빈번하게 등장하는 item set이 있다면 그것을 coaccess group으로 지정하고 coaccess group management ID를 부여, 관리한다.\n: (4) frequent item set mining에서의 confidence 개념을 도입하여 precursor의 존재 감지 시, 미래에 access되게 될 나머지 data들을 신속하게  (일부 데이터가 access되고 나면 나머지 데이터들도 지정된 시간 내에 함께 access될 확률을 의미하게 됨)\n: (4) run-time 시, coaccess group의 빠른 속도로 찾아낼 수 있도록 하기 위해, coaccess group \n\n\n : coaccess group 발굴이 되었다면, 어떻게 저장하고, 어떻게 불러내서(매칭시켜서) proactive data placement에 활용할 수 있게 할 것인가?\n\n\n* O(1)으로, 매칭되는 패턴을 찾을 수 있게 하려면?\n\n\n* proactive data placement를 위해 \n\n\n* Coaccess group pattern matching 방법\n\n\n* Fig{association graph, temporal occurance graph}\n: Frequent item set mining 분석 결과로 얻어진 association graph 및 시간에 따른 frequent item 발생 추이 graph를 이용하여 coaccess pattern의 실재함을 설명\n\n\n* Coaccess property는 다음과 같은 방식으로 활용될 수 있음\n:- 특정 coaccess pattern이 특정 workload에 대한 precursor가 되는 경우가 있을 수 있음. 이를 이용하여 다가올 workload의 prediction에 활용 가능 (예를 들어, coaccess group A가 access된 후 T1 시간 경과 후에 address range B를 random read하는 workload가 발생하고, T2 시간 경과 후에 address range C를 sequential write하는 workload가 오는 경향이 있다는 것을 알고 있다면, 이 정보를 기반으로 proactive하게 address range B를 fast media에 placement 시키고, address range C에 대해서는 caching  \n:- coaccess group 내의 일부 data가 access되면 그 coaccess group 내의 다른 data들을 미리 fast media에 가져다 놓음으로써 성능 향상 가능\n\n<!--\n* (for event-based I/O prediction) coaccess group이 access되는 순서를 event-based I/O prediction에 이용할 수도 있음. 예를 들어, coaccess group A가 access되고 나서 T1 시간 내로 coaccess group B가 access될 확률이 80% 이상이라고 하고, coaccess group B가 access되고 나서 T2 시간 내로 coaccess group C와 coaccess group D가 access될 확률이 90% 이상이라고 한다면, 이러한 정보를 기반으로 ... 무엇을 할 수 있나?\n-->\n\n\n* 이를 위해서 기존의 storage system 구조에 (1) coaccess pattern analysis engine, (2) coacccess pattern aware data placement controller가 추가되는 것이 필요하다.\n\n\n* coaccess pattern analyzer의 역할: workload 별로(???) coaccess pattern list 구축\n\n\n* coaccess pattern analyzer의 구조 및 동작 방식:\n\n\n* coaccess pattern aware data placement controller의 역할:\n\n\n* coaccess pattern aware data placement controller의 구조 및 동작 방식: coaccess pattern 정보를 받아들이기 위한 interface, interface로 받아들인 coaccess pattern 정보의 queuing? data placement의 I/O 단위 (granularity), data placement의 주기 (static? or dynamic?) 등 coaccess pattern 정보를 data placement 시 어떤 식으로 활용하는지\n\n=== Coaccess-based I/O Prediction의 성능 향상 효과 Estimation ===\n\n* 시뮬레이션 기반의 성능 향상 효과 비교\n:- Fast media의 4KB random read latency는 96.47 usec (SSD 830 128GB 기준), slow media의 4KB random read latency는 1,884.03 usec (HDD 7,200rpm 2TB 기준)으로 설정 (iozone에 의한 실측값 사용)\n:- 실험 시작 전에는 4GB 크기의 target data가 slow media (HDD)에 저장되어 있음.\n:- 여기에 1,000,000 개의 4KB random read request로 구성되는 workload를 인가. (각 read request들은 서로 중복되는 data를 가리키지 않으며, 4GB 크기의 target data를 한 번씩 다 request하게 된다고 가정)\n:- Workload 패턴 학습 결과, 4GB의 전체 data 중, \'가\' type의 coaccess 특성을 가지는 10MB 크기의 data 그룹 (data set 당 2,500개의 4KB data 포함)이 200개가 있으며, \'나\' type의 coaccess 특성을 가지는 1MB 크기의 data 그룹 (data set 당 250개의 4KB data 포함)이 1,000개 있는 것을 발견한 것으로 가정.\n:- \'가\' type의 coaccess 패턴은 해당 data 그룹 내 100개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 80%인 것으로 정의.\n:- \'나\' type의 coaccess 패턴을 해당 data 그룹 내 50개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 100%인 것으로 정의.\n\n\n\n\n본 기법을 적용한 경우와 그렇지 않은 경우의 성능 차이를 IOPS로 비교해보면 다음과 같다. \n\n: 예를 들어 설명하면 다음과 같다.\n: data 1, data 2, ..., data 10000으로 구성된 data set이 있으며, 각 data들의 크기는 4KB이다. 학습을 통해서, (1) 이 중 어느 한 data가 access되었을 때, 1분 이내에 나머지 data들도 같이 access되게 될 확률은 55%, (2) 이 중 2개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 80%, (3) 이 중 3개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 95%라는 것을 알고 있다고 가정한다.\n: fast media의 크기는 4GB 이며, 기존에 \n\n이라는 것을 알고 있다면, data 1, 2, 3이 access되었을 때 data 4~128까지도 미리 proactive하게 fast tier에 배치시킴으로써, 그로 인한 성능 이득을 얻을 수 있게 된다. (!!!실제 실험 사례?!!!)\n\n=== Real Data Analysis (MSN File Server) (5) ===\n\n* Analysis Target Workload Specification\n:- MSN FileServer Tracelog (2008-03-10, 약 5GB 데이터)\n:- 21720977703 microseconds (약 6시간)\n:- 29345085 IO operations / 21720 sec => 1351 IOPS\n\n\n* 첫 16.5 minutes (990 seconds) 동안의 I/O에 대해 10 seconds 단위의 Measurement Window로 측정\n\n\n* Frequent Item Sets Mining (Eclat algorithm)\n: support=0.3, minlen=1, maxlen=15\n <pre>\n> fi_s3 <- eclat(ciop_data, parameter=list(supp=0.3, maxlen=15, minlen=1, tidLists=T))\n\nparameter specification:\n tidLists support minlen maxlen            target   ext\n     TRUE     0.3      1     15 frequent itemsets FALSE\n\nalgorithmic control:\n sparse sort verbose\n      7   -2    TRUE\n\neclat - find frequent item sets with the eclat algorithm\nversion 2.6 (2004.08.16)         (c) 2002-2004   Christian Borgelt\ncreate itemset ...\nset transactions ...[1146629 item(s), 100 transaction(s)] done [1.31s].\nsorting and recoding items ... [21 item(s)] done [0.12s].\ncreating bit matrix ... [21 row(s), 100 column(s)] done [0.00s].\nwriting  ... [28761 set(s)] done [0.01s].\nCreating S4 object  ... done [0.25s].\n\n\n\n\n> summary(fi_s3)\nset of 28761 itemsets\n\nmost frequent items:\n  22159360   22155264 1504849920 3170566144 4086710272    (Other)\n     14372      14370      14336      14336      14336     123094\n\nelement (itemset/transaction) length distribution:sizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13\n  21  168  696 1951 3933 5808 6336 5115 3025 1276  364   63    5\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n  1.000   6.000   7.000   6.775   8.000  13.000\n\nsummary of quality measures:\n    support\n Min.   :0.300\n 1st Qu.:0.330\n Median :0.340\n Mean   :0.388\n 3rd Qu.:0.430\n Max.   :0.920\n\nincludes transaction ID lists: TRUE\n\nmining info:\n      data ntransactions support\n ciop_data           100     0.3\n> dim(tidLists(fi_s3))\n[1] 28761   100\n\n</pre>\n\n\n=== Brief Description of the Drawings (5) ===\n\n# [Concept] Coaccess-based I/O Prediction Concepts\n# [Architecture] Storage System Architecture with Coaccess-based Proactive Data Placement\n# [Concept] Coaccessness Analysis using Frequent Itemset Mining\n# [Flowchart] Proactive Data Placement based on Coaccessness\n# [Auxiliary] Experiment Results 1\n# [Auxiliary] Experiment Results 2\n# [Auxiliary] Performance Gain Estimation\n# [Auxiliary] Comparison with prior arts\n\n== Memo for {_BPT-004, _BPT-005} ==\n\n\n=== Think outside the box // Questions ===\n\n* 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n\n* Tiering을 좀 더 다양한 각도에서 바라보자\n:- 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 것이 기존의 tiering 개념임.\n:- SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n:- SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n:- access pattern-aware optimal placement (APOP) - 이런 것은 어떨까?\n:- Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n\n* write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반의 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지?\n\n\n* 만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? SSD-internal memory buffer를 이용해서 SSD를 NVRAM처럼 쓰도록 하는 것이 실제로 말이 되는 이야기일까?\n\n\n\n\n\n=== 기존 기술의 한계점 (Intelligent I/O Prediction 기술의 필요성) ===\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n\n <pre>\n실제 tracelog 사례를 보여주고 hot area가 변화하고 있음을 보일 것.\n</pre>\n\n* Data Placement (Tiering) 관점에서 본 EMC의 FAST, FAST-VP 기술의 한계점\n:- Coarse-grained Data Migration (768KB 까지 작게 나눌 수 있다고는 하지만, 실제로 그렇게 해서 성능이 얼마나 잘 나오는지는 확인 필요. 귀중한 자원인 DRAM을 관리를 위한 metadata로 가득 채우게 될 위험도 있음)\n:- Flexible하지 않은 Tiering 주기\n:- Busy-hour 지난 후 Idle-time에 Tiering 실시\n:- 낮은 지능의 미래 I/O 예측 능력\n:: data access count를 기반으로 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두도록 하는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n\n=== 본 발명의 차별화 포인트 ===\n\n* 본 기술의 특징 1: I/O를 triggering하는 전조 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Indicator는 Run-time에 저비용으로 감지될 수 있는 형태의 정보들로 구성\n:- 미래에 access될 data를 예측케 하는 Indicator간의 mapping은 Bayesian Inference 이용\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n=== 특허 검색/분석 결과 {storage I/O workload prediction} ===\n\n* 검색식: storage I/O workload prediction\n\n* http://portal.uspto.gov/pair/PublicPair 에서 특허 상태 확인 가능.\n\n* I/O prediction을 기반으로 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n:- 기존엔 proactive tiering이 없었나? -> IBM 특허가 나름 유사함. (US7930505, US7512766)\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n==== 1. 비교가 될만한 특허들 ====\n\n예측에 대한 언급이 되어 있는 스토리지 시스템 관련 특허들. 크게 두 가지로서, 그 중의 하나는 가상 volume에 대해 peak I/O가 발생하는 주기를 분석하여, peak I/O 발생시점이 닥치기 전에 미리 가상 volume의 data를 migration 시키는 방법에 대한 것이나, 어느 위치의 data가 언제 access될지에 대한 예측은 아님 (IBM 특허). 다른 하나는 I/O 예측을 기반으로 미래에 자주 access될만한 data를 미리 여러 노드에 분산시키는 방법에 대한 것이나, 예측 방법 자체에 대한 내용은 다루고 있지 않음 (HP 특허)\n\n\n; US7930505, US7512766 (IBM)\n\n* US7930505 (Application Number: US 12/053582), \"Controlling preemptive work balancing in data storage,\" IBM, 특허일 2011-04-19\n: storage network의 I/O 관찰을 통해 peak I/O activity의 repeating instance에 대한 signature를 VLU (virtual logical unit) 단위로 생성하고, 이에 기반하여 언제 heavy I/O가 도래할 것인지를 예측하여, heavy I/O가 오기 전에 미래 data를 faster media로 migration하도록 하는 tiering 기술에 대한 특허. (이미 I/O peak time이 온 후에 hot data를 옮기는 것은 오히려 시스템의 IO성능을 악화시킬 수도 있기 때문에 미리 migration을 하고자 하는 것임)\n: VLU 단위로, peak I/O activity가 어느 주기로 발생할지에 대한 예측임. 지능 수준과 Fine-granularity 측면에서 초보적인 수준임.\n\n* US7512766 (Application Number: US 11/213494), \"Controlling preemptive work balancing in data storage\", IBM, 특허일 2009-03-31\n: 위의 US7930505 특허와 동일 건으로 보임\n\n\n; US6912635 (HP)\n\n* US6912635 (Application Number: US 10/140968), \"Distributing workload evenly across storage media in a storage array\", HP, 특허일 2005-06-28\n: 미래에 자주 access될만한 data를 여러 arrayed storage device에 분산시키겠다는 기술. 그러나 I/O에 대한 예측 방법은 다루어지고 있지 않음.\n\n==== 2. 검색은 되었으나 관련은 없는 특허들 ====\n\n무언가를 예측한다는 측면, Storage System에 대한 기술이라는 측면에서는 관련이 있으나, 미래에 Access될 Data를 예측한다는 측면에서 제안 기술과 관련 없는 특허들\n\n\n; US7640231 (IBM)\n* US7640231 (Application Number: 11/280012), \"Approach based on self-evolving models for performance guarantees in a shared storage system\", IBM, 특허일 2009-12-29\n: network 기반 storage system에서, average latency 혹은 I/O request rate 등에 대한 주기적인 performance sampling을 기반으로, 주어진 objective function의 값을 최대화할 수 있도록 system resource를 제어할 수 있게 하는 기술.\n\n\n; US7774491, US7519725 (IBM)\n\n* US7774491 (Application Number: 12/359377), \"Utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2010-08-10\n: 복수의 클라이언트들을 대상으로 통계적인 성능 QoS를 보장하기 위한 informed throttling 방법\n: A system for utilizing informed throttling to guarantee quality of service to a plurality of clients\n\n* US7519725 (Application Number: US 10/444712), \"System and method for utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2009-04-14\n: resolve the problem of providing statistical performance guarantees for applications generating streams of read/write accesses (I/Os) on a shared, potentially distributed storage system of finite resources, by initiating throttling whenever an I/O stream is receiving insufficient resources\n\n\n; US7827283 (IBM)\n* US7827283 (Application Number: 10/367444), \"System for managing and controlling storage access requirements\", IBM, 2010-11-02\n: SAN 등의 환경에서, storage bandwidth를 관리하고 reserve하는 기술\n: A Resource Management and Reservation System (RMRS) for managing and reserving storage bandwidth, is a platform independent middleware layer that provides an interface to applications, their database management systems, or some other higher level data management systems like ADRS which does data management on behalf of the applications\n\n\n; US8433554, US8140319 (IBM)\n\n* US8433554, \"Predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2013-04-30\n: An approach for predicting performance and capacity of an information technology (IT) system before the IT system is built is described, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n* US8140319, \"Method and system for predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2012-03-20\n: A method and system for predicting performance and capacity of an information technology (IT) system before the IT system is built, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n\n\n; US8255915 (HP)\n* US8255915, \"Workload management for computer system with container hierarchy and workload-group policies\" HP, 특허일 2012-08-28\n\n\n; US7050956 (HP)\n* US7050956 (Application Number: US 09/843930), \"Method and apparatus for morphological modeling of complex systems to predict performance\", HP, 특허일 2006-05-23\n: I/O 성능 예측을 위한 complex storage system에서의 data flow 모델링 방법\n\n\n; US6718434 (HP)\n* US6718434 (Application Number: US 09/871043), \"Method and apparatus for assigning raid levels\", HP, 특허일 2004-04-06\n: Data를 disk array에 load하기 전에, workload 명세 및 device 명세에 기반하여, 적합한 RAID level을 판단하여 disk array에 할당 (RAID build)하는 방법\n\n\n; US8112586 (EMC)\n* US8112586 (Application Number: US 12/190071), \"Predicting and optimizing I/O performance characteristics in a multi-level caching system\", EMC, 특허일 2012-02-07\n: multi-level caching 시스템에서, cost대비 performance를 최적화할 수 있도록, workload에 따른 resource allocation design에 대한 recommendation 기술 (multi-level cache들 간의 workload propagation을 예측하는 것이 핵심)\n\n\n; US6629266 (IBM)\n* US6629266 (Application Number: US 09/442001), \"Method and system for transparent symptom-based selective software rejuvenation\", IBM, 특허일 2003-09-30\n: software가 언제 동작이 멈추는지를 예측함으로써, software의 dependability를 향상할 수 있게 하는 방법\n\n\n; US5088058 (Unisys)\n* US5088058 (Application Number: 07/237120), \"Apparatus and method for evaluating and predicting computer I/O performance using I/O workload snapshots for model input\", Unisys Corporation, 특허일 1992-02-11\n: I/O delay 등에 대한 확률적 모형을 기반으로 disk 시스템의 성능을 검증하고 예측하는 시뮬레이션 기반의 방법\n: A method of evaluating and predicting the performance of an I/O disk system comprised of one or more disk controllers and cooperating disk units using a simulation model containing a stact constructed such that delays are formulated as sets of stochastic processes\n\n=== Comparison of I/O Prediction Approaches ===\n\n미래에 access될 data의 위치와 시기를 예측하기 위한 방법으로 event-based I/O prediction, coaccess-based I/O prediction, periodicity-based I/O prediction 방식이 제안되며, 각각의 방식은 예측에 필요한 연산량 및 input data의 richness, 그리고 workload 변화에 대한 적응성 측면에서 장단점을 가지고 있음. 따라서, 상황에 따라서 적절한 방식을 이용하면 됨.\n\n{| style=\"border-collapse: collapse; border: 1px solid #000\" border=1\n| style=\"width: 25%\" | 비교 항목\n| style=\"width: 25%\" | periodicity-based I/O prediction\n| style=\"width: 25%\" | coaccess-based I/O prediction\n| style=\"width: 25%\" | event-based I/O prediction \n|-\n| context richness required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| computation overhead required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| adaptability to workload changes (lower is better)\n| high\n| high\n| high\n|-\n|}\n\n\n* 전조 기반 예측 vs. 주기성 기반 예측\n{| border=1\n| 비교 항목\n| 전조 기반 예측\n| 주기성 기반 예측\n|-\n| 기술 요약\n| 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식\n| address 별 access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식\n|-\n| 분석 오버헤드\n| High (학습/마이닝에 드는 비용임)\n| Low (주기성 파악 비용임)\n|-\n| Run-time 오버헤드\n| Mid (실시간 전조 모니터링 및 매칭 비용)\n| Low (주기성 계산)\n|-\n| Workload 변화 적응성\n| High\n| High (Workload 변경 사실 알림 hint 필요)\n|-\n| ...\n| ...\n| ...\n|-\n| Incrementality of Mining/Learning\n| ...\n| ...\n|}\n\n* 전조 기반 예측은, 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식임.  주기성 기반 예측은, data access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식임.\n\n\n=== Intelligent Data Placement ===\n\n* Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case\n: 여기에 HML이 사용될 수 있는 것이라면, 어떤 방식으로 어떤 이유로 그런 것일까?\n\n\n* I/O 예측 방식으로서 다음 방식이 제안되며, 각 방식에 따라서 prediction model, 수집되어야 하는 context data, I/O 패턴 DB에 저장되는 정보의 종류 및 형태가 달라질 수 있음. 예측에 이용할 수 있는 context 정보의 richness에 따라 적절한 방식을 사용할 수 있다. (서로 다른 발명이므로, 별도의 특허로 낸다)\n:- event-based I/O prediction\n:- coaccess-based I/O prediction\n:: 여기서, periodicity-based I/O prediction은 기존 특허에서 언급되었으므로, 본 발명에서는 크게 언급하지 않기로 한다.\n\n\n* I/O prediction 결과에 따라 적절한 위치에 data를 배치하게 되는데, 여기서 적절한 위치라 함은 SSD와 같은 fast tier가 될 수도 있겠으나, sequential read/write 같은 경우 striping된 HDD array가 될 수도 있고, 혹은 SAS HDD가 될 수도 있겠음. 또한 SSD 중에서도 SLC, MLC, TLC 로 구분되는 fine-grained fast tier로 구분될 수도 있음\n\n\n* 미래에 오게 될 I/O의 type도 알 수 있을까? 즉, {random/sequential}x{read/write} type 중 어느 것에 해당되는지를 안다면, 그리고 I/O의 intensiveness도 예상할 수 있다면 그에 맞춰서 placement 시 적절한 tier 혹은 적절한 node로 찾아가게 할 수 있을 것임.\n\n\n* context 정보는, 수집되는 위치에 따라, server-side에서 수집되는 context data와 storage-side에서 수집되는 context data로 분류될 수 있음. Event context data는 server-side에서 수집 가능하며, coaccess context data 및 periodicity context data는 storage-side에서 수집될 수 있음.\n\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n: 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n\n=== Design/Implementation Issues ===\n\n* workload type/class tagging 필요\n\n* cache와 tiering을 같이 쓸 때, DRAM cache가 아니라 Flash Cache의 경우 쓸데없이 모든 data가 Flash Cache에 다 들어가게 두기 보다는, tiering에서 cover하지 못하는 것들 위주로 caching하도록 하면 Flash Cache의 공간 낭비를 줄임으로써, 유효 cache 공간을 늘일 수 있게 되고, 그만큼의 성능 향상으로 연결될 수 있음.\n\n* DRAM 기반의 write cache를 적극 이용하여 마치 log-structured file system에서 제공하는 I/O merging and sequentialization 효과를 얻을 수 있음.\n\n* Tiering되는 data는 caching되는 data보다 상대적으로 coarse-grained하게 grouping하여 다룸으로써, 한꺼번에 많은 data를 move할 수 있도록 하고, meta data 관리에 필요한 overhead를 줄일 수 있게 한다. 당장 access되지 않더라도 일정 시간 내에 같이 access될 수 있는 data들은 함께 faster tier로 옮겨진다. 이때 필요한 정보가 coaccess 정보임.\n\n* Cache - Tiering 기법의 연동 방법: 예측이 실패했거나, target tier media의 가용 공간이 부족하여 tiering에서 놓치는 missing data가 있을 수 있음. 이러한 missing data들이 temporal locality가 있는 경우에는 당장은 faster tier에서 serve하지 못하더라도 cache에서 serve될 수 있는데, temporal locality가 적은 경우에는 cache만으로 cover되지 않는다. \n\n* Management table size optimization issue - 만약 모든 LBA 별로 event-based I/O prediction을 해야 한다면 현실적으로 사용 가능한 아키텍쳐가 아닐 수 있음. 만약 spatial locality가 있어서 LBA 1,000개 단위로 묶어서 대응할 수 있다면, 최소 1/1000 의 management table size 감소 효과를 얻을 수 있음. pattern mining 혹은 machine learning 실행 시에는 record 수가 감소함으로써 얻는 이득은 1,000배 이상이 될 수 있음. 관건은 얼마나 LBA 묶음을 크게 만들어서 예측에 사용할 수 있는지임.\n\n\n=== I/O Insight ===\n\n* ML 기반의 I/O Insight 발굴 (Data Placement를 위한 Macroscopic Guideline)\n: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:-> 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n\n* I/O Insight의 요건:\n:# indication, as simple as possible\n:# indication, as specific as possible\n:# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n:# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n\n* I/O Insight의 형태\n: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n=== X-Y Modeling ===\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n=== 예측 실패 대비 (별도 특허화 가능) ===\n\n* I/O 예측이 실패할 수 있으며, 이로 인한 성능 penalty를 최소화하기 위한 방법 필요.\n:- fast tier에 여유공간이 있다면 1타겟과 2타겟을 모두 올려놓아도 되지 않을까? 1타겟과 2타겟을 placement함으로써 예측 정확도를 95%까지 올릴 수 있다면, 꽤 쓸만할 수 있겠음. 그리고 1타겟과 2타겟의 모든 데이터를 다 fast tier에 올려두지 말고 일부만 fast tier에 올려두었다가, 둘 중에 하나의 타겟이 hit되었을 때, hit된 타겟의 나머지 data를 재빨리 fast tier에 올리도록 하는 incremental 방법을 사용할 수 있겠음.\n:- 그러나 이것 또한 예측 실패에 대한 완벽한 prevention이 될 수는 없다. 따라서 예측 실패가 발생하더라도 재빨리 그 성능 penalty를 mitigation할 수 있는 방법이 필요. \n:- cache를 이용하여 mitigation할 수 있을까?\n\n=== Data Read/Write 관련 Issues ===\n\n* read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n\n* 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n\n* write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간에, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면?\n\n* 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있음.\n\n* 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간이 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 scatter했던 I/O들을 나중에 다시 읽어들이려 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 원하는 고성능을 낼 수 있게 된다.\n\n* replica-based dynamic clustering\n: 위와 같은 write-and-then-read-again 상황에서, 해당 data를 저장했었던 HDD의 I/O busy/idle 여부에 상관 없이, 최고의 효과를 낼 수 있게 하기 위해, replica management를 적극적으로 활용하는 것은 어떨까? HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까?\n\n* data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 논의가 아님. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 논의임.\n\n* 그러나, 조심해야 할 것은 위의 논의는 상당히 case-dependency를 상당히 많이 탈 수도 있다는 것임. 책상 위의 아이디어로 끝날 수도 있기 때문에 조심스럽게 접근하는 것이 필요함.\n\n\n\n\n<br/>\n\n== _BPT-006 ==\n\n\n=== Data Lifetime (Valid-to-invalid) Interval 분석에 기반한 Data Placement (Tiering) ===\n\n\n* Data 별로, 혹은 LBA 별로, 혹은 Erase Block 별로? Data의 Valid to Invalid Time (Duration) 패턴을 분석할 수 있을까?\n: 이를 SSD 내에서 Low Overhead로 수행할 수 있나?\n: 혹은, 굳이 SSD 내에서 수행할 필요가 있나?\n: Host side에서 LBA 별로 분석 될 수 있다면, 이 정보를 기반으로 Tiering 시에 활용 할 수 있을 것임\n: 특정 Address에 대해서 W이 되고 나서, 다시 W이 되기까지의 시간을 측정하여 ARM 하면 될 듯. 그 두 W 간에 얼마나 많은 R들이 오는지에 대해서도 고려하여 placement 시 고려할 수 있을 것임.\n\nMeasurement Window 내에서 \'W 간의 Interval이 길다\'는 식으로 인식하는 것과, \'W의 횟수가 잦다\'고 인식하는 것 간에 어떤 차이가 있을 수 있을까?\n\n <pre>\n    01234567890123456789\na1: W-R-W-R-W-R-W-W-R--W : T20W06R04 : Short-Lifetime, Many-Reads : SLC\na2: W---W-R-W---W-W----W : T20W06R01 : Short-Lifetime, Few-Reads : SLC, DRAM?\na3: W-RRRR-W---RR-W-R-RW : T20W04R08 : Mid-Lifetime, Many-Reads : MLC\na4: W------W--R---W----W : T20W04R01 : Mid-Lifetime, Few-Reads : MLC\na5: W-R-R-R-RR-RRRR----R : T20W01R10 : Long-Lifetime, Many-Reads : MLC\na6: W------------R----R- : T20W01R02 : Long-Lifetime, Few-Reads : TLC\n</pre>\n\n=== Idle-time Prediction Based Free Space Management in SSD ===\n\n* Idle-time의 도래 시기와 지속 시간 패턴 분석에 기반한 SSD 내의 Free Space 확보 방법\n:- SSD 내에서 Low overhead (in computation, memory space)로 Idle-time 도래 시기 및 지속 시간 분석이 가능한가?\n:- 가능하다면, SSD 내에서 Garbage Collection 등으로 Free Space를 확보하는 데 있어서 어떤 장점을 가질 수 있는가?\n\n\n\n: 이를 기반으로 \n\n\n\n=== 요약 ===\n* \n\n\n=== 기술 상세 ===\n\n*\n\n\n\n\n\n== _BPT-007 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== _BPT-008 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== _BPT-009 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Ef?cient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== _BPT-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== _BPT-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== _BPT-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== _BPT-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== _BPT-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== _BPT-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== _BPT-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== _BPT-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n----\n=== (SmartSSD API) SSD-internal I/O pattern logging interface and mechanism ===\n\n\n==== Questions ====\n\n* SmartSSD에서 제공할 수 있는 logging 서비스로서 어떤 것들이 있을 수 있나?\n:- 어느? 정도의 free space를 필요로하는 critical (heavy) I/O가 어떤? 주기로 도래할 지 알 수 있을까?\n:- I/O prediction에 도움될 수 있는 정보를 SmartSSD가 기록했다가 필요한 때 (안전한 방법으로?) 줄 수 있을까?\n:- I/O prediction까지는 아니더라도 해당 SSD의 I/O wellness(어떻게 정의?)를 측정했다가 bottleneck을 회피하는 데 유용한 정보를 제공할 수 있을까?\n:- 어떤 SSD가 현재 주어지고 있는 I/O workload에 대해 list performance spec을 만족하는데 어려움을 겪고 있다는 것을 알려줄 수 있는 정보가 어떤 것이 있을까? (SSD 내 write buffer의 fullness 혹은 I/O queue의 유동성? amount of free space in over-provisioning area?) (-> I/O wellness 라는 metric을 이것과 연관시켜 정의할 수도 있으며, 결국 SLA 혹은 Performance QoS와 연관시킬 수 있음)\n:- 지금까지의 S.M.A.R.T.와 비교하였을 때, 어떤 차별점이 있는가? 기존의 S.M.A.R.T.가 제공하지 못하던 타입의 정보를 제공하고, 이로 인해 기존에는 불가능했던 새로운 알고리즘 구현 혹은 새로운 서비스 제공이 가능해짐을 보이면 좋겠음.\n\n\n\n:- workload의 history?\n:- 각 erase block이 erase되어야만 했던 주기?\n:- free space의 추이 (얼마나 많이 남아 돌던지)?\n:- I/O의 heaviness 패턴?\n:- SSD내 write buffer (혹은 write cache)의 utilization정보? (즉, write buffer가 넘칠 정도가 되어서 I/O wait을 해야만 했던 상황이 얼마나 자주 발생했는지, write buffer의 가득찬 정도를 백분율로 표현한다고 했을 때, 평균/표준편차 등으로 대표할 수 있는 normal distribution을 따르는지?\n:- 전체 sequential read/write \n\n\n==== 배경 ====\n\n* 각 SSD 모델별로, 또 동일한 모델의 SSD라 하더라도 어떤 workload에 노출되어왔는지에 따라, 현재 낼 수 있는 I/O performance가 동일하지 않을 수 있다 (!check! 관련 실험 결과 - SSD 830을 이용하여 같은 시간 동안 하나의 host에서 생성되는 workload들을 나누어 받았다 하더라도 내부적인 free block의 갯수 및 random write에 의한 block 내 파편화등의 상태가 상이하여 성능에 차이가 나는 것을 보여줄 것. uFLIP 혹은 filebench 활용 가능). 한 예로, SSD가 어느 호스트의 어느 file system에 매핑되었는지에 따라 그 SSD가 받는 workload의 특성이 다를 수 있다. file system mount point가 달라지거나, 기존과 다른 새로운 응용이 설치/서비스되는 상황이 되면 \n\n\n----\n=== I/O Prediction-based Proactive Data Placement ===\n\n* 미래에 access될 data를 예측하여 적소에 미리 가져다 놓는 기술로서, (1) I/O prediction을 위한 정보 수집 모듈, (2) 주어진 정보들에 기반하여 다가올 I/O를 예측하는 모듈, (3) data placement를 수행하는 모듈로 구성됨.\n\n\n* 본 기술의 특징 1: I/O를 triggering하는 indicator (전조) 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Run-time에 저비용으로 감지될 수 있는 Indicator 정보와, 이에 의해 access될 data 간의 mapping은 Bayesian Inference 이용\n\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n\n----\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n----\n\n=== Tiering Method Avoiding I/O Bottleneck ===\n\n* I/O Prediction에 의해 예측된 미래의 Hot Data를 Proactive하게 Placement한다고 할 때, 예상되는 access pattern으로 인해 발생 가능한 I/O Bottleneck을 최소화할 수 있도록 배치하는 방법\n\n* 예를 들어 미래의 hot data block 1 (200MB), hot data block 2 (100MB), hot data block 3 (400MB)이 1분 후에 access 될 것으로 예측되었다고 가정. data block 1에 대해서는 4KB 단위로 random read가 dominant한 access pattern이 예상되고, data block 2에 대해서는 1024KB 단위의 sequential read가 dominant한 access pattern이 예상되고, data block 3에 대해서는 4KB 단위의 random read와 512KB 단위의 sequential write이 각각 80:20의 비율로 mix된 형태의 access pattern이 예상된다고 했을 때, 각 data block을 어디에 어떤 형태로 placement시킬 것인가?\n\n: machine learning 혹은 pattern mining에 기반한 base workload pattern data를 기반으로 incremental하게 update될 수 있는 data access의 spatial locality와 periodicity\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n\n* 기존 기술과의 대비 (naive I/O prediction vs. full-fledged I/O prediction)\n: 기존 기술은 누적 hit count 정보에 의지하는 naive I/O prediction으로 볼 수 있음. 이러한 방식은 workload의 변화를 제대로 반영하지 못하기 때문에, static한 workload에 대해서는 잘 동작하지만, moving target처럼 dynamic한 workload에 대해서는 오히려 과거의 누적 hit count metric이 현재의 workload 패턴을 왜곡하는 문제점을 가지고 있음. (temporal locality와 spatial locality가 깨지는 순간이 여러번 존재할 수록 기존의 naive I/O prediction 방식으로 성능 향상은 어려워짐)\n\n\n\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== _BPT-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(281,'== _BPT-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== _BPT-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== _BPT-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n[[Bnote patidea 2013-003]]\n\n\n== _BPT-004 ==\n\n=== Event-based I/O Prediction (for Proactive Data Placement) ===\n\n=== 요약 (4) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n=== 기술 상세 (4) ===\n\n* I/O 발생 메커니즘의 모델링\n: I/O prediction을 위해, type-I event (indirect I/O trigger)와 type-II event (direct I/O trigger)를 관찰 및 분석함. Type-I event의 예로서, user의 log-in, 기 log-in되어 있던 user session의 상태 변경 (idle-to-busy), system service에 대한 invocation from local or remote node 등이 포함될 수 있으며, type-II event의 예로서, user application 의 실행, system service의 실행 등이 포함될 수 있음. Type-I event는 type-II event의 발생에 영향을 미치고, type-II event는 I/O request event (data access)의 발생에 영향을 미치므로, \'type-I event\' -> \'type-II event\' -> \'I/O request event\' 까지 이어지는 인과사슬 (cause-effect chain) 기반으로 I/O 발생 메커니즘을 모델링하였음.\n\n\n* Event 정보 수집: type-I event 정보 및 type-II event 정보 정의 및 수집 방법.\n\n\n* I/O 패턴 학습 (Offline Learning): 수집된 Event 정보를 기반으로 I/O 패턴을 learning하는 방법\n\n\n* Run-time I/O 예측 (Online Matching): 시스템 모니터링에서 얻어지는 최근 및 현재의 event들과 learning으로 얻어진 I/O 패턴 DB에 기반하여 Run-time으로 미래 I/O 예측\n\n\n* 시스템 구조: 정보 수집 및 패턴 학습 모듈의 시스템 내 위치 (I/O 패턴 DB 등의 위치 및 실시간 예측 모듈과의 관계)\n\n\n* 순서도: 수집부터 학습까지 전체적인 task의 흐름\n\n\n* 예상 효과: 성능적 측면에서 사용자가 체감하는 lower latency, higher throughput 효과, 이미 발생해버린 congestion time을 피해 미리 적절한 곳에 data를 배치시킴으로써 얻어지는 bottleneck prevention 효과.\n\n=== Data Collection (4) ===\n\n* 역할: I/O 패턴 학습/발굴에 필요한 정보 수집\n\n* 수집 정보 종류\n: 전조 기반 예측에 필요한 간접 I/O 유발자, 직접 I/O 유발자, I/O 발생 이벤트 등 I/O 예측에 도움이 될 수 있는 다양한 정보를 수집. 다음 정보들이 포함될 수 있음.\n:- indirect I/O trigger trace: {(user-id | user-type), status-type, before-status, after-status, event-time}\n:- direct I/O trigger trace: {(program-id | program-type), status-type, before-status, after-status, process-owner, event-time [, process-current-working-directory, files-opened]}\n:- I/O trace: {access-time, address-accessed, read/write-flag, io-size, file-path, process, co-accessed-addresses}\n\n\n* 수집 모듈의 위치\n: storage-server가 분리되어 있는 구조인 SAN/NAS 타입의 architecture인 경우에는 application이 구동되는 server-side에 정보 수집 모듈이 위치한다. storage-server가 결합되어 있는 converged-architecture 기반의 system인 경우에는 해당 system 내에 정보 수집 모듈이 위치한다.\n\n\n* 정보 수집 방식\n: 운영체체가 제공하는 시스템 상태 정보 제공 interface를 이용하거나, blktrace, lsof, debug file system 등 이용 (e.g., /sys/kernel/debug/ 등) 혹은 /proc file system 등을 이용 가능.\n\n== _BPT-005 ==\n\n=== Coaccess-based I/O Prediction (for Proactive Data Placement) ===\n\n* (영문) Coaccess-based I/O Prediction (for Proactive Data Placement)\n* (한글) Coaccess 기반의 I/O 예측 방법 및 시스템 구조\n\n=== 요약 (5) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 coaccess-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n\n=== 발명의 이용 분야 (5) ===\n\n데이터를 저장하고 처리하는 일반적인 컴퓨팅 시스템으로서,\n* 프론트엔드/백엔드 서버\n* 네트워크 스토리지 시스템\n* 컨버지드 서버-스토리지 시스템\n* 엔터프라이즈 혹은 데이터 센터 내의 IT 인프라\n등에서 활용 가능.\n\n\n=== 발명의 목적 (5) ===\n\n==== 기존 기술의 한계점 ====\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: Fast media로서 SSD가 사용되는 경우에 다음과 같은 부정적인 효과가 발생할 수 있음. 과거에는 자주 access되었던 data가 앞으로도 hot할 것으로 판단하고 tiering 시점에 fast media로 placement 시켰는데 tiering되고 난 후에 자주 access되지 않게 되면, 성능 향상 이득 없이 SSD의 wear-out만 초래하게 됨.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n==== Intelligent I/O Prediction의 필요성 ====\n\n\n\n\n=== 기술 상세 (5) ===\n\n* Data의 coaccess 특성에 기반한 proactive data placement 기법\n: 소정의 time window 내에 같이 access되는 확률이 높은 data 그룹 (coaccess group)이 있을 때, 그 그룹 내의 일부 data가 access되면 나머지 data들도 미리 fast media에 가져다 놓음으로써, 성능 향상을 얻을 수 있게 하는 방법임.\n\n\n* 개본 개념 정의 (coaccess property, coaccessness, coaccess group)\n: 지정된 시간 동안의 measurement window 내에서, 어떤 data가 access될 때 함께 access되는 경향을 가지는 다른 data들이 존재할 때 그 data들 간에 coaccess property 혹은 coaccessness가 있다고 정의한다.  이때, coaccess property를 가지는 data 그룹을 coaccess group이라고 한다. frequent item set mining 관점에서 보면 기준이 되는 support 값 이상의 확률로 관측되는 transaction들을 coaccess group으로 정의할 수 있다.\n\n\n* 추가 개념 정의 (merged coaccess group)\n: 서로 다른 coaccess group들을 개별적으로 다루기보다 이들을 묶어서 하나의 coaccess group으로 만들어 다룸으로써 meta data management의 효율성 및 proactive data placement의 효과 측면에서 장점이 발생하는 경우가 있다. 이렇게 만들어지는 coaccess group을 merged coaccess group이라 정의한다. \n: 서로 다른 coaccess group CG_A와 coaccess group CG_B가 있는데, CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합) 시켜서 새롭게 얻어진 CG_C = {CG_A, CG_B}를 하나의 coaccess group으로 다룬다.\n: computing resource의 한계로 인한 현실적인 문제로 인해 모든 경우의 frequent item set을 발굴하기 어려운 경우가 있음 (Frequent item set mining 시 실제로 maximum size of transaction을 한계 값으로 지정). merged/unionized coaccess group은 이러한 한계적인 상황을 극복할 수 있는 방법으로서 제안되는 개념임. (실제로 관측되었던 실험 data 및 graph를 이용하여 실재 근거를 제시할 것)\n\n<!--\nmerged coaccess group 관련해서.\n\n: CG_A = {a1, a2, ..., aM |supp=s1}, CG_B = {b1, b2, ..., bN |supp=s1}, CG_C = {union of CG_A and CG_B |supp=s1} 이 존재하고 발생 시기가 일치하는 것을 확인하였다면, CG_A, CG_B, CG_C 를 모두 개별적인 coaccess group으로 관리할 것이 아니라, CG_C 만을 coaccess group으로 관리한다. (조건 i) supp(CG_A) == supp(CG_B) == supp(CG_C), (조건 ii) union of CG_A and CG_B is identical to CG_C, (조건 iii) occurrence_time(CG_A) == occurrence_time(CG_B) == occurrence_time(CG_C) 이 있다고 하자. 상기 조건 i, ii, iii이 모두 만족되는 경우에는 CG_C 만을 coaccess group으로 관리하고 지정함으로써, coaccess group의 meta data management overhead를 감소시키고 coaccess-based proactive data placement의 operation efficiency를 증대시킴. /* 여기서, (조건 i)과 (조건 ii)가 만족되면 굳이 (조건 iii)까지 확인할 필요는 없을 수도 있음. 확인 필요 */\n* GCD (Greatest Common Divisor) / LCM(Least Common Multiple) / largest coaccess group 개념과 관련.\n\n* 유사 coaccess group 간 merging?\n: 예를 들어 coaccess group CG_A = {\'a\', \'b\', \'c\', \'d\'} 와 coaccess group CG_B = {\'b\', \'c\', \'d\', \'e\'} 가 있다고 했을 때, \'b\', \'c\', \'d\' data 가 겹치기는 하지만, CG_A와 CG_B는 동일한 coaccess group으로 볼 수는 없다. 그러나 만약 CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 (기준이 되는 확률 값은 시스템과 workload의 상황에 따라서 적절히 조절 가능) CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합)화 하여 단일 coaccess group으로 다루는 방식. (말이 되는 소리인지 확인 필요)\n-->\n\n\n* frequent item set mining을 활용한 coaccess group 발굴 방법\n: Coaccess group 발굴을 위해 frequent item set mining을 이용할 수 있으며, 구체적인 방법은 다음과 같다.\n: (1) 시스템의 resource 상황에 따라 적절한 support 값을 설정한다. 예를 들어 fast media (cache 혹은 tier 1 storage)의 저장 가능 용량에 따라, 얼마나 높은 등급의 hot data까지 fast media에 placement 시킬 수 있을지가 결정될 수 있다. 즉 fast media의 용량이 매우 넉넉하여 10번 중 2번의 빈도로 (support=0.2) 관찰되는 coaccess group 까지 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있고, 혹은 fast media의 용량이 여유롭지 않아서 10번 중 5번 이상의 빈도로 (support=0.5) 관찰되는 coaccess group까지만 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있다. support 값은 상황에 따라 dynamic하게 설정될 수 있으며, 시스템 관리자 혹은 data scientist등 사람에 의해 수동적으로 설정될 수도 있지만 algorithm에 의해 automatic하게 설정되도록 구현될 수도 있다.\n: (2) coaccess group에 대한 단위 관측 시간이 되는 transaction measurement window (TMW) 크기를 설정한다. transaction measurement window의 크기에 따라 coaccess group에 포함될 수 있는 data의 갯수가 달라질 수 있다. transaction measurement window 크기가 너무 작으면 함께 access되는 것으로 판단될 대상이 되는 data 갯수 자체가 감소하게 되므로 coaccessness 분석이 어렵게 된다. 반대로, transaction measurement window 크기가 너무 크면 coaccess group의 종류는 감소하고 각 coaccess group의 크기는 커지는 경향이 생기는데, 이 경우의 문제점은 data placement 시 fast media를 효율적으로 사용하는 것이 어려워질 수 있다는 것이다.\n: (3) access되는 data address 정보를 기반으로 frequent item set mining을 시작한다 (이를 위해서는 support 값과 transaction measurement window size 값이 설정되어 있어야 함).  transaction measurement window 동안 access 되는 address들의 집합을 하나의 transaction (basket)으로 묶는다. 전체 관찰 기간 동안 N 번의 transaction measurement window가 있었다면, N 개 만큼의 transaction이 만들어진다. 이 때 N 개의 transaction을 통틀어 support 값 확률 이상으로 빈번하게 등장하는 item set이 있다면 그것을 coaccess group으로 지정하고 coaccess group management ID를 부여, 관리한다.\n: (4) frequent item set mining에서의 confidence 개념을 도입하여 precursor의 존재 감지 시, 미래에 access되게 될 나머지 data들을 신속하게  (일부 데이터가 access되고 나면 나머지 데이터들도 지정된 시간 내에 함께 access될 확률을 의미하게 됨)\n: (4) run-time 시, coaccess group의 빠른 속도로 찾아낼 수 있도록 하기 위해, coaccess group \n\n\n : coaccess group 발굴이 되었다면, 어떻게 저장하고, 어떻게 불러내서(매칭시켜서) proactive data placement에 활용할 수 있게 할 것인가?\n\n\n* O(1)으로, 매칭되는 패턴을 찾을 수 있게 하려면?\n\n\n* proactive data placement를 위해 \n\n\n* Coaccess group pattern matching 방법\n\n\n* Fig{association graph, temporal occurance graph}\n: Frequent item set mining 분석 결과로 얻어진 association graph 및 시간에 따른 frequent item 발생 추이 graph를 이용하여 coaccess pattern의 실재함을 설명\n\n\n* Coaccess property는 다음과 같은 방식으로 활용될 수 있음\n:- 특정 coaccess pattern이 특정 workload에 대한 precursor가 되는 경우가 있을 수 있음. 이를 이용하여 다가올 workload의 prediction에 활용 가능 (예를 들어, coaccess group A가 access된 후 T1 시간 경과 후에 address range B를 random read하는 workload가 발생하고, T2 시간 경과 후에 address range C를 sequential write하는 workload가 오는 경향이 있다는 것을 알고 있다면, 이 정보를 기반으로 proactive하게 address range B를 fast media에 placement 시키고, address range C에 대해서는 caching  \n:- coaccess group 내의 일부 data가 access되면 그 coaccess group 내의 다른 data들을 미리 fast media에 가져다 놓음으로써 성능 향상 가능\n\n<!--\n* (for event-based I/O prediction) coaccess group이 access되는 순서를 event-based I/O prediction에 이용할 수도 있음. 예를 들어, coaccess group A가 access되고 나서 T1 시간 내로 coaccess group B가 access될 확률이 80% 이상이라고 하고, coaccess group B가 access되고 나서 T2 시간 내로 coaccess group C와 coaccess group D가 access될 확률이 90% 이상이라고 한다면, 이러한 정보를 기반으로 ... 무엇을 할 수 있나?\n-->\n\n\n* 이를 위해서 기존의 storage system 구조에 (1) coaccess pattern analysis engine, (2) coacccess pattern aware data placement controller가 추가되는 것이 필요하다.\n\n\n* coaccess pattern analyzer의 역할: workload 별로(???) coaccess pattern list 구축\n\n\n* coaccess pattern analyzer의 구조 및 동작 방식:\n\n\n* coaccess pattern aware data placement controller의 역할:\n\n\n* coaccess pattern aware data placement controller의 구조 및 동작 방식: coaccess pattern 정보를 받아들이기 위한 interface, interface로 받아들인 coaccess pattern 정보의 queuing? data placement의 I/O 단위 (granularity), data placement의 주기 (static? or dynamic?) 등 coaccess pattern 정보를 data placement 시 어떤 식으로 활용하는지\n\n=== Coaccess-based I/O Prediction의 성능 향상 효과 Estimation ===\n\n* 시뮬레이션 기반의 성능 향상 효과 비교\n:- Fast media의 4KB random read latency는 96.47 usec (SSD 830 128GB 기준), slow media의 4KB random read latency는 1,884.03 usec (HDD 7,200rpm 2TB 기준)으로 설정 (iozone에 의한 실측값 사용)\n:- 실험 시작 전에는 4GB 크기의 target data가 slow media (HDD)에 저장되어 있음.\n:- 여기에 1,000,000 개의 4KB random read request로 구성되는 workload를 인가. (각 read request들은 서로 중복되는 data를 가리키지 않으며, 4GB 크기의 target data를 한 번씩 다 request하게 된다고 가정)\n:- Workload 패턴 학습 결과, 4GB의 전체 data 중, \'가\' type의 coaccess 특성을 가지는 10MB 크기의 data 그룹 (data set 당 2,500개의 4KB data 포함)이 200개가 있으며, \'나\' type의 coaccess 특성을 가지는 1MB 크기의 data 그룹 (data set 당 250개의 4KB data 포함)이 1,000개 있는 것을 발견한 것으로 가정.\n:- \'가\' type의 coaccess 패턴은 해당 data 그룹 내 100개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 80%인 것으로 정의.\n:- \'나\' type의 coaccess 패턴을 해당 data 그룹 내 50개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 100%인 것으로 정의.\n\n\n\n\n본 기법을 적용한 경우와 그렇지 않은 경우의 성능 차이를 IOPS로 비교해보면 다음과 같다. \n\n: 예를 들어 설명하면 다음과 같다.\n: data 1, data 2, ..., data 10000으로 구성된 data set이 있으며, 각 data들의 크기는 4KB이다. 학습을 통해서, (1) 이 중 어느 한 data가 access되었을 때, 1분 이내에 나머지 data들도 같이 access되게 될 확률은 55%, (2) 이 중 2개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 80%, (3) 이 중 3개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 95%라는 것을 알고 있다고 가정한다.\n: fast media의 크기는 4GB 이며, 기존에 \n\n이라는 것을 알고 있다면, data 1, 2, 3이 access되었을 때 data 4~128까지도 미리 proactive하게 fast tier에 배치시킴으로써, 그로 인한 성능 이득을 얻을 수 있게 된다. (!!!실제 실험 사례?!!!)\n\n=== Real Data Analysis (MSN File Server) (5) ===\n\n* Analysis Target Workload Specification\n:- MSN FileServer Tracelog (2008-03-10, 약 5GB 데이터)\n:- 21720977703 microseconds (약 6시간)\n:- 29345085 IO operations / 21720 sec => 1351 IOPS\n\n\n* 첫 16.5 minutes (990 seconds) 동안의 I/O에 대해 10 seconds 단위의 Measurement Window로 측정\n\n\n* Frequent Item Sets Mining (Eclat algorithm)\n: support=0.3, minlen=1, maxlen=15\n <pre>\n> fi_s3 <- eclat(ciop_data, parameter=list(supp=0.3, maxlen=15, minlen=1, tidLists=T))\n\nparameter specification:\n tidLists support minlen maxlen            target   ext\n     TRUE     0.3      1     15 frequent itemsets FALSE\n\nalgorithmic control:\n sparse sort verbose\n      7   -2    TRUE\n\neclat - find frequent item sets with the eclat algorithm\nversion 2.6 (2004.08.16)         (c) 2002-2004   Christian Borgelt\ncreate itemset ...\nset transactions ...[1146629 item(s), 100 transaction(s)] done [1.31s].\nsorting and recoding items ... [21 item(s)] done [0.12s].\ncreating bit matrix ... [21 row(s), 100 column(s)] done [0.00s].\nwriting  ... [28761 set(s)] done [0.01s].\nCreating S4 object  ... done [0.25s].\n\n\n\n\n> summary(fi_s3)\nset of 28761 itemsets\n\nmost frequent items:\n  22159360   22155264 1504849920 3170566144 4086710272    (Other)\n     14372      14370      14336      14336      14336     123094\n\nelement (itemset/transaction) length distribution:sizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13\n  21  168  696 1951 3933 5808 6336 5115 3025 1276  364   63    5\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n  1.000   6.000   7.000   6.775   8.000  13.000\n\nsummary of quality measures:\n    support\n Min.   :0.300\n 1st Qu.:0.330\n Median :0.340\n Mean   :0.388\n 3rd Qu.:0.430\n Max.   :0.920\n\nincludes transaction ID lists: TRUE\n\nmining info:\n      data ntransactions support\n ciop_data           100     0.3\n> dim(tidLists(fi_s3))\n[1] 28761   100\n\n</pre>\n\n\n=== Brief Description of the Drawings (5) ===\n\n# [Concept] Coaccess-based I/O Prediction Concepts\n# [Architecture] Storage System Architecture with Coaccess-based Proactive Data Placement\n# [Concept] Coaccessness Analysis using Frequent Itemset Mining\n# [Flowchart] Proactive Data Placement based on Coaccessness\n# [Auxiliary] Experiment Results 1\n# [Auxiliary] Experiment Results 2\n# [Auxiliary] Performance Gain Estimation\n# [Auxiliary] Comparison with prior arts\n\n== Memo for {_BPT-004, _BPT-005} ==\n\n\n=== Think outside the box // Questions ===\n\n* 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n\n* Tiering을 좀 더 다양한 각도에서 바라보자\n:- 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 것이 기존의 tiering 개념임.\n:- SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n:- SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n:- access pattern-aware optimal placement (APOP) - 이런 것은 어떨까?\n:- Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n\n* write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반의 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지?\n\n\n* 만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? SSD-internal memory buffer를 이용해서 SSD를 NVRAM처럼 쓰도록 하는 것이 실제로 말이 되는 이야기일까?\n\n\n\n\n\n=== 기존 기술의 한계점 (Intelligent I/O Prediction 기술의 필요성) ===\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n\n <pre>\n실제 tracelog 사례를 보여주고 hot area가 변화하고 있음을 보일 것.\n</pre>\n\n* Data Placement (Tiering) 관점에서 본 EMC의 FAST, FAST-VP 기술의 한계점\n:- Coarse-grained Data Migration (768KB 까지 작게 나눌 수 있다고는 하지만, 실제로 그렇게 해서 성능이 얼마나 잘 나오는지는 확인 필요. 귀중한 자원인 DRAM을 관리를 위한 metadata로 가득 채우게 될 위험도 있음)\n:- Flexible하지 않은 Tiering 주기\n:- Busy-hour 지난 후 Idle-time에 Tiering 실시\n:- 낮은 지능의 미래 I/O 예측 능력\n:: data access count를 기반으로 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두도록 하는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n\n=== 본 발명의 차별화 포인트 ===\n\n* 본 기술의 특징 1: I/O를 triggering하는 전조 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Indicator는 Run-time에 저비용으로 감지될 수 있는 형태의 정보들로 구성\n:- 미래에 access될 data를 예측케 하는 Indicator간의 mapping은 Bayesian Inference 이용\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n=== 특허 검색/분석 결과 {storage I/O workload prediction} ===\n\n* 검색식: storage I/O workload prediction\n\n* http://portal.uspto.gov/pair/PublicPair 에서 특허 상태 확인 가능.\n\n* I/O prediction을 기반으로 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n:- 기존엔 proactive tiering이 없었나? -> IBM 특허가 나름 유사함. (US7930505, US7512766)\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n==== 1. 비교가 될만한 특허들 ====\n\n예측에 대한 언급이 되어 있는 스토리지 시스템 관련 특허들. 크게 두 가지로서, 그 중의 하나는 가상 volume에 대해 peak I/O가 발생하는 주기를 분석하여, peak I/O 발생시점이 닥치기 전에 미리 가상 volume의 data를 migration 시키는 방법에 대한 것이나, 어느 위치의 data가 언제 access될지에 대한 예측은 아님 (IBM 특허). 다른 하나는 I/O 예측을 기반으로 미래에 자주 access될만한 data를 미리 여러 노드에 분산시키는 방법에 대한 것이나, 예측 방법 자체에 대한 내용은 다루고 있지 않음 (HP 특허)\n\n\n; US7930505, US7512766 (IBM)\n\n* US7930505 (Application Number: US 12/053582), \"Controlling preemptive work balancing in data storage,\" IBM, 특허일 2011-04-19\n: storage network의 I/O 관찰을 통해 peak I/O activity의 repeating instance에 대한 signature를 VLU (virtual logical unit) 단위로 생성하고, 이에 기반하여 언제 heavy I/O가 도래할 것인지를 예측하여, heavy I/O가 오기 전에 미래 data를 faster media로 migration하도록 하는 tiering 기술에 대한 특허. (이미 I/O peak time이 온 후에 hot data를 옮기는 것은 오히려 시스템의 IO성능을 악화시킬 수도 있기 때문에 미리 migration을 하고자 하는 것임)\n: VLU 단위로, peak I/O activity가 어느 주기로 발생할지에 대한 예측임. 지능 수준과 Fine-granularity 측면에서 초보적인 수준임.\n\n* US7512766 (Application Number: US 11/213494), \"Controlling preemptive work balancing in data storage\", IBM, 특허일 2009-03-31\n: 위의 US7930505 특허와 동일 건으로 보임\n\n\n; US6912635 (HP)\n\n* US6912635 (Application Number: US 10/140968), \"Distributing workload evenly across storage media in a storage array\", HP, 특허일 2005-06-28\n: 미래에 자주 access될만한 data를 여러 arrayed storage device에 분산시키겠다는 기술. 그러나 I/O에 대한 예측 방법은 다루어지고 있지 않음.\n\n==== 2. 검색은 되었으나 관련은 없는 특허들 ====\n\n무언가를 예측한다는 측면, Storage System에 대한 기술이라는 측면에서는 관련이 있으나, 미래에 Access될 Data를 예측한다는 측면에서 제안 기술과 관련 없는 특허들\n\n\n; US7640231 (IBM)\n* US7640231 (Application Number: 11/280012), \"Approach based on self-evolving models for performance guarantees in a shared storage system\", IBM, 특허일 2009-12-29\n: network 기반 storage system에서, average latency 혹은 I/O request rate 등에 대한 주기적인 performance sampling을 기반으로, 주어진 objective function의 값을 최대화할 수 있도록 system resource를 제어할 수 있게 하는 기술.\n\n\n; US7774491, US7519725 (IBM)\n\n* US7774491 (Application Number: 12/359377), \"Utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2010-08-10\n: 복수의 클라이언트들을 대상으로 통계적인 성능 QoS를 보장하기 위한 informed throttling 방법\n: A system for utilizing informed throttling to guarantee quality of service to a plurality of clients\n\n* US7519725 (Application Number: US 10/444712), \"System and method for utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2009-04-14\n: resolve the problem of providing statistical performance guarantees for applications generating streams of read/write accesses (I/Os) on a shared, potentially distributed storage system of finite resources, by initiating throttling whenever an I/O stream is receiving insufficient resources\n\n\n; US7827283 (IBM)\n* US7827283 (Application Number: 10/367444), \"System for managing and controlling storage access requirements\", IBM, 2010-11-02\n: SAN 등의 환경에서, storage bandwidth를 관리하고 reserve하는 기술\n: A Resource Management and Reservation System (RMRS) for managing and reserving storage bandwidth, is a platform independent middleware layer that provides an interface to applications, their database management systems, or some other higher level data management systems like ADRS which does data management on behalf of the applications\n\n\n; US8433554, US8140319 (IBM)\n\n* US8433554, \"Predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2013-04-30\n: An approach for predicting performance and capacity of an information technology (IT) system before the IT system is built is described, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n* US8140319, \"Method and system for predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2012-03-20\n: A method and system for predicting performance and capacity of an information technology (IT) system before the IT system is built, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n\n\n; US8255915 (HP)\n* US8255915, \"Workload management for computer system with container hierarchy and workload-group policies\" HP, 특허일 2012-08-28\n\n\n; US7050956 (HP)\n* US7050956 (Application Number: US 09/843930), \"Method and apparatus for morphological modeling of complex systems to predict performance\", HP, 특허일 2006-05-23\n: I/O 성능 예측을 위한 complex storage system에서의 data flow 모델링 방법\n\n\n; US6718434 (HP)\n* US6718434 (Application Number: US 09/871043), \"Method and apparatus for assigning raid levels\", HP, 특허일 2004-04-06\n: Data를 disk array에 load하기 전에, workload 명세 및 device 명세에 기반하여, 적합한 RAID level을 판단하여 disk array에 할당 (RAID build)하는 방법\n\n\n; US8112586 (EMC)\n* US8112586 (Application Number: US 12/190071), \"Predicting and optimizing I/O performance characteristics in a multi-level caching system\", EMC, 특허일 2012-02-07\n: multi-level caching 시스템에서, cost대비 performance를 최적화할 수 있도록, workload에 따른 resource allocation design에 대한 recommendation 기술 (multi-level cache들 간의 workload propagation을 예측하는 것이 핵심)\n\n\n; US6629266 (IBM)\n* US6629266 (Application Number: US 09/442001), \"Method and system for transparent symptom-based selective software rejuvenation\", IBM, 특허일 2003-09-30\n: software가 언제 동작이 멈추는지를 예측함으로써, software의 dependability를 향상할 수 있게 하는 방법\n\n\n; US5088058 (Unisys)\n* US5088058 (Application Number: 07/237120), \"Apparatus and method for evaluating and predicting computer I/O performance using I/O workload snapshots for model input\", Unisys Corporation, 특허일 1992-02-11\n: I/O delay 등에 대한 확률적 모형을 기반으로 disk 시스템의 성능을 검증하고 예측하는 시뮬레이션 기반의 방법\n: A method of evaluating and predicting the performance of an I/O disk system comprised of one or more disk controllers and cooperating disk units using a simulation model containing a stact constructed such that delays are formulated as sets of stochastic processes\n\n=== Comparison of I/O Prediction Approaches ===\n\n미래에 access될 data의 위치와 시기를 예측하기 위한 방법으로 event-based I/O prediction, coaccess-based I/O prediction, periodicity-based I/O prediction 방식이 제안되며, 각각의 방식은 예측에 필요한 연산량 및 input data의 richness, 그리고 workload 변화에 대한 적응성 측면에서 장단점을 가지고 있음. 따라서, 상황에 따라서 적절한 방식을 이용하면 됨.\n\n{| style=\"border-collapse: collapse; border: 1px solid #000\" border=1\n| style=\"width: 25%\" | 비교 항목\n| style=\"width: 25%\" | periodicity-based I/O prediction\n| style=\"width: 25%\" | coaccess-based I/O prediction\n| style=\"width: 25%\" | event-based I/O prediction \n|-\n| context richness required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| computation overhead required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| adaptability to workload changes (lower is better)\n| high\n| high\n| high\n|-\n|}\n\n\n* 전조 기반 예측 vs. 주기성 기반 예측\n{| border=1\n| 비교 항목\n| 전조 기반 예측\n| 주기성 기반 예측\n|-\n| 기술 요약\n| 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식\n| address 별 access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식\n|-\n| 분석 오버헤드\n| High (학습/마이닝에 드는 비용임)\n| Low (주기성 파악 비용임)\n|-\n| Run-time 오버헤드\n| Mid (실시간 전조 모니터링 및 매칭 비용)\n| Low (주기성 계산)\n|-\n| Workload 변화 적응성\n| High\n| High (Workload 변경 사실 알림 hint 필요)\n|-\n| ...\n| ...\n| ...\n|-\n| Incrementality of Mining/Learning\n| ...\n| ...\n|}\n\n* 전조 기반 예측은, 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식임.  주기성 기반 예측은, data access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식임.\n\n\n=== Intelligent Data Placement ===\n\n* Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case\n: 여기에 HML이 사용될 수 있는 것이라면, 어떤 방식으로 어떤 이유로 그런 것일까?\n\n\n* I/O 예측 방식으로서 다음 방식이 제안되며, 각 방식에 따라서 prediction model, 수집되어야 하는 context data, I/O 패턴 DB에 저장되는 정보의 종류 및 형태가 달라질 수 있음. 예측에 이용할 수 있는 context 정보의 richness에 따라 적절한 방식을 사용할 수 있다. (서로 다른 발명이므로, 별도의 특허로 낸다)\n:- event-based I/O prediction\n:- coaccess-based I/O prediction\n:: 여기서, periodicity-based I/O prediction은 기존 특허에서 언급되었으므로, 본 발명에서는 크게 언급하지 않기로 한다.\n\n\n* I/O prediction 결과에 따라 적절한 위치에 data를 배치하게 되는데, 여기서 적절한 위치라 함은 SSD와 같은 fast tier가 될 수도 있겠으나, sequential read/write 같은 경우 striping된 HDD array가 될 수도 있고, 혹은 SAS HDD가 될 수도 있겠음. 또한 SSD 중에서도 SLC, MLC, TLC 로 구분되는 fine-grained fast tier로 구분될 수도 있음\n\n\n* 미래에 오게 될 I/O의 type도 알 수 있을까? 즉, {random/sequential}x{read/write} type 중 어느 것에 해당되는지를 안다면, 그리고 I/O의 intensiveness도 예상할 수 있다면 그에 맞춰서 placement 시 적절한 tier 혹은 적절한 node로 찾아가게 할 수 있을 것임.\n\n\n* context 정보는, 수집되는 위치에 따라, server-side에서 수집되는 context data와 storage-side에서 수집되는 context data로 분류될 수 있음. Event context data는 server-side에서 수집 가능하며, coaccess context data 및 periodicity context data는 storage-side에서 수집될 수 있음.\n\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n: 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n\n=== Design/Implementation Issues ===\n\n* workload type/class tagging 필요\n\n* cache와 tiering을 같이 쓸 때, DRAM cache가 아니라 Flash Cache의 경우 쓸데없이 모든 data가 Flash Cache에 다 들어가게 두기 보다는, tiering에서 cover하지 못하는 것들 위주로 caching하도록 하면 Flash Cache의 공간 낭비를 줄임으로써, 유효 cache 공간을 늘일 수 있게 되고, 그만큼의 성능 향상으로 연결될 수 있음.\n\n* DRAM 기반의 write cache를 적극 이용하여 마치 log-structured file system에서 제공하는 I/O merging and sequentialization 효과를 얻을 수 있음.\n\n* Tiering되는 data는 caching되는 data보다 상대적으로 coarse-grained하게 grouping하여 다룸으로써, 한꺼번에 많은 data를 move할 수 있도록 하고, meta data 관리에 필요한 overhead를 줄일 수 있게 한다. 당장 access되지 않더라도 일정 시간 내에 같이 access될 수 있는 data들은 함께 faster tier로 옮겨진다. 이때 필요한 정보가 coaccess 정보임.\n\n* Cache - Tiering 기법의 연동 방법: 예측이 실패했거나, target tier media의 가용 공간이 부족하여 tiering에서 놓치는 missing data가 있을 수 있음. 이러한 missing data들이 temporal locality가 있는 경우에는 당장은 faster tier에서 serve하지 못하더라도 cache에서 serve될 수 있는데, temporal locality가 적은 경우에는 cache만으로 cover되지 않는다. \n\n* Management table size optimization issue - 만약 모든 LBA 별로 event-based I/O prediction을 해야 한다면 현실적으로 사용 가능한 아키텍쳐가 아닐 수 있음. 만약 spatial locality가 있어서 LBA 1,000개 단위로 묶어서 대응할 수 있다면, 최소 1/1000 의 management table size 감소 효과를 얻을 수 있음. pattern mining 혹은 machine learning 실행 시에는 record 수가 감소함으로써 얻는 이득은 1,000배 이상이 될 수 있음. 관건은 얼마나 LBA 묶음을 크게 만들어서 예측에 사용할 수 있는지임.\n\n\n=== I/O Insight ===\n\n* ML 기반의 I/O Insight 발굴 (Data Placement를 위한 Macroscopic Guideline)\n: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:-> 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n\n* I/O Insight의 요건:\n:# indication, as simple as possible\n:# indication, as specific as possible\n:# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n:# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n\n* I/O Insight의 형태\n: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n=== X-Y Modeling ===\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n=== 예측 실패 대비 (별도 특허화 가능) ===\n\n* I/O 예측이 실패할 수 있으며, 이로 인한 성능 penalty를 최소화하기 위한 방법 필요.\n:- fast tier에 여유공간이 있다면 1타겟과 2타겟을 모두 올려놓아도 되지 않을까? 1타겟과 2타겟을 placement함으로써 예측 정확도를 95%까지 올릴 수 있다면, 꽤 쓸만할 수 있겠음. 그리고 1타겟과 2타겟의 모든 데이터를 다 fast tier에 올려두지 말고 일부만 fast tier에 올려두었다가, 둘 중에 하나의 타겟이 hit되었을 때, hit된 타겟의 나머지 data를 재빨리 fast tier에 올리도록 하는 incremental 방법을 사용할 수 있겠음.\n:- 그러나 이것 또한 예측 실패에 대한 완벽한 prevention이 될 수는 없다. 따라서 예측 실패가 발생하더라도 재빨리 그 성능 penalty를 mitigation할 수 있는 방법이 필요. \n:- cache를 이용하여 mitigation할 수 있을까?\n\n=== Data Read/Write 관련 Issues ===\n\n* read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n\n* 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n\n* write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간에, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면?\n\n* 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있음.\n\n* 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간이 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 scatter했던 I/O들을 나중에 다시 읽어들이려 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 원하는 고성능을 낼 수 있게 된다.\n\n* replica-based dynamic clustering\n: 위와 같은 write-and-then-read-again 상황에서, 해당 data를 저장했었던 HDD의 I/O busy/idle 여부에 상관 없이, 최고의 효과를 낼 수 있게 하기 위해, replica management를 적극적으로 활용하는 것은 어떨까? HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까?\n\n* data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 논의가 아님. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 논의임.\n\n* 그러나, 조심해야 할 것은 위의 논의는 상당히 case-dependency를 상당히 많이 탈 수도 있다는 것임. 책상 위의 아이디어로 끝날 수도 있기 때문에 조심스럽게 접근하는 것이 필요함.\n\n\n\n\n<br/>\n\n== _BPT-006 ==\n\n\n=== Data Lifetime (Valid-to-invalid) Interval 분석에 기반한 Data Placement (Tiering) ===\n\n\n* Data 별로, 혹은 LBA 별로, 혹은 Erase Block 별로? Data의 Valid to Invalid Time (Duration) 패턴을 분석할 수 있을까?\n: 이를 SSD 내에서 Low Overhead로 수행할 수 있나?\n: 혹은, 굳이 SSD 내에서 수행할 필요가 있나?\n: Host side에서 LBA 별로 분석 될 수 있다면, 이 정보를 기반으로 Tiering 시에 활용 할 수 있을 것임\n: 특정 Address에 대해서 W이 되고 나서, 다시 W이 되기까지의 시간을 측정하여 ARM 하면 될 듯. 그 두 W 간에 얼마나 많은 R들이 오는지에 대해서도 고려하여 placement 시 고려할 수 있을 것임.\n\nMeasurement Window 내에서 \'W 간의 Interval이 길다\'는 식으로 인식하는 것과, \'W의 횟수가 잦다\'고 인식하는 것 간에 어떤 차이가 있을 수 있을까?\n\n <pre>\n    01234567890123456789\na1: W-R-W-R-W-R-W-W-R--W : T20W06R04 : Short-Lifetime, Many-Reads : SLC\na2: W---W-R-W---W-W----W : T20W06R01 : Short-Lifetime, Few-Reads : SLC, DRAM?\na3: W-RRRR-W---RR-W-R-RW : T20W04R08 : Mid-Lifetime, Many-Reads : MLC\na4: W------W--R---W----W : T20W04R01 : Mid-Lifetime, Few-Reads : MLC\na5: W-R-R-R-RR-RRRR----R : T20W01R10 : Long-Lifetime, Many-Reads : MLC\na6: W------------R----R- : T20W01R02 : Long-Lifetime, Few-Reads : TLC\n</pre>\n\n=== Idle-time Prediction Based Free Space Management in SSD ===\n\n* Idle-time의 도래 시기와 지속 시간 패턴 분석에 기반한 SSD 내의 Free Space 확보 방법\n:- SSD 내에서 Low overhead (in computation, memory space)로 Idle-time 도래 시기 및 지속 시간 분석이 가능한가?\n:- 가능하다면, SSD 내에서 Garbage Collection 등으로 Free Space를 확보하는 데 있어서 어떤 장점을 가질 수 있는가?\n\n\n\n: 이를 기반으로 \n\n\n\n=== 요약 ===\n* \n\n\n=== 기술 상세 ===\n\n*\n\n\n\n\n\n== _BPT-007 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== _BPT-008 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== _BPT-009 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Ef?cient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== _BPT-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== _BPT-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== _BPT-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== _BPT-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== _BPT-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== _BPT-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== _BPT-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== _BPT-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n----\n=== (SmartSSD API) SSD-internal I/O pattern logging interface and mechanism ===\n\n\n==== Questions ====\n\n* SmartSSD에서 제공할 수 있는 logging 서비스로서 어떤 것들이 있을 수 있나?\n:- 어느? 정도의 free space를 필요로하는 critical (heavy) I/O가 어떤? 주기로 도래할 지 알 수 있을까?\n:- I/O prediction에 도움될 수 있는 정보를 SmartSSD가 기록했다가 필요한 때 (안전한 방법으로?) 줄 수 있을까?\n:- I/O prediction까지는 아니더라도 해당 SSD의 I/O wellness(어떻게 정의?)를 측정했다가 bottleneck을 회피하는 데 유용한 정보를 제공할 수 있을까?\n:- 어떤 SSD가 현재 주어지고 있는 I/O workload에 대해 list performance spec을 만족하는데 어려움을 겪고 있다는 것을 알려줄 수 있는 정보가 어떤 것이 있을까? (SSD 내 write buffer의 fullness 혹은 I/O queue의 유동성? amount of free space in over-provisioning area?) (-> I/O wellness 라는 metric을 이것과 연관시켜 정의할 수도 있으며, 결국 SLA 혹은 Performance QoS와 연관시킬 수 있음)\n:- 지금까지의 S.M.A.R.T.와 비교하였을 때, 어떤 차별점이 있는가? 기존의 S.M.A.R.T.가 제공하지 못하던 타입의 정보를 제공하고, 이로 인해 기존에는 불가능했던 새로운 알고리즘 구현 혹은 새로운 서비스 제공이 가능해짐을 보이면 좋겠음.\n\n\n\n:- workload의 history?\n:- 각 erase block이 erase되어야만 했던 주기?\n:- free space의 추이 (얼마나 많이 남아 돌던지)?\n:- I/O의 heaviness 패턴?\n:- SSD내 write buffer (혹은 write cache)의 utilization정보? (즉, write buffer가 넘칠 정도가 되어서 I/O wait을 해야만 했던 상황이 얼마나 자주 발생했는지, write buffer의 가득찬 정도를 백분율로 표현한다고 했을 때, 평균/표준편차 등으로 대표할 수 있는 normal distribution을 따르는지?\n:- 전체 sequential read/write \n\n\n==== 배경 ====\n\n* 각 SSD 모델별로, 또 동일한 모델의 SSD라 하더라도 어떤 workload에 노출되어왔는지에 따라, 현재 낼 수 있는 I/O performance가 동일하지 않을 수 있다 (!check! 관련 실험 결과 - SSD 830을 이용하여 같은 시간 동안 하나의 host에서 생성되는 workload들을 나누어 받았다 하더라도 내부적인 free block의 갯수 및 random write에 의한 block 내 파편화등의 상태가 상이하여 성능에 차이가 나는 것을 보여줄 것. uFLIP 혹은 filebench 활용 가능). 한 예로, SSD가 어느 호스트의 어느 file system에 매핑되었는지에 따라 그 SSD가 받는 workload의 특성이 다를 수 있다. file system mount point가 달라지거나, 기존과 다른 새로운 응용이 설치/서비스되는 상황이 되면 \n\n\n----\n=== I/O Prediction-based Proactive Data Placement ===\n\n* 미래에 access될 data를 예측하여 적소에 미리 가져다 놓는 기술로서, (1) I/O prediction을 위한 정보 수집 모듈, (2) 주어진 정보들에 기반하여 다가올 I/O를 예측하는 모듈, (3) data placement를 수행하는 모듈로 구성됨.\n\n\n* 본 기술의 특징 1: I/O를 triggering하는 indicator (전조) 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Run-time에 저비용으로 감지될 수 있는 Indicator 정보와, 이에 의해 access될 data 간의 mapping은 Bayesian Inference 이용\n\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n\n----\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n----\n\n=== Tiering Method Avoiding I/O Bottleneck ===\n\n* I/O Prediction에 의해 예측된 미래의 Hot Data를 Proactive하게 Placement한다고 할 때, 예상되는 access pattern으로 인해 발생 가능한 I/O Bottleneck을 최소화할 수 있도록 배치하는 방법\n\n* 예를 들어 미래의 hot data block 1 (200MB), hot data block 2 (100MB), hot data block 3 (400MB)이 1분 후에 access 될 것으로 예측되었다고 가정. data block 1에 대해서는 4KB 단위로 random read가 dominant한 access pattern이 예상되고, data block 2에 대해서는 1024KB 단위의 sequential read가 dominant한 access pattern이 예상되고, data block 3에 대해서는 4KB 단위의 random read와 512KB 단위의 sequential write이 각각 80:20의 비율로 mix된 형태의 access pattern이 예상된다고 했을 때, 각 data block을 어디에 어떤 형태로 placement시킬 것인가?\n\n: machine learning 혹은 pattern mining에 기반한 base workload pattern data를 기반으로 incremental하게 update될 수 있는 data access의 spatial locality와 periodicity\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n\n* 기존 기술과의 대비 (naive I/O prediction vs. full-fledged I/O prediction)\n: 기존 기술은 누적 hit count 정보에 의지하는 naive I/O prediction으로 볼 수 있음. 이러한 방식은 workload의 변화를 제대로 반영하지 못하기 때문에, static한 workload에 대해서는 잘 동작하지만, moving target처럼 dynamic한 workload에 대해서는 오히려 과거의 누적 hit count metric이 현재의 workload 패턴을 왜곡하는 문제점을 가지고 있음. (temporal locality와 spatial locality가 깨지는 순간이 여러번 존재할 수록 기존의 naive I/O prediction 방식으로 성능 향상은 어려워짐)\n\n\n\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== _BPT-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(282,'== _BPT-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== _BPT-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== _BPT-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n[[Bnote patidea 2013-003]]\n\n\n== _BPT-004 ==\n\n=== Event-based I/O Prediction (for Proactive Data Placement) ===\n\n=== 요약 (4) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n=== 기술 상세 (4) ===\n\n* I/O 발생 메커니즘의 모델링\n: I/O prediction을 위해, type-I event (indirect I/O trigger)와 type-II event (direct I/O trigger)를 관찰 및 분석함. Type-I event의 예로서, user의 log-in, 기 log-in되어 있던 user session의 상태 변경 (idle-to-busy), system service에 대한 invocation from local or remote node 등이 포함될 수 있으며, type-II event의 예로서, user application 의 실행, system service의 실행 등이 포함될 수 있음. Type-I event는 type-II event의 발생에 영향을 미치고, type-II event는 I/O request event (data access)의 발생에 영향을 미치므로, \'type-I event\' -> \'type-II event\' -> \'I/O request event\' 까지 이어지는 인과사슬 (cause-effect chain) 기반으로 I/O 발생 메커니즘을 모델링하였음.\n\n\n* Event 정보 수집: type-I event 정보 및 type-II event 정보 정의 및 수집 방법.\n\n\n* I/O 패턴 학습 (Offline Learning): 수집된 Event 정보를 기반으로 I/O 패턴을 learning하는 방법\n\n\n* Run-time I/O 예측 (Online Matching): 시스템 모니터링에서 얻어지는 최근 및 현재의 event들과 learning으로 얻어진 I/O 패턴 DB에 기반하여 Run-time으로 미래 I/O 예측\n\n\n* 시스템 구조: 정보 수집 및 패턴 학습 모듈의 시스템 내 위치 (I/O 패턴 DB 등의 위치 및 실시간 예측 모듈과의 관계)\n\n\n* 순서도: 수집부터 학습까지 전체적인 task의 흐름\n\n\n* 예상 효과: 성능적 측면에서 사용자가 체감하는 lower latency, higher throughput 효과, 이미 발생해버린 congestion time을 피해 미리 적절한 곳에 data를 배치시킴으로써 얻어지는 bottleneck prevention 효과.\n\n=== Data Collection (4) ===\n\n* 역할: I/O 패턴 학습/발굴에 필요한 정보 수집\n\n* 수집 정보 종류\n: 전조 기반 예측에 필요한 간접 I/O 유발자, 직접 I/O 유발자, I/O 발생 이벤트 등 I/O 예측에 도움이 될 수 있는 다양한 정보를 수집. 다음 정보들이 포함될 수 있음.\n:- indirect I/O trigger trace: {(user-id | user-type), status-type, before-status, after-status, event-time}\n:- direct I/O trigger trace: {(program-id | program-type), status-type, before-status, after-status, process-owner, event-time [, process-current-working-directory, files-opened]}\n:- I/O trace: {access-time, address-accessed, read/write-flag, io-size, file-path, process, co-accessed-addresses}\n\n\n* 수집 모듈의 위치\n: storage-server가 분리되어 있는 구조인 SAN/NAS 타입의 architecture인 경우에는 application이 구동되는 server-side에 정보 수집 모듈이 위치한다. storage-server가 결합되어 있는 converged-architecture 기반의 system인 경우에는 해당 system 내에 정보 수집 모듈이 위치한다.\n\n\n* 정보 수집 방식\n: 운영체체가 제공하는 시스템 상태 정보 제공 interface를 이용하거나, blktrace, lsof, debug file system 등 이용 (e.g., /sys/kernel/debug/ 등) 혹은 /proc file system 등을 이용 가능.\n\n== _BPT-005 ==\n\n=== Coaccess-based I/O Prediction (for Proactive Data Placement) ===\n\n* (영문) Coaccess-based I/O Prediction (for Proactive Data Placement)\n* (한글) Coaccess 기반의 I/O 예측 방법 및 시스템 구조\n\n=== 요약 (5) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 coaccess-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n* 본 발명은 address 정보 기반의 I/O prediction 기술로서, client-side의 rich information을 확보가 제한적인 경우에도 periodicity 및 coaccessness 특성을 기반으로 미래의 I/O를 예측할 수 있도록 하는 기술임.\n\n\n* 소정의 Confidence 값 이상인 X=>Y 관계를 활용하여, signature X (Y에 대한 precursor)가 access되면 어느 정도 확률로 이후에\n\n=== 발명의 이용 분야 (5) ===\n\n데이터를 저장하고 처리하는 일반적인 컴퓨팅 시스템으로서,\n* 프론트엔드/백엔드 서버\n* 네트워크 스토리지 시스템\n* 컨버지드 서버-스토리지 시스템\n* 엔터프라이즈 혹은 데이터 센터 내의 IT 인프라\n등에서 활용 가능.\n\n\n=== 발명의 목적 (5) ===\n\n==== 기존 기술의 한계점 ====\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: Fast media로서 SSD가 사용되는 경우에 다음과 같은 부정적인 효과가 발생할 수 있음. 과거에는 자주 access되었던 data가 앞으로도 hot할 것으로 판단하고 tiering 시점에 fast media로 placement 시켰는데 tiering되고 난 후에 자주 access되지 않게 되면, 성능 향상 이득 없이 SSD의 wear-out만 초래하게 됨.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n==== Intelligent I/O Prediction의 필요성 ====\n\n\n\n\n=== 기술 상세 (5) ===\n\n* Data의 coaccess 특성에 기반한 proactive data placement 기법\n: 소정의 time window 내에 같이 access되는 확률이 높은 data 그룹 (coaccess group)이 있을 때, 그 그룹 내의 일부 data가 access되면 나머지 data들도 미리 fast media에 가져다 놓음으로써, 성능 향상을 얻을 수 있게 하는 방법임.\n\n\n* 개본 개념 정의 (coaccess property, coaccessness, coaccess group)\n: 지정된 시간 동안의 measurement window 내에서, 어떤 data가 access될 때 함께 access되는 경향을 가지는 다른 data들이 존재할 때 그 data들 간에 coaccess property 혹은 coaccessness가 있다고 정의한다.  이때, coaccess property를 가지는 data 그룹을 coaccess group이라고 한다. frequent item set mining 관점에서 보면 기준이 되는 support 값 이상의 확률로 관측되는 transaction들을 coaccess group으로 정의할 수 있다.\n\n\n* 추가 개념 정의 (merged coaccess group)\n: 서로 다른 coaccess group들을 개별적으로 다루기보다 이들을 묶어서 하나의 coaccess group으로 만들어 다룸으로써 meta data management의 효율성 및 proactive data placement의 효과 측면에서 장점이 발생하는 경우가 있다. 이렇게 만들어지는 coaccess group을 merged coaccess group이라 정의한다. \n: 서로 다른 coaccess group CG_A와 coaccess group CG_B가 있는데, CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합) 시켜서 새롭게 얻어진 CG_C = {CG_A, CG_B}를 하나의 coaccess group으로 다룬다.\n: computing resource의 한계로 인한 현실적인 문제로 인해 모든 경우의 frequent item set을 발굴하기 어려운 경우가 있음 (Frequent item set mining 시 실제로 maximum size of transaction을 한계 값으로 지정). merged/unionized coaccess group은 이러한 한계적인 상황을 극복할 수 있는 방법으로서 제안되는 개념임. (실제로 관측되었던 실험 data 및 graph를 이용하여 실재 근거를 제시할 것)\n\n<!--\nmerged coaccess group 관련해서.\n\n: CG_A = {a1, a2, ..., aM |supp=s1}, CG_B = {b1, b2, ..., bN |supp=s1}, CG_C = {union of CG_A and CG_B |supp=s1} 이 존재하고 발생 시기가 일치하는 것을 확인하였다면, CG_A, CG_B, CG_C 를 모두 개별적인 coaccess group으로 관리할 것이 아니라, CG_C 만을 coaccess group으로 관리한다. (조건 i) supp(CG_A) == supp(CG_B) == supp(CG_C), (조건 ii) union of CG_A and CG_B is identical to CG_C, (조건 iii) occurrence_time(CG_A) == occurrence_time(CG_B) == occurrence_time(CG_C) 이 있다고 하자. 상기 조건 i, ii, iii이 모두 만족되는 경우에는 CG_C 만을 coaccess group으로 관리하고 지정함으로써, coaccess group의 meta data management overhead를 감소시키고 coaccess-based proactive data placement의 operation efficiency를 증대시킴. /* 여기서, (조건 i)과 (조건 ii)가 만족되면 굳이 (조건 iii)까지 확인할 필요는 없을 수도 있음. 확인 필요 */\n* GCD (Greatest Common Divisor) / LCM(Least Common Multiple) / largest coaccess group 개념과 관련.\n\n* 유사 coaccess group 간 merging?\n: 예를 들어 coaccess group CG_A = {\'a\', \'b\', \'c\', \'d\'} 와 coaccess group CG_B = {\'b\', \'c\', \'d\', \'e\'} 가 있다고 했을 때, \'b\', \'c\', \'d\' data 가 겹치기는 하지만, CG_A와 CG_B는 동일한 coaccess group으로 볼 수는 없다. 그러나 만약 CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 (기준이 되는 확률 값은 시스템과 workload의 상황에 따라서 적절히 조절 가능) CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합)화 하여 단일 coaccess group으로 다루는 방식. (말이 되는 소리인지 확인 필요)\n-->\n\n\n* frequent item set mining을 활용한 coaccess group 발굴 방법\n: Coaccess group 발굴을 위해 frequent item set mining을 이용할 수 있으며, 구체적인 방법은 다음과 같다.\n: (1) 시스템의 resource 상황에 따라 적절한 support 값을 설정한다. 예를 들어 fast media (cache 혹은 tier 1 storage)의 저장 가능 용량에 따라, 얼마나 높은 등급의 hot data까지 fast media에 placement 시킬 수 있을지가 결정될 수 있다. 즉 fast media의 용량이 매우 넉넉하여 10번 중 2번의 빈도로 (support=0.2) 관찰되는 coaccess group 까지 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있고, 혹은 fast media의 용량이 여유롭지 않아서 10번 중 5번 이상의 빈도로 (support=0.5) 관찰되는 coaccess group까지만 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있다. support 값은 상황에 따라 dynamic하게 설정될 수 있으며, 시스템 관리자 혹은 data scientist등 사람에 의해 수동적으로 설정될 수도 있지만 algorithm에 의해 automatic하게 설정되도록 구현될 수도 있다.\n: (2) coaccess group에 대한 단위 관측 시간이 되는 transaction measurement window (TMW) 크기를 설정한다. transaction measurement window의 크기에 따라 coaccess group에 포함될 수 있는 data의 갯수가 달라질 수 있다. transaction measurement window 크기가 너무 작으면 함께 access되는 것으로 판단될 대상이 되는 data 갯수 자체가 감소하게 되므로 coaccessness 분석이 어렵게 된다. 반대로, transaction measurement window 크기가 너무 크면 coaccess group의 종류는 감소하고 각 coaccess group의 크기는 커지는 경향이 생기는데, 이 경우의 문제점은 data placement 시 fast media를 효율적으로 사용하는 것이 어려워질 수 있다는 것이다.\n: (3) access되는 data address 정보를 기반으로 frequent item set mining을 시작한다 (이를 위해서는 support 값과 transaction measurement window size 값이 설정되어 있어야 함).  transaction measurement window 동안 access 되는 address들의 집합을 하나의 transaction (basket)으로 묶는다. 전체 관찰 기간 동안 N 번의 transaction measurement window가 있었다면, N 개 만큼의 transaction이 만들어진다. 이 때 N 개의 transaction을 통틀어 support 값 확률 이상으로 빈번하게 등장하는 item set이 있다면 그것을 coaccess group으로 지정하고 coaccess group management ID를 부여, 관리한다.\n: (4) frequent item set mining에서의 confidence 개념을 도입하여 precursor의 존재 감지 시, 미래에 access되게 될 나머지 data들을 신속하게  (일부 데이터가 access되고 나면 나머지 데이터들도 지정된 시간 내에 함께 access될 확률을 의미하게 됨)\n: (4) run-time 시, coaccess group의 빠른 속도로 찾아낼 수 있도록 하기 위해, coaccess group \n\n\n : coaccess group 발굴이 되었다면, 어떻게 저장하고, 어떻게 불러내서(매칭시켜서) proactive data placement에 활용할 수 있게 할 것인가?\n\n\n* O(1)으로, 매칭되는 패턴을 찾을 수 있게 하려면?\n\n\n* proactive data placement를 위해 \n\n\n* Coaccess group pattern matching 방법\n\n\n* Fig{association graph, temporal occurance graph}\n: Frequent item set mining 분석 결과로 얻어진 association graph 및 시간에 따른 frequent item 발생 추이 graph를 이용하여 coaccess pattern의 실재함을 설명\n\n\n* Coaccess property는 다음과 같은 방식으로 활용될 수 있음\n:- 특정 coaccess pattern이 특정 workload에 대한 precursor가 되는 경우가 있을 수 있음. 이를 이용하여 다가올 workload의 prediction에 활용 가능 (예를 들어, coaccess group A가 access된 후 T1 시간 경과 후에 address range B를 random read하는 workload가 발생하고, T2 시간 경과 후에 address range C를 sequential write하는 workload가 오는 경향이 있다는 것을 알고 있다면, 이 정보를 기반으로 proactive하게 address range B를 fast media에 placement 시키고, address range C에 대해서는 caching  \n:- coaccess group 내의 일부 data가 access되면 그 coaccess group 내의 다른 data들을 미리 fast media에 가져다 놓음으로써 성능 향상 가능\n\n<!--\n* (for event-based I/O prediction) coaccess group이 access되는 순서를 event-based I/O prediction에 이용할 수도 있음. 예를 들어, coaccess group A가 access되고 나서 T1 시간 내로 coaccess group B가 access될 확률이 80% 이상이라고 하고, coaccess group B가 access되고 나서 T2 시간 내로 coaccess group C와 coaccess group D가 access될 확률이 90% 이상이라고 한다면, 이러한 정보를 기반으로 ... 무엇을 할 수 있나?\n-->\n\n\n* 이를 위해서 기존의 storage system 구조에 (1) coaccess pattern analysis engine, (2) coacccess pattern aware data placement controller가 추가되는 것이 필요하다.\n\n\n* coaccess pattern analyzer의 역할: workload 별로(???) coaccess pattern list 구축\n\n\n* coaccess pattern analyzer의 구조 및 동작 방식:\n\n\n* coaccess pattern aware data placement controller의 역할:\n\n\n* coaccess pattern aware data placement controller의 구조 및 동작 방식: coaccess pattern 정보를 받아들이기 위한 interface, interface로 받아들인 coaccess pattern 정보의 queuing? data placement의 I/O 단위 (granularity), data placement의 주기 (static? or dynamic?) 등 coaccess pattern 정보를 data placement 시 어떤 식으로 활용하는지\n\n=== Coaccess-based I/O Prediction의 성능 향상 효과 Estimation ===\n\n* 시뮬레이션 기반의 성능 향상 효과 비교\n:- Fast media의 4KB random read latency는 96.47 usec (SSD 830 128GB 기준), slow media의 4KB random read latency는 1,884.03 usec (HDD 7,200rpm 2TB 기준)으로 설정 (iozone에 의한 실측값 사용)\n:- 실험 시작 전에는 4GB 크기의 target data가 slow media (HDD)에 저장되어 있음.\n:- 여기에 1,000,000 개의 4KB random read request로 구성되는 workload를 인가. (각 read request들은 서로 중복되는 data를 가리키지 않으며, 4GB 크기의 target data를 한 번씩 다 request하게 된다고 가정)\n:- Workload 패턴 학습 결과, 4GB의 전체 data 중, \'가\' type의 coaccess 특성을 가지는 10MB 크기의 data 그룹 (data set 당 2,500개의 4KB data 포함)이 200개가 있으며, \'나\' type의 coaccess 특성을 가지는 1MB 크기의 data 그룹 (data set 당 250개의 4KB data 포함)이 1,000개 있는 것을 발견한 것으로 가정.\n:- \'가\' type의 coaccess 패턴은 해당 data 그룹 내 100개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 80%인 것으로 정의.\n:- \'나\' type의 coaccess 패턴을 해당 data 그룹 내 50개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 100%인 것으로 정의.\n\n\n\n\n본 기법을 적용한 경우와 그렇지 않은 경우의 성능 차이를 IOPS로 비교해보면 다음과 같다. \n\n: 예를 들어 설명하면 다음과 같다.\n: data 1, data 2, ..., data 10000으로 구성된 data set이 있으며, 각 data들의 크기는 4KB이다. 학습을 통해서, (1) 이 중 어느 한 data가 access되었을 때, 1분 이내에 나머지 data들도 같이 access되게 될 확률은 55%, (2) 이 중 2개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 80%, (3) 이 중 3개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 95%라는 것을 알고 있다고 가정한다.\n: fast media의 크기는 4GB 이며, 기존에 \n\n이라는 것을 알고 있다면, data 1, 2, 3이 access되었을 때 data 4~128까지도 미리 proactive하게 fast tier에 배치시킴으로써, 그로 인한 성능 이득을 얻을 수 있게 된다. (!!!실제 실험 사례?!!!)\n\n=== Real Data Analysis (MSN File Server) (5) ===\n\n* Analysis Target Workload Specification\n:- MSN FileServer Tracelog (2008-03-10, 약 5GB 데이터)\n:- 21720977703 microseconds (약 6시간)\n:- 29345085 IO operations / 21720 sec => 1351 IOPS\n\n\n* 첫 16.5 minutes (990 seconds) 동안의 I/O에 대해 10 seconds 단위의 Measurement Window로 측정\n\n\n* Frequent Item Sets Mining (Eclat algorithm)\n: support=0.3, minlen=1, maxlen=15\n <pre>\n> fi_s3 <- eclat(ciop_data, parameter=list(supp=0.3, maxlen=15, minlen=1, tidLists=T))\n\nparameter specification:\n tidLists support minlen maxlen            target   ext\n     TRUE     0.3      1     15 frequent itemsets FALSE\n\nalgorithmic control:\n sparse sort verbose\n      7   -2    TRUE\n\neclat - find frequent item sets with the eclat algorithm\nversion 2.6 (2004.08.16)         (c) 2002-2004   Christian Borgelt\ncreate itemset ...\nset transactions ...[1146629 item(s), 100 transaction(s)] done [1.31s].\nsorting and recoding items ... [21 item(s)] done [0.12s].\ncreating bit matrix ... [21 row(s), 100 column(s)] done [0.00s].\nwriting  ... [28761 set(s)] done [0.01s].\nCreating S4 object  ... done [0.25s].\n\n\n\n\n> summary(fi_s3)\nset of 28761 itemsets\n\nmost frequent items:\n  22159360   22155264 1504849920 3170566144 4086710272    (Other)\n     14372      14370      14336      14336      14336     123094\n\nelement (itemset/transaction) length distribution:sizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13\n  21  168  696 1951 3933 5808 6336 5115 3025 1276  364   63    5\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n  1.000   6.000   7.000   6.775   8.000  13.000\n\nsummary of quality measures:\n    support\n Min.   :0.300\n 1st Qu.:0.330\n Median :0.340\n Mean   :0.388\n 3rd Qu.:0.430\n Max.   :0.920\n\nincludes transaction ID lists: TRUE\n\nmining info:\n      data ntransactions support\n ciop_data           100     0.3\n> dim(tidLists(fi_s3))\n[1] 28761   100\n\n</pre>\n\n\n=== Brief Description of the Drawings (5) ===\n\n# [Concept] Coaccess-based I/O Prediction Concepts\n# [Architecture] Storage System Architecture with Coaccess-based Proactive Data Placement\n# [Concept] Coaccessness Analysis using Frequent Itemset Mining\n# [Flowchart] Proactive Data Placement based on Coaccessness\n# [Auxiliary] Experiment Results 1\n# [Auxiliary] Experiment Results 2\n# [Auxiliary] Performance Gain Estimation\n# [Auxiliary] Comparison with prior arts\n\n== Memo for {_BPT-004, _BPT-005} ==\n\n\n=== Think outside the box // Questions ===\n\n* 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n\n* Tiering을 좀 더 다양한 각도에서 바라보자\n:- 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 것이 기존의 tiering 개념임.\n:- SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n:- SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n:- access pattern-aware optimal placement (APOP) - 이런 것은 어떨까?\n:- Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n\n* write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반의 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지?\n\n\n* 만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? SSD-internal memory buffer를 이용해서 SSD를 NVRAM처럼 쓰도록 하는 것이 실제로 말이 되는 이야기일까?\n\n\n\n\n\n=== 기존 기술의 한계점 (Intelligent I/O Prediction 기술의 필요성) ===\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n\n <pre>\n실제 tracelog 사례를 보여주고 hot area가 변화하고 있음을 보일 것.\n</pre>\n\n* Data Placement (Tiering) 관점에서 본 EMC의 FAST, FAST-VP 기술의 한계점\n:- Coarse-grained Data Migration (768KB 까지 작게 나눌 수 있다고는 하지만, 실제로 그렇게 해서 성능이 얼마나 잘 나오는지는 확인 필요. 귀중한 자원인 DRAM을 관리를 위한 metadata로 가득 채우게 될 위험도 있음)\n:- Flexible하지 않은 Tiering 주기\n:- Busy-hour 지난 후 Idle-time에 Tiering 실시\n:- 낮은 지능의 미래 I/O 예측 능력\n:: data access count를 기반으로 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두도록 하는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n\n=== 본 발명의 차별화 포인트 ===\n\n* 본 기술의 특징 1: I/O를 triggering하는 전조 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Indicator는 Run-time에 저비용으로 감지될 수 있는 형태의 정보들로 구성\n:- 미래에 access될 data를 예측케 하는 Indicator간의 mapping은 Bayesian Inference 이용\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n=== 특허 검색/분석 결과 {storage I/O workload prediction} ===\n\n* 검색식: storage I/O workload prediction\n\n* http://portal.uspto.gov/pair/PublicPair 에서 특허 상태 확인 가능.\n\n* I/O prediction을 기반으로 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n:- 기존엔 proactive tiering이 없었나? -> IBM 특허가 나름 유사함. (US7930505, US7512766)\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n==== 1. 비교가 될만한 특허들 ====\n\n예측에 대한 언급이 되어 있는 스토리지 시스템 관련 특허들. 크게 두 가지로서, 그 중의 하나는 가상 volume에 대해 peak I/O가 발생하는 주기를 분석하여, peak I/O 발생시점이 닥치기 전에 미리 가상 volume의 data를 migration 시키는 방법에 대한 것이나, 어느 위치의 data가 언제 access될지에 대한 예측은 아님 (IBM 특허). 다른 하나는 I/O 예측을 기반으로 미래에 자주 access될만한 data를 미리 여러 노드에 분산시키는 방법에 대한 것이나, 예측 방법 자체에 대한 내용은 다루고 있지 않음 (HP 특허)\n\n\n; US7930505, US7512766 (IBM)\n\n* US7930505 (Application Number: US 12/053582), \"Controlling preemptive work balancing in data storage,\" IBM, 특허일 2011-04-19\n: storage network의 I/O 관찰을 통해 peak I/O activity의 repeating instance에 대한 signature를 VLU (virtual logical unit) 단위로 생성하고, 이에 기반하여 언제 heavy I/O가 도래할 것인지를 예측하여, heavy I/O가 오기 전에 미래 data를 faster media로 migration하도록 하는 tiering 기술에 대한 특허. (이미 I/O peak time이 온 후에 hot data를 옮기는 것은 오히려 시스템의 IO성능을 악화시킬 수도 있기 때문에 미리 migration을 하고자 하는 것임)\n: VLU 단위로, peak I/O activity가 어느 주기로 발생할지에 대한 예측임. 지능 수준과 Fine-granularity 측면에서 초보적인 수준임.\n\n* US7512766 (Application Number: US 11/213494), \"Controlling preemptive work balancing in data storage\", IBM, 특허일 2009-03-31\n: 위의 US7930505 특허와 동일 건으로 보임\n\n\n; US6912635 (HP)\n\n* US6912635 (Application Number: US 10/140968), \"Distributing workload evenly across storage media in a storage array\", HP, 특허일 2005-06-28\n: 미래에 자주 access될만한 data를 여러 arrayed storage device에 분산시키겠다는 기술. 그러나 I/O에 대한 예측 방법은 다루어지고 있지 않음.\n\n==== 2. 검색은 되었으나 관련은 없는 특허들 ====\n\n무언가를 예측한다는 측면, Storage System에 대한 기술이라는 측면에서는 관련이 있으나, 미래에 Access될 Data를 예측한다는 측면에서 제안 기술과 관련 없는 특허들\n\n\n; US7640231 (IBM)\n* US7640231 (Application Number: 11/280012), \"Approach based on self-evolving models for performance guarantees in a shared storage system\", IBM, 특허일 2009-12-29\n: network 기반 storage system에서, average latency 혹은 I/O request rate 등에 대한 주기적인 performance sampling을 기반으로, 주어진 objective function의 값을 최대화할 수 있도록 system resource를 제어할 수 있게 하는 기술.\n\n\n; US7774491, US7519725 (IBM)\n\n* US7774491 (Application Number: 12/359377), \"Utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2010-08-10\n: 복수의 클라이언트들을 대상으로 통계적인 성능 QoS를 보장하기 위한 informed throttling 방법\n: A system for utilizing informed throttling to guarantee quality of service to a plurality of clients\n\n* US7519725 (Application Number: US 10/444712), \"System and method for utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2009-04-14\n: resolve the problem of providing statistical performance guarantees for applications generating streams of read/write accesses (I/Os) on a shared, potentially distributed storage system of finite resources, by initiating throttling whenever an I/O stream is receiving insufficient resources\n\n\n; US7827283 (IBM)\n* US7827283 (Application Number: 10/367444), \"System for managing and controlling storage access requirements\", IBM, 2010-11-02\n: SAN 등의 환경에서, storage bandwidth를 관리하고 reserve하는 기술\n: A Resource Management and Reservation System (RMRS) for managing and reserving storage bandwidth, is a platform independent middleware layer that provides an interface to applications, their database management systems, or some other higher level data management systems like ADRS which does data management on behalf of the applications\n\n\n; US8433554, US8140319 (IBM)\n\n* US8433554, \"Predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2013-04-30\n: An approach for predicting performance and capacity of an information technology (IT) system before the IT system is built is described, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n* US8140319, \"Method and system for predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2012-03-20\n: A method and system for predicting performance and capacity of an information technology (IT) system before the IT system is built, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n\n\n; US8255915 (HP)\n* US8255915, \"Workload management for computer system with container hierarchy and workload-group policies\" HP, 특허일 2012-08-28\n\n\n; US7050956 (HP)\n* US7050956 (Application Number: US 09/843930), \"Method and apparatus for morphological modeling of complex systems to predict performance\", HP, 특허일 2006-05-23\n: I/O 성능 예측을 위한 complex storage system에서의 data flow 모델링 방법\n\n\n; US6718434 (HP)\n* US6718434 (Application Number: US 09/871043), \"Method and apparatus for assigning raid levels\", HP, 특허일 2004-04-06\n: Data를 disk array에 load하기 전에, workload 명세 및 device 명세에 기반하여, 적합한 RAID level을 판단하여 disk array에 할당 (RAID build)하는 방법\n\n\n; US8112586 (EMC)\n* US8112586 (Application Number: US 12/190071), \"Predicting and optimizing I/O performance characteristics in a multi-level caching system\", EMC, 특허일 2012-02-07\n: multi-level caching 시스템에서, cost대비 performance를 최적화할 수 있도록, workload에 따른 resource allocation design에 대한 recommendation 기술 (multi-level cache들 간의 workload propagation을 예측하는 것이 핵심)\n\n\n; US6629266 (IBM)\n* US6629266 (Application Number: US 09/442001), \"Method and system for transparent symptom-based selective software rejuvenation\", IBM, 특허일 2003-09-30\n: software가 언제 동작이 멈추는지를 예측함으로써, software의 dependability를 향상할 수 있게 하는 방법\n\n\n; US5088058 (Unisys)\n* US5088058 (Application Number: 07/237120), \"Apparatus and method for evaluating and predicting computer I/O performance using I/O workload snapshots for model input\", Unisys Corporation, 특허일 1992-02-11\n: I/O delay 등에 대한 확률적 모형을 기반으로 disk 시스템의 성능을 검증하고 예측하는 시뮬레이션 기반의 방법\n: A method of evaluating and predicting the performance of an I/O disk system comprised of one or more disk controllers and cooperating disk units using a simulation model containing a stact constructed such that delays are formulated as sets of stochastic processes\n\n=== Comparison of I/O Prediction Approaches ===\n\n미래에 access될 data의 위치와 시기를 예측하기 위한 방법으로 event-based I/O prediction, coaccess-based I/O prediction, periodicity-based I/O prediction 방식이 제안되며, 각각의 방식은 예측에 필요한 연산량 및 input data의 richness, 그리고 workload 변화에 대한 적응성 측면에서 장단점을 가지고 있음. 따라서, 상황에 따라서 적절한 방식을 이용하면 됨.\n\n{| style=\"border-collapse: collapse; border: 1px solid #000\" border=1\n| style=\"width: 25%\" | 비교 항목\n| style=\"width: 25%\" | periodicity-based I/O prediction\n| style=\"width: 25%\" | coaccess-based I/O prediction\n| style=\"width: 25%\" | event-based I/O prediction \n|-\n| context richness required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| computation overhead required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| adaptability to workload changes (lower is better)\n| high\n| high\n| high\n|-\n|}\n\n\n* 전조 기반 예측 vs. 주기성 기반 예측\n{| border=1\n| 비교 항목\n| 전조 기반 예측\n| 주기성 기반 예측\n|-\n| 기술 요약\n| 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식\n| address 별 access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식\n|-\n| 분석 오버헤드\n| High (학습/마이닝에 드는 비용임)\n| Low (주기성 파악 비용임)\n|-\n| Run-time 오버헤드\n| Mid (실시간 전조 모니터링 및 매칭 비용)\n| Low (주기성 계산)\n|-\n| Workload 변화 적응성\n| High\n| High (Workload 변경 사실 알림 hint 필요)\n|-\n| ...\n| ...\n| ...\n|-\n| Incrementality of Mining/Learning\n| ...\n| ...\n|}\n\n* 전조 기반 예측은, 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식임.  주기성 기반 예측은, data access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식임.\n\n\n=== Intelligent Data Placement ===\n\n* Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case\n: 여기에 HML이 사용될 수 있는 것이라면, 어떤 방식으로 어떤 이유로 그런 것일까?\n\n\n* I/O 예측 방식으로서 다음 방식이 제안되며, 각 방식에 따라서 prediction model, 수집되어야 하는 context data, I/O 패턴 DB에 저장되는 정보의 종류 및 형태가 달라질 수 있음. 예측에 이용할 수 있는 context 정보의 richness에 따라 적절한 방식을 사용할 수 있다. (서로 다른 발명이므로, 별도의 특허로 낸다)\n:- event-based I/O prediction\n:- coaccess-based I/O prediction\n:: 여기서, periodicity-based I/O prediction은 기존 특허에서 언급되었으므로, 본 발명에서는 크게 언급하지 않기로 한다.\n\n\n* I/O prediction 결과에 따라 적절한 위치에 data를 배치하게 되는데, 여기서 적절한 위치라 함은 SSD와 같은 fast tier가 될 수도 있겠으나, sequential read/write 같은 경우 striping된 HDD array가 될 수도 있고, 혹은 SAS HDD가 될 수도 있겠음. 또한 SSD 중에서도 SLC, MLC, TLC 로 구분되는 fine-grained fast tier로 구분될 수도 있음\n\n\n* 미래에 오게 될 I/O의 type도 알 수 있을까? 즉, {random/sequential}x{read/write} type 중 어느 것에 해당되는지를 안다면, 그리고 I/O의 intensiveness도 예상할 수 있다면 그에 맞춰서 placement 시 적절한 tier 혹은 적절한 node로 찾아가게 할 수 있을 것임.\n\n\n* context 정보는, 수집되는 위치에 따라, server-side에서 수집되는 context data와 storage-side에서 수집되는 context data로 분류될 수 있음. Event context data는 server-side에서 수집 가능하며, coaccess context data 및 periodicity context data는 storage-side에서 수집될 수 있음.\n\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n: 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n\n=== Design/Implementation Issues ===\n\n* workload type/class tagging 필요\n\n* cache와 tiering을 같이 쓸 때, DRAM cache가 아니라 Flash Cache의 경우 쓸데없이 모든 data가 Flash Cache에 다 들어가게 두기 보다는, tiering에서 cover하지 못하는 것들 위주로 caching하도록 하면 Flash Cache의 공간 낭비를 줄임으로써, 유효 cache 공간을 늘일 수 있게 되고, 그만큼의 성능 향상으로 연결될 수 있음.\n\n* DRAM 기반의 write cache를 적극 이용하여 마치 log-structured file system에서 제공하는 I/O merging and sequentialization 효과를 얻을 수 있음.\n\n* Tiering되는 data는 caching되는 data보다 상대적으로 coarse-grained하게 grouping하여 다룸으로써, 한꺼번에 많은 data를 move할 수 있도록 하고, meta data 관리에 필요한 overhead를 줄일 수 있게 한다. 당장 access되지 않더라도 일정 시간 내에 같이 access될 수 있는 data들은 함께 faster tier로 옮겨진다. 이때 필요한 정보가 coaccess 정보임.\n\n* Cache - Tiering 기법의 연동 방법: 예측이 실패했거나, target tier media의 가용 공간이 부족하여 tiering에서 놓치는 missing data가 있을 수 있음. 이러한 missing data들이 temporal locality가 있는 경우에는 당장은 faster tier에서 serve하지 못하더라도 cache에서 serve될 수 있는데, temporal locality가 적은 경우에는 cache만으로 cover되지 않는다. \n\n* Management table size optimization issue - 만약 모든 LBA 별로 event-based I/O prediction을 해야 한다면 현실적으로 사용 가능한 아키텍쳐가 아닐 수 있음. 만약 spatial locality가 있어서 LBA 1,000개 단위로 묶어서 대응할 수 있다면, 최소 1/1000 의 management table size 감소 효과를 얻을 수 있음. pattern mining 혹은 machine learning 실행 시에는 record 수가 감소함으로써 얻는 이득은 1,000배 이상이 될 수 있음. 관건은 얼마나 LBA 묶음을 크게 만들어서 예측에 사용할 수 있는지임.\n\n\n=== I/O Insight ===\n\n* ML 기반의 I/O Insight 발굴 (Data Placement를 위한 Macroscopic Guideline)\n: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:-> 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n\n* I/O Insight의 요건:\n:# indication, as simple as possible\n:# indication, as specific as possible\n:# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n:# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n\n* I/O Insight의 형태\n: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n=== X-Y Modeling ===\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n=== 예측 실패 대비 (별도 특허화 가능) ===\n\n* I/O 예측이 실패할 수 있으며, 이로 인한 성능 penalty를 최소화하기 위한 방법 필요.\n:- fast tier에 여유공간이 있다면 1타겟과 2타겟을 모두 올려놓아도 되지 않을까? 1타겟과 2타겟을 placement함으로써 예측 정확도를 95%까지 올릴 수 있다면, 꽤 쓸만할 수 있겠음. 그리고 1타겟과 2타겟의 모든 데이터를 다 fast tier에 올려두지 말고 일부만 fast tier에 올려두었다가, 둘 중에 하나의 타겟이 hit되었을 때, hit된 타겟의 나머지 data를 재빨리 fast tier에 올리도록 하는 incremental 방법을 사용할 수 있겠음.\n:- 그러나 이것 또한 예측 실패에 대한 완벽한 prevention이 될 수는 없다. 따라서 예측 실패가 발생하더라도 재빨리 그 성능 penalty를 mitigation할 수 있는 방법이 필요. \n:- cache를 이용하여 mitigation할 수 있을까?\n\n=== Data Read/Write 관련 Issues ===\n\n* read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n\n* 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n\n* write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간에, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면?\n\n* 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있음.\n\n* 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간이 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 scatter했던 I/O들을 나중에 다시 읽어들이려 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 원하는 고성능을 낼 수 있게 된다.\n\n* replica-based dynamic clustering\n: 위와 같은 write-and-then-read-again 상황에서, 해당 data를 저장했었던 HDD의 I/O busy/idle 여부에 상관 없이, 최고의 효과를 낼 수 있게 하기 위해, replica management를 적극적으로 활용하는 것은 어떨까? HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까?\n\n* data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 논의가 아님. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 논의임.\n\n* 그러나, 조심해야 할 것은 위의 논의는 상당히 case-dependency를 상당히 많이 탈 수도 있다는 것임. 책상 위의 아이디어로 끝날 수도 있기 때문에 조심스럽게 접근하는 것이 필요함.\n\n\n\n\n<br/>\n\n== _BPT-006 ==\n\n\n=== Data Lifetime (Valid-to-invalid) Interval 분석에 기반한 Data Placement (Tiering) ===\n\n\n* Data 별로, 혹은 LBA 별로, 혹은 Erase Block 별로? Data의 Valid to Invalid Time (Duration) 패턴을 분석할 수 있을까?\n: 이를 SSD 내에서 Low Overhead로 수행할 수 있나?\n: 혹은, 굳이 SSD 내에서 수행할 필요가 있나?\n: Host side에서 LBA 별로 분석 될 수 있다면, 이 정보를 기반으로 Tiering 시에 활용 할 수 있을 것임\n: 특정 Address에 대해서 W이 되고 나서, 다시 W이 되기까지의 시간을 측정하여 ARM 하면 될 듯. 그 두 W 간에 얼마나 많은 R들이 오는지에 대해서도 고려하여 placement 시 고려할 수 있을 것임.\n\nMeasurement Window 내에서 \'W 간의 Interval이 길다\'는 식으로 인식하는 것과, \'W의 횟수가 잦다\'고 인식하는 것 간에 어떤 차이가 있을 수 있을까?\n\n <pre>\n    01234567890123456789\na1: W-R-W-R-W-R-W-W-R--W : T20W06R04 : Short-Lifetime, Many-Reads : SLC\na2: W---W-R-W---W-W----W : T20W06R01 : Short-Lifetime, Few-Reads : SLC, DRAM?\na3: W-RRRR-W---RR-W-R-RW : T20W04R08 : Mid-Lifetime, Many-Reads : MLC\na4: W------W--R---W----W : T20W04R01 : Mid-Lifetime, Few-Reads : MLC\na5: W-R-R-R-RR-RRRR----R : T20W01R10 : Long-Lifetime, Many-Reads : MLC\na6: W------------R----R- : T20W01R02 : Long-Lifetime, Few-Reads : TLC\n</pre>\n\n=== Idle-time Prediction Based Free Space Management in SSD ===\n\n* Idle-time의 도래 시기와 지속 시간 패턴 분석에 기반한 SSD 내의 Free Space 확보 방법\n:- SSD 내에서 Low overhead (in computation, memory space)로 Idle-time 도래 시기 및 지속 시간 분석이 가능한가?\n:- 가능하다면, SSD 내에서 Garbage Collection 등으로 Free Space를 확보하는 데 있어서 어떤 장점을 가질 수 있는가?\n\n\n\n: 이를 기반으로 \n\n\n\n=== 요약 ===\n* \n\n\n=== 기술 상세 ===\n\n*\n\n\n\n\n\n== _BPT-007 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== _BPT-008 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== _BPT-009 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Ef?cient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== _BPT-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== _BPT-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== _BPT-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== _BPT-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== _BPT-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== _BPT-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== _BPT-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== _BPT-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n----\n=== (SmartSSD API) SSD-internal I/O pattern logging interface and mechanism ===\n\n\n==== Questions ====\n\n* SmartSSD에서 제공할 수 있는 logging 서비스로서 어떤 것들이 있을 수 있나?\n:- 어느? 정도의 free space를 필요로하는 critical (heavy) I/O가 어떤? 주기로 도래할 지 알 수 있을까?\n:- I/O prediction에 도움될 수 있는 정보를 SmartSSD가 기록했다가 필요한 때 (안전한 방법으로?) 줄 수 있을까?\n:- I/O prediction까지는 아니더라도 해당 SSD의 I/O wellness(어떻게 정의?)를 측정했다가 bottleneck을 회피하는 데 유용한 정보를 제공할 수 있을까?\n:- 어떤 SSD가 현재 주어지고 있는 I/O workload에 대해 list performance spec을 만족하는데 어려움을 겪고 있다는 것을 알려줄 수 있는 정보가 어떤 것이 있을까? (SSD 내 write buffer의 fullness 혹은 I/O queue의 유동성? amount of free space in over-provisioning area?) (-> I/O wellness 라는 metric을 이것과 연관시켜 정의할 수도 있으며, 결국 SLA 혹은 Performance QoS와 연관시킬 수 있음)\n:- 지금까지의 S.M.A.R.T.와 비교하였을 때, 어떤 차별점이 있는가? 기존의 S.M.A.R.T.가 제공하지 못하던 타입의 정보를 제공하고, 이로 인해 기존에는 불가능했던 새로운 알고리즘 구현 혹은 새로운 서비스 제공이 가능해짐을 보이면 좋겠음.\n\n\n\n:- workload의 history?\n:- 각 erase block이 erase되어야만 했던 주기?\n:- free space의 추이 (얼마나 많이 남아 돌던지)?\n:- I/O의 heaviness 패턴?\n:- SSD내 write buffer (혹은 write cache)의 utilization정보? (즉, write buffer가 넘칠 정도가 되어서 I/O wait을 해야만 했던 상황이 얼마나 자주 발생했는지, write buffer의 가득찬 정도를 백분율로 표현한다고 했을 때, 평균/표준편차 등으로 대표할 수 있는 normal distribution을 따르는지?\n:- 전체 sequential read/write \n\n\n==== 배경 ====\n\n* 각 SSD 모델별로, 또 동일한 모델의 SSD라 하더라도 어떤 workload에 노출되어왔는지에 따라, 현재 낼 수 있는 I/O performance가 동일하지 않을 수 있다 (!check! 관련 실험 결과 - SSD 830을 이용하여 같은 시간 동안 하나의 host에서 생성되는 workload들을 나누어 받았다 하더라도 내부적인 free block의 갯수 및 random write에 의한 block 내 파편화등의 상태가 상이하여 성능에 차이가 나는 것을 보여줄 것. uFLIP 혹은 filebench 활용 가능). 한 예로, SSD가 어느 호스트의 어느 file system에 매핑되었는지에 따라 그 SSD가 받는 workload의 특성이 다를 수 있다. file system mount point가 달라지거나, 기존과 다른 새로운 응용이 설치/서비스되는 상황이 되면 \n\n\n----\n=== I/O Prediction-based Proactive Data Placement ===\n\n* 미래에 access될 data를 예측하여 적소에 미리 가져다 놓는 기술로서, (1) I/O prediction을 위한 정보 수집 모듈, (2) 주어진 정보들에 기반하여 다가올 I/O를 예측하는 모듈, (3) data placement를 수행하는 모듈로 구성됨.\n\n\n* 본 기술의 특징 1: I/O를 triggering하는 indicator (전조) 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Run-time에 저비용으로 감지될 수 있는 Indicator 정보와, 이에 의해 access될 data 간의 mapping은 Bayesian Inference 이용\n\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n\n----\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n----\n\n=== Tiering Method Avoiding I/O Bottleneck ===\n\n* I/O Prediction에 의해 예측된 미래의 Hot Data를 Proactive하게 Placement한다고 할 때, 예상되는 access pattern으로 인해 발생 가능한 I/O Bottleneck을 최소화할 수 있도록 배치하는 방법\n\n* 예를 들어 미래의 hot data block 1 (200MB), hot data block 2 (100MB), hot data block 3 (400MB)이 1분 후에 access 될 것으로 예측되었다고 가정. data block 1에 대해서는 4KB 단위로 random read가 dominant한 access pattern이 예상되고, data block 2에 대해서는 1024KB 단위의 sequential read가 dominant한 access pattern이 예상되고, data block 3에 대해서는 4KB 단위의 random read와 512KB 단위의 sequential write이 각각 80:20의 비율로 mix된 형태의 access pattern이 예상된다고 했을 때, 각 data block을 어디에 어떤 형태로 placement시킬 것인가?\n\n: machine learning 혹은 pattern mining에 기반한 base workload pattern data를 기반으로 incremental하게 update될 수 있는 data access의 spatial locality와 periodicity\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n\n* 기존 기술과의 대비 (naive I/O prediction vs. full-fledged I/O prediction)\n: 기존 기술은 누적 hit count 정보에 의지하는 naive I/O prediction으로 볼 수 있음. 이러한 방식은 workload의 변화를 제대로 반영하지 못하기 때문에, static한 workload에 대해서는 잘 동작하지만, moving target처럼 dynamic한 workload에 대해서는 오히려 과거의 누적 hit count metric이 현재의 workload 패턴을 왜곡하는 문제점을 가지고 있음. (temporal locality와 spatial locality가 깨지는 순간이 여러번 존재할 수록 기존의 naive I/O prediction 방식으로 성능 향상은 어려워짐)\n\n\n\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== _BPT-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(283,'== _BPT-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== _BPT-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== _BPT-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n[[Bnote patidea 2013-003]]\n\n\n== _BPT-004 ==\n\n=== Event-based I/O Prediction (for Proactive Data Placement) ===\n\n=== 요약 (4) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n=== 기술 상세 (4) ===\n\n* I/O 발생 메커니즘의 모델링\n: I/O prediction을 위해, type-I event (indirect I/O trigger)와 type-II event (direct I/O trigger)를 관찰 및 분석함. Type-I event의 예로서, user의 log-in, 기 log-in되어 있던 user session의 상태 변경 (idle-to-busy), system service에 대한 invocation from local or remote node 등이 포함될 수 있으며, type-II event의 예로서, user application 의 실행, system service의 실행 등이 포함될 수 있음. Type-I event는 type-II event의 발생에 영향을 미치고, type-II event는 I/O request event (data access)의 발생에 영향을 미치므로, \'type-I event\' -> \'type-II event\' -> \'I/O request event\' 까지 이어지는 인과사슬 (cause-effect chain) 기반으로 I/O 발생 메커니즘을 모델링하였음.\n\n\n* Event 정보 수집: type-I event 정보 및 type-II event 정보 정의 및 수집 방법.\n\n\n* I/O 패턴 학습 (Offline Learning): 수집된 Event 정보를 기반으로 I/O 패턴을 learning하는 방법\n\n\n* Run-time I/O 예측 (Online Matching): 시스템 모니터링에서 얻어지는 최근 및 현재의 event들과 learning으로 얻어진 I/O 패턴 DB에 기반하여 Run-time으로 미래 I/O 예측\n\n\n* 시스템 구조: 정보 수집 및 패턴 학습 모듈의 시스템 내 위치 (I/O 패턴 DB 등의 위치 및 실시간 예측 모듈과의 관계)\n\n\n* 순서도: 수집부터 학습까지 전체적인 task의 흐름\n\n\n* 예상 효과: 성능적 측면에서 사용자가 체감하는 lower latency, higher throughput 효과, 이미 발생해버린 congestion time을 피해 미리 적절한 곳에 data를 배치시킴으로써 얻어지는 bottleneck prevention 효과.\n\n=== Data Collection (4) ===\n\n* 역할: I/O 패턴 학습/발굴에 필요한 정보 수집\n\n* 수집 정보 종류\n: 전조 기반 예측에 필요한 간접 I/O 유발자, 직접 I/O 유발자, I/O 발생 이벤트 등 I/O 예측에 도움이 될 수 있는 다양한 정보를 수집. 다음 정보들이 포함될 수 있음.\n:- indirect I/O trigger trace: {(user-id | user-type), status-type, before-status, after-status, event-time}\n:- direct I/O trigger trace: {(program-id | program-type), status-type, before-status, after-status, process-owner, event-time [, process-current-working-directory, files-opened]}\n:- I/O trace: {access-time, address-accessed, read/write-flag, io-size, file-path, process, co-accessed-addresses}\n\n\n* 수집 모듈의 위치\n: storage-server가 분리되어 있는 구조인 SAN/NAS 타입의 architecture인 경우에는 application이 구동되는 server-side에 정보 수집 모듈이 위치한다. storage-server가 결합되어 있는 converged-architecture 기반의 system인 경우에는 해당 system 내에 정보 수집 모듈이 위치한다.\n\n\n* 정보 수집 방식\n: 운영체체가 제공하는 시스템 상태 정보 제공 interface를 이용하거나, blktrace, lsof, debug file system 등 이용 (e.g., /sys/kernel/debug/ 등) 혹은 /proc file system 등을 이용 가능.\n\n== _BPT-005 ==\n\n=== Coaccess-based I/O Prediction (for Proactive Data Placement) ===\n\n* (영문) Coaccess-based I/O Prediction (for Proactive Data Placement)\n* (한글) Coaccess 기반의 I/O 예측 방법 및 시스템 구조\n\n=== 요약 (5) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 coaccess-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n\n\n* 소정의 Confidence 값 이상인 X=>Y 관계를 활용하여, signature X (Y에 대한 precursor)가 access되면 어느 정도 확률로 이후에\n\n=== 발명의 이용 분야 (5) ===\n\n데이터를 저장하고 처리하는 일반적인 컴퓨팅 시스템으로서,\n* 프론트엔드/백엔드 서버\n* 네트워크 스토리지 시스템\n* 컨버지드 서버-스토리지 시스템\n* 엔터프라이즈 혹은 데이터 센터 내의 IT 인프라\n등에서 활용 가능.\n\n\n=== 발명의 목적 (5) ===\n\n==== 기존 기술의 한계점 ====\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: Fast media로서 SSD가 사용되는 경우에 다음과 같은 부정적인 효과가 발생할 수 있음. 과거에는 자주 access되었던 data가 앞으로도 hot할 것으로 판단하고 tiering 시점에 fast media로 placement 시켰는데 tiering되고 난 후에 자주 access되지 않게 되면, 성능 향상 이득 없이 SSD의 wear-out만 초래하게 됨.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n==== Intelligent I/O Prediction의 필요성 ====\n\n\n\n\n=== 기술 상세 (5) ===\n\n* Data의 coaccess 특성에 기반한 proactive data placement 기법\n: 소정의 time window 내에 같이 access되는 확률이 높은 data 그룹 (coaccess group)이 있을 때, 그 그룹 내의 일부 data가 access되면 나머지 data들도 미리 fast media에 가져다 놓음으로써, 성능 향상을 얻을 수 있게 하는 방법임.\n\n\n* 개본 개념 정의 (coaccess property, coaccessness, coaccess group)\n: 지정된 시간 동안의 measurement window 내에서, 어떤 data가 access될 때 함께 access되는 경향을 가지는 다른 data들이 존재할 때 그 data들 간에 coaccess property 혹은 coaccessness가 있다고 정의한다.  이때, coaccess property를 가지는 data 그룹을 coaccess group이라고 한다. frequent item set mining 관점에서 보면 기준이 되는 support 값 이상의 확률로 관측되는 transaction들을 coaccess group으로 정의할 수 있다.\n\n\n* 추가 개념 정의 (merged coaccess group)\n: 서로 다른 coaccess group들을 개별적으로 다루기보다 이들을 묶어서 하나의 coaccess group으로 만들어 다룸으로써 meta data management의 효율성 및 proactive data placement의 효과 측면에서 장점이 발생하는 경우가 있다. 이렇게 만들어지는 coaccess group을 merged coaccess group이라 정의한다. \n: 서로 다른 coaccess group CG_A와 coaccess group CG_B가 있는데, CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합) 시켜서 새롭게 얻어진 CG_C = {CG_A, CG_B}를 하나의 coaccess group으로 다룬다.\n: computing resource의 한계로 인한 현실적인 문제로 인해 모든 경우의 frequent item set을 발굴하기 어려운 경우가 있음 (Frequent item set mining 시 실제로 maximum size of transaction을 한계 값으로 지정). merged/unionized coaccess group은 이러한 한계적인 상황을 극복할 수 있는 방법으로서 제안되는 개념임. (실제로 관측되었던 실험 data 및 graph를 이용하여 실재 근거를 제시할 것)\n\n<!--\nmerged coaccess group 관련해서.\n\n: CG_A = {a1, a2, ..., aM |supp=s1}, CG_B = {b1, b2, ..., bN |supp=s1}, CG_C = {union of CG_A and CG_B |supp=s1} 이 존재하고 발생 시기가 일치하는 것을 확인하였다면, CG_A, CG_B, CG_C 를 모두 개별적인 coaccess group으로 관리할 것이 아니라, CG_C 만을 coaccess group으로 관리한다. (조건 i) supp(CG_A) == supp(CG_B) == supp(CG_C), (조건 ii) union of CG_A and CG_B is identical to CG_C, (조건 iii) occurrence_time(CG_A) == occurrence_time(CG_B) == occurrence_time(CG_C) 이 있다고 하자. 상기 조건 i, ii, iii이 모두 만족되는 경우에는 CG_C 만을 coaccess group으로 관리하고 지정함으로써, coaccess group의 meta data management overhead를 감소시키고 coaccess-based proactive data placement의 operation efficiency를 증대시킴. /* 여기서, (조건 i)과 (조건 ii)가 만족되면 굳이 (조건 iii)까지 확인할 필요는 없을 수도 있음. 확인 필요 */\n* GCD (Greatest Common Divisor) / LCM(Least Common Multiple) / largest coaccess group 개념과 관련.\n\n* 유사 coaccess group 간 merging?\n: 예를 들어 coaccess group CG_A = {\'a\', \'b\', \'c\', \'d\'} 와 coaccess group CG_B = {\'b\', \'c\', \'d\', \'e\'} 가 있다고 했을 때, \'b\', \'c\', \'d\' data 가 겹치기는 하지만, CG_A와 CG_B는 동일한 coaccess group으로 볼 수는 없다. 그러나 만약 CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 (기준이 되는 확률 값은 시스템과 workload의 상황에 따라서 적절히 조절 가능) CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합)화 하여 단일 coaccess group으로 다루는 방식. (말이 되는 소리인지 확인 필요)\n-->\n\n\n* frequent item set mining을 활용한 coaccess group 발굴 방법\n: Coaccess group 발굴을 위해 frequent item set mining을 이용할 수 있으며, 구체적인 방법은 다음과 같다.\n: (1) 시스템의 resource 상황에 따라 적절한 support 값을 설정한다. 예를 들어 fast media (cache 혹은 tier 1 storage)의 저장 가능 용량에 따라, 얼마나 높은 등급의 hot data까지 fast media에 placement 시킬 수 있을지가 결정될 수 있다. 즉 fast media의 용량이 매우 넉넉하여 10번 중 2번의 빈도로 (support=0.2) 관찰되는 coaccess group 까지 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있고, 혹은 fast media의 용량이 여유롭지 않아서 10번 중 5번 이상의 빈도로 (support=0.5) 관찰되는 coaccess group까지만 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있다. support 값은 상황에 따라 dynamic하게 설정될 수 있으며, 시스템 관리자 혹은 data scientist등 사람에 의해 수동적으로 설정될 수도 있지만 algorithm에 의해 automatic하게 설정되도록 구현될 수도 있다.\n: (2) coaccess group에 대한 단위 관측 시간이 되는 transaction measurement window (TMW) 크기를 설정한다. transaction measurement window의 크기에 따라 coaccess group에 포함될 수 있는 data의 갯수가 달라질 수 있다. transaction measurement window 크기가 너무 작으면 함께 access되는 것으로 판단될 대상이 되는 data 갯수 자체가 감소하게 되므로 coaccessness 분석이 어렵게 된다. 반대로, transaction measurement window 크기가 너무 크면 coaccess group의 종류는 감소하고 각 coaccess group의 크기는 커지는 경향이 생기는데, 이 경우의 문제점은 data placement 시 fast media를 효율적으로 사용하는 것이 어려워질 수 있다는 것이다.\n: (3) access되는 data address 정보를 기반으로 frequent item set mining을 시작한다 (이를 위해서는 support 값과 transaction measurement window size 값이 설정되어 있어야 함).  transaction measurement window 동안 access 되는 address들의 집합을 하나의 transaction (basket)으로 묶는다. 전체 관찰 기간 동안 N 번의 transaction measurement window가 있었다면, N 개 만큼의 transaction이 만들어진다. 이 때 N 개의 transaction을 통틀어 support 값 확률 이상으로 빈번하게 등장하는 item set이 있다면 그것을 coaccess group으로 지정하고 coaccess group management ID를 부여, 관리한다.\n: (4) frequent item set mining에서의 confidence 개념을 도입하여 precursor의 존재 감지 시, 미래에 access되게 될 나머지 data들을 신속하게  (일부 데이터가 access되고 나면 나머지 데이터들도 지정된 시간 내에 함께 access될 확률을 의미하게 됨)\n: (4) run-time 시, coaccess group의 빠른 속도로 찾아낼 수 있도록 하기 위해, coaccess group \n\n\n : coaccess group 발굴이 되었다면, 어떻게 저장하고, 어떻게 불러내서(매칭시켜서) proactive data placement에 활용할 수 있게 할 것인가?\n\n\n* O(1)으로, 매칭되는 패턴을 찾을 수 있게 하려면?\n\n\n* proactive data placement를 위해 \n\n\n* Coaccess group pattern matching 방법\n\n\n* Fig{association graph, temporal occurance graph}\n: Frequent item set mining 분석 결과로 얻어진 association graph 및 시간에 따른 frequent item 발생 추이 graph를 이용하여 coaccess pattern의 실재함을 설명\n\n\n* Coaccess property는 다음과 같은 방식으로 활용될 수 있음\n:- 특정 coaccess pattern이 특정 workload에 대한 precursor가 되는 경우가 있을 수 있음. 이를 이용하여 다가올 workload의 prediction에 활용 가능 (예를 들어, coaccess group A가 access된 후 T1 시간 경과 후에 address range B를 random read하는 workload가 발생하고, T2 시간 경과 후에 address range C를 sequential write하는 workload가 오는 경향이 있다는 것을 알고 있다면, 이 정보를 기반으로 proactive하게 address range B를 fast media에 placement 시키고, address range C에 대해서는 caching  \n:- coaccess group 내의 일부 data가 access되면 그 coaccess group 내의 다른 data들을 미리 fast media에 가져다 놓음으로써 성능 향상 가능\n\n<!--\n* (for event-based I/O prediction) coaccess group이 access되는 순서를 event-based I/O prediction에 이용할 수도 있음. 예를 들어, coaccess group A가 access되고 나서 T1 시간 내로 coaccess group B가 access될 확률이 80% 이상이라고 하고, coaccess group B가 access되고 나서 T2 시간 내로 coaccess group C와 coaccess group D가 access될 확률이 90% 이상이라고 한다면, 이러한 정보를 기반으로 ... 무엇을 할 수 있나?\n-->\n\n\n* 이를 위해서 기존의 storage system 구조에 (1) coaccess pattern analysis engine, (2) coacccess pattern aware data placement controller가 추가되는 것이 필요하다.\n\n\n* coaccess pattern analyzer의 역할: workload 별로(???) coaccess pattern list 구축\n\n\n* coaccess pattern analyzer의 구조 및 동작 방식:\n\n\n* coaccess pattern aware data placement controller의 역할:\n\n\n* coaccess pattern aware data placement controller의 구조 및 동작 방식: coaccess pattern 정보를 받아들이기 위한 interface, interface로 받아들인 coaccess pattern 정보의 queuing? data placement의 I/O 단위 (granularity), data placement의 주기 (static? or dynamic?) 등 coaccess pattern 정보를 data placement 시 어떤 식으로 활용하는지\n\n=== Coaccess-based I/O Prediction의 성능 향상 효과 Estimation ===\n\n* 시뮬레이션 기반의 성능 향상 효과 비교\n:- Fast media의 4KB random read latency는 96.47 usec (SSD 830 128GB 기준), slow media의 4KB random read latency는 1,884.03 usec (HDD 7,200rpm 2TB 기준)으로 설정 (iozone에 의한 실측값 사용)\n:- 실험 시작 전에는 4GB 크기의 target data가 slow media (HDD)에 저장되어 있음.\n:- 여기에 1,000,000 개의 4KB random read request로 구성되는 workload를 인가. (각 read request들은 서로 중복되는 data를 가리키지 않으며, 4GB 크기의 target data를 한 번씩 다 request하게 된다고 가정)\n:- Workload 패턴 학습 결과, 4GB의 전체 data 중, \'가\' type의 coaccess 특성을 가지는 10MB 크기의 data 그룹 (data set 당 2,500개의 4KB data 포함)이 200개가 있으며, \'나\' type의 coaccess 특성을 가지는 1MB 크기의 data 그룹 (data set 당 250개의 4KB data 포함)이 1,000개 있는 것을 발견한 것으로 가정.\n:- \'가\' type의 coaccess 패턴은 해당 data 그룹 내 100개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 80%인 것으로 정의.\n:- \'나\' type의 coaccess 패턴을 해당 data 그룹 내 50개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 100%인 것으로 정의.\n\n\n\n\n본 기법을 적용한 경우와 그렇지 않은 경우의 성능 차이를 IOPS로 비교해보면 다음과 같다. \n\n: 예를 들어 설명하면 다음과 같다.\n: data 1, data 2, ..., data 10000으로 구성된 data set이 있으며, 각 data들의 크기는 4KB이다. 학습을 통해서, (1) 이 중 어느 한 data가 access되었을 때, 1분 이내에 나머지 data들도 같이 access되게 될 확률은 55%, (2) 이 중 2개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 80%, (3) 이 중 3개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 95%라는 것을 알고 있다고 가정한다.\n: fast media의 크기는 4GB 이며, 기존에 \n\n이라는 것을 알고 있다면, data 1, 2, 3이 access되었을 때 data 4~128까지도 미리 proactive하게 fast tier에 배치시킴으로써, 그로 인한 성능 이득을 얻을 수 있게 된다. (!!!실제 실험 사례?!!!)\n\n=== Real Data Analysis (MSN File Server) (5) ===\n\n* Analysis Target Workload Specification\n:- MSN FileServer Tracelog (2008-03-10, 약 5GB 데이터)\n:- 21720977703 microseconds (약 6시간)\n:- 29345085 IO operations / 21720 sec => 1351 IOPS\n\n\n* 첫 16.5 minutes (990 seconds) 동안의 I/O에 대해 10 seconds 단위의 Measurement Window로 측정\n\n\n* Frequent Item Sets Mining (Eclat algorithm)\n: support=0.3, minlen=1, maxlen=15\n <pre>\n> fi_s3 <- eclat(ciop_data, parameter=list(supp=0.3, maxlen=15, minlen=1, tidLists=T))\n\nparameter specification:\n tidLists support minlen maxlen            target   ext\n     TRUE     0.3      1     15 frequent itemsets FALSE\n\nalgorithmic control:\n sparse sort verbose\n      7   -2    TRUE\n\neclat - find frequent item sets with the eclat algorithm\nversion 2.6 (2004.08.16)         (c) 2002-2004   Christian Borgelt\ncreate itemset ...\nset transactions ...[1146629 item(s), 100 transaction(s)] done [1.31s].\nsorting and recoding items ... [21 item(s)] done [0.12s].\ncreating bit matrix ... [21 row(s), 100 column(s)] done [0.00s].\nwriting  ... [28761 set(s)] done [0.01s].\nCreating S4 object  ... done [0.25s].\n\n\n\n\n> summary(fi_s3)\nset of 28761 itemsets\n\nmost frequent items:\n  22159360   22155264 1504849920 3170566144 4086710272    (Other)\n     14372      14370      14336      14336      14336     123094\n\nelement (itemset/transaction) length distribution:sizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13\n  21  168  696 1951 3933 5808 6336 5115 3025 1276  364   63    5\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n  1.000   6.000   7.000   6.775   8.000  13.000\n\nsummary of quality measures:\n    support\n Min.   :0.300\n 1st Qu.:0.330\n Median :0.340\n Mean   :0.388\n 3rd Qu.:0.430\n Max.   :0.920\n\nincludes transaction ID lists: TRUE\n\nmining info:\n      data ntransactions support\n ciop_data           100     0.3\n> dim(tidLists(fi_s3))\n[1] 28761   100\n\n</pre>\n\n\n=== Brief Description of the Drawings (5) ===\n\n# [Concept] Coaccess-based I/O Prediction Concepts\n# [Architecture] Storage System Architecture with Coaccess-based Proactive Data Placement\n# [Concept] Coaccessness Analysis using Frequent Itemset Mining\n# [Flowchart] Proactive Data Placement based on Coaccessness\n# [Auxiliary] Experiment Results 1\n# [Auxiliary] Experiment Results 2\n# [Auxiliary] Performance Gain Estimation\n# [Auxiliary] Comparison with prior arts\n\n== Memo for {_BPT-004, _BPT-005} ==\n\n\n=== Think outside the box // Questions ===\n\n* 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n\n* Tiering을 좀 더 다양한 각도에서 바라보자\n:- 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 것이 기존의 tiering 개념임.\n:- SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n:- SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n:- access pattern-aware optimal placement (APOP) - 이런 것은 어떨까?\n:- Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n\n* write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반의 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지?\n\n\n* 만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? SSD-internal memory buffer를 이용해서 SSD를 NVRAM처럼 쓰도록 하는 것이 실제로 말이 되는 이야기일까?\n\n\n\n\n\n=== 기존 기술의 한계점 (Intelligent I/O Prediction 기술의 필요성) ===\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n\n <pre>\n실제 tracelog 사례를 보여주고 hot area가 변화하고 있음을 보일 것.\n</pre>\n\n* Data Placement (Tiering) 관점에서 본 EMC의 FAST, FAST-VP 기술의 한계점\n:- Coarse-grained Data Migration (768KB 까지 작게 나눌 수 있다고는 하지만, 실제로 그렇게 해서 성능이 얼마나 잘 나오는지는 확인 필요. 귀중한 자원인 DRAM을 관리를 위한 metadata로 가득 채우게 될 위험도 있음)\n:- Flexible하지 않은 Tiering 주기\n:- Busy-hour 지난 후 Idle-time에 Tiering 실시\n:- 낮은 지능의 미래 I/O 예측 능력\n:: data access count를 기반으로 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두도록 하는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n\n=== 본 발명의 차별화 포인트 ===\n\n* 본 기술의 특징 1: I/O를 triggering하는 전조 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Indicator는 Run-time에 저비용으로 감지될 수 있는 형태의 정보들로 구성\n:- 미래에 access될 data를 예측케 하는 Indicator간의 mapping은 Bayesian Inference 이용\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n=== 특허 검색/분석 결과 {storage I/O workload prediction} ===\n\n* 검색식: storage I/O workload prediction\n\n* http://portal.uspto.gov/pair/PublicPair 에서 특허 상태 확인 가능.\n\n* I/O prediction을 기반으로 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n:- 기존엔 proactive tiering이 없었나? -> IBM 특허가 나름 유사함. (US7930505, US7512766)\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n==== 1. 비교가 될만한 특허들 ====\n\n예측에 대한 언급이 되어 있는 스토리지 시스템 관련 특허들. 크게 두 가지로서, 그 중의 하나는 가상 volume에 대해 peak I/O가 발생하는 주기를 분석하여, peak I/O 발생시점이 닥치기 전에 미리 가상 volume의 data를 migration 시키는 방법에 대한 것이나, 어느 위치의 data가 언제 access될지에 대한 예측은 아님 (IBM 특허). 다른 하나는 I/O 예측을 기반으로 미래에 자주 access될만한 data를 미리 여러 노드에 분산시키는 방법에 대한 것이나, 예측 방법 자체에 대한 내용은 다루고 있지 않음 (HP 특허)\n\n\n; US7930505, US7512766 (IBM)\n\n* US7930505 (Application Number: US 12/053582), \"Controlling preemptive work balancing in data storage,\" IBM, 특허일 2011-04-19\n: storage network의 I/O 관찰을 통해 peak I/O activity의 repeating instance에 대한 signature를 VLU (virtual logical unit) 단위로 생성하고, 이에 기반하여 언제 heavy I/O가 도래할 것인지를 예측하여, heavy I/O가 오기 전에 미래 data를 faster media로 migration하도록 하는 tiering 기술에 대한 특허. (이미 I/O peak time이 온 후에 hot data를 옮기는 것은 오히려 시스템의 IO성능을 악화시킬 수도 있기 때문에 미리 migration을 하고자 하는 것임)\n: VLU 단위로, peak I/O activity가 어느 주기로 발생할지에 대한 예측임. 지능 수준과 Fine-granularity 측면에서 초보적인 수준임.\n\n* US7512766 (Application Number: US 11/213494), \"Controlling preemptive work balancing in data storage\", IBM, 특허일 2009-03-31\n: 위의 US7930505 특허와 동일 건으로 보임\n\n\n; US6912635 (HP)\n\n* US6912635 (Application Number: US 10/140968), \"Distributing workload evenly across storage media in a storage array\", HP, 특허일 2005-06-28\n: 미래에 자주 access될만한 data를 여러 arrayed storage device에 분산시키겠다는 기술. 그러나 I/O에 대한 예측 방법은 다루어지고 있지 않음.\n\n==== 2. 검색은 되었으나 관련은 없는 특허들 ====\n\n무언가를 예측한다는 측면, Storage System에 대한 기술이라는 측면에서는 관련이 있으나, 미래에 Access될 Data를 예측한다는 측면에서 제안 기술과 관련 없는 특허들\n\n\n; US7640231 (IBM)\n* US7640231 (Application Number: 11/280012), \"Approach based on self-evolving models for performance guarantees in a shared storage system\", IBM, 특허일 2009-12-29\n: network 기반 storage system에서, average latency 혹은 I/O request rate 등에 대한 주기적인 performance sampling을 기반으로, 주어진 objective function의 값을 최대화할 수 있도록 system resource를 제어할 수 있게 하는 기술.\n\n\n; US7774491, US7519725 (IBM)\n\n* US7774491 (Application Number: 12/359377), \"Utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2010-08-10\n: 복수의 클라이언트들을 대상으로 통계적인 성능 QoS를 보장하기 위한 informed throttling 방법\n: A system for utilizing informed throttling to guarantee quality of service to a plurality of clients\n\n* US7519725 (Application Number: US 10/444712), \"System and method for utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2009-04-14\n: resolve the problem of providing statistical performance guarantees for applications generating streams of read/write accesses (I/Os) on a shared, potentially distributed storage system of finite resources, by initiating throttling whenever an I/O stream is receiving insufficient resources\n\n\n; US7827283 (IBM)\n* US7827283 (Application Number: 10/367444), \"System for managing and controlling storage access requirements\", IBM, 2010-11-02\n: SAN 등의 환경에서, storage bandwidth를 관리하고 reserve하는 기술\n: A Resource Management and Reservation System (RMRS) for managing and reserving storage bandwidth, is a platform independent middleware layer that provides an interface to applications, their database management systems, or some other higher level data management systems like ADRS which does data management on behalf of the applications\n\n\n; US8433554, US8140319 (IBM)\n\n* US8433554, \"Predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2013-04-30\n: An approach for predicting performance and capacity of an information technology (IT) system before the IT system is built is described, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n* US8140319, \"Method and system for predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2012-03-20\n: A method and system for predicting performance and capacity of an information technology (IT) system before the IT system is built, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n\n\n; US8255915 (HP)\n* US8255915, \"Workload management for computer system with container hierarchy and workload-group policies\" HP, 특허일 2012-08-28\n\n\n; US7050956 (HP)\n* US7050956 (Application Number: US 09/843930), \"Method and apparatus for morphological modeling of complex systems to predict performance\", HP, 특허일 2006-05-23\n: I/O 성능 예측을 위한 complex storage system에서의 data flow 모델링 방법\n\n\n; US6718434 (HP)\n* US6718434 (Application Number: US 09/871043), \"Method and apparatus for assigning raid levels\", HP, 특허일 2004-04-06\n: Data를 disk array에 load하기 전에, workload 명세 및 device 명세에 기반하여, 적합한 RAID level을 판단하여 disk array에 할당 (RAID build)하는 방법\n\n\n; US8112586 (EMC)\n* US8112586 (Application Number: US 12/190071), \"Predicting and optimizing I/O performance characteristics in a multi-level caching system\", EMC, 특허일 2012-02-07\n: multi-level caching 시스템에서, cost대비 performance를 최적화할 수 있도록, workload에 따른 resource allocation design에 대한 recommendation 기술 (multi-level cache들 간의 workload propagation을 예측하는 것이 핵심)\n\n\n; US6629266 (IBM)\n* US6629266 (Application Number: US 09/442001), \"Method and system for transparent symptom-based selective software rejuvenation\", IBM, 특허일 2003-09-30\n: software가 언제 동작이 멈추는지를 예측함으로써, software의 dependability를 향상할 수 있게 하는 방법\n\n\n; US5088058 (Unisys)\n* US5088058 (Application Number: 07/237120), \"Apparatus and method for evaluating and predicting computer I/O performance using I/O workload snapshots for model input\", Unisys Corporation, 특허일 1992-02-11\n: I/O delay 등에 대한 확률적 모형을 기반으로 disk 시스템의 성능을 검증하고 예측하는 시뮬레이션 기반의 방법\n: A method of evaluating and predicting the performance of an I/O disk system comprised of one or more disk controllers and cooperating disk units using a simulation model containing a stact constructed such that delays are formulated as sets of stochastic processes\n\n=== Comparison of I/O Prediction Approaches ===\n\n미래에 access될 data의 위치와 시기를 예측하기 위한 방법으로 event-based I/O prediction, coaccess-based I/O prediction, periodicity-based I/O prediction 방식이 제안되며, 각각의 방식은 예측에 필요한 연산량 및 input data의 richness, 그리고 workload 변화에 대한 적응성 측면에서 장단점을 가지고 있음. 따라서, 상황에 따라서 적절한 방식을 이용하면 됨.\n\n{| style=\"border-collapse: collapse; border: 1px solid #000\" border=1\n| style=\"width: 25%\" | 비교 항목\n| style=\"width: 25%\" | periodicity-based I/O prediction\n| style=\"width: 25%\" | coaccess-based I/O prediction\n| style=\"width: 25%\" | event-based I/O prediction \n|-\n| context richness required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| computation overhead required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| adaptability to workload changes (lower is better)\n| high\n| high\n| high\n|-\n|}\n\n\n* 전조 기반 예측 vs. 주기성 기반 예측\n{| border=1\n| 비교 항목\n| 전조 기반 예측\n| 주기성 기반 예측\n|-\n| 기술 요약\n| 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식\n| address 별 access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식\n|-\n| 분석 오버헤드\n| High (학습/마이닝에 드는 비용임)\n| Low (주기성 파악 비용임)\n|-\n| Run-time 오버헤드\n| Mid (실시간 전조 모니터링 및 매칭 비용)\n| Low (주기성 계산)\n|-\n| Workload 변화 적응성\n| High\n| High (Workload 변경 사실 알림 hint 필요)\n|-\n| ...\n| ...\n| ...\n|-\n| Incrementality of Mining/Learning\n| ...\n| ...\n|}\n\n* 전조 기반 예측은, 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식임.  주기성 기반 예측은, data access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식임.\n\n\n=== Intelligent Data Placement ===\n\n* Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case\n: 여기에 HML이 사용될 수 있는 것이라면, 어떤 방식으로 어떤 이유로 그런 것일까?\n\n\n* I/O 예측 방식으로서 다음 방식이 제안되며, 각 방식에 따라서 prediction model, 수집되어야 하는 context data, I/O 패턴 DB에 저장되는 정보의 종류 및 형태가 달라질 수 있음. 예측에 이용할 수 있는 context 정보의 richness에 따라 적절한 방식을 사용할 수 있다. (서로 다른 발명이므로, 별도의 특허로 낸다)\n:- event-based I/O prediction\n:- coaccess-based I/O prediction\n:: 여기서, periodicity-based I/O prediction은 기존 특허에서 언급되었으므로, 본 발명에서는 크게 언급하지 않기로 한다.\n\n\n* I/O prediction 결과에 따라 적절한 위치에 data를 배치하게 되는데, 여기서 적절한 위치라 함은 SSD와 같은 fast tier가 될 수도 있겠으나, sequential read/write 같은 경우 striping된 HDD array가 될 수도 있고, 혹은 SAS HDD가 될 수도 있겠음. 또한 SSD 중에서도 SLC, MLC, TLC 로 구분되는 fine-grained fast tier로 구분될 수도 있음\n\n\n* 미래에 오게 될 I/O의 type도 알 수 있을까? 즉, {random/sequential}x{read/write} type 중 어느 것에 해당되는지를 안다면, 그리고 I/O의 intensiveness도 예상할 수 있다면 그에 맞춰서 placement 시 적절한 tier 혹은 적절한 node로 찾아가게 할 수 있을 것임.\n\n\n* context 정보는, 수집되는 위치에 따라, server-side에서 수집되는 context data와 storage-side에서 수집되는 context data로 분류될 수 있음. Event context data는 server-side에서 수집 가능하며, coaccess context data 및 periodicity context data는 storage-side에서 수집될 수 있음.\n\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n: 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n\n=== Design/Implementation Issues ===\n\n* workload type/class tagging 필요\n\n* cache와 tiering을 같이 쓸 때, DRAM cache가 아니라 Flash Cache의 경우 쓸데없이 모든 data가 Flash Cache에 다 들어가게 두기 보다는, tiering에서 cover하지 못하는 것들 위주로 caching하도록 하면 Flash Cache의 공간 낭비를 줄임으로써, 유효 cache 공간을 늘일 수 있게 되고, 그만큼의 성능 향상으로 연결될 수 있음.\n\n* DRAM 기반의 write cache를 적극 이용하여 마치 log-structured file system에서 제공하는 I/O merging and sequentialization 효과를 얻을 수 있음.\n\n* Tiering되는 data는 caching되는 data보다 상대적으로 coarse-grained하게 grouping하여 다룸으로써, 한꺼번에 많은 data를 move할 수 있도록 하고, meta data 관리에 필요한 overhead를 줄일 수 있게 한다. 당장 access되지 않더라도 일정 시간 내에 같이 access될 수 있는 data들은 함께 faster tier로 옮겨진다. 이때 필요한 정보가 coaccess 정보임.\n\n* Cache - Tiering 기법의 연동 방법: 예측이 실패했거나, target tier media의 가용 공간이 부족하여 tiering에서 놓치는 missing data가 있을 수 있음. 이러한 missing data들이 temporal locality가 있는 경우에는 당장은 faster tier에서 serve하지 못하더라도 cache에서 serve될 수 있는데, temporal locality가 적은 경우에는 cache만으로 cover되지 않는다. \n\n* Management table size optimization issue - 만약 모든 LBA 별로 event-based I/O prediction을 해야 한다면 현실적으로 사용 가능한 아키텍쳐가 아닐 수 있음. 만약 spatial locality가 있어서 LBA 1,000개 단위로 묶어서 대응할 수 있다면, 최소 1/1000 의 management table size 감소 효과를 얻을 수 있음. pattern mining 혹은 machine learning 실행 시에는 record 수가 감소함으로써 얻는 이득은 1,000배 이상이 될 수 있음. 관건은 얼마나 LBA 묶음을 크게 만들어서 예측에 사용할 수 있는지임.\n\n\n=== I/O Insight ===\n\n* ML 기반의 I/O Insight 발굴 (Data Placement를 위한 Macroscopic Guideline)\n: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:-> 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n\n* I/O Insight의 요건:\n:# indication, as simple as possible\n:# indication, as specific as possible\n:# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n:# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n\n* I/O Insight의 형태\n: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n=== X-Y Modeling ===\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n=== 예측 실패 대비 (별도 특허화 가능) ===\n\n* I/O 예측이 실패할 수 있으며, 이로 인한 성능 penalty를 최소화하기 위한 방법 필요.\n:- fast tier에 여유공간이 있다면 1타겟과 2타겟을 모두 올려놓아도 되지 않을까? 1타겟과 2타겟을 placement함으로써 예측 정확도를 95%까지 올릴 수 있다면, 꽤 쓸만할 수 있겠음. 그리고 1타겟과 2타겟의 모든 데이터를 다 fast tier에 올려두지 말고 일부만 fast tier에 올려두었다가, 둘 중에 하나의 타겟이 hit되었을 때, hit된 타겟의 나머지 data를 재빨리 fast tier에 올리도록 하는 incremental 방법을 사용할 수 있겠음.\n:- 그러나 이것 또한 예측 실패에 대한 완벽한 prevention이 될 수는 없다. 따라서 예측 실패가 발생하더라도 재빨리 그 성능 penalty를 mitigation할 수 있는 방법이 필요. \n:- cache를 이용하여 mitigation할 수 있을까?\n\n=== Data Read/Write 관련 Issues ===\n\n* read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n\n* 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n\n* write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간에, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면?\n\n* 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있음.\n\n* 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간이 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 scatter했던 I/O들을 나중에 다시 읽어들이려 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 원하는 고성능을 낼 수 있게 된다.\n\n* replica-based dynamic clustering\n: 위와 같은 write-and-then-read-again 상황에서, 해당 data를 저장했었던 HDD의 I/O busy/idle 여부에 상관 없이, 최고의 효과를 낼 수 있게 하기 위해, replica management를 적극적으로 활용하는 것은 어떨까? HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까?\n\n* data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 논의가 아님. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 논의임.\n\n* 그러나, 조심해야 할 것은 위의 논의는 상당히 case-dependency를 상당히 많이 탈 수도 있다는 것임. 책상 위의 아이디어로 끝날 수도 있기 때문에 조심스럽게 접근하는 것이 필요함.\n\n\n\n\n<br/>\n\n== _BPT-006 ==\n\n\n=== Data Lifetime (Valid-to-invalid) Interval 분석에 기반한 Data Placement (Tiering) ===\n\n\n* Data 별로, 혹은 LBA 별로, 혹은 Erase Block 별로? Data의 Valid to Invalid Time (Duration) 패턴을 분석할 수 있을까?\n: 이를 SSD 내에서 Low Overhead로 수행할 수 있나?\n: 혹은, 굳이 SSD 내에서 수행할 필요가 있나?\n: Host side에서 LBA 별로 분석 될 수 있다면, 이 정보를 기반으로 Tiering 시에 활용 할 수 있을 것임\n: 특정 Address에 대해서 W이 되고 나서, 다시 W이 되기까지의 시간을 측정하여 ARM 하면 될 듯. 그 두 W 간에 얼마나 많은 R들이 오는지에 대해서도 고려하여 placement 시 고려할 수 있을 것임.\n\nMeasurement Window 내에서 \'W 간의 Interval이 길다\'는 식으로 인식하는 것과, \'W의 횟수가 잦다\'고 인식하는 것 간에 어떤 차이가 있을 수 있을까?\n\n <pre>\n    01234567890123456789\na1: W-R-W-R-W-R-W-W-R--W : T20W06R04 : Short-Lifetime, Many-Reads : SLC\na2: W---W-R-W---W-W----W : T20W06R01 : Short-Lifetime, Few-Reads : SLC, DRAM?\na3: W-RRRR-W---RR-W-R-RW : T20W04R08 : Mid-Lifetime, Many-Reads : MLC\na4: W------W--R---W----W : T20W04R01 : Mid-Lifetime, Few-Reads : MLC\na5: W-R-R-R-RR-RRRR----R : T20W01R10 : Long-Lifetime, Many-Reads : MLC\na6: W------------R----R- : T20W01R02 : Long-Lifetime, Few-Reads : TLC\n</pre>\n\n=== Idle-time Prediction Based Free Space Management in SSD ===\n\n* Idle-time의 도래 시기와 지속 시간 패턴 분석에 기반한 SSD 내의 Free Space 확보 방법\n:- SSD 내에서 Low overhead (in computation, memory space)로 Idle-time 도래 시기 및 지속 시간 분석이 가능한가?\n:- 가능하다면, SSD 내에서 Garbage Collection 등으로 Free Space를 확보하는 데 있어서 어떤 장점을 가질 수 있는가?\n\n\n\n: 이를 기반으로 \n\n\n\n=== 요약 ===\n* \n\n\n=== 기술 상세 ===\n\n*\n\n\n\n\n\n== _BPT-007 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== _BPT-008 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== _BPT-009 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Ef?cient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== _BPT-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== _BPT-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== _BPT-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== _BPT-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== _BPT-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== _BPT-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== _BPT-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== _BPT-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n----\n=== (SmartSSD API) SSD-internal I/O pattern logging interface and mechanism ===\n\n\n==== Questions ====\n\n* SmartSSD에서 제공할 수 있는 logging 서비스로서 어떤 것들이 있을 수 있나?\n:- 어느? 정도의 free space를 필요로하는 critical (heavy) I/O가 어떤? 주기로 도래할 지 알 수 있을까?\n:- I/O prediction에 도움될 수 있는 정보를 SmartSSD가 기록했다가 필요한 때 (안전한 방법으로?) 줄 수 있을까?\n:- I/O prediction까지는 아니더라도 해당 SSD의 I/O wellness(어떻게 정의?)를 측정했다가 bottleneck을 회피하는 데 유용한 정보를 제공할 수 있을까?\n:- 어떤 SSD가 현재 주어지고 있는 I/O workload에 대해 list performance spec을 만족하는데 어려움을 겪고 있다는 것을 알려줄 수 있는 정보가 어떤 것이 있을까? (SSD 내 write buffer의 fullness 혹은 I/O queue의 유동성? amount of free space in over-provisioning area?) (-> I/O wellness 라는 metric을 이것과 연관시켜 정의할 수도 있으며, 결국 SLA 혹은 Performance QoS와 연관시킬 수 있음)\n:- 지금까지의 S.M.A.R.T.와 비교하였을 때, 어떤 차별점이 있는가? 기존의 S.M.A.R.T.가 제공하지 못하던 타입의 정보를 제공하고, 이로 인해 기존에는 불가능했던 새로운 알고리즘 구현 혹은 새로운 서비스 제공이 가능해짐을 보이면 좋겠음.\n\n\n\n:- workload의 history?\n:- 각 erase block이 erase되어야만 했던 주기?\n:- free space의 추이 (얼마나 많이 남아 돌던지)?\n:- I/O의 heaviness 패턴?\n:- SSD내 write buffer (혹은 write cache)의 utilization정보? (즉, write buffer가 넘칠 정도가 되어서 I/O wait을 해야만 했던 상황이 얼마나 자주 발생했는지, write buffer의 가득찬 정도를 백분율로 표현한다고 했을 때, 평균/표준편차 등으로 대표할 수 있는 normal distribution을 따르는지?\n:- 전체 sequential read/write \n\n\n==== 배경 ====\n\n* 각 SSD 모델별로, 또 동일한 모델의 SSD라 하더라도 어떤 workload에 노출되어왔는지에 따라, 현재 낼 수 있는 I/O performance가 동일하지 않을 수 있다 (!check! 관련 실험 결과 - SSD 830을 이용하여 같은 시간 동안 하나의 host에서 생성되는 workload들을 나누어 받았다 하더라도 내부적인 free block의 갯수 및 random write에 의한 block 내 파편화등의 상태가 상이하여 성능에 차이가 나는 것을 보여줄 것. uFLIP 혹은 filebench 활용 가능). 한 예로, SSD가 어느 호스트의 어느 file system에 매핑되었는지에 따라 그 SSD가 받는 workload의 특성이 다를 수 있다. file system mount point가 달라지거나, 기존과 다른 새로운 응용이 설치/서비스되는 상황이 되면 \n\n\n----\n=== I/O Prediction-based Proactive Data Placement ===\n\n* 미래에 access될 data를 예측하여 적소에 미리 가져다 놓는 기술로서, (1) I/O prediction을 위한 정보 수집 모듈, (2) 주어진 정보들에 기반하여 다가올 I/O를 예측하는 모듈, (3) data placement를 수행하는 모듈로 구성됨.\n\n\n* 본 기술의 특징 1: I/O를 triggering하는 indicator (전조) 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Run-time에 저비용으로 감지될 수 있는 Indicator 정보와, 이에 의해 access될 data 간의 mapping은 Bayesian Inference 이용\n\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n\n----\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n----\n\n=== Tiering Method Avoiding I/O Bottleneck ===\n\n* I/O Prediction에 의해 예측된 미래의 Hot Data를 Proactive하게 Placement한다고 할 때, 예상되는 access pattern으로 인해 발생 가능한 I/O Bottleneck을 최소화할 수 있도록 배치하는 방법\n\n* 예를 들어 미래의 hot data block 1 (200MB), hot data block 2 (100MB), hot data block 3 (400MB)이 1분 후에 access 될 것으로 예측되었다고 가정. data block 1에 대해서는 4KB 단위로 random read가 dominant한 access pattern이 예상되고, data block 2에 대해서는 1024KB 단위의 sequential read가 dominant한 access pattern이 예상되고, data block 3에 대해서는 4KB 단위의 random read와 512KB 단위의 sequential write이 각각 80:20의 비율로 mix된 형태의 access pattern이 예상된다고 했을 때, 각 data block을 어디에 어떤 형태로 placement시킬 것인가?\n\n: machine learning 혹은 pattern mining에 기반한 base workload pattern data를 기반으로 incremental하게 update될 수 있는 data access의 spatial locality와 periodicity\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n\n* 기존 기술과의 대비 (naive I/O prediction vs. full-fledged I/O prediction)\n: 기존 기술은 누적 hit count 정보에 의지하는 naive I/O prediction으로 볼 수 있음. 이러한 방식은 workload의 변화를 제대로 반영하지 못하기 때문에, static한 workload에 대해서는 잘 동작하지만, moving target처럼 dynamic한 workload에 대해서는 오히려 과거의 누적 hit count metric이 현재의 workload 패턴을 왜곡하는 문제점을 가지고 있음. (temporal locality와 spatial locality가 깨지는 순간이 여러번 존재할 수록 기존의 naive I/O prediction 방식으로 성능 향상은 어려워짐)\n\n\n\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== _BPT-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(284,'== _BPT-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== _BPT-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== _BPT-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n[[Bnote patidea 2013-003]]\n\n\n== _BPT-004 ==\n\n=== Event-based I/O Prediction (for Proactive Data Placement) ===\n\n=== 요약 (4) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n=== 기술 상세 (4) ===\n\n* I/O 발생 메커니즘의 모델링\n: I/O prediction을 위해, type-I event (indirect I/O trigger)와 type-II event (direct I/O trigger)를 관찰 및 분석함. Type-I event의 예로서, user의 log-in, 기 log-in되어 있던 user session의 상태 변경 (idle-to-busy), system service에 대한 invocation from local or remote node 등이 포함될 수 있으며, type-II event의 예로서, user application 의 실행, system service의 실행 등이 포함될 수 있음. Type-I event는 type-II event의 발생에 영향을 미치고, type-II event는 I/O request event (data access)의 발생에 영향을 미치므로, \'type-I event\' -> \'type-II event\' -> \'I/O request event\' 까지 이어지는 인과사슬 (cause-effect chain) 기반으로 I/O 발생 메커니즘을 모델링하였음.\n\n\n* Event 정보 수집: type-I event 정보 및 type-II event 정보 정의 및 수집 방법.\n\n\n* I/O 패턴 학습 (Offline Learning): 수집된 Event 정보를 기반으로 I/O 패턴을 learning하는 방법\n\n\n* Run-time I/O 예측 (Online Matching): 시스템 모니터링에서 얻어지는 최근 및 현재의 event들과 learning으로 얻어진 I/O 패턴 DB에 기반하여 Run-time으로 미래 I/O 예측\n\n\n* 시스템 구조: 정보 수집 및 패턴 학습 모듈의 시스템 내 위치 (I/O 패턴 DB 등의 위치 및 실시간 예측 모듈과의 관계)\n\n\n* 순서도: 수집부터 학습까지 전체적인 task의 흐름\n\n\n* 예상 효과: 성능적 측면에서 사용자가 체감하는 lower latency, higher throughput 효과, 이미 발생해버린 congestion time을 피해 미리 적절한 곳에 data를 배치시킴으로써 얻어지는 bottleneck prevention 효과.\n\n=== Data Collection (4) ===\n\n* 역할: I/O 패턴 학습/발굴에 필요한 정보 수집\n\n* 수집 정보 종류\n: 전조 기반 예측에 필요한 간접 I/O 유발자, 직접 I/O 유발자, I/O 발생 이벤트 등 I/O 예측에 도움이 될 수 있는 다양한 정보를 수집. 다음 정보들이 포함될 수 있음.\n:- indirect I/O trigger trace: {(user-id | user-type), status-type, before-status, after-status, event-time}\n:- direct I/O trigger trace: {(program-id | program-type), status-type, before-status, after-status, process-owner, event-time [, process-current-working-directory, files-opened]}\n:- I/O trace: {access-time, address-accessed, read/write-flag, io-size, file-path, process, co-accessed-addresses}\n\n\n* 수집 모듈의 위치\n: storage-server가 분리되어 있는 구조인 SAN/NAS 타입의 architecture인 경우에는 application이 구동되는 server-side에 정보 수집 모듈이 위치한다. storage-server가 결합되어 있는 converged-architecture 기반의 system인 경우에는 해당 system 내에 정보 수집 모듈이 위치한다.\n\n\n* 정보 수집 방식\n: 운영체체가 제공하는 시스템 상태 정보 제공 interface를 이용하거나, blktrace, lsof, debug file system 등 이용 (e.g., /sys/kernel/debug/ 등) 혹은 /proc file system 등을 이용 가능.\n\n== _BPT-005 ==\n\n=== Coaccess-based I/O Prediction (for Proactive Data Placement) ===\n\n* (영문) Coaccess-based I/O Prediction (for Proactive Data Placement)\n* (한글) Coaccess 기반의 I/O 예측 방법 및 시스템 구조\n\n=== 요약 (5) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 coaccess-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n\n\n* 소정의 Confidence 값 이상인 X=>Y 관계를 활용하여, signature X (Y에 대한 precursor)가 access되면 어느 정도 확률로 이후에\n\n=== 발명의 이용 분야 (5) ===\n\n데이터를 저장하고 처리하는 일반적인 컴퓨팅 시스템으로서,\n* 프론트엔드/백엔드 서버\n* 네트워크 스토리지 시스템\n* 컨버지드 서버-스토리지 시스템\n* 엔터프라이즈 혹은 데이터 센터 내의 IT 인프라\n등에서 활용 가능.\n\n\n=== 발명의 목적 (5) ===\n\n==== 기존 기술의 한계점 ====\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: Fast media로서 SSD가 사용되는 경우에 다음과 같은 부정적인 효과가 발생할 수 있음. 과거에는 자주 access되었던 data가 앞으로도 hot할 것으로 판단하고 tiering 시점에 fast media로 placement 시켰는데 tiering되고 난 후에 자주 access되지 않게 되면, 성능 향상 이득 없이 SSD의 wear-out만 초래하게 됨.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n\n* 본 발명은 address 정보 기반의 I/O prediction 기술로서, client-side의 rich information을 확보가 제한적인 경우에도 periodicity 및 coaccessness 특성을 기반으로 미래의 I/O를 예측할 수 있도록 하는 기술임.\n\n=== 기술 상세 (5) ===\n\n* Data의 coaccess 특성에 기반한 proactive data placement 기법\n: 소정의 time window 내에 같이 access되는 확률이 높은 data 그룹 (coaccess group)이 있을 때, 그 그룹 내의 일부 data가 access되면 나머지 data들도 미리 fast media에 가져다 놓음으로써, 성능 향상을 얻을 수 있게 하는 방법임.\n\n\n* 개본 개념 정의 (coaccess property, coaccessness, coaccess group)\n: 지정된 시간 동안의 measurement window 내에서, 어떤 data가 access될 때 함께 access되는 경향을 가지는 다른 data들이 존재할 때 그 data들 간에 coaccess property 혹은 coaccessness가 있다고 정의한다.  이때, coaccess property를 가지는 data 그룹을 coaccess group이라고 한다. frequent item set mining 관점에서 보면 기준이 되는 support 값 이상의 확률로 관측되는 transaction들을 coaccess group으로 정의할 수 있다.\n\n\n* 추가 개념 정의 (merged coaccess group)\n: 서로 다른 coaccess group들을 개별적으로 다루기보다 이들을 묶어서 하나의 coaccess group으로 만들어 다룸으로써 meta data management의 효율성 및 proactive data placement의 효과 측면에서 장점이 발생하는 경우가 있다. 이렇게 만들어지는 coaccess group을 merged coaccess group이라 정의한다. \n: 서로 다른 coaccess group CG_A와 coaccess group CG_B가 있는데, CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합) 시켜서 새롭게 얻어진 CG_C = {CG_A, CG_B}를 하나의 coaccess group으로 다룬다.\n: computing resource의 한계로 인한 현실적인 문제로 인해 모든 경우의 frequent item set을 발굴하기 어려운 경우가 있음 (Frequent item set mining 시 실제로 maximum size of transaction을 한계 값으로 지정). merged/unionized coaccess group은 이러한 한계적인 상황을 극복할 수 있는 방법으로서 제안되는 개념임. (실제로 관측되었던 실험 data 및 graph를 이용하여 실재 근거를 제시할 것)\n\n<!--\nmerged coaccess group 관련해서.\n\n: CG_A = {a1, a2, ..., aM |supp=s1}, CG_B = {b1, b2, ..., bN |supp=s1}, CG_C = {union of CG_A and CG_B |supp=s1} 이 존재하고 발생 시기가 일치하는 것을 확인하였다면, CG_A, CG_B, CG_C 를 모두 개별적인 coaccess group으로 관리할 것이 아니라, CG_C 만을 coaccess group으로 관리한다. (조건 i) supp(CG_A) == supp(CG_B) == supp(CG_C), (조건 ii) union of CG_A and CG_B is identical to CG_C, (조건 iii) occurrence_time(CG_A) == occurrence_time(CG_B) == occurrence_time(CG_C) 이 있다고 하자. 상기 조건 i, ii, iii이 모두 만족되는 경우에는 CG_C 만을 coaccess group으로 관리하고 지정함으로써, coaccess group의 meta data management overhead를 감소시키고 coaccess-based proactive data placement의 operation efficiency를 증대시킴. /* 여기서, (조건 i)과 (조건 ii)가 만족되면 굳이 (조건 iii)까지 확인할 필요는 없을 수도 있음. 확인 필요 */\n* GCD (Greatest Common Divisor) / LCM(Least Common Multiple) / largest coaccess group 개념과 관련.\n\n* 유사 coaccess group 간 merging?\n: 예를 들어 coaccess group CG_A = {\'a\', \'b\', \'c\', \'d\'} 와 coaccess group CG_B = {\'b\', \'c\', \'d\', \'e\'} 가 있다고 했을 때, \'b\', \'c\', \'d\' data 가 겹치기는 하지만, CG_A와 CG_B는 동일한 coaccess group으로 볼 수는 없다. 그러나 만약 CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 (기준이 되는 확률 값은 시스템과 workload의 상황에 따라서 적절히 조절 가능) CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합)화 하여 단일 coaccess group으로 다루는 방식. (말이 되는 소리인지 확인 필요)\n-->\n\n\n* frequent item set mining을 활용한 coaccess group 발굴 방법\n: Coaccess group 발굴을 위해 frequent item set mining을 이용할 수 있으며, 구체적인 방법은 다음과 같다.\n: (1) 시스템의 resource 상황에 따라 적절한 support 값을 설정한다. 예를 들어 fast media (cache 혹은 tier 1 storage)의 저장 가능 용량에 따라, 얼마나 높은 등급의 hot data까지 fast media에 placement 시킬 수 있을지가 결정될 수 있다. 즉 fast media의 용량이 매우 넉넉하여 10번 중 2번의 빈도로 (support=0.2) 관찰되는 coaccess group 까지 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있고, 혹은 fast media의 용량이 여유롭지 않아서 10번 중 5번 이상의 빈도로 (support=0.5) 관찰되는 coaccess group까지만 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있다. support 값은 상황에 따라 dynamic하게 설정될 수 있으며, 시스템 관리자 혹은 data scientist등 사람에 의해 수동적으로 설정될 수도 있지만 algorithm에 의해 automatic하게 설정되도록 구현될 수도 있다.\n: (2) coaccess group에 대한 단위 관측 시간이 되는 transaction measurement window (TMW) 크기를 설정한다. transaction measurement window의 크기에 따라 coaccess group에 포함될 수 있는 data의 갯수가 달라질 수 있다. transaction measurement window 크기가 너무 작으면 함께 access되는 것으로 판단될 대상이 되는 data 갯수 자체가 감소하게 되므로 coaccessness 분석이 어렵게 된다. 반대로, transaction measurement window 크기가 너무 크면 coaccess group의 종류는 감소하고 각 coaccess group의 크기는 커지는 경향이 생기는데, 이 경우의 문제점은 data placement 시 fast media를 효율적으로 사용하는 것이 어려워질 수 있다는 것이다.\n: (3) access되는 data address 정보를 기반으로 frequent item set mining을 시작한다 (이를 위해서는 support 값과 transaction measurement window size 값이 설정되어 있어야 함).  transaction measurement window 동안 access 되는 address들의 집합을 하나의 transaction (basket)으로 묶는다. 전체 관찰 기간 동안 N 번의 transaction measurement window가 있었다면, N 개 만큼의 transaction이 만들어진다. 이 때 N 개의 transaction을 통틀어 support 값 확률 이상으로 빈번하게 등장하는 item set이 있다면 그것을 coaccess group으로 지정하고 coaccess group management ID를 부여, 관리한다.\n: (4) frequent item set mining에서의 confidence 개념을 도입하여 precursor의 존재 감지 시, 미래에 access되게 될 나머지 data들을 신속하게  (일부 데이터가 access되고 나면 나머지 데이터들도 지정된 시간 내에 함께 access될 확률을 의미하게 됨)\n: (4) run-time 시, coaccess group의 빠른 속도로 찾아낼 수 있도록 하기 위해, coaccess group \n\n\n : coaccess group 발굴이 되었다면, 어떻게 저장하고, 어떻게 불러내서(매칭시켜서) proactive data placement에 활용할 수 있게 할 것인가?\n\n\n* O(1)으로, 매칭되는 패턴을 찾을 수 있게 하려면?\n\n\n* proactive data placement를 위해 \n\n\n* Coaccess group pattern matching 방법\n\n\n* Fig{association graph, temporal occurance graph}\n: Frequent item set mining 분석 결과로 얻어진 association graph 및 시간에 따른 frequent item 발생 추이 graph를 이용하여 coaccess pattern의 실재함을 설명\n\n\n* Coaccess property는 다음과 같은 방식으로 활용될 수 있음\n:- 특정 coaccess pattern이 특정 workload에 대한 precursor가 되는 경우가 있을 수 있음. 이를 이용하여 다가올 workload의 prediction에 활용 가능 (예를 들어, coaccess group A가 access된 후 T1 시간 경과 후에 address range B를 random read하는 workload가 발생하고, T2 시간 경과 후에 address range C를 sequential write하는 workload가 오는 경향이 있다는 것을 알고 있다면, 이 정보를 기반으로 proactive하게 address range B를 fast media에 placement 시키고, address range C에 대해서는 caching  \n:- coaccess group 내의 일부 data가 access되면 그 coaccess group 내의 다른 data들을 미리 fast media에 가져다 놓음으로써 성능 향상 가능\n\n<!--\n* (for event-based I/O prediction) coaccess group이 access되는 순서를 event-based I/O prediction에 이용할 수도 있음. 예를 들어, coaccess group A가 access되고 나서 T1 시간 내로 coaccess group B가 access될 확률이 80% 이상이라고 하고, coaccess group B가 access되고 나서 T2 시간 내로 coaccess group C와 coaccess group D가 access될 확률이 90% 이상이라고 한다면, 이러한 정보를 기반으로 ... 무엇을 할 수 있나?\n-->\n\n\n* 이를 위해서 기존의 storage system 구조에 (1) coaccess pattern analysis engine, (2) coacccess pattern aware data placement controller가 추가되는 것이 필요하다.\n\n\n* coaccess pattern analyzer의 역할: workload 별로(???) coaccess pattern list 구축\n\n\n* coaccess pattern analyzer의 구조 및 동작 방식:\n\n\n* coaccess pattern aware data placement controller의 역할:\n\n\n* coaccess pattern aware data placement controller의 구조 및 동작 방식: coaccess pattern 정보를 받아들이기 위한 interface, interface로 받아들인 coaccess pattern 정보의 queuing? data placement의 I/O 단위 (granularity), data placement의 주기 (static? or dynamic?) 등 coaccess pattern 정보를 data placement 시 어떤 식으로 활용하는지\n\n=== Coaccess-based I/O Prediction의 성능 향상 효과 Estimation ===\n\n* 시뮬레이션 기반의 성능 향상 효과 비교\n:- Fast media의 4KB random read latency는 96.47 usec (SSD 830 128GB 기준), slow media의 4KB random read latency는 1,884.03 usec (HDD 7,200rpm 2TB 기준)으로 설정 (iozone에 의한 실측값 사용)\n:- 실험 시작 전에는 4GB 크기의 target data가 slow media (HDD)에 저장되어 있음.\n:- 여기에 1,000,000 개의 4KB random read request로 구성되는 workload를 인가. (각 read request들은 서로 중복되는 data를 가리키지 않으며, 4GB 크기의 target data를 한 번씩 다 request하게 된다고 가정)\n:- Workload 패턴 학습 결과, 4GB의 전체 data 중, \'가\' type의 coaccess 특성을 가지는 10MB 크기의 data 그룹 (data set 당 2,500개의 4KB data 포함)이 200개가 있으며, \'나\' type의 coaccess 특성을 가지는 1MB 크기의 data 그룹 (data set 당 250개의 4KB data 포함)이 1,000개 있는 것을 발견한 것으로 가정.\n:- \'가\' type의 coaccess 패턴은 해당 data 그룹 내 100개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 80%인 것으로 정의.\n:- \'나\' type의 coaccess 패턴을 해당 data 그룹 내 50개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 100%인 것으로 정의.\n\n\n\n\n본 기법을 적용한 경우와 그렇지 않은 경우의 성능 차이를 IOPS로 비교해보면 다음과 같다. \n\n: 예를 들어 설명하면 다음과 같다.\n: data 1, data 2, ..., data 10000으로 구성된 data set이 있으며, 각 data들의 크기는 4KB이다. 학습을 통해서, (1) 이 중 어느 한 data가 access되었을 때, 1분 이내에 나머지 data들도 같이 access되게 될 확률은 55%, (2) 이 중 2개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 80%, (3) 이 중 3개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 95%라는 것을 알고 있다고 가정한다.\n: fast media의 크기는 4GB 이며, 기존에 \n\n이라는 것을 알고 있다면, data 1, 2, 3이 access되었을 때 data 4~128까지도 미리 proactive하게 fast tier에 배치시킴으로써, 그로 인한 성능 이득을 얻을 수 있게 된다. (!!!실제 실험 사례?!!!)\n\n=== Real Data Analysis (MSN File Server) (5) ===\n\n* Analysis Target Workload Specification\n:- MSN FileServer Tracelog (2008-03-10, 약 5GB 데이터)\n:- 21720977703 microseconds (약 6시간)\n:- 29345085 IO operations / 21720 sec => 1351 IOPS\n\n\n* 첫 16.5 minutes (990 seconds) 동안의 I/O에 대해 10 seconds 단위의 Measurement Window로 측정\n\n\n* Frequent Item Sets Mining (Eclat algorithm)\n: support=0.3, minlen=1, maxlen=15\n <pre>\n> fi_s3 <- eclat(ciop_data, parameter=list(supp=0.3, maxlen=15, minlen=1, tidLists=T))\n\nparameter specification:\n tidLists support minlen maxlen            target   ext\n     TRUE     0.3      1     15 frequent itemsets FALSE\n\nalgorithmic control:\n sparse sort verbose\n      7   -2    TRUE\n\neclat - find frequent item sets with the eclat algorithm\nversion 2.6 (2004.08.16)         (c) 2002-2004   Christian Borgelt\ncreate itemset ...\nset transactions ...[1146629 item(s), 100 transaction(s)] done [1.31s].\nsorting and recoding items ... [21 item(s)] done [0.12s].\ncreating bit matrix ... [21 row(s), 100 column(s)] done [0.00s].\nwriting  ... [28761 set(s)] done [0.01s].\nCreating S4 object  ... done [0.25s].\n\n\n\n\n> summary(fi_s3)\nset of 28761 itemsets\n\nmost frequent items:\n  22159360   22155264 1504849920 3170566144 4086710272    (Other)\n     14372      14370      14336      14336      14336     123094\n\nelement (itemset/transaction) length distribution:sizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13\n  21  168  696 1951 3933 5808 6336 5115 3025 1276  364   63    5\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n  1.000   6.000   7.000   6.775   8.000  13.000\n\nsummary of quality measures:\n    support\n Min.   :0.300\n 1st Qu.:0.330\n Median :0.340\n Mean   :0.388\n 3rd Qu.:0.430\n Max.   :0.920\n\nincludes transaction ID lists: TRUE\n\nmining info:\n      data ntransactions support\n ciop_data           100     0.3\n> dim(tidLists(fi_s3))\n[1] 28761   100\n\n</pre>\n\n\n=== Brief Description of the Drawings (5) ===\n\n# [Concept] Coaccess-based I/O Prediction Concepts\n# [Architecture] Storage System Architecture with Coaccess-based Proactive Data Placement\n# [Concept] Coaccessness Analysis using Frequent Itemset Mining\n# [Flowchart] Proactive Data Placement based on Coaccessness\n# [Auxiliary] Experiment Results 1\n# [Auxiliary] Experiment Results 2\n# [Auxiliary] Performance Gain Estimation\n# [Auxiliary] Comparison with prior arts\n\n== Memo for {_BPT-004, _BPT-005} ==\n\n\n=== Think outside the box // Questions ===\n\n* 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n\n* Tiering을 좀 더 다양한 각도에서 바라보자\n:- 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 것이 기존의 tiering 개념임.\n:- SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n:- SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n:- access pattern-aware optimal placement (APOP) - 이런 것은 어떨까?\n:- Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n\n* write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반의 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지?\n\n\n* 만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? SSD-internal memory buffer를 이용해서 SSD를 NVRAM처럼 쓰도록 하는 것이 실제로 말이 되는 이야기일까?\n\n\n\n\n\n=== 기존 기술의 한계점 (Intelligent I/O Prediction 기술의 필요성) ===\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n\n <pre>\n실제 tracelog 사례를 보여주고 hot area가 변화하고 있음을 보일 것.\n</pre>\n\n* Data Placement (Tiering) 관점에서 본 EMC의 FAST, FAST-VP 기술의 한계점\n:- Coarse-grained Data Migration (768KB 까지 작게 나눌 수 있다고는 하지만, 실제로 그렇게 해서 성능이 얼마나 잘 나오는지는 확인 필요. 귀중한 자원인 DRAM을 관리를 위한 metadata로 가득 채우게 될 위험도 있음)\n:- Flexible하지 않은 Tiering 주기\n:- Busy-hour 지난 후 Idle-time에 Tiering 실시\n:- 낮은 지능의 미래 I/O 예측 능력\n:: data access count를 기반으로 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두도록 하는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n\n=== 본 발명의 차별화 포인트 ===\n\n* 본 기술의 특징 1: I/O를 triggering하는 전조 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Indicator는 Run-time에 저비용으로 감지될 수 있는 형태의 정보들로 구성\n:- 미래에 access될 data를 예측케 하는 Indicator간의 mapping은 Bayesian Inference 이용\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n=== 특허 검색/분석 결과 {storage I/O workload prediction} ===\n\n* 검색식: storage I/O workload prediction\n\n* http://portal.uspto.gov/pair/PublicPair 에서 특허 상태 확인 가능.\n\n* I/O prediction을 기반으로 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n:- 기존엔 proactive tiering이 없었나? -> IBM 특허가 나름 유사함. (US7930505, US7512766)\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n==== 1. 비교가 될만한 특허들 ====\n\n예측에 대한 언급이 되어 있는 스토리지 시스템 관련 특허들. 크게 두 가지로서, 그 중의 하나는 가상 volume에 대해 peak I/O가 발생하는 주기를 분석하여, peak I/O 발생시점이 닥치기 전에 미리 가상 volume의 data를 migration 시키는 방법에 대한 것이나, 어느 위치의 data가 언제 access될지에 대한 예측은 아님 (IBM 특허). 다른 하나는 I/O 예측을 기반으로 미래에 자주 access될만한 data를 미리 여러 노드에 분산시키는 방법에 대한 것이나, 예측 방법 자체에 대한 내용은 다루고 있지 않음 (HP 특허)\n\n\n; US7930505, US7512766 (IBM)\n\n* US7930505 (Application Number: US 12/053582), \"Controlling preemptive work balancing in data storage,\" IBM, 특허일 2011-04-19\n: storage network의 I/O 관찰을 통해 peak I/O activity의 repeating instance에 대한 signature를 VLU (virtual logical unit) 단위로 생성하고, 이에 기반하여 언제 heavy I/O가 도래할 것인지를 예측하여, heavy I/O가 오기 전에 미래 data를 faster media로 migration하도록 하는 tiering 기술에 대한 특허. (이미 I/O peak time이 온 후에 hot data를 옮기는 것은 오히려 시스템의 IO성능을 악화시킬 수도 있기 때문에 미리 migration을 하고자 하는 것임)\n: VLU 단위로, peak I/O activity가 어느 주기로 발생할지에 대한 예측임. 지능 수준과 Fine-granularity 측면에서 초보적인 수준임.\n\n* US7512766 (Application Number: US 11/213494), \"Controlling preemptive work balancing in data storage\", IBM, 특허일 2009-03-31\n: 위의 US7930505 특허와 동일 건으로 보임\n\n\n; US6912635 (HP)\n\n* US6912635 (Application Number: US 10/140968), \"Distributing workload evenly across storage media in a storage array\", HP, 특허일 2005-06-28\n: 미래에 자주 access될만한 data를 여러 arrayed storage device에 분산시키겠다는 기술. 그러나 I/O에 대한 예측 방법은 다루어지고 있지 않음.\n\n==== 2. 검색은 되었으나 관련은 없는 특허들 ====\n\n무언가를 예측한다는 측면, Storage System에 대한 기술이라는 측면에서는 관련이 있으나, 미래에 Access될 Data를 예측한다는 측면에서 제안 기술과 관련 없는 특허들\n\n\n; US7640231 (IBM)\n* US7640231 (Application Number: 11/280012), \"Approach based on self-evolving models for performance guarantees in a shared storage system\", IBM, 특허일 2009-12-29\n: network 기반 storage system에서, average latency 혹은 I/O request rate 등에 대한 주기적인 performance sampling을 기반으로, 주어진 objective function의 값을 최대화할 수 있도록 system resource를 제어할 수 있게 하는 기술.\n\n\n; US7774491, US7519725 (IBM)\n\n* US7774491 (Application Number: 12/359377), \"Utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2010-08-10\n: 복수의 클라이언트들을 대상으로 통계적인 성능 QoS를 보장하기 위한 informed throttling 방법\n: A system for utilizing informed throttling to guarantee quality of service to a plurality of clients\n\n* US7519725 (Application Number: US 10/444712), \"System and method for utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2009-04-14\n: resolve the problem of providing statistical performance guarantees for applications generating streams of read/write accesses (I/Os) on a shared, potentially distributed storage system of finite resources, by initiating throttling whenever an I/O stream is receiving insufficient resources\n\n\n; US7827283 (IBM)\n* US7827283 (Application Number: 10/367444), \"System for managing and controlling storage access requirements\", IBM, 2010-11-02\n: SAN 등의 환경에서, storage bandwidth를 관리하고 reserve하는 기술\n: A Resource Management and Reservation System (RMRS) for managing and reserving storage bandwidth, is a platform independent middleware layer that provides an interface to applications, their database management systems, or some other higher level data management systems like ADRS which does data management on behalf of the applications\n\n\n; US8433554, US8140319 (IBM)\n\n* US8433554, \"Predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2013-04-30\n: An approach for predicting performance and capacity of an information technology (IT) system before the IT system is built is described, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n* US8140319, \"Method and system for predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2012-03-20\n: A method and system for predicting performance and capacity of an information technology (IT) system before the IT system is built, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n\n\n; US8255915 (HP)\n* US8255915, \"Workload management for computer system with container hierarchy and workload-group policies\" HP, 특허일 2012-08-28\n\n\n; US7050956 (HP)\n* US7050956 (Application Number: US 09/843930), \"Method and apparatus for morphological modeling of complex systems to predict performance\", HP, 특허일 2006-05-23\n: I/O 성능 예측을 위한 complex storage system에서의 data flow 모델링 방법\n\n\n; US6718434 (HP)\n* US6718434 (Application Number: US 09/871043), \"Method and apparatus for assigning raid levels\", HP, 특허일 2004-04-06\n: Data를 disk array에 load하기 전에, workload 명세 및 device 명세에 기반하여, 적합한 RAID level을 판단하여 disk array에 할당 (RAID build)하는 방법\n\n\n; US8112586 (EMC)\n* US8112586 (Application Number: US 12/190071), \"Predicting and optimizing I/O performance characteristics in a multi-level caching system\", EMC, 특허일 2012-02-07\n: multi-level caching 시스템에서, cost대비 performance를 최적화할 수 있도록, workload에 따른 resource allocation design에 대한 recommendation 기술 (multi-level cache들 간의 workload propagation을 예측하는 것이 핵심)\n\n\n; US6629266 (IBM)\n* US6629266 (Application Number: US 09/442001), \"Method and system for transparent symptom-based selective software rejuvenation\", IBM, 특허일 2003-09-30\n: software가 언제 동작이 멈추는지를 예측함으로써, software의 dependability를 향상할 수 있게 하는 방법\n\n\n; US5088058 (Unisys)\n* US5088058 (Application Number: 07/237120), \"Apparatus and method for evaluating and predicting computer I/O performance using I/O workload snapshots for model input\", Unisys Corporation, 특허일 1992-02-11\n: I/O delay 등에 대한 확률적 모형을 기반으로 disk 시스템의 성능을 검증하고 예측하는 시뮬레이션 기반의 방법\n: A method of evaluating and predicting the performance of an I/O disk system comprised of one or more disk controllers and cooperating disk units using a simulation model containing a stact constructed such that delays are formulated as sets of stochastic processes\n\n=== Comparison of I/O Prediction Approaches ===\n\n미래에 access될 data의 위치와 시기를 예측하기 위한 방법으로 event-based I/O prediction, coaccess-based I/O prediction, periodicity-based I/O prediction 방식이 제안되며, 각각의 방식은 예측에 필요한 연산량 및 input data의 richness, 그리고 workload 변화에 대한 적응성 측면에서 장단점을 가지고 있음. 따라서, 상황에 따라서 적절한 방식을 이용하면 됨.\n\n{| style=\"border-collapse: collapse; border: 1px solid #000\" border=1\n| style=\"width: 25%\" | 비교 항목\n| style=\"width: 25%\" | periodicity-based I/O prediction\n| style=\"width: 25%\" | coaccess-based I/O prediction\n| style=\"width: 25%\" | event-based I/O prediction \n|-\n| context richness required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| computation overhead required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| adaptability to workload changes (lower is better)\n| high\n| high\n| high\n|-\n|}\n\n\n* 전조 기반 예측 vs. 주기성 기반 예측\n{| border=1\n| 비교 항목\n| 전조 기반 예측\n| 주기성 기반 예측\n|-\n| 기술 요약\n| 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식\n| address 별 access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식\n|-\n| 분석 오버헤드\n| High (학습/마이닝에 드는 비용임)\n| Low (주기성 파악 비용임)\n|-\n| Run-time 오버헤드\n| Mid (실시간 전조 모니터링 및 매칭 비용)\n| Low (주기성 계산)\n|-\n| Workload 변화 적응성\n| High\n| High (Workload 변경 사실 알림 hint 필요)\n|-\n| ...\n| ...\n| ...\n|-\n| Incrementality of Mining/Learning\n| ...\n| ...\n|}\n\n* 전조 기반 예측은, 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식임.  주기성 기반 예측은, data access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식임.\n\n\n=== Intelligent Data Placement ===\n\n* Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case\n: 여기에 HML이 사용될 수 있는 것이라면, 어떤 방식으로 어떤 이유로 그런 것일까?\n\n\n* I/O 예측 방식으로서 다음 방식이 제안되며, 각 방식에 따라서 prediction model, 수집되어야 하는 context data, I/O 패턴 DB에 저장되는 정보의 종류 및 형태가 달라질 수 있음. 예측에 이용할 수 있는 context 정보의 richness에 따라 적절한 방식을 사용할 수 있다. (서로 다른 발명이므로, 별도의 특허로 낸다)\n:- event-based I/O prediction\n:- coaccess-based I/O prediction\n:: 여기서, periodicity-based I/O prediction은 기존 특허에서 언급되었으므로, 본 발명에서는 크게 언급하지 않기로 한다.\n\n\n* I/O prediction 결과에 따라 적절한 위치에 data를 배치하게 되는데, 여기서 적절한 위치라 함은 SSD와 같은 fast tier가 될 수도 있겠으나, sequential read/write 같은 경우 striping된 HDD array가 될 수도 있고, 혹은 SAS HDD가 될 수도 있겠음. 또한 SSD 중에서도 SLC, MLC, TLC 로 구분되는 fine-grained fast tier로 구분될 수도 있음\n\n\n* 미래에 오게 될 I/O의 type도 알 수 있을까? 즉, {random/sequential}x{read/write} type 중 어느 것에 해당되는지를 안다면, 그리고 I/O의 intensiveness도 예상할 수 있다면 그에 맞춰서 placement 시 적절한 tier 혹은 적절한 node로 찾아가게 할 수 있을 것임.\n\n\n* context 정보는, 수집되는 위치에 따라, server-side에서 수집되는 context data와 storage-side에서 수집되는 context data로 분류될 수 있음. Event context data는 server-side에서 수집 가능하며, coaccess context data 및 periodicity context data는 storage-side에서 수집될 수 있음.\n\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n: 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n\n=== Design/Implementation Issues ===\n\n* workload type/class tagging 필요\n\n* cache와 tiering을 같이 쓸 때, DRAM cache가 아니라 Flash Cache의 경우 쓸데없이 모든 data가 Flash Cache에 다 들어가게 두기 보다는, tiering에서 cover하지 못하는 것들 위주로 caching하도록 하면 Flash Cache의 공간 낭비를 줄임으로써, 유효 cache 공간을 늘일 수 있게 되고, 그만큼의 성능 향상으로 연결될 수 있음.\n\n* DRAM 기반의 write cache를 적극 이용하여 마치 log-structured file system에서 제공하는 I/O merging and sequentialization 효과를 얻을 수 있음.\n\n* Tiering되는 data는 caching되는 data보다 상대적으로 coarse-grained하게 grouping하여 다룸으로써, 한꺼번에 많은 data를 move할 수 있도록 하고, meta data 관리에 필요한 overhead를 줄일 수 있게 한다. 당장 access되지 않더라도 일정 시간 내에 같이 access될 수 있는 data들은 함께 faster tier로 옮겨진다. 이때 필요한 정보가 coaccess 정보임.\n\n* Cache - Tiering 기법의 연동 방법: 예측이 실패했거나, target tier media의 가용 공간이 부족하여 tiering에서 놓치는 missing data가 있을 수 있음. 이러한 missing data들이 temporal locality가 있는 경우에는 당장은 faster tier에서 serve하지 못하더라도 cache에서 serve될 수 있는데, temporal locality가 적은 경우에는 cache만으로 cover되지 않는다. \n\n* Management table size optimization issue - 만약 모든 LBA 별로 event-based I/O prediction을 해야 한다면 현실적으로 사용 가능한 아키텍쳐가 아닐 수 있음. 만약 spatial locality가 있어서 LBA 1,000개 단위로 묶어서 대응할 수 있다면, 최소 1/1000 의 management table size 감소 효과를 얻을 수 있음. pattern mining 혹은 machine learning 실행 시에는 record 수가 감소함으로써 얻는 이득은 1,000배 이상이 될 수 있음. 관건은 얼마나 LBA 묶음을 크게 만들어서 예측에 사용할 수 있는지임.\n\n\n=== I/O Insight ===\n\n* ML 기반의 I/O Insight 발굴 (Data Placement를 위한 Macroscopic Guideline)\n: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:-> 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n\n* I/O Insight의 요건:\n:# indication, as simple as possible\n:# indication, as specific as possible\n:# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n:# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n\n* I/O Insight의 형태\n: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n=== X-Y Modeling ===\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n=== 예측 실패 대비 (별도 특허화 가능) ===\n\n* I/O 예측이 실패할 수 있으며, 이로 인한 성능 penalty를 최소화하기 위한 방법 필요.\n:- fast tier에 여유공간이 있다면 1타겟과 2타겟을 모두 올려놓아도 되지 않을까? 1타겟과 2타겟을 placement함으로써 예측 정확도를 95%까지 올릴 수 있다면, 꽤 쓸만할 수 있겠음. 그리고 1타겟과 2타겟의 모든 데이터를 다 fast tier에 올려두지 말고 일부만 fast tier에 올려두었다가, 둘 중에 하나의 타겟이 hit되었을 때, hit된 타겟의 나머지 data를 재빨리 fast tier에 올리도록 하는 incremental 방법을 사용할 수 있겠음.\n:- 그러나 이것 또한 예측 실패에 대한 완벽한 prevention이 될 수는 없다. 따라서 예측 실패가 발생하더라도 재빨리 그 성능 penalty를 mitigation할 수 있는 방법이 필요. \n:- cache를 이용하여 mitigation할 수 있을까?\n\n=== Data Read/Write 관련 Issues ===\n\n* read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n\n* 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n\n* write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간에, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면?\n\n* 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있음.\n\n* 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간이 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 scatter했던 I/O들을 나중에 다시 읽어들이려 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 원하는 고성능을 낼 수 있게 된다.\n\n* replica-based dynamic clustering\n: 위와 같은 write-and-then-read-again 상황에서, 해당 data를 저장했었던 HDD의 I/O busy/idle 여부에 상관 없이, 최고의 효과를 낼 수 있게 하기 위해, replica management를 적극적으로 활용하는 것은 어떨까? HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까?\n\n* data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 논의가 아님. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 논의임.\n\n* 그러나, 조심해야 할 것은 위의 논의는 상당히 case-dependency를 상당히 많이 탈 수도 있다는 것임. 책상 위의 아이디어로 끝날 수도 있기 때문에 조심스럽게 접근하는 것이 필요함.\n\n\n\n\n<br/>\n\n== _BPT-006 ==\n\n\n=== Data Lifetime (Valid-to-invalid) Interval 분석에 기반한 Data Placement (Tiering) ===\n\n\n* Data 별로, 혹은 LBA 별로, 혹은 Erase Block 별로? Data의 Valid to Invalid Time (Duration) 패턴을 분석할 수 있을까?\n: 이를 SSD 내에서 Low Overhead로 수행할 수 있나?\n: 혹은, 굳이 SSD 내에서 수행할 필요가 있나?\n: Host side에서 LBA 별로 분석 될 수 있다면, 이 정보를 기반으로 Tiering 시에 활용 할 수 있을 것임\n: 특정 Address에 대해서 W이 되고 나서, 다시 W이 되기까지의 시간을 측정하여 ARM 하면 될 듯. 그 두 W 간에 얼마나 많은 R들이 오는지에 대해서도 고려하여 placement 시 고려할 수 있을 것임.\n\nMeasurement Window 내에서 \'W 간의 Interval이 길다\'는 식으로 인식하는 것과, \'W의 횟수가 잦다\'고 인식하는 것 간에 어떤 차이가 있을 수 있을까?\n\n <pre>\n    01234567890123456789\na1: W-R-W-R-W-R-W-W-R--W : T20W06R04 : Short-Lifetime, Many-Reads : SLC\na2: W---W-R-W---W-W----W : T20W06R01 : Short-Lifetime, Few-Reads : SLC, DRAM?\na3: W-RRRR-W---RR-W-R-RW : T20W04R08 : Mid-Lifetime, Many-Reads : MLC\na4: W------W--R---W----W : T20W04R01 : Mid-Lifetime, Few-Reads : MLC\na5: W-R-R-R-RR-RRRR----R : T20W01R10 : Long-Lifetime, Many-Reads : MLC\na6: W------------R----R- : T20W01R02 : Long-Lifetime, Few-Reads : TLC\n</pre>\n\n=== Idle-time Prediction Based Free Space Management in SSD ===\n\n* Idle-time의 도래 시기와 지속 시간 패턴 분석에 기반한 SSD 내의 Free Space 확보 방법\n:- SSD 내에서 Low overhead (in computation, memory space)로 Idle-time 도래 시기 및 지속 시간 분석이 가능한가?\n:- 가능하다면, SSD 내에서 Garbage Collection 등으로 Free Space를 확보하는 데 있어서 어떤 장점을 가질 수 있는가?\n\n\n\n: 이를 기반으로 \n\n\n\n=== 요약 ===\n* \n\n\n=== 기술 상세 ===\n\n*\n\n\n\n\n\n== _BPT-007 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== _BPT-008 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== _BPT-009 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Ef?cient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== _BPT-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== _BPT-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== _BPT-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== _BPT-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== _BPT-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== _BPT-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== _BPT-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== _BPT-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n----\n=== (SmartSSD API) SSD-internal I/O pattern logging interface and mechanism ===\n\n\n==== Questions ====\n\n* SmartSSD에서 제공할 수 있는 logging 서비스로서 어떤 것들이 있을 수 있나?\n:- 어느? 정도의 free space를 필요로하는 critical (heavy) I/O가 어떤? 주기로 도래할 지 알 수 있을까?\n:- I/O prediction에 도움될 수 있는 정보를 SmartSSD가 기록했다가 필요한 때 (안전한 방법으로?) 줄 수 있을까?\n:- I/O prediction까지는 아니더라도 해당 SSD의 I/O wellness(어떻게 정의?)를 측정했다가 bottleneck을 회피하는 데 유용한 정보를 제공할 수 있을까?\n:- 어떤 SSD가 현재 주어지고 있는 I/O workload에 대해 list performance spec을 만족하는데 어려움을 겪고 있다는 것을 알려줄 수 있는 정보가 어떤 것이 있을까? (SSD 내 write buffer의 fullness 혹은 I/O queue의 유동성? amount of free space in over-provisioning area?) (-> I/O wellness 라는 metric을 이것과 연관시켜 정의할 수도 있으며, 결국 SLA 혹은 Performance QoS와 연관시킬 수 있음)\n:- 지금까지의 S.M.A.R.T.와 비교하였을 때, 어떤 차별점이 있는가? 기존의 S.M.A.R.T.가 제공하지 못하던 타입의 정보를 제공하고, 이로 인해 기존에는 불가능했던 새로운 알고리즘 구현 혹은 새로운 서비스 제공이 가능해짐을 보이면 좋겠음.\n\n\n\n:- workload의 history?\n:- 각 erase block이 erase되어야만 했던 주기?\n:- free space의 추이 (얼마나 많이 남아 돌던지)?\n:- I/O의 heaviness 패턴?\n:- SSD내 write buffer (혹은 write cache)의 utilization정보? (즉, write buffer가 넘칠 정도가 되어서 I/O wait을 해야만 했던 상황이 얼마나 자주 발생했는지, write buffer의 가득찬 정도를 백분율로 표현한다고 했을 때, 평균/표준편차 등으로 대표할 수 있는 normal distribution을 따르는지?\n:- 전체 sequential read/write \n\n\n==== 배경 ====\n\n* 각 SSD 모델별로, 또 동일한 모델의 SSD라 하더라도 어떤 workload에 노출되어왔는지에 따라, 현재 낼 수 있는 I/O performance가 동일하지 않을 수 있다 (!check! 관련 실험 결과 - SSD 830을 이용하여 같은 시간 동안 하나의 host에서 생성되는 workload들을 나누어 받았다 하더라도 내부적인 free block의 갯수 및 random write에 의한 block 내 파편화등의 상태가 상이하여 성능에 차이가 나는 것을 보여줄 것. uFLIP 혹은 filebench 활용 가능). 한 예로, SSD가 어느 호스트의 어느 file system에 매핑되었는지에 따라 그 SSD가 받는 workload의 특성이 다를 수 있다. file system mount point가 달라지거나, 기존과 다른 새로운 응용이 설치/서비스되는 상황이 되면 \n\n\n----\n=== I/O Prediction-based Proactive Data Placement ===\n\n* 미래에 access될 data를 예측하여 적소에 미리 가져다 놓는 기술로서, (1) I/O prediction을 위한 정보 수집 모듈, (2) 주어진 정보들에 기반하여 다가올 I/O를 예측하는 모듈, (3) data placement를 수행하는 모듈로 구성됨.\n\n\n* 본 기술의 특징 1: I/O를 triggering하는 indicator (전조) 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Run-time에 저비용으로 감지될 수 있는 Indicator 정보와, 이에 의해 access될 data 간의 mapping은 Bayesian Inference 이용\n\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n\n----\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n----\n\n=== Tiering Method Avoiding I/O Bottleneck ===\n\n* I/O Prediction에 의해 예측된 미래의 Hot Data를 Proactive하게 Placement한다고 할 때, 예상되는 access pattern으로 인해 발생 가능한 I/O Bottleneck을 최소화할 수 있도록 배치하는 방법\n\n* 예를 들어 미래의 hot data block 1 (200MB), hot data block 2 (100MB), hot data block 3 (400MB)이 1분 후에 access 될 것으로 예측되었다고 가정. data block 1에 대해서는 4KB 단위로 random read가 dominant한 access pattern이 예상되고, data block 2에 대해서는 1024KB 단위의 sequential read가 dominant한 access pattern이 예상되고, data block 3에 대해서는 4KB 단위의 random read와 512KB 단위의 sequential write이 각각 80:20의 비율로 mix된 형태의 access pattern이 예상된다고 했을 때, 각 data block을 어디에 어떤 형태로 placement시킬 것인가?\n\n: machine learning 혹은 pattern mining에 기반한 base workload pattern data를 기반으로 incremental하게 update될 수 있는 data access의 spatial locality와 periodicity\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n\n* 기존 기술과의 대비 (naive I/O prediction vs. full-fledged I/O prediction)\n: 기존 기술은 누적 hit count 정보에 의지하는 naive I/O prediction으로 볼 수 있음. 이러한 방식은 workload의 변화를 제대로 반영하지 못하기 때문에, static한 workload에 대해서는 잘 동작하지만, moving target처럼 dynamic한 workload에 대해서는 오히려 과거의 누적 hit count metric이 현재의 workload 패턴을 왜곡하는 문제점을 가지고 있음. (temporal locality와 spatial locality가 깨지는 순간이 여러번 존재할 수록 기존의 naive I/O prediction 방식으로 성능 향상은 어려워짐)\n\n\n\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== _BPT-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(285,'== _BPT-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== _BPT-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== _BPT-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n[[Bnote patidea 2013-003]]\n\n\n== _BPT-004 ==\n\n=== Event-based I/O Prediction (for Proactive Data Placement) ===\n\n=== 요약 (4) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n=== 기술 상세 (4) ===\n\n* I/O 발생 메커니즘의 모델링\n: I/O prediction을 위해, type-I event (indirect I/O trigger)와 type-II event (direct I/O trigger)를 관찰 및 분석함. Type-I event의 예로서, user의 log-in, 기 log-in되어 있던 user session의 상태 변경 (idle-to-busy), system service에 대한 invocation from local or remote node 등이 포함될 수 있으며, type-II event의 예로서, user application 의 실행, system service의 실행 등이 포함될 수 있음. Type-I event는 type-II event의 발생에 영향을 미치고, type-II event는 I/O request event (data access)의 발생에 영향을 미치므로, \'type-I event\' -> \'type-II event\' -> \'I/O request event\' 까지 이어지는 인과사슬 (cause-effect chain) 기반으로 I/O 발생 메커니즘을 모델링하였음.\n\n\n* Event 정보 수집: type-I event 정보 및 type-II event 정보 정의 및 수집 방법.\n\n\n* I/O 패턴 학습 (Offline Learning): 수집된 Event 정보를 기반으로 I/O 패턴을 learning하는 방법\n\n\n* Run-time I/O 예측 (Online Matching): 시스템 모니터링에서 얻어지는 최근 및 현재의 event들과 learning으로 얻어진 I/O 패턴 DB에 기반하여 Run-time으로 미래 I/O 예측\n\n\n* 시스템 구조: 정보 수집 및 패턴 학습 모듈의 시스템 내 위치 (I/O 패턴 DB 등의 위치 및 실시간 예측 모듈과의 관계)\n\n\n* 순서도: 수집부터 학습까지 전체적인 task의 흐름\n\n\n* 예상 효과: 성능적 측면에서 사용자가 체감하는 lower latency, higher throughput 효과, 이미 발생해버린 congestion time을 피해 미리 적절한 곳에 data를 배치시킴으로써 얻어지는 bottleneck prevention 효과.\n\n=== Data Collection (4) ===\n\n* 역할: I/O 패턴 학습/발굴에 필요한 정보 수집\n\n* 수집 정보 종류\n: 전조 기반 예측에 필요한 간접 I/O 유발자, 직접 I/O 유발자, I/O 발생 이벤트 등 I/O 예측에 도움이 될 수 있는 다양한 정보를 수집. 다음 정보들이 포함될 수 있음.\n:- indirect I/O trigger trace: {(user-id | user-type), status-type, before-status, after-status, event-time}\n:- direct I/O trigger trace: {(program-id | program-type), status-type, before-status, after-status, process-owner, event-time [, process-current-working-directory, files-opened]}\n:- I/O trace: {access-time, address-accessed, read/write-flag, io-size, file-path, process, co-accessed-addresses}\n\n\n* 수집 모듈의 위치\n: storage-server가 분리되어 있는 구조인 SAN/NAS 타입의 architecture인 경우에는 application이 구동되는 server-side에 정보 수집 모듈이 위치한다. storage-server가 결합되어 있는 converged-architecture 기반의 system인 경우에는 해당 system 내에 정보 수집 모듈이 위치한다.\n\n\n* 정보 수집 방식\n: 운영체체가 제공하는 시스템 상태 정보 제공 interface를 이용하거나, blktrace, lsof, debug file system 등 이용 (e.g., /sys/kernel/debug/ 등) 혹은 /proc file system 등을 이용 가능.\n\n== _BPT-005 ==\n\n=== Coaccess-based I/O Prediction (for Proactive Data Placement) ===\n\n* (영문) Coaccess-based I/O Prediction (for Proactive Data Placement)\n* (한글) Coaccess 기반의 I/O 예측 방법 및 시스템 구조\n\n=== 요약 (5) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 coaccess-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n\n\n* 소정의 Confidence 값 이상인 X=>Y 관계를 활용하여, signature X (Y에 대한 precursor)가 access되면 어느 정도 확률로 이후에\n\n=== 발명의 이용 분야 (5) ===\n\n데이터를 저장하고 처리하는 일반적인 컴퓨팅 시스템으로서,\n* 프론트엔드/백엔드 서버\n* 네트워크 스토리지 시스템\n* 컨버지드 서버-스토리지 시스템\n* 엔터프라이즈 혹은 데이터 센터 내의 IT 인프라\n등에서 활용 가능.\n\n\n=== 발명의 목적 (5) ===\n\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: Fast media로서 SSD가 사용되는 경우에 다음과 같은 부정적인 효과가 발생할 수 있음. 과거에는 자주 access되었던 data가 앞으로도 hot할 것으로 판단하고 tiering 시점에 fast media로 placement 시켰는데 tiering되고 난 후에 자주 access되지 않게 되면, 성능 향상 이득 없이 SSD의 wear-out만 초래하게 됨.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n\n* 본 발명의 필요성/의미\n: 본 발명은 data access 정보 기반의 I/O prediction 기술로서, client-side의 rich information을 확보가 제한적인 경우에도 periodicity 및 coaccessness 특성을 기반으로 미래에 access될 data의 위치와 시점을 예측할 수 있도록 하는 기술임.\n\n=== 기술 상세 (5) ===\n\n* Data의 coaccess 특성에 기반한 proactive data placement 기법\n: 소정의 time window 내에 같이 access되는 확률이 높은 data 그룹 (coaccess group)이 있을 때, 그 그룹 내의 일부 data가 access되면 나머지 data들도 미리 fast media에 가져다 놓음으로써, 성능 향상을 얻을 수 있게 하는 방법임.\n\n\n* 개본 개념 정의 (coaccess property, coaccessness, coaccess group)\n: 지정된 시간 동안의 measurement window 내에서, 어떤 data가 access될 때 함께 access되는 경향을 가지는 다른 data들이 존재할 때 그 data들 간에 coaccess property 혹은 coaccessness가 있다고 정의한다.  이때, coaccess property를 가지는 data 그룹을 coaccess group이라고 한다. frequent item set mining 관점에서 보면 기준이 되는 support 값 이상의 확률로 관측되는 transaction들을 coaccess group으로 정의할 수 있다.\n\n\n* 추가 개념 정의 (merged coaccess group)\n: 서로 다른 coaccess group들을 개별적으로 다루기보다 이들을 묶어서 하나의 coaccess group으로 만들어 다룸으로써 meta data management의 효율성 및 proactive data placement의 효과 측면에서 장점이 발생하는 경우가 있다. 이렇게 만들어지는 coaccess group을 merged coaccess group이라 정의한다. \n: 서로 다른 coaccess group CG_A와 coaccess group CG_B가 있는데, CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합) 시켜서 새롭게 얻어진 CG_C = {CG_A, CG_B}를 하나의 coaccess group으로 다룬다.\n: computing resource의 한계로 인한 현실적인 문제로 인해 모든 경우의 frequent item set을 발굴하기 어려운 경우가 있음 (Frequent item set mining 시 실제로 maximum size of transaction을 한계 값으로 지정). merged/unionized coaccess group은 이러한 한계적인 상황을 극복할 수 있는 방법으로서 제안되는 개념임. (실제로 관측되었던 실험 data 및 graph를 이용하여 실재 근거를 제시할 것)\n\n<!--\nmerged coaccess group 관련해서.\n\n: CG_A = {a1, a2, ..., aM |supp=s1}, CG_B = {b1, b2, ..., bN |supp=s1}, CG_C = {union of CG_A and CG_B |supp=s1} 이 존재하고 발생 시기가 일치하는 것을 확인하였다면, CG_A, CG_B, CG_C 를 모두 개별적인 coaccess group으로 관리할 것이 아니라, CG_C 만을 coaccess group으로 관리한다. (조건 i) supp(CG_A) == supp(CG_B) == supp(CG_C), (조건 ii) union of CG_A and CG_B is identical to CG_C, (조건 iii) occurrence_time(CG_A) == occurrence_time(CG_B) == occurrence_time(CG_C) 이 있다고 하자. 상기 조건 i, ii, iii이 모두 만족되는 경우에는 CG_C 만을 coaccess group으로 관리하고 지정함으로써, coaccess group의 meta data management overhead를 감소시키고 coaccess-based proactive data placement의 operation efficiency를 증대시킴. /* 여기서, (조건 i)과 (조건 ii)가 만족되면 굳이 (조건 iii)까지 확인할 필요는 없을 수도 있음. 확인 필요 */\n* GCD (Greatest Common Divisor) / LCM(Least Common Multiple) / largest coaccess group 개념과 관련.\n\n* 유사 coaccess group 간 merging?\n: 예를 들어 coaccess group CG_A = {\'a\', \'b\', \'c\', \'d\'} 와 coaccess group CG_B = {\'b\', \'c\', \'d\', \'e\'} 가 있다고 했을 때, \'b\', \'c\', \'d\' data 가 겹치기는 하지만, CG_A와 CG_B는 동일한 coaccess group으로 볼 수는 없다. 그러나 만약 CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 (기준이 되는 확률 값은 시스템과 workload의 상황에 따라서 적절히 조절 가능) CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합)화 하여 단일 coaccess group으로 다루는 방식. (말이 되는 소리인지 확인 필요)\n-->\n\n\n* frequent item set mining을 활용한 coaccess group 발굴 방법\n: Coaccess group 발굴을 위해 frequent item set mining을 이용할 수 있으며, 구체적인 방법은 다음과 같다.\n: (1) 시스템의 resource 상황에 따라 적절한 support 값을 설정한다. 예를 들어 fast media (cache 혹은 tier 1 storage)의 저장 가능 용량에 따라, 얼마나 높은 등급의 hot data까지 fast media에 placement 시킬 수 있을지가 결정될 수 있다. 즉 fast media의 용량이 매우 넉넉하여 10번 중 2번의 빈도로 (support=0.2) 관찰되는 coaccess group 까지 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있고, 혹은 fast media의 용량이 여유롭지 않아서 10번 중 5번 이상의 빈도로 (support=0.5) 관찰되는 coaccess group까지만 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있다. support 값은 상황에 따라 dynamic하게 설정될 수 있으며, 시스템 관리자 혹은 data scientist등 사람에 의해 수동적으로 설정될 수도 있지만 algorithm에 의해 automatic하게 설정되도록 구현될 수도 있다.\n: (2) coaccess group에 대한 단위 관측 시간이 되는 transaction measurement window (TMW) 크기를 설정한다. transaction measurement window의 크기에 따라 coaccess group에 포함될 수 있는 data의 갯수가 달라질 수 있다. transaction measurement window 크기가 너무 작으면 함께 access되는 것으로 판단될 대상이 되는 data 갯수 자체가 감소하게 되므로 coaccessness 분석이 어렵게 된다. 반대로, transaction measurement window 크기가 너무 크면 coaccess group의 종류는 감소하고 각 coaccess group의 크기는 커지는 경향이 생기는데, 이 경우의 문제점은 data placement 시 fast media를 효율적으로 사용하는 것이 어려워질 수 있다는 것이다.\n: (3) access되는 data address 정보를 기반으로 frequent item set mining을 시작한다 (이를 위해서는 support 값과 transaction measurement window size 값이 설정되어 있어야 함).  transaction measurement window 동안 access 되는 address들의 집합을 하나의 transaction (basket)으로 묶는다. 전체 관찰 기간 동안 N 번의 transaction measurement window가 있었다면, N 개 만큼의 transaction이 만들어진다. 이 때 N 개의 transaction을 통틀어 support 값 확률 이상으로 빈번하게 등장하는 item set이 있다면 그것을 coaccess group으로 지정하고 coaccess group management ID를 부여, 관리한다.\n: (4) frequent item set mining에서의 confidence 개념을 도입하여 precursor의 존재 감지 시, 미래에 access되게 될 나머지 data들을 신속하게  (일부 데이터가 access되고 나면 나머지 데이터들도 지정된 시간 내에 함께 access될 확률을 의미하게 됨)\n: (4) run-time 시, coaccess group의 빠른 속도로 찾아낼 수 있도록 하기 위해, coaccess group \n\n\n : coaccess group 발굴이 되었다면, 어떻게 저장하고, 어떻게 불러내서(매칭시켜서) proactive data placement에 활용할 수 있게 할 것인가?\n\n\n* O(1)으로, 매칭되는 패턴을 찾을 수 있게 하려면?\n\n\n* proactive data placement를 위해 \n\n\n* Coaccess group pattern matching 방법\n\n\n* Fig{association graph, temporal occurance graph}\n: Frequent item set mining 분석 결과로 얻어진 association graph 및 시간에 따른 frequent item 발생 추이 graph를 이용하여 coaccess pattern의 실재함을 설명\n\n\n* Coaccess property는 다음과 같은 방식으로 활용될 수 있음\n:- 특정 coaccess pattern이 특정 workload에 대한 precursor가 되는 경우가 있을 수 있음. 이를 이용하여 다가올 workload의 prediction에 활용 가능 (예를 들어, coaccess group A가 access된 후 T1 시간 경과 후에 address range B를 random read하는 workload가 발생하고, T2 시간 경과 후에 address range C를 sequential write하는 workload가 오는 경향이 있다는 것을 알고 있다면, 이 정보를 기반으로 proactive하게 address range B를 fast media에 placement 시키고, address range C에 대해서는 caching  \n:- coaccess group 내의 일부 data가 access되면 그 coaccess group 내의 다른 data들을 미리 fast media에 가져다 놓음으로써 성능 향상 가능\n\n<!--\n* (for event-based I/O prediction) coaccess group이 access되는 순서를 event-based I/O prediction에 이용할 수도 있음. 예를 들어, coaccess group A가 access되고 나서 T1 시간 내로 coaccess group B가 access될 확률이 80% 이상이라고 하고, coaccess group B가 access되고 나서 T2 시간 내로 coaccess group C와 coaccess group D가 access될 확률이 90% 이상이라고 한다면, 이러한 정보를 기반으로 ... 무엇을 할 수 있나?\n-->\n\n\n* 이를 위해서 기존의 storage system 구조에 (1) coaccess pattern analysis engine, (2) coacccess pattern aware data placement controller가 추가되는 것이 필요하다.\n\n\n* coaccess pattern analyzer의 역할: workload 별로(???) coaccess pattern list 구축\n\n\n* coaccess pattern analyzer의 구조 및 동작 방식:\n\n\n* coaccess pattern aware data placement controller의 역할:\n\n\n* coaccess pattern aware data placement controller의 구조 및 동작 방식: coaccess pattern 정보를 받아들이기 위한 interface, interface로 받아들인 coaccess pattern 정보의 queuing? data placement의 I/O 단위 (granularity), data placement의 주기 (static? or dynamic?) 등 coaccess pattern 정보를 data placement 시 어떤 식으로 활용하는지\n\n=== Coaccess-based I/O Prediction의 성능 향상 효과 Estimation ===\n\n* 시뮬레이션 기반의 성능 향상 효과 비교\n:- Fast media의 4KB random read latency는 96.47 usec (SSD 830 128GB 기준), slow media의 4KB random read latency는 1,884.03 usec (HDD 7,200rpm 2TB 기준)으로 설정 (iozone에 의한 실측값 사용)\n:- 실험 시작 전에는 4GB 크기의 target data가 slow media (HDD)에 저장되어 있음.\n:- 여기에 1,000,000 개의 4KB random read request로 구성되는 workload를 인가. (각 read request들은 서로 중복되는 data를 가리키지 않으며, 4GB 크기의 target data를 한 번씩 다 request하게 된다고 가정)\n:- Workload 패턴 학습 결과, 4GB의 전체 data 중, \'가\' type의 coaccess 특성을 가지는 10MB 크기의 data 그룹 (data set 당 2,500개의 4KB data 포함)이 200개가 있으며, \'나\' type의 coaccess 특성을 가지는 1MB 크기의 data 그룹 (data set 당 250개의 4KB data 포함)이 1,000개 있는 것을 발견한 것으로 가정.\n:- \'가\' type의 coaccess 패턴은 해당 data 그룹 내 100개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 80%인 것으로 정의.\n:- \'나\' type의 coaccess 패턴을 해당 data 그룹 내 50개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 100%인 것으로 정의.\n\n\n\n\n본 기법을 적용한 경우와 그렇지 않은 경우의 성능 차이를 IOPS로 비교해보면 다음과 같다. \n\n: 예를 들어 설명하면 다음과 같다.\n: data 1, data 2, ..., data 10000으로 구성된 data set이 있으며, 각 data들의 크기는 4KB이다. 학습을 통해서, (1) 이 중 어느 한 data가 access되었을 때, 1분 이내에 나머지 data들도 같이 access되게 될 확률은 55%, (2) 이 중 2개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 80%, (3) 이 중 3개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 95%라는 것을 알고 있다고 가정한다.\n: fast media의 크기는 4GB 이며, 기존에 \n\n이라는 것을 알고 있다면, data 1, 2, 3이 access되었을 때 data 4~128까지도 미리 proactive하게 fast tier에 배치시킴으로써, 그로 인한 성능 이득을 얻을 수 있게 된다. (!!!실제 실험 사례?!!!)\n\n=== Real Data Analysis (MSN File Server) (5) ===\n\n* Analysis Target Workload Specification\n:- MSN FileServer Tracelog (2008-03-10, 약 5GB 데이터)\n:- 21720977703 microseconds (약 6시간)\n:- 29345085 IO operations / 21720 sec => 1351 IOPS\n\n\n* 첫 16.5 minutes (990 seconds) 동안의 I/O에 대해 10 seconds 단위의 Measurement Window로 측정\n\n\n* Frequent Item Sets Mining (Eclat algorithm)\n: support=0.3, minlen=1, maxlen=15\n <pre>\n> fi_s3 <- eclat(ciop_data, parameter=list(supp=0.3, maxlen=15, minlen=1, tidLists=T))\n\nparameter specification:\n tidLists support minlen maxlen            target   ext\n     TRUE     0.3      1     15 frequent itemsets FALSE\n\nalgorithmic control:\n sparse sort verbose\n      7   -2    TRUE\n\neclat - find frequent item sets with the eclat algorithm\nversion 2.6 (2004.08.16)         (c) 2002-2004   Christian Borgelt\ncreate itemset ...\nset transactions ...[1146629 item(s), 100 transaction(s)] done [1.31s].\nsorting and recoding items ... [21 item(s)] done [0.12s].\ncreating bit matrix ... [21 row(s), 100 column(s)] done [0.00s].\nwriting  ... [28761 set(s)] done [0.01s].\nCreating S4 object  ... done [0.25s].\n\n\n\n\n> summary(fi_s3)\nset of 28761 itemsets\n\nmost frequent items:\n  22159360   22155264 1504849920 3170566144 4086710272    (Other)\n     14372      14370      14336      14336      14336     123094\n\nelement (itemset/transaction) length distribution:sizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13\n  21  168  696 1951 3933 5808 6336 5115 3025 1276  364   63    5\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n  1.000   6.000   7.000   6.775   8.000  13.000\n\nsummary of quality measures:\n    support\n Min.   :0.300\n 1st Qu.:0.330\n Median :0.340\n Mean   :0.388\n 3rd Qu.:0.430\n Max.   :0.920\n\nincludes transaction ID lists: TRUE\n\nmining info:\n      data ntransactions support\n ciop_data           100     0.3\n> dim(tidLists(fi_s3))\n[1] 28761   100\n\n</pre>\n\n\n=== Brief Description of the Drawings (5) ===\n\n# [Concept] Coaccess-based I/O Prediction Concepts\n# [Architecture] Storage System Architecture with Coaccess-based Proactive Data Placement\n# [Concept] Coaccessness Analysis using Frequent Itemset Mining\n# [Flowchart] Proactive Data Placement based on Coaccessness\n# [Auxiliary] Experiment Results 1\n# [Auxiliary] Experiment Results 2\n# [Auxiliary] Performance Gain Estimation\n# [Auxiliary] Comparison with prior arts\n\n== Memo for {_BPT-004, _BPT-005} ==\n\n\n=== Think outside the box // Questions ===\n\n* 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n\n* Tiering을 좀 더 다양한 각도에서 바라보자\n:- 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 것이 기존의 tiering 개념임.\n:- SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n:- SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n:- access pattern-aware optimal placement (APOP) - 이런 것은 어떨까?\n:- Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n\n* write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반의 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지?\n\n\n* 만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? SSD-internal memory buffer를 이용해서 SSD를 NVRAM처럼 쓰도록 하는 것이 실제로 말이 되는 이야기일까?\n\n\n\n\n\n=== 기존 기술의 한계점 (Intelligent I/O Prediction 기술의 필요성) ===\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n\n <pre>\n실제 tracelog 사례를 보여주고 hot area가 변화하고 있음을 보일 것.\n</pre>\n\n* Data Placement (Tiering) 관점에서 본 EMC의 FAST, FAST-VP 기술의 한계점\n:- Coarse-grained Data Migration (768KB 까지 작게 나눌 수 있다고는 하지만, 실제로 그렇게 해서 성능이 얼마나 잘 나오는지는 확인 필요. 귀중한 자원인 DRAM을 관리를 위한 metadata로 가득 채우게 될 위험도 있음)\n:- Flexible하지 않은 Tiering 주기\n:- Busy-hour 지난 후 Idle-time에 Tiering 실시\n:- 낮은 지능의 미래 I/O 예측 능력\n:: data access count를 기반으로 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두도록 하는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n\n=== 본 발명의 차별화 포인트 ===\n\n* 본 기술의 특징 1: I/O를 triggering하는 전조 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Indicator는 Run-time에 저비용으로 감지될 수 있는 형태의 정보들로 구성\n:- 미래에 access될 data를 예측케 하는 Indicator간의 mapping은 Bayesian Inference 이용\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n=== 특허 검색/분석 결과 {storage I/O workload prediction} ===\n\n* 검색식: storage I/O workload prediction\n\n* http://portal.uspto.gov/pair/PublicPair 에서 특허 상태 확인 가능.\n\n* I/O prediction을 기반으로 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n:- 기존엔 proactive tiering이 없었나? -> IBM 특허가 나름 유사함. (US7930505, US7512766)\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n==== 1. 비교가 될만한 특허들 ====\n\n예측에 대한 언급이 되어 있는 스토리지 시스템 관련 특허들. 크게 두 가지로서, 그 중의 하나는 가상 volume에 대해 peak I/O가 발생하는 주기를 분석하여, peak I/O 발생시점이 닥치기 전에 미리 가상 volume의 data를 migration 시키는 방법에 대한 것이나, 어느 위치의 data가 언제 access될지에 대한 예측은 아님 (IBM 특허). 다른 하나는 I/O 예측을 기반으로 미래에 자주 access될만한 data를 미리 여러 노드에 분산시키는 방법에 대한 것이나, 예측 방법 자체에 대한 내용은 다루고 있지 않음 (HP 특허)\n\n\n; US7930505, US7512766 (IBM)\n\n* US7930505 (Application Number: US 12/053582), \"Controlling preemptive work balancing in data storage,\" IBM, 특허일 2011-04-19\n: storage network의 I/O 관찰을 통해 peak I/O activity의 repeating instance에 대한 signature를 VLU (virtual logical unit) 단위로 생성하고, 이에 기반하여 언제 heavy I/O가 도래할 것인지를 예측하여, heavy I/O가 오기 전에 미래 data를 faster media로 migration하도록 하는 tiering 기술에 대한 특허. (이미 I/O peak time이 온 후에 hot data를 옮기는 것은 오히려 시스템의 IO성능을 악화시킬 수도 있기 때문에 미리 migration을 하고자 하는 것임)\n: VLU 단위로, peak I/O activity가 어느 주기로 발생할지에 대한 예측임. 지능 수준과 Fine-granularity 측면에서 초보적인 수준임.\n\n* US7512766 (Application Number: US 11/213494), \"Controlling preemptive work balancing in data storage\", IBM, 특허일 2009-03-31\n: 위의 US7930505 특허와 동일 건으로 보임\n\n\n; US6912635 (HP)\n\n* US6912635 (Application Number: US 10/140968), \"Distributing workload evenly across storage media in a storage array\", HP, 특허일 2005-06-28\n: 미래에 자주 access될만한 data를 여러 arrayed storage device에 분산시키겠다는 기술. 그러나 I/O에 대한 예측 방법은 다루어지고 있지 않음.\n\n==== 2. 검색은 되었으나 관련은 없는 특허들 ====\n\n무언가를 예측한다는 측면, Storage System에 대한 기술이라는 측면에서는 관련이 있으나, 미래에 Access될 Data를 예측한다는 측면에서 제안 기술과 관련 없는 특허들\n\n\n; US7640231 (IBM)\n* US7640231 (Application Number: 11/280012), \"Approach based on self-evolving models for performance guarantees in a shared storage system\", IBM, 특허일 2009-12-29\n: network 기반 storage system에서, average latency 혹은 I/O request rate 등에 대한 주기적인 performance sampling을 기반으로, 주어진 objective function의 값을 최대화할 수 있도록 system resource를 제어할 수 있게 하는 기술.\n\n\n; US7774491, US7519725 (IBM)\n\n* US7774491 (Application Number: 12/359377), \"Utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2010-08-10\n: 복수의 클라이언트들을 대상으로 통계적인 성능 QoS를 보장하기 위한 informed throttling 방법\n: A system for utilizing informed throttling to guarantee quality of service to a plurality of clients\n\n* US7519725 (Application Number: US 10/444712), \"System and method for utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2009-04-14\n: resolve the problem of providing statistical performance guarantees for applications generating streams of read/write accesses (I/Os) on a shared, potentially distributed storage system of finite resources, by initiating throttling whenever an I/O stream is receiving insufficient resources\n\n\n; US7827283 (IBM)\n* US7827283 (Application Number: 10/367444), \"System for managing and controlling storage access requirements\", IBM, 2010-11-02\n: SAN 등의 환경에서, storage bandwidth를 관리하고 reserve하는 기술\n: A Resource Management and Reservation System (RMRS) for managing and reserving storage bandwidth, is a platform independent middleware layer that provides an interface to applications, their database management systems, or some other higher level data management systems like ADRS which does data management on behalf of the applications\n\n\n; US8433554, US8140319 (IBM)\n\n* US8433554, \"Predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2013-04-30\n: An approach for predicting performance and capacity of an information technology (IT) system before the IT system is built is described, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n* US8140319, \"Method and system for predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2012-03-20\n: A method and system for predicting performance and capacity of an information technology (IT) system before the IT system is built, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n\n\n; US8255915 (HP)\n* US8255915, \"Workload management for computer system with container hierarchy and workload-group policies\" HP, 특허일 2012-08-28\n\n\n; US7050956 (HP)\n* US7050956 (Application Number: US 09/843930), \"Method and apparatus for morphological modeling of complex systems to predict performance\", HP, 특허일 2006-05-23\n: I/O 성능 예측을 위한 complex storage system에서의 data flow 모델링 방법\n\n\n; US6718434 (HP)\n* US6718434 (Application Number: US 09/871043), \"Method and apparatus for assigning raid levels\", HP, 특허일 2004-04-06\n: Data를 disk array에 load하기 전에, workload 명세 및 device 명세에 기반하여, 적합한 RAID level을 판단하여 disk array에 할당 (RAID build)하는 방법\n\n\n; US8112586 (EMC)\n* US8112586 (Application Number: US 12/190071), \"Predicting and optimizing I/O performance characteristics in a multi-level caching system\", EMC, 특허일 2012-02-07\n: multi-level caching 시스템에서, cost대비 performance를 최적화할 수 있도록, workload에 따른 resource allocation design에 대한 recommendation 기술 (multi-level cache들 간의 workload propagation을 예측하는 것이 핵심)\n\n\n; US6629266 (IBM)\n* US6629266 (Application Number: US 09/442001), \"Method and system for transparent symptom-based selective software rejuvenation\", IBM, 특허일 2003-09-30\n: software가 언제 동작이 멈추는지를 예측함으로써, software의 dependability를 향상할 수 있게 하는 방법\n\n\n; US5088058 (Unisys)\n* US5088058 (Application Number: 07/237120), \"Apparatus and method for evaluating and predicting computer I/O performance using I/O workload snapshots for model input\", Unisys Corporation, 특허일 1992-02-11\n: I/O delay 등에 대한 확률적 모형을 기반으로 disk 시스템의 성능을 검증하고 예측하는 시뮬레이션 기반의 방법\n: A method of evaluating and predicting the performance of an I/O disk system comprised of one or more disk controllers and cooperating disk units using a simulation model containing a stact constructed such that delays are formulated as sets of stochastic processes\n\n=== Comparison of I/O Prediction Approaches ===\n\n미래에 access될 data의 위치와 시기를 예측하기 위한 방법으로 event-based I/O prediction, coaccess-based I/O prediction, periodicity-based I/O prediction 방식이 제안되며, 각각의 방식은 예측에 필요한 연산량 및 input data의 richness, 그리고 workload 변화에 대한 적응성 측면에서 장단점을 가지고 있음. 따라서, 상황에 따라서 적절한 방식을 이용하면 됨.\n\n{| style=\"border-collapse: collapse; border: 1px solid #000\" border=1\n| style=\"width: 25%\" | 비교 항목\n| style=\"width: 25%\" | periodicity-based I/O prediction\n| style=\"width: 25%\" | coaccess-based I/O prediction\n| style=\"width: 25%\" | event-based I/O prediction \n|-\n| context richness required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| computation overhead required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| adaptability to workload changes (lower is better)\n| high\n| high\n| high\n|-\n|}\n\n\n* 전조 기반 예측 vs. 주기성 기반 예측\n{| border=1\n| 비교 항목\n| 전조 기반 예측\n| 주기성 기반 예측\n|-\n| 기술 요약\n| 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식\n| address 별 access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식\n|-\n| 분석 오버헤드\n| High (학습/마이닝에 드는 비용임)\n| Low (주기성 파악 비용임)\n|-\n| Run-time 오버헤드\n| Mid (실시간 전조 모니터링 및 매칭 비용)\n| Low (주기성 계산)\n|-\n| Workload 변화 적응성\n| High\n| High (Workload 변경 사실 알림 hint 필요)\n|-\n| ...\n| ...\n| ...\n|-\n| Incrementality of Mining/Learning\n| ...\n| ...\n|}\n\n* 전조 기반 예측은, 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식임.  주기성 기반 예측은, data access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식임.\n\n\n=== Intelligent Data Placement ===\n\n* Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case\n: 여기에 HML이 사용될 수 있는 것이라면, 어떤 방식으로 어떤 이유로 그런 것일까?\n\n\n* I/O 예측 방식으로서 다음 방식이 제안되며, 각 방식에 따라서 prediction model, 수집되어야 하는 context data, I/O 패턴 DB에 저장되는 정보의 종류 및 형태가 달라질 수 있음. 예측에 이용할 수 있는 context 정보의 richness에 따라 적절한 방식을 사용할 수 있다. (서로 다른 발명이므로, 별도의 특허로 낸다)\n:- event-based I/O prediction\n:- coaccess-based I/O prediction\n:: 여기서, periodicity-based I/O prediction은 기존 특허에서 언급되었으므로, 본 발명에서는 크게 언급하지 않기로 한다.\n\n\n* I/O prediction 결과에 따라 적절한 위치에 data를 배치하게 되는데, 여기서 적절한 위치라 함은 SSD와 같은 fast tier가 될 수도 있겠으나, sequential read/write 같은 경우 striping된 HDD array가 될 수도 있고, 혹은 SAS HDD가 될 수도 있겠음. 또한 SSD 중에서도 SLC, MLC, TLC 로 구분되는 fine-grained fast tier로 구분될 수도 있음\n\n\n* 미래에 오게 될 I/O의 type도 알 수 있을까? 즉, {random/sequential}x{read/write} type 중 어느 것에 해당되는지를 안다면, 그리고 I/O의 intensiveness도 예상할 수 있다면 그에 맞춰서 placement 시 적절한 tier 혹은 적절한 node로 찾아가게 할 수 있을 것임.\n\n\n* context 정보는, 수집되는 위치에 따라, server-side에서 수집되는 context data와 storage-side에서 수집되는 context data로 분류될 수 있음. Event context data는 server-side에서 수집 가능하며, coaccess context data 및 periodicity context data는 storage-side에서 수집될 수 있음.\n\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n: 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n\n=== Design/Implementation Issues ===\n\n* workload type/class tagging 필요\n\n* cache와 tiering을 같이 쓸 때, DRAM cache가 아니라 Flash Cache의 경우 쓸데없이 모든 data가 Flash Cache에 다 들어가게 두기 보다는, tiering에서 cover하지 못하는 것들 위주로 caching하도록 하면 Flash Cache의 공간 낭비를 줄임으로써, 유효 cache 공간을 늘일 수 있게 되고, 그만큼의 성능 향상으로 연결될 수 있음.\n\n* DRAM 기반의 write cache를 적극 이용하여 마치 log-structured file system에서 제공하는 I/O merging and sequentialization 효과를 얻을 수 있음.\n\n* Tiering되는 data는 caching되는 data보다 상대적으로 coarse-grained하게 grouping하여 다룸으로써, 한꺼번에 많은 data를 move할 수 있도록 하고, meta data 관리에 필요한 overhead를 줄일 수 있게 한다. 당장 access되지 않더라도 일정 시간 내에 같이 access될 수 있는 data들은 함께 faster tier로 옮겨진다. 이때 필요한 정보가 coaccess 정보임.\n\n* Cache - Tiering 기법의 연동 방법: 예측이 실패했거나, target tier media의 가용 공간이 부족하여 tiering에서 놓치는 missing data가 있을 수 있음. 이러한 missing data들이 temporal locality가 있는 경우에는 당장은 faster tier에서 serve하지 못하더라도 cache에서 serve될 수 있는데, temporal locality가 적은 경우에는 cache만으로 cover되지 않는다. \n\n* Management table size optimization issue - 만약 모든 LBA 별로 event-based I/O prediction을 해야 한다면 현실적으로 사용 가능한 아키텍쳐가 아닐 수 있음. 만약 spatial locality가 있어서 LBA 1,000개 단위로 묶어서 대응할 수 있다면, 최소 1/1000 의 management table size 감소 효과를 얻을 수 있음. pattern mining 혹은 machine learning 실행 시에는 record 수가 감소함으로써 얻는 이득은 1,000배 이상이 될 수 있음. 관건은 얼마나 LBA 묶음을 크게 만들어서 예측에 사용할 수 있는지임.\n\n\n=== I/O Insight ===\n\n* ML 기반의 I/O Insight 발굴 (Data Placement를 위한 Macroscopic Guideline)\n: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:-> 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n\n* I/O Insight의 요건:\n:# indication, as simple as possible\n:# indication, as specific as possible\n:# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n:# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n\n* I/O Insight의 형태\n: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n=== X-Y Modeling ===\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n=== 예측 실패 대비 (별도 특허화 가능) ===\n\n* I/O 예측이 실패할 수 있으며, 이로 인한 성능 penalty를 최소화하기 위한 방법 필요.\n:- fast tier에 여유공간이 있다면 1타겟과 2타겟을 모두 올려놓아도 되지 않을까? 1타겟과 2타겟을 placement함으로써 예측 정확도를 95%까지 올릴 수 있다면, 꽤 쓸만할 수 있겠음. 그리고 1타겟과 2타겟의 모든 데이터를 다 fast tier에 올려두지 말고 일부만 fast tier에 올려두었다가, 둘 중에 하나의 타겟이 hit되었을 때, hit된 타겟의 나머지 data를 재빨리 fast tier에 올리도록 하는 incremental 방법을 사용할 수 있겠음.\n:- 그러나 이것 또한 예측 실패에 대한 완벽한 prevention이 될 수는 없다. 따라서 예측 실패가 발생하더라도 재빨리 그 성능 penalty를 mitigation할 수 있는 방법이 필요. \n:- cache를 이용하여 mitigation할 수 있을까?\n\n=== Data Read/Write 관련 Issues ===\n\n* read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n\n* 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n\n* write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간에, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면?\n\n* 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있음.\n\n* 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간이 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 scatter했던 I/O들을 나중에 다시 읽어들이려 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 원하는 고성능을 낼 수 있게 된다.\n\n* replica-based dynamic clustering\n: 위와 같은 write-and-then-read-again 상황에서, 해당 data를 저장했었던 HDD의 I/O busy/idle 여부에 상관 없이, 최고의 효과를 낼 수 있게 하기 위해, replica management를 적극적으로 활용하는 것은 어떨까? HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까?\n\n* data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 논의가 아님. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 논의임.\n\n* 그러나, 조심해야 할 것은 위의 논의는 상당히 case-dependency를 상당히 많이 탈 수도 있다는 것임. 책상 위의 아이디어로 끝날 수도 있기 때문에 조심스럽게 접근하는 것이 필요함.\n\n\n\n\n<br/>\n\n== _BPT-006 ==\n\n\n=== Data Lifetime (Valid-to-invalid) Interval 분석에 기반한 Data Placement (Tiering) ===\n\n\n* Data 별로, 혹은 LBA 별로, 혹은 Erase Block 별로? Data의 Valid to Invalid Time (Duration) 패턴을 분석할 수 있을까?\n: 이를 SSD 내에서 Low Overhead로 수행할 수 있나?\n: 혹은, 굳이 SSD 내에서 수행할 필요가 있나?\n: Host side에서 LBA 별로 분석 될 수 있다면, 이 정보를 기반으로 Tiering 시에 활용 할 수 있을 것임\n: 특정 Address에 대해서 W이 되고 나서, 다시 W이 되기까지의 시간을 측정하여 ARM 하면 될 듯. 그 두 W 간에 얼마나 많은 R들이 오는지에 대해서도 고려하여 placement 시 고려할 수 있을 것임.\n\nMeasurement Window 내에서 \'W 간의 Interval이 길다\'는 식으로 인식하는 것과, \'W의 횟수가 잦다\'고 인식하는 것 간에 어떤 차이가 있을 수 있을까?\n\n <pre>\n    01234567890123456789\na1: W-R-W-R-W-R-W-W-R--W : T20W06R04 : Short-Lifetime, Many-Reads : SLC\na2: W---W-R-W---W-W----W : T20W06R01 : Short-Lifetime, Few-Reads : SLC, DRAM?\na3: W-RRRR-W---RR-W-R-RW : T20W04R08 : Mid-Lifetime, Many-Reads : MLC\na4: W------W--R---W----W : T20W04R01 : Mid-Lifetime, Few-Reads : MLC\na5: W-R-R-R-RR-RRRR----R : T20W01R10 : Long-Lifetime, Many-Reads : MLC\na6: W------------R----R- : T20W01R02 : Long-Lifetime, Few-Reads : TLC\n</pre>\n\n=== Idle-time Prediction Based Free Space Management in SSD ===\n\n* Idle-time의 도래 시기와 지속 시간 패턴 분석에 기반한 SSD 내의 Free Space 확보 방법\n:- SSD 내에서 Low overhead (in computation, memory space)로 Idle-time 도래 시기 및 지속 시간 분석이 가능한가?\n:- 가능하다면, SSD 내에서 Garbage Collection 등으로 Free Space를 확보하는 데 있어서 어떤 장점을 가질 수 있는가?\n\n\n\n: 이를 기반으로 \n\n\n\n=== 요약 ===\n* \n\n\n=== 기술 상세 ===\n\n*\n\n\n\n\n\n== _BPT-007 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== _BPT-008 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== _BPT-009 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Ef?cient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== _BPT-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== _BPT-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== _BPT-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== _BPT-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== _BPT-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== _BPT-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== _BPT-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== _BPT-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n----\n=== (SmartSSD API) SSD-internal I/O pattern logging interface and mechanism ===\n\n\n==== Questions ====\n\n* SmartSSD에서 제공할 수 있는 logging 서비스로서 어떤 것들이 있을 수 있나?\n:- 어느? 정도의 free space를 필요로하는 critical (heavy) I/O가 어떤? 주기로 도래할 지 알 수 있을까?\n:- I/O prediction에 도움될 수 있는 정보를 SmartSSD가 기록했다가 필요한 때 (안전한 방법으로?) 줄 수 있을까?\n:- I/O prediction까지는 아니더라도 해당 SSD의 I/O wellness(어떻게 정의?)를 측정했다가 bottleneck을 회피하는 데 유용한 정보를 제공할 수 있을까?\n:- 어떤 SSD가 현재 주어지고 있는 I/O workload에 대해 list performance spec을 만족하는데 어려움을 겪고 있다는 것을 알려줄 수 있는 정보가 어떤 것이 있을까? (SSD 내 write buffer의 fullness 혹은 I/O queue의 유동성? amount of free space in over-provisioning area?) (-> I/O wellness 라는 metric을 이것과 연관시켜 정의할 수도 있으며, 결국 SLA 혹은 Performance QoS와 연관시킬 수 있음)\n:- 지금까지의 S.M.A.R.T.와 비교하였을 때, 어떤 차별점이 있는가? 기존의 S.M.A.R.T.가 제공하지 못하던 타입의 정보를 제공하고, 이로 인해 기존에는 불가능했던 새로운 알고리즘 구현 혹은 새로운 서비스 제공이 가능해짐을 보이면 좋겠음.\n\n\n\n:- workload의 history?\n:- 각 erase block이 erase되어야만 했던 주기?\n:- free space의 추이 (얼마나 많이 남아 돌던지)?\n:- I/O의 heaviness 패턴?\n:- SSD내 write buffer (혹은 write cache)의 utilization정보? (즉, write buffer가 넘칠 정도가 되어서 I/O wait을 해야만 했던 상황이 얼마나 자주 발생했는지, write buffer의 가득찬 정도를 백분율로 표현한다고 했을 때, 평균/표준편차 등으로 대표할 수 있는 normal distribution을 따르는지?\n:- 전체 sequential read/write \n\n\n==== 배경 ====\n\n* 각 SSD 모델별로, 또 동일한 모델의 SSD라 하더라도 어떤 workload에 노출되어왔는지에 따라, 현재 낼 수 있는 I/O performance가 동일하지 않을 수 있다 (!check! 관련 실험 결과 - SSD 830을 이용하여 같은 시간 동안 하나의 host에서 생성되는 workload들을 나누어 받았다 하더라도 내부적인 free block의 갯수 및 random write에 의한 block 내 파편화등의 상태가 상이하여 성능에 차이가 나는 것을 보여줄 것. uFLIP 혹은 filebench 활용 가능). 한 예로, SSD가 어느 호스트의 어느 file system에 매핑되었는지에 따라 그 SSD가 받는 workload의 특성이 다를 수 있다. file system mount point가 달라지거나, 기존과 다른 새로운 응용이 설치/서비스되는 상황이 되면 \n\n\n----\n=== I/O Prediction-based Proactive Data Placement ===\n\n* 미래에 access될 data를 예측하여 적소에 미리 가져다 놓는 기술로서, (1) I/O prediction을 위한 정보 수집 모듈, (2) 주어진 정보들에 기반하여 다가올 I/O를 예측하는 모듈, (3) data placement를 수행하는 모듈로 구성됨.\n\n\n* 본 기술의 특징 1: I/O를 triggering하는 indicator (전조) 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Run-time에 저비용으로 감지될 수 있는 Indicator 정보와, 이에 의해 access될 data 간의 mapping은 Bayesian Inference 이용\n\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n\n----\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n----\n\n=== Tiering Method Avoiding I/O Bottleneck ===\n\n* I/O Prediction에 의해 예측된 미래의 Hot Data를 Proactive하게 Placement한다고 할 때, 예상되는 access pattern으로 인해 발생 가능한 I/O Bottleneck을 최소화할 수 있도록 배치하는 방법\n\n* 예를 들어 미래의 hot data block 1 (200MB), hot data block 2 (100MB), hot data block 3 (400MB)이 1분 후에 access 될 것으로 예측되었다고 가정. data block 1에 대해서는 4KB 단위로 random read가 dominant한 access pattern이 예상되고, data block 2에 대해서는 1024KB 단위의 sequential read가 dominant한 access pattern이 예상되고, data block 3에 대해서는 4KB 단위의 random read와 512KB 단위의 sequential write이 각각 80:20의 비율로 mix된 형태의 access pattern이 예상된다고 했을 때, 각 data block을 어디에 어떤 형태로 placement시킬 것인가?\n\n: machine learning 혹은 pattern mining에 기반한 base workload pattern data를 기반으로 incremental하게 update될 수 있는 data access의 spatial locality와 periodicity\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n\n* 기존 기술과의 대비 (naive I/O prediction vs. full-fledged I/O prediction)\n: 기존 기술은 누적 hit count 정보에 의지하는 naive I/O prediction으로 볼 수 있음. 이러한 방식은 workload의 변화를 제대로 반영하지 못하기 때문에, static한 workload에 대해서는 잘 동작하지만, moving target처럼 dynamic한 workload에 대해서는 오히려 과거의 누적 hit count metric이 현재의 workload 패턴을 왜곡하는 문제점을 가지고 있음. (temporal locality와 spatial locality가 깨지는 순간이 여러번 존재할 수록 기존의 naive I/O prediction 방식으로 성능 향상은 어려워짐)\n\n\n\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== _BPT-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(286,'== _BPT-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== _BPT-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== _BPT-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n[[Bnote patidea 2013-003]]\n\n\n== _BPT-004 ==\n\n=== Event-based I/O Prediction (for Proactive Data Placement) ===\n\n=== 요약 (4) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n=== 기술 상세 (4) ===\n\n* I/O 발생 메커니즘의 모델링\n: I/O prediction을 위해, type-I event (indirect I/O trigger)와 type-II event (direct I/O trigger)를 관찰 및 분석함. Type-I event의 예로서, user의 log-in, 기 log-in되어 있던 user session의 상태 변경 (idle-to-busy), system service에 대한 invocation from local or remote node 등이 포함될 수 있으며, type-II event의 예로서, user application 의 실행, system service의 실행 등이 포함될 수 있음. Type-I event는 type-II event의 발생에 영향을 미치고, type-II event는 I/O request event (data access)의 발생에 영향을 미치므로, \'type-I event\' -> \'type-II event\' -> \'I/O request event\' 까지 이어지는 인과사슬 (cause-effect chain) 기반으로 I/O 발생 메커니즘을 모델링하였음.\n\n\n* Event 정보 수집: type-I event 정보 및 type-II event 정보 정의 및 수집 방법.\n\n\n* I/O 패턴 학습 (Offline Learning): 수집된 Event 정보를 기반으로 I/O 패턴을 learning하는 방법\n\n\n* Run-time I/O 예측 (Online Matching): 시스템 모니터링에서 얻어지는 최근 및 현재의 event들과 learning으로 얻어진 I/O 패턴 DB에 기반하여 Run-time으로 미래 I/O 예측\n\n\n* 시스템 구조: 정보 수집 및 패턴 학습 모듈의 시스템 내 위치 (I/O 패턴 DB 등의 위치 및 실시간 예측 모듈과의 관계)\n\n\n* 순서도: 수집부터 학습까지 전체적인 task의 흐름\n\n\n* 예상 효과: 성능적 측면에서 사용자가 체감하는 lower latency, higher throughput 효과, 이미 발생해버린 congestion time을 피해 미리 적절한 곳에 data를 배치시킴으로써 얻어지는 bottleneck prevention 효과.\n\n=== Data Collection (4) ===\n\n* 역할: I/O 패턴 학습/발굴에 필요한 정보 수집\n\n* 수집 정보 종류\n: 전조 기반 예측에 필요한 간접 I/O 유발자, 직접 I/O 유발자, I/O 발생 이벤트 등 I/O 예측에 도움이 될 수 있는 다양한 정보를 수집. 다음 정보들이 포함될 수 있음.\n:- indirect I/O trigger trace: {(user-id | user-type), status-type, before-status, after-status, event-time}\n:- direct I/O trigger trace: {(program-id | program-type), status-type, before-status, after-status, process-owner, event-time [, process-current-working-directory, files-opened]}\n:- I/O trace: {access-time, address-accessed, read/write-flag, io-size, file-path, process, co-accessed-addresses}\n\n\n* 수집 모듈의 위치\n: storage-server가 분리되어 있는 구조인 SAN/NAS 타입의 architecture인 경우에는 application이 구동되는 server-side에 정보 수집 모듈이 위치한다. storage-server가 결합되어 있는 converged-architecture 기반의 system인 경우에는 해당 system 내에 정보 수집 모듈이 위치한다.\n\n\n* 정보 수집 방식\n: 운영체체가 제공하는 시스템 상태 정보 제공 interface를 이용하거나, blktrace, lsof, debug file system 등 이용 (e.g., /sys/kernel/debug/ 등) 혹은 /proc file system 등을 이용 가능.\n\n== _BPT-005 ==\n\n=== Coaccess-based I/O Prediction (for Proactive Data Placement) ===\n\n* (영문) Coaccess-based I/O Prediction (for Proactive Data Placement)\n* (한글) Coaccess 기반의 I/O 예측 방법 및 시스템 구조\n\n=== 요약 (5) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 coaccess-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n\n\n* 소정의 Confidence 값 이상인 X=>Y 관계를 활용하여, signature X (Y에 대한 precursor)가 access되면 어느 정도 확률로 이후에\n\n=== 발명의 이용 분야 (5) ===\n\n데이터를 저장하고 처리하는 일반적인 컴퓨팅 시스템으로서,\n* 프론트엔드/백엔드 서버\n* 네트워크 스토리지 시스템\n* 컨버지드 서버-스토리지 시스템\n* 엔터프라이즈 혹은 데이터 센터 내의 IT 인프라\n등에서 활용 가능.\n\n\n=== 발명의 목적 (5) ===\n\n\n* 기존 I/O 예측 방식의 한계: Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n\n* 본 발명의 필요성/의미\n: 본 발명은 data access 정보 기반의 I/O prediction 기술로서, client-side의 rich information을 확보가 제한적인 경우에도 periodicity 및 coaccessness 특성을 기반으로 미래에 access될 data의 위치와 시점을 예측할 수 있도록 하는 기술임.\n\n=== 기술 상세 (5) ===\n\n* Data의 coaccess 특성에 기반한 proactive data placement 기법\n: 소정의 time window 내에 같이 access되는 확률이 높은 data 그룹 (coaccess group)이 있을 때, 그 그룹 내의 일부 data가 access되면 나머지 data들도 미리 fast media에 가져다 놓음으로써, 성능 향상을 얻을 수 있게 하는 방법임.\n\n\n* 개본 개념 정의 (coaccess property, coaccessness, coaccess group)\n: 지정된 시간 동안의 measurement window 내에서, 어떤 data가 access될 때 함께 access되는 경향을 가지는 다른 data들이 존재할 때 그 data들 간에 coaccess property 혹은 coaccessness가 있다고 정의한다.  이때, coaccess property를 가지는 data 그룹을 coaccess group이라고 한다. frequent item set mining 관점에서 보면 기준이 되는 support 값 이상의 확률로 관측되는 transaction들을 coaccess group으로 정의할 수 있다.\n\n\n* 추가 개념 정의 (merged coaccess group)\n: 서로 다른 coaccess group들을 개별적으로 다루기보다 이들을 묶어서 하나의 coaccess group으로 만들어 다룸으로써 meta data management의 효율성 및 proactive data placement의 효과 측면에서 장점이 발생하는 경우가 있다. 이렇게 만들어지는 coaccess group을 merged coaccess group이라 정의한다. \n: 서로 다른 coaccess group CG_A와 coaccess group CG_B가 있는데, CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합) 시켜서 새롭게 얻어진 CG_C = {CG_A, CG_B}를 하나의 coaccess group으로 다룬다.\n: computing resource의 한계로 인한 현실적인 문제로 인해 모든 경우의 frequent item set을 발굴하기 어려운 경우가 있음 (Frequent item set mining 시 실제로 maximum size of transaction을 한계 값으로 지정). merged/unionized coaccess group은 이러한 한계적인 상황을 극복할 수 있는 방법으로서 제안되는 개념임. (실제로 관측되었던 실험 data 및 graph를 이용하여 실재 근거를 제시할 것)\n\n<!--\nmerged coaccess group 관련해서.\n\n: CG_A = {a1, a2, ..., aM |supp=s1}, CG_B = {b1, b2, ..., bN |supp=s1}, CG_C = {union of CG_A and CG_B |supp=s1} 이 존재하고 발생 시기가 일치하는 것을 확인하였다면, CG_A, CG_B, CG_C 를 모두 개별적인 coaccess group으로 관리할 것이 아니라, CG_C 만을 coaccess group으로 관리한다. (조건 i) supp(CG_A) == supp(CG_B) == supp(CG_C), (조건 ii) union of CG_A and CG_B is identical to CG_C, (조건 iii) occurrence_time(CG_A) == occurrence_time(CG_B) == occurrence_time(CG_C) 이 있다고 하자. 상기 조건 i, ii, iii이 모두 만족되는 경우에는 CG_C 만을 coaccess group으로 관리하고 지정함으로써, coaccess group의 meta data management overhead를 감소시키고 coaccess-based proactive data placement의 operation efficiency를 증대시킴. /* 여기서, (조건 i)과 (조건 ii)가 만족되면 굳이 (조건 iii)까지 확인할 필요는 없을 수도 있음. 확인 필요 */\n* GCD (Greatest Common Divisor) / LCM(Least Common Multiple) / largest coaccess group 개념과 관련.\n\n* 유사 coaccess group 간 merging?\n: 예를 들어 coaccess group CG_A = {\'a\', \'b\', \'c\', \'d\'} 와 coaccess group CG_B = {\'b\', \'c\', \'d\', \'e\'} 가 있다고 했을 때, \'b\', \'c\', \'d\' data 가 겹치기는 하지만, CG_A와 CG_B는 동일한 coaccess group으로 볼 수는 없다. 그러나 만약 CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 (기준이 되는 확률 값은 시스템과 workload의 상황에 따라서 적절히 조절 가능) CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합)화 하여 단일 coaccess group으로 다루는 방식. (말이 되는 소리인지 확인 필요)\n-->\n\n\n* frequent item set mining을 활용한 coaccess group 발굴 방법\n: Coaccess group 발굴을 위해 frequent item set mining을 이용할 수 있으며, 구체적인 방법은 다음과 같다.\n: (1) 시스템의 resource 상황에 따라 적절한 support 값을 설정한다. 예를 들어 fast media (cache 혹은 tier 1 storage)의 저장 가능 용량에 따라, 얼마나 높은 등급의 hot data까지 fast media에 placement 시킬 수 있을지가 결정될 수 있다. 즉 fast media의 용량이 매우 넉넉하여 10번 중 2번의 빈도로 (support=0.2) 관찰되는 coaccess group 까지 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있고, 혹은 fast media의 용량이 여유롭지 않아서 10번 중 5번 이상의 빈도로 (support=0.5) 관찰되는 coaccess group까지만 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있다. support 값은 상황에 따라 dynamic하게 설정될 수 있으며, 시스템 관리자 혹은 data scientist등 사람에 의해 수동적으로 설정될 수도 있지만 algorithm에 의해 automatic하게 설정되도록 구현될 수도 있다.\n: (2) coaccess group에 대한 단위 관측 시간이 되는 transaction measurement window (TMW) 크기를 설정한다. transaction measurement window의 크기에 따라 coaccess group에 포함될 수 있는 data의 갯수가 달라질 수 있다. transaction measurement window 크기가 너무 작으면 함께 access되는 것으로 판단될 대상이 되는 data 갯수 자체가 감소하게 되므로 coaccessness 분석이 어렵게 된다. 반대로, transaction measurement window 크기가 너무 크면 coaccess group의 종류는 감소하고 각 coaccess group의 크기는 커지는 경향이 생기는데, 이 경우의 문제점은 data placement 시 fast media를 효율적으로 사용하는 것이 어려워질 수 있다는 것이다.\n: (3) access되는 data address 정보를 기반으로 frequent item set mining을 시작한다 (이를 위해서는 support 값과 transaction measurement window size 값이 설정되어 있어야 함).  transaction measurement window 동안 access 되는 address들의 집합을 하나의 transaction (basket)으로 묶는다. 전체 관찰 기간 동안 N 번의 transaction measurement window가 있었다면, N 개 만큼의 transaction이 만들어진다. 이 때 N 개의 transaction을 통틀어 support 값 확률 이상으로 빈번하게 등장하는 item set이 있다면 그것을 coaccess group으로 지정하고 coaccess group management ID를 부여, 관리한다.\n: (4) frequent item set mining에서의 confidence 개념을 도입하여 precursor의 존재 감지 시, 미래에 access되게 될 나머지 data들을 신속하게  (일부 데이터가 access되고 나면 나머지 데이터들도 지정된 시간 내에 함께 access될 확률을 의미하게 됨)\n: (4) run-time 시, coaccess group의 빠른 속도로 찾아낼 수 있도록 하기 위해, coaccess group \n\n\n : coaccess group 발굴이 되었다면, 어떻게 저장하고, 어떻게 불러내서(매칭시켜서) proactive data placement에 활용할 수 있게 할 것인가?\n\n\n* O(1)으로, 매칭되는 패턴을 찾을 수 있게 하려면?\n\n\n* proactive data placement를 위해 \n\n\n* Coaccess group pattern matching 방법\n\n\n* Fig{association graph, temporal occurance graph}\n: Frequent item set mining 분석 결과로 얻어진 association graph 및 시간에 따른 frequent item 발생 추이 graph를 이용하여 coaccess pattern의 실재함을 설명\n\n\n* Coaccess property는 다음과 같은 방식으로 활용될 수 있음\n:- 특정 coaccess pattern이 특정 workload에 대한 precursor가 되는 경우가 있을 수 있음. 이를 이용하여 다가올 workload의 prediction에 활용 가능 (예를 들어, coaccess group A가 access된 후 T1 시간 경과 후에 address range B를 random read하는 workload가 발생하고, T2 시간 경과 후에 address range C를 sequential write하는 workload가 오는 경향이 있다는 것을 알고 있다면, 이 정보를 기반으로 proactive하게 address range B를 fast media에 placement 시키고, address range C에 대해서는 caching  \n:- coaccess group 내의 일부 data가 access되면 그 coaccess group 내의 다른 data들을 미리 fast media에 가져다 놓음으로써 성능 향상 가능\n\n<!--\n* (for event-based I/O prediction) coaccess group이 access되는 순서를 event-based I/O prediction에 이용할 수도 있음. 예를 들어, coaccess group A가 access되고 나서 T1 시간 내로 coaccess group B가 access될 확률이 80% 이상이라고 하고, coaccess group B가 access되고 나서 T2 시간 내로 coaccess group C와 coaccess group D가 access될 확률이 90% 이상이라고 한다면, 이러한 정보를 기반으로 ... 무엇을 할 수 있나?\n-->\n\n\n* 이를 위해서 기존의 storage system 구조에 (1) coaccess pattern analysis engine, (2) coacccess pattern aware data placement controller가 추가되는 것이 필요하다.\n\n\n* coaccess pattern analyzer의 역할: workload 별로(???) coaccess pattern list 구축\n\n\n* coaccess pattern analyzer의 구조 및 동작 방식:\n\n\n* coaccess pattern aware data placement controller의 역할:\n\n\n* coaccess pattern aware data placement controller의 구조 및 동작 방식: coaccess pattern 정보를 받아들이기 위한 interface, interface로 받아들인 coaccess pattern 정보의 queuing? data placement의 I/O 단위 (granularity), data placement의 주기 (static? or dynamic?) 등 coaccess pattern 정보를 data placement 시 어떤 식으로 활용하는지\n\n=== Coaccess-based I/O Prediction의 성능 향상 효과 Estimation ===\n\n* 시뮬레이션 기반의 성능 향상 효과 비교\n:- Fast media의 4KB random read latency는 96.47 usec (SSD 830 128GB 기준), slow media의 4KB random read latency는 1,884.03 usec (HDD 7,200rpm 2TB 기준)으로 설정 (iozone에 의한 실측값 사용)\n:- 실험 시작 전에는 4GB 크기의 target data가 slow media (HDD)에 저장되어 있음.\n:- 여기에 1,000,000 개의 4KB random read request로 구성되는 workload를 인가. (각 read request들은 서로 중복되는 data를 가리키지 않으며, 4GB 크기의 target data를 한 번씩 다 request하게 된다고 가정)\n:- Workload 패턴 학습 결과, 4GB의 전체 data 중, \'가\' type의 coaccess 특성을 가지는 10MB 크기의 data 그룹 (data set 당 2,500개의 4KB data 포함)이 200개가 있으며, \'나\' type의 coaccess 특성을 가지는 1MB 크기의 data 그룹 (data set 당 250개의 4KB data 포함)이 1,000개 있는 것을 발견한 것으로 가정.\n:- \'가\' type의 coaccess 패턴은 해당 data 그룹 내 100개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 80%인 것으로 정의.\n:- \'나\' type의 coaccess 패턴을 해당 data 그룹 내 50개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 100%인 것으로 정의.\n\n\n\n\n본 기법을 적용한 경우와 그렇지 않은 경우의 성능 차이를 IOPS로 비교해보면 다음과 같다. \n\n: 예를 들어 설명하면 다음과 같다.\n: data 1, data 2, ..., data 10000으로 구성된 data set이 있으며, 각 data들의 크기는 4KB이다. 학습을 통해서, (1) 이 중 어느 한 data가 access되었을 때, 1분 이내에 나머지 data들도 같이 access되게 될 확률은 55%, (2) 이 중 2개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 80%, (3) 이 중 3개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 95%라는 것을 알고 있다고 가정한다.\n: fast media의 크기는 4GB 이며, 기존에 \n\n이라는 것을 알고 있다면, data 1, 2, 3이 access되었을 때 data 4~128까지도 미리 proactive하게 fast tier에 배치시킴으로써, 그로 인한 성능 이득을 얻을 수 있게 된다. (!!!실제 실험 사례?!!!)\n\n=== Real Data Analysis (MSN File Server) (5) ===\n\n* Analysis Target Workload Specification\n:- MSN FileServer Tracelog (2008-03-10, 약 5GB 데이터)\n:- 21720977703 microseconds (약 6시간)\n:- 29345085 IO operations / 21720 sec => 1351 IOPS\n\n\n* 첫 16.5 minutes (990 seconds) 동안의 I/O에 대해 10 seconds 단위의 Measurement Window로 측정\n\n\n* Frequent Item Sets Mining (Eclat algorithm)\n: support=0.3, minlen=1, maxlen=15\n <pre>\n> fi_s3 <- eclat(ciop_data, parameter=list(supp=0.3, maxlen=15, minlen=1, tidLists=T))\n\nparameter specification:\n tidLists support minlen maxlen            target   ext\n     TRUE     0.3      1     15 frequent itemsets FALSE\n\nalgorithmic control:\n sparse sort verbose\n      7   -2    TRUE\n\neclat - find frequent item sets with the eclat algorithm\nversion 2.6 (2004.08.16)         (c) 2002-2004   Christian Borgelt\ncreate itemset ...\nset transactions ...[1146629 item(s), 100 transaction(s)] done [1.31s].\nsorting and recoding items ... [21 item(s)] done [0.12s].\ncreating bit matrix ... [21 row(s), 100 column(s)] done [0.00s].\nwriting  ... [28761 set(s)] done [0.01s].\nCreating S4 object  ... done [0.25s].\n\n\n\n\n> summary(fi_s3)\nset of 28761 itemsets\n\nmost frequent items:\n  22159360   22155264 1504849920 3170566144 4086710272    (Other)\n     14372      14370      14336      14336      14336     123094\n\nelement (itemset/transaction) length distribution:sizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13\n  21  168  696 1951 3933 5808 6336 5115 3025 1276  364   63    5\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n  1.000   6.000   7.000   6.775   8.000  13.000\n\nsummary of quality measures:\n    support\n Min.   :0.300\n 1st Qu.:0.330\n Median :0.340\n Mean   :0.388\n 3rd Qu.:0.430\n Max.   :0.920\n\nincludes transaction ID lists: TRUE\n\nmining info:\n      data ntransactions support\n ciop_data           100     0.3\n> dim(tidLists(fi_s3))\n[1] 28761   100\n\n</pre>\n\n\n=== Brief Description of the Drawings (5) ===\n\n# [Concept] Coaccess-based I/O Prediction Concepts\n# [Architecture] Storage System Architecture with Coaccess-based Proactive Data Placement\n# [Concept] Coaccessness Analysis using Frequent Itemset Mining\n# [Flowchart] Proactive Data Placement based on Coaccessness\n# [Auxiliary] Experiment Results 1\n# [Auxiliary] Experiment Results 2\n# [Auxiliary] Performance Gain Estimation\n# [Auxiliary] Comparison with prior arts\n\n== Memo for {_BPT-004, _BPT-005} ==\n\n\n=== Think outside the box // Questions ===\n\n* 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n\n* Tiering을 좀 더 다양한 각도에서 바라보자\n:- 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 것이 기존의 tiering 개념임.\n:- SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n:- SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n:- access pattern-aware optimal placement (APOP) - 이런 것은 어떨까?\n:- Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n\n* write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반의 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지?\n\n\n* 만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? SSD-internal memory buffer를 이용해서 SSD를 NVRAM처럼 쓰도록 하는 것이 실제로 말이 되는 이야기일까?\n\n\n\n\n\n=== 기존 기술의 한계점 (Intelligent I/O Prediction 기술의 필요성) ===\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n\n <pre>\n실제 tracelog 사례를 보여주고 hot area가 변화하고 있음을 보일 것.\n</pre>\n\n* Data Placement (Tiering) 관점에서 본 EMC의 FAST, FAST-VP 기술의 한계점\n:- Coarse-grained Data Migration (768KB 까지 작게 나눌 수 있다고는 하지만, 실제로 그렇게 해서 성능이 얼마나 잘 나오는지는 확인 필요. 귀중한 자원인 DRAM을 관리를 위한 metadata로 가득 채우게 될 위험도 있음)\n:- Flexible하지 않은 Tiering 주기\n:- Busy-hour 지난 후 Idle-time에 Tiering 실시\n:- 낮은 지능의 미래 I/O 예측 능력\n:: data access count를 기반으로 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두도록 하는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n\n=== 본 발명의 차별화 포인트 ===\n\n* 본 기술의 특징 1: I/O를 triggering하는 전조 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Indicator는 Run-time에 저비용으로 감지될 수 있는 형태의 정보들로 구성\n:- 미래에 access될 data를 예측케 하는 Indicator간의 mapping은 Bayesian Inference 이용\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n=== 특허 검색/분석 결과 {storage I/O workload prediction} ===\n\n* 검색식: storage I/O workload prediction\n\n* http://portal.uspto.gov/pair/PublicPair 에서 특허 상태 확인 가능.\n\n* I/O prediction을 기반으로 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n:- 기존엔 proactive tiering이 없었나? -> IBM 특허가 나름 유사함. (US7930505, US7512766)\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n==== 1. 비교가 될만한 특허들 ====\n\n예측에 대한 언급이 되어 있는 스토리지 시스템 관련 특허들. 크게 두 가지로서, 그 중의 하나는 가상 volume에 대해 peak I/O가 발생하는 주기를 분석하여, peak I/O 발생시점이 닥치기 전에 미리 가상 volume의 data를 migration 시키는 방법에 대한 것이나, 어느 위치의 data가 언제 access될지에 대한 예측은 아님 (IBM 특허). 다른 하나는 I/O 예측을 기반으로 미래에 자주 access될만한 data를 미리 여러 노드에 분산시키는 방법에 대한 것이나, 예측 방법 자체에 대한 내용은 다루고 있지 않음 (HP 특허)\n\n\n; US7930505, US7512766 (IBM)\n\n* US7930505 (Application Number: US 12/053582), \"Controlling preemptive work balancing in data storage,\" IBM, 특허일 2011-04-19\n: storage network의 I/O 관찰을 통해 peak I/O activity의 repeating instance에 대한 signature를 VLU (virtual logical unit) 단위로 생성하고, 이에 기반하여 언제 heavy I/O가 도래할 것인지를 예측하여, heavy I/O가 오기 전에 미래 data를 faster media로 migration하도록 하는 tiering 기술에 대한 특허. (이미 I/O peak time이 온 후에 hot data를 옮기는 것은 오히려 시스템의 IO성능을 악화시킬 수도 있기 때문에 미리 migration을 하고자 하는 것임)\n: VLU 단위로, peak I/O activity가 어느 주기로 발생할지에 대한 예측임. 지능 수준과 Fine-granularity 측면에서 초보적인 수준임.\n\n* US7512766 (Application Number: US 11/213494), \"Controlling preemptive work balancing in data storage\", IBM, 특허일 2009-03-31\n: 위의 US7930505 특허와 동일 건으로 보임\n\n\n; US6912635 (HP)\n\n* US6912635 (Application Number: US 10/140968), \"Distributing workload evenly across storage media in a storage array\", HP, 특허일 2005-06-28\n: 미래에 자주 access될만한 data를 여러 arrayed storage device에 분산시키겠다는 기술. 그러나 I/O에 대한 예측 방법은 다루어지고 있지 않음.\n\n==== 2. 검색은 되었으나 관련은 없는 특허들 ====\n\n무언가를 예측한다는 측면, Storage System에 대한 기술이라는 측면에서는 관련이 있으나, 미래에 Access될 Data를 예측한다는 측면에서 제안 기술과 관련 없는 특허들\n\n\n; US7640231 (IBM)\n* US7640231 (Application Number: 11/280012), \"Approach based on self-evolving models for performance guarantees in a shared storage system\", IBM, 특허일 2009-12-29\n: network 기반 storage system에서, average latency 혹은 I/O request rate 등에 대한 주기적인 performance sampling을 기반으로, 주어진 objective function의 값을 최대화할 수 있도록 system resource를 제어할 수 있게 하는 기술.\n\n\n; US7774491, US7519725 (IBM)\n\n* US7774491 (Application Number: 12/359377), \"Utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2010-08-10\n: 복수의 클라이언트들을 대상으로 통계적인 성능 QoS를 보장하기 위한 informed throttling 방법\n: A system for utilizing informed throttling to guarantee quality of service to a plurality of clients\n\n* US7519725 (Application Number: US 10/444712), \"System and method for utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2009-04-14\n: resolve the problem of providing statistical performance guarantees for applications generating streams of read/write accesses (I/Os) on a shared, potentially distributed storage system of finite resources, by initiating throttling whenever an I/O stream is receiving insufficient resources\n\n\n; US7827283 (IBM)\n* US7827283 (Application Number: 10/367444), \"System for managing and controlling storage access requirements\", IBM, 2010-11-02\n: SAN 등의 환경에서, storage bandwidth를 관리하고 reserve하는 기술\n: A Resource Management and Reservation System (RMRS) for managing and reserving storage bandwidth, is a platform independent middleware layer that provides an interface to applications, their database management systems, or some other higher level data management systems like ADRS which does data management on behalf of the applications\n\n\n; US8433554, US8140319 (IBM)\n\n* US8433554, \"Predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2013-04-30\n: An approach for predicting performance and capacity of an information technology (IT) system before the IT system is built is described, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n* US8140319, \"Method and system for predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2012-03-20\n: A method and system for predicting performance and capacity of an information technology (IT) system before the IT system is built, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n\n\n; US8255915 (HP)\n* US8255915, \"Workload management for computer system with container hierarchy and workload-group policies\" HP, 특허일 2012-08-28\n\n\n; US7050956 (HP)\n* US7050956 (Application Number: US 09/843930), \"Method and apparatus for morphological modeling of complex systems to predict performance\", HP, 특허일 2006-05-23\n: I/O 성능 예측을 위한 complex storage system에서의 data flow 모델링 방법\n\n\n; US6718434 (HP)\n* US6718434 (Application Number: US 09/871043), \"Method and apparatus for assigning raid levels\", HP, 특허일 2004-04-06\n: Data를 disk array에 load하기 전에, workload 명세 및 device 명세에 기반하여, 적합한 RAID level을 판단하여 disk array에 할당 (RAID build)하는 방법\n\n\n; US8112586 (EMC)\n* US8112586 (Application Number: US 12/190071), \"Predicting and optimizing I/O performance characteristics in a multi-level caching system\", EMC, 특허일 2012-02-07\n: multi-level caching 시스템에서, cost대비 performance를 최적화할 수 있도록, workload에 따른 resource allocation design에 대한 recommendation 기술 (multi-level cache들 간의 workload propagation을 예측하는 것이 핵심)\n\n\n; US6629266 (IBM)\n* US6629266 (Application Number: US 09/442001), \"Method and system for transparent symptom-based selective software rejuvenation\", IBM, 특허일 2003-09-30\n: software가 언제 동작이 멈추는지를 예측함으로써, software의 dependability를 향상할 수 있게 하는 방법\n\n\n; US5088058 (Unisys)\n* US5088058 (Application Number: 07/237120), \"Apparatus and method for evaluating and predicting computer I/O performance using I/O workload snapshots for model input\", Unisys Corporation, 특허일 1992-02-11\n: I/O delay 등에 대한 확률적 모형을 기반으로 disk 시스템의 성능을 검증하고 예측하는 시뮬레이션 기반의 방법\n: A method of evaluating and predicting the performance of an I/O disk system comprised of one or more disk controllers and cooperating disk units using a simulation model containing a stact constructed such that delays are formulated as sets of stochastic processes\n\n=== Comparison of I/O Prediction Approaches ===\n\n미래에 access될 data의 위치와 시기를 예측하기 위한 방법으로 event-based I/O prediction, coaccess-based I/O prediction, periodicity-based I/O prediction 방식이 제안되며, 각각의 방식은 예측에 필요한 연산량 및 input data의 richness, 그리고 workload 변화에 대한 적응성 측면에서 장단점을 가지고 있음. 따라서, 상황에 따라서 적절한 방식을 이용하면 됨.\n\n{| style=\"border-collapse: collapse; border: 1px solid #000\" border=1\n| style=\"width: 25%\" | 비교 항목\n| style=\"width: 25%\" | periodicity-based I/O prediction\n| style=\"width: 25%\" | coaccess-based I/O prediction\n| style=\"width: 25%\" | event-based I/O prediction \n|-\n| context richness required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| computation overhead required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| adaptability to workload changes (lower is better)\n| high\n| high\n| high\n|-\n|}\n\n\n* 전조 기반 예측 vs. 주기성 기반 예측\n{| border=1\n| 비교 항목\n| 전조 기반 예측\n| 주기성 기반 예측\n|-\n| 기술 요약\n| 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식\n| address 별 access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식\n|-\n| 분석 오버헤드\n| High (학습/마이닝에 드는 비용임)\n| Low (주기성 파악 비용임)\n|-\n| Run-time 오버헤드\n| Mid (실시간 전조 모니터링 및 매칭 비용)\n| Low (주기성 계산)\n|-\n| Workload 변화 적응성\n| High\n| High (Workload 변경 사실 알림 hint 필요)\n|-\n| ...\n| ...\n| ...\n|-\n| Incrementality of Mining/Learning\n| ...\n| ...\n|}\n\n* 전조 기반 예측은, 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식임.  주기성 기반 예측은, data access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식임.\n\n\n=== Intelligent Data Placement ===\n\n* Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case\n: 여기에 HML이 사용될 수 있는 것이라면, 어떤 방식으로 어떤 이유로 그런 것일까?\n\n\n* I/O 예측 방식으로서 다음 방식이 제안되며, 각 방식에 따라서 prediction model, 수집되어야 하는 context data, I/O 패턴 DB에 저장되는 정보의 종류 및 형태가 달라질 수 있음. 예측에 이용할 수 있는 context 정보의 richness에 따라 적절한 방식을 사용할 수 있다. (서로 다른 발명이므로, 별도의 특허로 낸다)\n:- event-based I/O prediction\n:- coaccess-based I/O prediction\n:: 여기서, periodicity-based I/O prediction은 기존 특허에서 언급되었으므로, 본 발명에서는 크게 언급하지 않기로 한다.\n\n\n* I/O prediction 결과에 따라 적절한 위치에 data를 배치하게 되는데, 여기서 적절한 위치라 함은 SSD와 같은 fast tier가 될 수도 있겠으나, sequential read/write 같은 경우 striping된 HDD array가 될 수도 있고, 혹은 SAS HDD가 될 수도 있겠음. 또한 SSD 중에서도 SLC, MLC, TLC 로 구분되는 fine-grained fast tier로 구분될 수도 있음\n\n\n* 미래에 오게 될 I/O의 type도 알 수 있을까? 즉, {random/sequential}x{read/write} type 중 어느 것에 해당되는지를 안다면, 그리고 I/O의 intensiveness도 예상할 수 있다면 그에 맞춰서 placement 시 적절한 tier 혹은 적절한 node로 찾아가게 할 수 있을 것임.\n\n\n* context 정보는, 수집되는 위치에 따라, server-side에서 수집되는 context data와 storage-side에서 수집되는 context data로 분류될 수 있음. Event context data는 server-side에서 수집 가능하며, coaccess context data 및 periodicity context data는 storage-side에서 수집될 수 있음.\n\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n: 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n\n=== Design/Implementation Issues ===\n\n* workload type/class tagging 필요\n\n* cache와 tiering을 같이 쓸 때, DRAM cache가 아니라 Flash Cache의 경우 쓸데없이 모든 data가 Flash Cache에 다 들어가게 두기 보다는, tiering에서 cover하지 못하는 것들 위주로 caching하도록 하면 Flash Cache의 공간 낭비를 줄임으로써, 유효 cache 공간을 늘일 수 있게 되고, 그만큼의 성능 향상으로 연결될 수 있음.\n\n* DRAM 기반의 write cache를 적극 이용하여 마치 log-structured file system에서 제공하는 I/O merging and sequentialization 효과를 얻을 수 있음.\n\n* Tiering되는 data는 caching되는 data보다 상대적으로 coarse-grained하게 grouping하여 다룸으로써, 한꺼번에 많은 data를 move할 수 있도록 하고, meta data 관리에 필요한 overhead를 줄일 수 있게 한다. 당장 access되지 않더라도 일정 시간 내에 같이 access될 수 있는 data들은 함께 faster tier로 옮겨진다. 이때 필요한 정보가 coaccess 정보임.\n\n* Cache - Tiering 기법의 연동 방법: 예측이 실패했거나, target tier media의 가용 공간이 부족하여 tiering에서 놓치는 missing data가 있을 수 있음. 이러한 missing data들이 temporal locality가 있는 경우에는 당장은 faster tier에서 serve하지 못하더라도 cache에서 serve될 수 있는데, temporal locality가 적은 경우에는 cache만으로 cover되지 않는다. \n\n* Management table size optimization issue - 만약 모든 LBA 별로 event-based I/O prediction을 해야 한다면 현실적으로 사용 가능한 아키텍쳐가 아닐 수 있음. 만약 spatial locality가 있어서 LBA 1,000개 단위로 묶어서 대응할 수 있다면, 최소 1/1000 의 management table size 감소 효과를 얻을 수 있음. pattern mining 혹은 machine learning 실행 시에는 record 수가 감소함으로써 얻는 이득은 1,000배 이상이 될 수 있음. 관건은 얼마나 LBA 묶음을 크게 만들어서 예측에 사용할 수 있는지임.\n\n\n=== I/O Insight ===\n\n* ML 기반의 I/O Insight 발굴 (Data Placement를 위한 Macroscopic Guideline)\n: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:-> 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n\n* I/O Insight의 요건:\n:# indication, as simple as possible\n:# indication, as specific as possible\n:# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n:# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n\n* I/O Insight의 형태\n: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n=== X-Y Modeling ===\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n=== 예측 실패 대비 (별도 특허화 가능) ===\n\n* I/O 예측이 실패할 수 있으며, 이로 인한 성능 penalty를 최소화하기 위한 방법 필요.\n:- fast tier에 여유공간이 있다면 1타겟과 2타겟을 모두 올려놓아도 되지 않을까? 1타겟과 2타겟을 placement함으로써 예측 정확도를 95%까지 올릴 수 있다면, 꽤 쓸만할 수 있겠음. 그리고 1타겟과 2타겟의 모든 데이터를 다 fast tier에 올려두지 말고 일부만 fast tier에 올려두었다가, 둘 중에 하나의 타겟이 hit되었을 때, hit된 타겟의 나머지 data를 재빨리 fast tier에 올리도록 하는 incremental 방법을 사용할 수 있겠음.\n:- 그러나 이것 또한 예측 실패에 대한 완벽한 prevention이 될 수는 없다. 따라서 예측 실패가 발생하더라도 재빨리 그 성능 penalty를 mitigation할 수 있는 방법이 필요. \n:- cache를 이용하여 mitigation할 수 있을까?\n\n=== Data Read/Write 관련 Issues ===\n\n* read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n\n* 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n\n* write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간에, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면?\n\n* 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있음.\n\n* 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간이 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 scatter했던 I/O들을 나중에 다시 읽어들이려 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 원하는 고성능을 낼 수 있게 된다.\n\n* replica-based dynamic clustering\n: 위와 같은 write-and-then-read-again 상황에서, 해당 data를 저장했었던 HDD의 I/O busy/idle 여부에 상관 없이, 최고의 효과를 낼 수 있게 하기 위해, replica management를 적극적으로 활용하는 것은 어떨까? HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까?\n\n* data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 논의가 아님. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 논의임.\n\n* 그러나, 조심해야 할 것은 위의 논의는 상당히 case-dependency를 상당히 많이 탈 수도 있다는 것임. 책상 위의 아이디어로 끝날 수도 있기 때문에 조심스럽게 접근하는 것이 필요함.\n\n\n\n\n<br/>\n\n== _BPT-006 ==\n\n\n=== Data Lifetime (Valid-to-invalid) Interval 분석에 기반한 Data Placement (Tiering) ===\n\n\n* Data 별로, 혹은 LBA 별로, 혹은 Erase Block 별로? Data의 Valid to Invalid Time (Duration) 패턴을 분석할 수 있을까?\n: 이를 SSD 내에서 Low Overhead로 수행할 수 있나?\n: 혹은, 굳이 SSD 내에서 수행할 필요가 있나?\n: Host side에서 LBA 별로 분석 될 수 있다면, 이 정보를 기반으로 Tiering 시에 활용 할 수 있을 것임\n: 특정 Address에 대해서 W이 되고 나서, 다시 W이 되기까지의 시간을 측정하여 ARM 하면 될 듯. 그 두 W 간에 얼마나 많은 R들이 오는지에 대해서도 고려하여 placement 시 고려할 수 있을 것임.\n\nMeasurement Window 내에서 \'W 간의 Interval이 길다\'는 식으로 인식하는 것과, \'W의 횟수가 잦다\'고 인식하는 것 간에 어떤 차이가 있을 수 있을까?\n\n <pre>\n    01234567890123456789\na1: W-R-W-R-W-R-W-W-R--W : T20W06R04 : Short-Lifetime, Many-Reads : SLC\na2: W---W-R-W---W-W----W : T20W06R01 : Short-Lifetime, Few-Reads : SLC, DRAM?\na3: W-RRRR-W---RR-W-R-RW : T20W04R08 : Mid-Lifetime, Many-Reads : MLC\na4: W------W--R---W----W : T20W04R01 : Mid-Lifetime, Few-Reads : MLC\na5: W-R-R-R-RR-RRRR----R : T20W01R10 : Long-Lifetime, Many-Reads : MLC\na6: W------------R----R- : T20W01R02 : Long-Lifetime, Few-Reads : TLC\n</pre>\n\n=== Idle-time Prediction Based Free Space Management in SSD ===\n\n* Idle-time의 도래 시기와 지속 시간 패턴 분석에 기반한 SSD 내의 Free Space 확보 방법\n:- SSD 내에서 Low overhead (in computation, memory space)로 Idle-time 도래 시기 및 지속 시간 분석이 가능한가?\n:- 가능하다면, SSD 내에서 Garbage Collection 등으로 Free Space를 확보하는 데 있어서 어떤 장점을 가질 수 있는가?\n\n\n\n: 이를 기반으로 \n\n\n\n=== 요약 ===\n* \n\n\n=== 기술 상세 ===\n\n*\n\n\n\n\n\n== _BPT-007 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== _BPT-008 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== _BPT-009 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Ef?cient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== _BPT-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== _BPT-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== _BPT-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== _BPT-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== _BPT-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== _BPT-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== _BPT-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== _BPT-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n----\n=== (SmartSSD API) SSD-internal I/O pattern logging interface and mechanism ===\n\n\n==== Questions ====\n\n* SmartSSD에서 제공할 수 있는 logging 서비스로서 어떤 것들이 있을 수 있나?\n:- 어느? 정도의 free space를 필요로하는 critical (heavy) I/O가 어떤? 주기로 도래할 지 알 수 있을까?\n:- I/O prediction에 도움될 수 있는 정보를 SmartSSD가 기록했다가 필요한 때 (안전한 방법으로?) 줄 수 있을까?\n:- I/O prediction까지는 아니더라도 해당 SSD의 I/O wellness(어떻게 정의?)를 측정했다가 bottleneck을 회피하는 데 유용한 정보를 제공할 수 있을까?\n:- 어떤 SSD가 현재 주어지고 있는 I/O workload에 대해 list performance spec을 만족하는데 어려움을 겪고 있다는 것을 알려줄 수 있는 정보가 어떤 것이 있을까? (SSD 내 write buffer의 fullness 혹은 I/O queue의 유동성? amount of free space in over-provisioning area?) (-> I/O wellness 라는 metric을 이것과 연관시켜 정의할 수도 있으며, 결국 SLA 혹은 Performance QoS와 연관시킬 수 있음)\n:- 지금까지의 S.M.A.R.T.와 비교하였을 때, 어떤 차별점이 있는가? 기존의 S.M.A.R.T.가 제공하지 못하던 타입의 정보를 제공하고, 이로 인해 기존에는 불가능했던 새로운 알고리즘 구현 혹은 새로운 서비스 제공이 가능해짐을 보이면 좋겠음.\n\n\n\n:- workload의 history?\n:- 각 erase block이 erase되어야만 했던 주기?\n:- free space의 추이 (얼마나 많이 남아 돌던지)?\n:- I/O의 heaviness 패턴?\n:- SSD내 write buffer (혹은 write cache)의 utilization정보? (즉, write buffer가 넘칠 정도가 되어서 I/O wait을 해야만 했던 상황이 얼마나 자주 발생했는지, write buffer의 가득찬 정도를 백분율로 표현한다고 했을 때, 평균/표준편차 등으로 대표할 수 있는 normal distribution을 따르는지?\n:- 전체 sequential read/write \n\n\n==== 배경 ====\n\n* 각 SSD 모델별로, 또 동일한 모델의 SSD라 하더라도 어떤 workload에 노출되어왔는지에 따라, 현재 낼 수 있는 I/O performance가 동일하지 않을 수 있다 (!check! 관련 실험 결과 - SSD 830을 이용하여 같은 시간 동안 하나의 host에서 생성되는 workload들을 나누어 받았다 하더라도 내부적인 free block의 갯수 및 random write에 의한 block 내 파편화등의 상태가 상이하여 성능에 차이가 나는 것을 보여줄 것. uFLIP 혹은 filebench 활용 가능). 한 예로, SSD가 어느 호스트의 어느 file system에 매핑되었는지에 따라 그 SSD가 받는 workload의 특성이 다를 수 있다. file system mount point가 달라지거나, 기존과 다른 새로운 응용이 설치/서비스되는 상황이 되면 \n\n\n----\n=== I/O Prediction-based Proactive Data Placement ===\n\n* 미래에 access될 data를 예측하여 적소에 미리 가져다 놓는 기술로서, (1) I/O prediction을 위한 정보 수집 모듈, (2) 주어진 정보들에 기반하여 다가올 I/O를 예측하는 모듈, (3) data placement를 수행하는 모듈로 구성됨.\n\n\n* 본 기술의 특징 1: I/O를 triggering하는 indicator (전조) 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Run-time에 저비용으로 감지될 수 있는 Indicator 정보와, 이에 의해 access될 data 간의 mapping은 Bayesian Inference 이용\n\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n\n----\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n----\n\n=== Tiering Method Avoiding I/O Bottleneck ===\n\n* I/O Prediction에 의해 예측된 미래의 Hot Data를 Proactive하게 Placement한다고 할 때, 예상되는 access pattern으로 인해 발생 가능한 I/O Bottleneck을 최소화할 수 있도록 배치하는 방법\n\n* 예를 들어 미래의 hot data block 1 (200MB), hot data block 2 (100MB), hot data block 3 (400MB)이 1분 후에 access 될 것으로 예측되었다고 가정. data block 1에 대해서는 4KB 단위로 random read가 dominant한 access pattern이 예상되고, data block 2에 대해서는 1024KB 단위의 sequential read가 dominant한 access pattern이 예상되고, data block 3에 대해서는 4KB 단위의 random read와 512KB 단위의 sequential write이 각각 80:20의 비율로 mix된 형태의 access pattern이 예상된다고 했을 때, 각 data block을 어디에 어떤 형태로 placement시킬 것인가?\n\n: machine learning 혹은 pattern mining에 기반한 base workload pattern data를 기반으로 incremental하게 update될 수 있는 data access의 spatial locality와 periodicity\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n\n* 기존 기술과의 대비 (naive I/O prediction vs. full-fledged I/O prediction)\n: 기존 기술은 누적 hit count 정보에 의지하는 naive I/O prediction으로 볼 수 있음. 이러한 방식은 workload의 변화를 제대로 반영하지 못하기 때문에, static한 workload에 대해서는 잘 동작하지만, moving target처럼 dynamic한 workload에 대해서는 오히려 과거의 누적 hit count metric이 현재의 workload 패턴을 왜곡하는 문제점을 가지고 있음. (temporal locality와 spatial locality가 깨지는 순간이 여러번 존재할 수록 기존의 naive I/O prediction 방식으로 성능 향상은 어려워짐)\n\n\n\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== _BPT-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(287,'== _BPT-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== _BPT-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== _BPT-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n[[Bnote patidea 2013-003]]\n\n\n== _BPT-004 ==\n\n=== Event-based I/O Prediction (for Proactive Data Placement) ===\n\n=== 요약 (4) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n=== 기술 상세 (4) ===\n\n* I/O 발생 메커니즘의 모델링\n: I/O prediction을 위해, type-I event (indirect I/O trigger)와 type-II event (direct I/O trigger)를 관찰 및 분석함. Type-I event의 예로서, user의 log-in, 기 log-in되어 있던 user session의 상태 변경 (idle-to-busy), system service에 대한 invocation from local or remote node 등이 포함될 수 있으며, type-II event의 예로서, user application 의 실행, system service의 실행 등이 포함될 수 있음. Type-I event는 type-II event의 발생에 영향을 미치고, type-II event는 I/O request event (data access)의 발생에 영향을 미치므로, \'type-I event\' -> \'type-II event\' -> \'I/O request event\' 까지 이어지는 인과사슬 (cause-effect chain) 기반으로 I/O 발생 메커니즘을 모델링하였음.\n\n\n* Event 정보 수집: type-I event 정보 및 type-II event 정보 정의 및 수집 방법.\n\n\n* I/O 패턴 학습 (Offline Learning): 수집된 Event 정보를 기반으로 I/O 패턴을 learning하는 방법\n\n\n* Run-time I/O 예측 (Online Matching): 시스템 모니터링에서 얻어지는 최근 및 현재의 event들과 learning으로 얻어진 I/O 패턴 DB에 기반하여 Run-time으로 미래 I/O 예측\n\n\n* 시스템 구조: 정보 수집 및 패턴 학습 모듈의 시스템 내 위치 (I/O 패턴 DB 등의 위치 및 실시간 예측 모듈과의 관계)\n\n\n* 순서도: 수집부터 학습까지 전체적인 task의 흐름\n\n\n* 예상 효과: 성능적 측면에서 사용자가 체감하는 lower latency, higher throughput 효과, 이미 발생해버린 congestion time을 피해 미리 적절한 곳에 data를 배치시킴으로써 얻어지는 bottleneck prevention 효과.\n\n=== Data Collection (4) ===\n\n* 역할: I/O 패턴 학습/발굴에 필요한 정보 수집\n\n* 수집 정보 종류\n: 전조 기반 예측에 필요한 간접 I/O 유발자, 직접 I/O 유발자, I/O 발생 이벤트 등 I/O 예측에 도움이 될 수 있는 다양한 정보를 수집. 다음 정보들이 포함될 수 있음.\n:- indirect I/O trigger trace: {(user-id | user-type), status-type, before-status, after-status, event-time}\n:- direct I/O trigger trace: {(program-id | program-type), status-type, before-status, after-status, process-owner, event-time [, process-current-working-directory, files-opened]}\n:- I/O trace: {access-time, address-accessed, read/write-flag, io-size, file-path, process, co-accessed-addresses}\n\n\n* 수집 모듈의 위치\n: storage-server가 분리되어 있는 구조인 SAN/NAS 타입의 architecture인 경우에는 application이 구동되는 server-side에 정보 수집 모듈이 위치한다. storage-server가 결합되어 있는 converged-architecture 기반의 system인 경우에는 해당 system 내에 정보 수집 모듈이 위치한다.\n\n\n* 정보 수집 방식\n: 운영체체가 제공하는 시스템 상태 정보 제공 interface를 이용하거나, blktrace, lsof, debug file system 등 이용 (e.g., /sys/kernel/debug/ 등) 혹은 /proc file system 등을 이용 가능.\n\n== _BPT-005 ==\n\n=== Coaccess-based I/O Prediction (for Proactive Data Placement) ===\n\n* (영문) Coaccess-based I/O Prediction (for Proactive Data Placement)\n* (한글) Coaccess 기반의 I/O 예측 방법 및 시스템 구조\n\n=== 요약 (5) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 coaccess-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n\n\n* 소정의 Confidence 값 이상인 X=>Y 관계를 활용하여, signature X (Y에 대한 precursor)가 access되면 어느 정도 확률로 이후에\n\n=== 발명의 이용 분야 (5) ===\n\n데이터를 저장하고 처리하는 일반적인 컴퓨팅 시스템으로서,\n* 프론트엔드/백엔드 서버\n* 네트워크 스토리지 시스템\n* 컨버지드 서버-스토리지 시스템\n* 엔터프라이즈 혹은 데이터 센터 내의 IT 인프라\n등에서 활용 가능.\n\n\n=== 발명의 목적 (5) ===\n\n\n* 기존 I/O 예측 방식의 한계: Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n\n* 본 발명의 필요성/의미\n: 본 발명은 data access 정보 기반의 I/O prediction 기술로서, client-side의 rich information을 확보가 제한적인 경우에도 periodicity 및 coaccessness 특성을 기반으로 미래에 access될 data의 위치와 시점을 예측할 수 있도록 하는 기술임.\n\n=== 기술 상세 (5) ===\n\n* Data의 coaccess 특성에 기반한 proactive data placement 기법\n: 소정의 time window 내에 같이 access되는 확률이 높은 data 그룹 (coaccess group)이 있을 때, 그 그룹 내의 일부 data가 access되면 나머지 data들도 미리 fast media에 가져다 놓음으로써, 성능 향상을 얻을 수 있게 하는 방법임.\n\n\n* 개본 개념 정의 (coaccess property, coaccessness, coaccess group)\n: 지정된 시간 동안의 measurement window 내에서, 어떤 data가 access될 때 함께 access되는 경향을 가지는 다른 data들이 존재할 때 그 data들 간에 coaccess property 혹은 coaccessness가 있다고 정의한다.  이때, coaccess property를 가지는 data 그룹을 coaccess group이라고 한다. frequent item set mining 관점에서 보면 기준이 되는 support 값 이상의 확률로 관측되는 transaction들을 coaccess group으로 정의할 수 있다.\n\n\n* 추가 개념 정의 (merged coaccess group)\n: 서로 다른 coaccess group들을 개별적으로 다루기보다 이들을 묶어서 하나의 coaccess group으로 만들어 다룸으로써 meta data management의 효율성 및 proactive data placement의 효과 측면에서 장점이 발생하는 경우가 있다. 이렇게 만들어지는 coaccess group을 merged coaccess group이라 정의한다. \n: 서로 다른 coaccess group CG_A와 coaccess group CG_B가 있는데, CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합) 시켜서 새롭게 얻어진 CG_C = {CG_A, CG_B}를 하나의 coaccess group으로 다룬다.\n: computing resource의 한계로 인한 현실적인 문제로 인해 모든 경우의 frequent item set을 발굴하기 어려운 경우가 있음 (Frequent item set mining 시 실제로 maximum size of transaction을 한계 값으로 지정). merged/unionized coaccess group은 이러한 한계적인 상황을 극복할 수 있는 방법으로서 제안되는 개념임. (실제로 관측되었던 실험 data 및 graph를 이용하여 실재 근거를 제시할 것)\n\n<!--\nmerged coaccess group 관련해서.\n\n: CG_A = {a1, a2, ..., aM |supp=s1}, CG_B = {b1, b2, ..., bN |supp=s1}, CG_C = {union of CG_A and CG_B |supp=s1} 이 존재하고 발생 시기가 일치하는 것을 확인하였다면, CG_A, CG_B, CG_C 를 모두 개별적인 coaccess group으로 관리할 것이 아니라, CG_C 만을 coaccess group으로 관리한다. (조건 i) supp(CG_A) == supp(CG_B) == supp(CG_C), (조건 ii) union of CG_A and CG_B is identical to CG_C, (조건 iii) occurrence_time(CG_A) == occurrence_time(CG_B) == occurrence_time(CG_C) 이 있다고 하자. 상기 조건 i, ii, iii이 모두 만족되는 경우에는 CG_C 만을 coaccess group으로 관리하고 지정함으로써, coaccess group의 meta data management overhead를 감소시키고 coaccess-based proactive data placement의 operation efficiency를 증대시킴. /* 여기서, (조건 i)과 (조건 ii)가 만족되면 굳이 (조건 iii)까지 확인할 필요는 없을 수도 있음. 확인 필요 */\n* GCD (Greatest Common Divisor) / LCM(Least Common Multiple) / largest coaccess group 개념과 관련.\n\n* 유사 coaccess group 간 merging?\n: 예를 들어 coaccess group CG_A = {\'a\', \'b\', \'c\', \'d\'} 와 coaccess group CG_B = {\'b\', \'c\', \'d\', \'e\'} 가 있다고 했을 때, \'b\', \'c\', \'d\' data 가 겹치기는 하지만, CG_A와 CG_B는 동일한 coaccess group으로 볼 수는 없다. 그러나 만약 CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 (기준이 되는 확률 값은 시스템과 workload의 상황에 따라서 적절히 조절 가능) CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합)화 하여 단일 coaccess group으로 다루는 방식. (말이 되는 소리인지 확인 필요)\n-->\n\n\n* frequent item set mining을 활용한 coaccess group 발굴 방법\n: Coaccess group 발굴을 위해 frequent item set mining을 이용할 수 있으며, 구체적인 방법은 다음과 같다.\n: (1) 시스템의 resource 상황에 따라 적절한 support 값을 설정한다. 예를 들어 fast media (cache 혹은 tier 1 storage)의 저장 가능 용량에 따라, 얼마나 높은 등급의 hot data까지 fast media에 placement 시킬 수 있을지가 결정될 수 있다. 즉 fast media의 용량이 매우 넉넉하여 10번 중 2번의 빈도로 (support=0.2) 관찰되는 coaccess group 까지 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있고, 혹은 fast media의 용량이 여유롭지 않아서 10번 중 5번 이상의 빈도로 (support=0.5) 관찰되는 coaccess group까지만 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있다. support 값은 상황에 따라 dynamic하게 설정될 수 있으며, 시스템 관리자 혹은 data scientist등 사람에 의해 수동적으로 설정될 수도 있지만 algorithm에 의해 automatic하게 설정되도록 구현될 수도 있다.\n: (2) coaccess group에 대한 단위 관측 시간이 되는 transaction measurement window (TMW) 크기를 설정한다. transaction measurement window의 크기에 따라 coaccess group에 포함될 수 있는 data의 갯수가 달라질 수 있다. transaction measurement window 크기가 너무 작으면 함께 access되는 것으로 판단될 대상이 되는 data 갯수 자체가 감소하게 되므로 coaccessness 분석이 어렵게 된다. 반대로, transaction measurement window 크기가 너무 크면 coaccess group의 종류는 감소하고 각 coaccess group의 크기는 커지는 경향이 생기는데, 이 경우의 문제점은 data placement 시 fast media를 효율적으로 사용하는 것이 어려워질 수 있다는 것이다.\n: (3) access되는 data address 정보를 기반으로 frequent item set mining을 시작한다 (이를 위해서는 support 값과 transaction measurement window size 값이 설정되어 있어야 함).  transaction measurement window 동안 access 되는 address들의 집합을 하나의 transaction (basket)으로 묶는다. 전체 관찰 기간 동안 N 번의 transaction measurement window가 있었다면, N 개 만큼의 transaction이 만들어진다. 이 때 N 개의 transaction을 통틀어 support 값 확률 이상으로 빈번하게 등장하는 item set이 있다면 그것을 coaccess group으로 지정하고 coaccess group management ID를 부여, 관리한다.\n: (4) frequent item set mining에서의 confidence 개념을 도입하여 precursor의 존재 감지 시, 미래에 access되게 될 나머지 data들을 신속하게  (일부 데이터가 access되고 나면 나머지 데이터들도 지정된 시간 내에 함께 access될 확률을 의미하게 됨)\n: (4) run-time 시, coaccess group의 빠른 속도로 찾아낼 수 있도록 하기 위해, coaccess group \n\n\n : coaccess group 발굴이 되었다면, 어떻게 저장하고, 어떻게 불러내서(매칭시켜서) proactive data placement에 활용할 수 있게 할 것인가?\n\n\n* O(1)으로, 매칭되는 패턴을 찾을 수 있게 하려면?\n\n\n* proactive data placement를 위해 \n\n\n* Coaccess group pattern matching 방법\n\n\n* Fig{association graph, temporal occurance graph}\n: Frequent item set mining 분석 결과로 얻어진 association graph 및 시간에 따른 frequent item 발생 추이 graph를 이용하여 coaccess pattern의 실재함을 설명\n\n\n* Coaccess property는 다음과 같은 방식으로 활용될 수 있음\n:- 특정 coaccess pattern이 특정 workload에 대한 precursor가 되는 경우가 있을 수 있음. 이를 이용하여 다가올 workload의 prediction에 활용 가능 (예를 들어, coaccess group A가 access된 후 T1 시간 경과 후에 address range B를 random read하는 workload가 발생하고, T2 시간 경과 후에 address range C를 sequential write하는 workload가 오는 경향이 있다는 것을 알고 있다면, 이 정보를 기반으로 proactive하게 address range B를 fast media에 placement 시키고, address range C에 대해서는 caching  \n:- coaccess group 내의 일부 data가 access되면 그 coaccess group 내의 다른 data들을 미리 fast media에 가져다 놓음으로써 성능 향상 가능\n\n<!--\n* (for event-based I/O prediction) coaccess group이 access되는 순서를 event-based I/O prediction에 이용할 수도 있음. 예를 들어, coaccess group A가 access되고 나서 T1 시간 내로 coaccess group B가 access될 확률이 80% 이상이라고 하고, coaccess group B가 access되고 나서 T2 시간 내로 coaccess group C와 coaccess group D가 access될 확률이 90% 이상이라고 한다면, 이러한 정보를 기반으로 ... 무엇을 할 수 있나?\n-->\n\n\n* 이를 위해서 기존의 storage system 구조에 (1) coaccess pattern analysis engine, (2) coacccess pattern aware data placement controller가 추가되는 것이 필요하다.\n\n\n* coaccess pattern analyzer의 역할: workload 별로(???) coaccess pattern list 구축\n\n\n* coaccess pattern analyzer의 구조 및 동작 방식:\n\n\n* coaccess pattern aware data placement controller의 역할:\n\n\n* coaccess pattern aware data placement controller의 구조 및 동작 방식: coaccess pattern 정보를 받아들이기 위한 interface, interface로 받아들인 coaccess pattern 정보의 queuing? data placement의 I/O 단위 (granularity), data placement의 주기 (static? or dynamic?) 등 coaccess pattern 정보를 data placement 시 어떤 식으로 활용하는지\n\n=== Coaccess-based I/O Prediction의 성능 향상 효과 Estimation ===\n\n* 시뮬레이션 기반의 성능 향상 효과 비교\n:- Fast media의 4KB random read latency는 96.47 usec (SSD 830 128GB 기준), slow media의 4KB random read latency는 1,884.03 usec (HDD 7,200rpm 2TB 기준)으로 설정 (iozone에 의한 실측값 사용)\n:- 실험 시작 전에는 4GB 크기의 target data가 slow media (HDD)에 저장되어 있음.\n:- 여기에 1,000,000 개의 4KB random read request로 구성되는 workload를 인가. (각 read request들은 서로 중복되는 data를 가리키지 않으며, 4GB 크기의 target data를 한 번씩 다 request하게 된다고 가정)\n:- Workload 패턴 학습 결과, 4GB의 전체 data 중, \'가\' type의 coaccess 특성을 가지는 10MB 크기의 data 그룹 (data set 당 2,500개의 4KB data 포함)이 200개가 있으며, \'나\' type의 coaccess 특성을 가지는 1MB 크기의 data 그룹 (data set 당 250개의 4KB data 포함)이 1,000개 있는 것을 발견한 것으로 가정.\n:- \'가\' type의 coaccess 패턴은 해당 data 그룹 내 100개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 80%인 것으로 정의.\n:- \'나\' type의 coaccess 패턴을 해당 data 그룹 내 50개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 100%인 것으로 정의.\n\n\n\n\n본 기법을 적용한 경우와 그렇지 않은 경우의 성능 차이를 IOPS로 비교해보면 다음과 같다. \n\n: 예를 들어 설명하면 다음과 같다.\n: data 1, data 2, ..., data 10000으로 구성된 data set이 있으며, 각 data들의 크기는 4KB이다. 학습을 통해서, (1) 이 중 어느 한 data가 access되었을 때, 1분 이내에 나머지 data들도 같이 access되게 될 확률은 55%, (2) 이 중 2개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 80%, (3) 이 중 3개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 95%라는 것을 알고 있다고 가정한다.\n: fast media의 크기는 4GB 이며, 기존에 \n\n이라는 것을 알고 있다면, data 1, 2, 3이 access되었을 때 data 4~128까지도 미리 proactive하게 fast tier에 배치시킴으로써, 그로 인한 성능 이득을 얻을 수 있게 된다. (!!!실제 실험 사례?!!!)\n\n=== Real Data Analysis (MSN File Server) (5) ===\n\n* Analysis Target Workload Specification\n:- MSN FileServer Tracelog (2008-03-10, 약 5GB 데이터)\n:- 21720977703 microseconds (약 6시간)\n:- 29345085 IO operations / 21720 sec => 1351 IOPS\n\n\n* 첫 16.5 minutes (990 seconds) 동안의 I/O에 대해 10 seconds 단위의 Measurement Window로 측정\n\n\n* Frequent Item Sets Mining (Eclat algorithm)\n: support=0.3, minlen=1, maxlen=15\n <pre>\n> fi_s3 <- eclat(ciop_data, parameter=list(supp=0.3, maxlen=15, minlen=1, tidLists=T))\n\nparameter specification:\n tidLists support minlen maxlen            target   ext\n     TRUE     0.3      1     15 frequent itemsets FALSE\n\nalgorithmic control:\n sparse sort verbose\n      7   -2    TRUE\n\neclat - find frequent item sets with the eclat algorithm\nversion 2.6 (2004.08.16)         (c) 2002-2004   Christian Borgelt\ncreate itemset ...\nset transactions ...[1146629 item(s), 100 transaction(s)] done [1.31s].\nsorting and recoding items ... [21 item(s)] done [0.12s].\ncreating bit matrix ... [21 row(s), 100 column(s)] done [0.00s].\nwriting  ... [28761 set(s)] done [0.01s].\nCreating S4 object  ... done [0.25s].\n\n\n\n\n> summary(fi_s3)\nset of 28761 itemsets\n\nmost frequent items:\n  22159360   22155264 1504849920 3170566144 4086710272    (Other)\n     14372      14370      14336      14336      14336     123094\n\nelement (itemset/transaction) length distribution:sizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13\n  21  168  696 1951 3933 5808 6336 5115 3025 1276  364   63    5\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n  1.000   6.000   7.000   6.775   8.000  13.000\n\nsummary of quality measures:\n    support\n Min.   :0.300\n 1st Qu.:0.330\n Median :0.340\n Mean   :0.388\n 3rd Qu.:0.430\n Max.   :0.920\n\nincludes transaction ID lists: TRUE\n\nmining info:\n      data ntransactions support\n ciop_data           100     0.3\n> dim(tidLists(fi_s3))\n[1] 28761   100\n\n</pre>\n\n\n=== Brief Description of the Drawings (5) ===\n\n# [Concept] Coaccess-based I/O Prediction Concepts\n# [Architecture] Storage System Architecture with Coaccess-based Proactive Data Placement\n# [Concept] Coaccessness Analysis using Frequent Itemset Mining\n# [Flowchart] Proactive Data Placement based on Coaccessness\n# [Auxiliary] Experiment Results 1\n# [Auxiliary] Experiment Results 2\n# [Auxiliary] Performance Gain Estimation\n# [Auxiliary] Comparison with prior arts\n\n== Memo for {_BPT-004, _BPT-005} ==\n\n\n=== Think outside the box // Questions ===\n\n* 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n\n* Tiering을 좀 더 다양한 각도에서 바라보자\n:- 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 것이 기존의 tiering 개념임.\n:- SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n:- SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n:- access pattern-aware optimal placement (APOP) - 이런 것은 어떨까?\n:- Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n\n* write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반의 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지?\n\n\n* 만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? SSD-internal memory buffer를 이용해서 SSD를 NVRAM처럼 쓰도록 하는 것이 실제로 말이 되는 이야기일까?\n\n\n\n\n\n=== 기존 기술의 한계점 (Intelligent I/O Prediction 기술의 필요성) ===\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: Fast media로서 SSD가 사용되는 경우에 다음과 같은 부정적인 효과가 발생할 수 있음. 과거에는 자주 access되었던 data가 앞으로도 hot할 것으로 판단하고 tiering 시점에 fast media로 placement 시켰는데 tiering되고 난 후에 자주 access되지 않게 되면, 성능 향상 이득 없이 SSD의 wear-out만 초래하게 됨.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n\n\n\n\n <pre>\n실제 tracelog 사례를 보여주고 hot area가 변화하고 있음을 보일 것.\n</pre>\n\n* Data Placement (Tiering) 관점에서 본 EMC의 FAST, FAST-VP 기술의 한계점\n:- Coarse-grained Data Migration (768KB 까지 작게 나눌 수 있다고는 하지만, 실제로 그렇게 해서 성능이 얼마나 잘 나오는지는 확인 필요. 귀중한 자원인 DRAM을 관리를 위한 metadata로 가득 채우게 될 위험도 있음)\n:- Flexible하지 않은 Tiering 주기\n:- Busy-hour 지난 후 Idle-time에 Tiering 실시\n:- 낮은 지능의 미래 I/O 예측 능력\n:: data access count를 기반으로 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두도록 하는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n\n=== 본 발명의 차별화 포인트 ===\n\n* 본 기술의 특징 1: I/O를 triggering하는 전조 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Indicator는 Run-time에 저비용으로 감지될 수 있는 형태의 정보들로 구성\n:- 미래에 access될 data를 예측케 하는 Indicator간의 mapping은 Bayesian Inference 이용\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n=== 특허 검색/분석 결과 {storage I/O workload prediction} ===\n\n* 검색식: storage I/O workload prediction\n\n* http://portal.uspto.gov/pair/PublicPair 에서 특허 상태 확인 가능.\n\n* I/O prediction을 기반으로 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n:- 기존엔 proactive tiering이 없었나? -> IBM 특허가 나름 유사함. (US7930505, US7512766)\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n==== 1. 비교가 될만한 특허들 ====\n\n예측에 대한 언급이 되어 있는 스토리지 시스템 관련 특허들. 크게 두 가지로서, 그 중의 하나는 가상 volume에 대해 peak I/O가 발생하는 주기를 분석하여, peak I/O 발생시점이 닥치기 전에 미리 가상 volume의 data를 migration 시키는 방법에 대한 것이나, 어느 위치의 data가 언제 access될지에 대한 예측은 아님 (IBM 특허). 다른 하나는 I/O 예측을 기반으로 미래에 자주 access될만한 data를 미리 여러 노드에 분산시키는 방법에 대한 것이나, 예측 방법 자체에 대한 내용은 다루고 있지 않음 (HP 특허)\n\n\n; US7930505, US7512766 (IBM)\n\n* US7930505 (Application Number: US 12/053582), \"Controlling preemptive work balancing in data storage,\" IBM, 특허일 2011-04-19\n: storage network의 I/O 관찰을 통해 peak I/O activity의 repeating instance에 대한 signature를 VLU (virtual logical unit) 단위로 생성하고, 이에 기반하여 언제 heavy I/O가 도래할 것인지를 예측하여, heavy I/O가 오기 전에 미래 data를 faster media로 migration하도록 하는 tiering 기술에 대한 특허. (이미 I/O peak time이 온 후에 hot data를 옮기는 것은 오히려 시스템의 IO성능을 악화시킬 수도 있기 때문에 미리 migration을 하고자 하는 것임)\n: VLU 단위로, peak I/O activity가 어느 주기로 발생할지에 대한 예측임. 지능 수준과 Fine-granularity 측면에서 초보적인 수준임.\n\n* US7512766 (Application Number: US 11/213494), \"Controlling preemptive work balancing in data storage\", IBM, 특허일 2009-03-31\n: 위의 US7930505 특허와 동일 건으로 보임\n\n\n; US6912635 (HP)\n\n* US6912635 (Application Number: US 10/140968), \"Distributing workload evenly across storage media in a storage array\", HP, 특허일 2005-06-28\n: 미래에 자주 access될만한 data를 여러 arrayed storage device에 분산시키겠다는 기술. 그러나 I/O에 대한 예측 방법은 다루어지고 있지 않음.\n\n==== 2. 검색은 되었으나 관련은 없는 특허들 ====\n\n무언가를 예측한다는 측면, Storage System에 대한 기술이라는 측면에서는 관련이 있으나, 미래에 Access될 Data를 예측한다는 측면에서 제안 기술과 관련 없는 특허들\n\n\n; US7640231 (IBM)\n* US7640231 (Application Number: 11/280012), \"Approach based on self-evolving models for performance guarantees in a shared storage system\", IBM, 특허일 2009-12-29\n: network 기반 storage system에서, average latency 혹은 I/O request rate 등에 대한 주기적인 performance sampling을 기반으로, 주어진 objective function의 값을 최대화할 수 있도록 system resource를 제어할 수 있게 하는 기술.\n\n\n; US7774491, US7519725 (IBM)\n\n* US7774491 (Application Number: 12/359377), \"Utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2010-08-10\n: 복수의 클라이언트들을 대상으로 통계적인 성능 QoS를 보장하기 위한 informed throttling 방법\n: A system for utilizing informed throttling to guarantee quality of service to a plurality of clients\n\n* US7519725 (Application Number: US 10/444712), \"System and method for utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2009-04-14\n: resolve the problem of providing statistical performance guarantees for applications generating streams of read/write accesses (I/Os) on a shared, potentially distributed storage system of finite resources, by initiating throttling whenever an I/O stream is receiving insufficient resources\n\n\n; US7827283 (IBM)\n* US7827283 (Application Number: 10/367444), \"System for managing and controlling storage access requirements\", IBM, 2010-11-02\n: SAN 등의 환경에서, storage bandwidth를 관리하고 reserve하는 기술\n: A Resource Management and Reservation System (RMRS) for managing and reserving storage bandwidth, is a platform independent middleware layer that provides an interface to applications, their database management systems, or some other higher level data management systems like ADRS which does data management on behalf of the applications\n\n\n; US8433554, US8140319 (IBM)\n\n* US8433554, \"Predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2013-04-30\n: An approach for predicting performance and capacity of an information technology (IT) system before the IT system is built is described, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n* US8140319, \"Method and system for predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2012-03-20\n: A method and system for predicting performance and capacity of an information technology (IT) system before the IT system is built, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n\n\n; US8255915 (HP)\n* US8255915, \"Workload management for computer system with container hierarchy and workload-group policies\" HP, 특허일 2012-08-28\n\n\n; US7050956 (HP)\n* US7050956 (Application Number: US 09/843930), \"Method and apparatus for morphological modeling of complex systems to predict performance\", HP, 특허일 2006-05-23\n: I/O 성능 예측을 위한 complex storage system에서의 data flow 모델링 방법\n\n\n; US6718434 (HP)\n* US6718434 (Application Number: US 09/871043), \"Method and apparatus for assigning raid levels\", HP, 특허일 2004-04-06\n: Data를 disk array에 load하기 전에, workload 명세 및 device 명세에 기반하여, 적합한 RAID level을 판단하여 disk array에 할당 (RAID build)하는 방법\n\n\n; US8112586 (EMC)\n* US8112586 (Application Number: US 12/190071), \"Predicting and optimizing I/O performance characteristics in a multi-level caching system\", EMC, 특허일 2012-02-07\n: multi-level caching 시스템에서, cost대비 performance를 최적화할 수 있도록, workload에 따른 resource allocation design에 대한 recommendation 기술 (multi-level cache들 간의 workload propagation을 예측하는 것이 핵심)\n\n\n; US6629266 (IBM)\n* US6629266 (Application Number: US 09/442001), \"Method and system for transparent symptom-based selective software rejuvenation\", IBM, 특허일 2003-09-30\n: software가 언제 동작이 멈추는지를 예측함으로써, software의 dependability를 향상할 수 있게 하는 방법\n\n\n; US5088058 (Unisys)\n* US5088058 (Application Number: 07/237120), \"Apparatus and method for evaluating and predicting computer I/O performance using I/O workload snapshots for model input\", Unisys Corporation, 특허일 1992-02-11\n: I/O delay 등에 대한 확률적 모형을 기반으로 disk 시스템의 성능을 검증하고 예측하는 시뮬레이션 기반의 방법\n: A method of evaluating and predicting the performance of an I/O disk system comprised of one or more disk controllers and cooperating disk units using a simulation model containing a stact constructed such that delays are formulated as sets of stochastic processes\n\n=== Comparison of I/O Prediction Approaches ===\n\n미래에 access될 data의 위치와 시기를 예측하기 위한 방법으로 event-based I/O prediction, coaccess-based I/O prediction, periodicity-based I/O prediction 방식이 제안되며, 각각의 방식은 예측에 필요한 연산량 및 input data의 richness, 그리고 workload 변화에 대한 적응성 측면에서 장단점을 가지고 있음. 따라서, 상황에 따라서 적절한 방식을 이용하면 됨.\n\n{| style=\"border-collapse: collapse; border: 1px solid #000\" border=1\n| style=\"width: 25%\" | 비교 항목\n| style=\"width: 25%\" | periodicity-based I/O prediction\n| style=\"width: 25%\" | coaccess-based I/O prediction\n| style=\"width: 25%\" | event-based I/O prediction \n|-\n| context richness required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| computation overhead required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| adaptability to workload changes (lower is better)\n| high\n| high\n| high\n|-\n|}\n\n\n* 전조 기반 예측 vs. 주기성 기반 예측\n{| border=1\n| 비교 항목\n| 전조 기반 예측\n| 주기성 기반 예측\n|-\n| 기술 요약\n| 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식\n| address 별 access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식\n|-\n| 분석 오버헤드\n| High (학습/마이닝에 드는 비용임)\n| Low (주기성 파악 비용임)\n|-\n| Run-time 오버헤드\n| Mid (실시간 전조 모니터링 및 매칭 비용)\n| Low (주기성 계산)\n|-\n| Workload 변화 적응성\n| High\n| High (Workload 변경 사실 알림 hint 필요)\n|-\n| ...\n| ...\n| ...\n|-\n| Incrementality of Mining/Learning\n| ...\n| ...\n|}\n\n* 전조 기반 예측은, 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식임.  주기성 기반 예측은, data access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식임.\n\n\n=== Intelligent Data Placement ===\n\n* Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case\n: 여기에 HML이 사용될 수 있는 것이라면, 어떤 방식으로 어떤 이유로 그런 것일까?\n\n\n* I/O 예측 방식으로서 다음 방식이 제안되며, 각 방식에 따라서 prediction model, 수집되어야 하는 context data, I/O 패턴 DB에 저장되는 정보의 종류 및 형태가 달라질 수 있음. 예측에 이용할 수 있는 context 정보의 richness에 따라 적절한 방식을 사용할 수 있다. (서로 다른 발명이므로, 별도의 특허로 낸다)\n:- event-based I/O prediction\n:- coaccess-based I/O prediction\n:: 여기서, periodicity-based I/O prediction은 기존 특허에서 언급되었으므로, 본 발명에서는 크게 언급하지 않기로 한다.\n\n\n* I/O prediction 결과에 따라 적절한 위치에 data를 배치하게 되는데, 여기서 적절한 위치라 함은 SSD와 같은 fast tier가 될 수도 있겠으나, sequential read/write 같은 경우 striping된 HDD array가 될 수도 있고, 혹은 SAS HDD가 될 수도 있겠음. 또한 SSD 중에서도 SLC, MLC, TLC 로 구분되는 fine-grained fast tier로 구분될 수도 있음\n\n\n* 미래에 오게 될 I/O의 type도 알 수 있을까? 즉, {random/sequential}x{read/write} type 중 어느 것에 해당되는지를 안다면, 그리고 I/O의 intensiveness도 예상할 수 있다면 그에 맞춰서 placement 시 적절한 tier 혹은 적절한 node로 찾아가게 할 수 있을 것임.\n\n\n* context 정보는, 수집되는 위치에 따라, server-side에서 수집되는 context data와 storage-side에서 수집되는 context data로 분류될 수 있음. Event context data는 server-side에서 수집 가능하며, coaccess context data 및 periodicity context data는 storage-side에서 수집될 수 있음.\n\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n: 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n\n=== Design/Implementation Issues ===\n\n* workload type/class tagging 필요\n\n* cache와 tiering을 같이 쓸 때, DRAM cache가 아니라 Flash Cache의 경우 쓸데없이 모든 data가 Flash Cache에 다 들어가게 두기 보다는, tiering에서 cover하지 못하는 것들 위주로 caching하도록 하면 Flash Cache의 공간 낭비를 줄임으로써, 유효 cache 공간을 늘일 수 있게 되고, 그만큼의 성능 향상으로 연결될 수 있음.\n\n* DRAM 기반의 write cache를 적극 이용하여 마치 log-structured file system에서 제공하는 I/O merging and sequentialization 효과를 얻을 수 있음.\n\n* Tiering되는 data는 caching되는 data보다 상대적으로 coarse-grained하게 grouping하여 다룸으로써, 한꺼번에 많은 data를 move할 수 있도록 하고, meta data 관리에 필요한 overhead를 줄일 수 있게 한다. 당장 access되지 않더라도 일정 시간 내에 같이 access될 수 있는 data들은 함께 faster tier로 옮겨진다. 이때 필요한 정보가 coaccess 정보임.\n\n* Cache - Tiering 기법의 연동 방법: 예측이 실패했거나, target tier media의 가용 공간이 부족하여 tiering에서 놓치는 missing data가 있을 수 있음. 이러한 missing data들이 temporal locality가 있는 경우에는 당장은 faster tier에서 serve하지 못하더라도 cache에서 serve될 수 있는데, temporal locality가 적은 경우에는 cache만으로 cover되지 않는다. \n\n* Management table size optimization issue - 만약 모든 LBA 별로 event-based I/O prediction을 해야 한다면 현실적으로 사용 가능한 아키텍쳐가 아닐 수 있음. 만약 spatial locality가 있어서 LBA 1,000개 단위로 묶어서 대응할 수 있다면, 최소 1/1000 의 management table size 감소 효과를 얻을 수 있음. pattern mining 혹은 machine learning 실행 시에는 record 수가 감소함으로써 얻는 이득은 1,000배 이상이 될 수 있음. 관건은 얼마나 LBA 묶음을 크게 만들어서 예측에 사용할 수 있는지임.\n\n\n=== I/O Insight ===\n\n* ML 기반의 I/O Insight 발굴 (Data Placement를 위한 Macroscopic Guideline)\n: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:-> 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n\n* I/O Insight의 요건:\n:# indication, as simple as possible\n:# indication, as specific as possible\n:# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n:# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n\n* I/O Insight의 형태\n: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n=== X-Y Modeling ===\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n=== 예측 실패 대비 (별도 특허화 가능) ===\n\n* I/O 예측이 실패할 수 있으며, 이로 인한 성능 penalty를 최소화하기 위한 방법 필요.\n:- fast tier에 여유공간이 있다면 1타겟과 2타겟을 모두 올려놓아도 되지 않을까? 1타겟과 2타겟을 placement함으로써 예측 정확도를 95%까지 올릴 수 있다면, 꽤 쓸만할 수 있겠음. 그리고 1타겟과 2타겟의 모든 데이터를 다 fast tier에 올려두지 말고 일부만 fast tier에 올려두었다가, 둘 중에 하나의 타겟이 hit되었을 때, hit된 타겟의 나머지 data를 재빨리 fast tier에 올리도록 하는 incremental 방법을 사용할 수 있겠음.\n:- 그러나 이것 또한 예측 실패에 대한 완벽한 prevention이 될 수는 없다. 따라서 예측 실패가 발생하더라도 재빨리 그 성능 penalty를 mitigation할 수 있는 방법이 필요. \n:- cache를 이용하여 mitigation할 수 있을까?\n\n=== Data Read/Write 관련 Issues ===\n\n* read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n\n* 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n\n* write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간에, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면?\n\n* 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있음.\n\n* 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간이 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 scatter했던 I/O들을 나중에 다시 읽어들이려 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 원하는 고성능을 낼 수 있게 된다.\n\n* replica-based dynamic clustering\n: 위와 같은 write-and-then-read-again 상황에서, 해당 data를 저장했었던 HDD의 I/O busy/idle 여부에 상관 없이, 최고의 효과를 낼 수 있게 하기 위해, replica management를 적극적으로 활용하는 것은 어떨까? HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까?\n\n* data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 논의가 아님. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 논의임.\n\n* 그러나, 조심해야 할 것은 위의 논의는 상당히 case-dependency를 상당히 많이 탈 수도 있다는 것임. 책상 위의 아이디어로 끝날 수도 있기 때문에 조심스럽게 접근하는 것이 필요함.\n\n\n\n\n<br/>\n\n== _BPT-006 ==\n\n\n=== Data Lifetime (Valid-to-invalid) Interval 분석에 기반한 Data Placement (Tiering) ===\n\n\n* Data 별로, 혹은 LBA 별로, 혹은 Erase Block 별로? Data의 Valid to Invalid Time (Duration) 패턴을 분석할 수 있을까?\n: 이를 SSD 내에서 Low Overhead로 수행할 수 있나?\n: 혹은, 굳이 SSD 내에서 수행할 필요가 있나?\n: Host side에서 LBA 별로 분석 될 수 있다면, 이 정보를 기반으로 Tiering 시에 활용 할 수 있을 것임\n: 특정 Address에 대해서 W이 되고 나서, 다시 W이 되기까지의 시간을 측정하여 ARM 하면 될 듯. 그 두 W 간에 얼마나 많은 R들이 오는지에 대해서도 고려하여 placement 시 고려할 수 있을 것임.\n\nMeasurement Window 내에서 \'W 간의 Interval이 길다\'는 식으로 인식하는 것과, \'W의 횟수가 잦다\'고 인식하는 것 간에 어떤 차이가 있을 수 있을까?\n\n <pre>\n    01234567890123456789\na1: W-R-W-R-W-R-W-W-R--W : T20W06R04 : Short-Lifetime, Many-Reads : SLC\na2: W---W-R-W---W-W----W : T20W06R01 : Short-Lifetime, Few-Reads : SLC, DRAM?\na3: W-RRRR-W---RR-W-R-RW : T20W04R08 : Mid-Lifetime, Many-Reads : MLC\na4: W------W--R---W----W : T20W04R01 : Mid-Lifetime, Few-Reads : MLC\na5: W-R-R-R-RR-RRRR----R : T20W01R10 : Long-Lifetime, Many-Reads : MLC\na6: W------------R----R- : T20W01R02 : Long-Lifetime, Few-Reads : TLC\n</pre>\n\n=== Idle-time Prediction Based Free Space Management in SSD ===\n\n* Idle-time의 도래 시기와 지속 시간 패턴 분석에 기반한 SSD 내의 Free Space 확보 방법\n:- SSD 내에서 Low overhead (in computation, memory space)로 Idle-time 도래 시기 및 지속 시간 분석이 가능한가?\n:- 가능하다면, SSD 내에서 Garbage Collection 등으로 Free Space를 확보하는 데 있어서 어떤 장점을 가질 수 있는가?\n\n\n\n: 이를 기반으로 \n\n\n\n=== 요약 ===\n* \n\n\n=== 기술 상세 ===\n\n*\n\n\n\n\n\n== _BPT-007 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== _BPT-008 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== _BPT-009 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Ef?cient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== _BPT-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== _BPT-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== _BPT-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== _BPT-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== _BPT-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== _BPT-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== _BPT-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== _BPT-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n----\n=== (SmartSSD API) SSD-internal I/O pattern logging interface and mechanism ===\n\n\n==== Questions ====\n\n* SmartSSD에서 제공할 수 있는 logging 서비스로서 어떤 것들이 있을 수 있나?\n:- 어느? 정도의 free space를 필요로하는 critical (heavy) I/O가 어떤? 주기로 도래할 지 알 수 있을까?\n:- I/O prediction에 도움될 수 있는 정보를 SmartSSD가 기록했다가 필요한 때 (안전한 방법으로?) 줄 수 있을까?\n:- I/O prediction까지는 아니더라도 해당 SSD의 I/O wellness(어떻게 정의?)를 측정했다가 bottleneck을 회피하는 데 유용한 정보를 제공할 수 있을까?\n:- 어떤 SSD가 현재 주어지고 있는 I/O workload에 대해 list performance spec을 만족하는데 어려움을 겪고 있다는 것을 알려줄 수 있는 정보가 어떤 것이 있을까? (SSD 내 write buffer의 fullness 혹은 I/O queue의 유동성? amount of free space in over-provisioning area?) (-> I/O wellness 라는 metric을 이것과 연관시켜 정의할 수도 있으며, 결국 SLA 혹은 Performance QoS와 연관시킬 수 있음)\n:- 지금까지의 S.M.A.R.T.와 비교하였을 때, 어떤 차별점이 있는가? 기존의 S.M.A.R.T.가 제공하지 못하던 타입의 정보를 제공하고, 이로 인해 기존에는 불가능했던 새로운 알고리즘 구현 혹은 새로운 서비스 제공이 가능해짐을 보이면 좋겠음.\n\n\n\n:- workload의 history?\n:- 각 erase block이 erase되어야만 했던 주기?\n:- free space의 추이 (얼마나 많이 남아 돌던지)?\n:- I/O의 heaviness 패턴?\n:- SSD내 write buffer (혹은 write cache)의 utilization정보? (즉, write buffer가 넘칠 정도가 되어서 I/O wait을 해야만 했던 상황이 얼마나 자주 발생했는지, write buffer의 가득찬 정도를 백분율로 표현한다고 했을 때, 평균/표준편차 등으로 대표할 수 있는 normal distribution을 따르는지?\n:- 전체 sequential read/write \n\n\n==== 배경 ====\n\n* 각 SSD 모델별로, 또 동일한 모델의 SSD라 하더라도 어떤 workload에 노출되어왔는지에 따라, 현재 낼 수 있는 I/O performance가 동일하지 않을 수 있다 (!check! 관련 실험 결과 - SSD 830을 이용하여 같은 시간 동안 하나의 host에서 생성되는 workload들을 나누어 받았다 하더라도 내부적인 free block의 갯수 및 random write에 의한 block 내 파편화등의 상태가 상이하여 성능에 차이가 나는 것을 보여줄 것. uFLIP 혹은 filebench 활용 가능). 한 예로, SSD가 어느 호스트의 어느 file system에 매핑되었는지에 따라 그 SSD가 받는 workload의 특성이 다를 수 있다. file system mount point가 달라지거나, 기존과 다른 새로운 응용이 설치/서비스되는 상황이 되면 \n\n\n----\n=== I/O Prediction-based Proactive Data Placement ===\n\n* 미래에 access될 data를 예측하여 적소에 미리 가져다 놓는 기술로서, (1) I/O prediction을 위한 정보 수집 모듈, (2) 주어진 정보들에 기반하여 다가올 I/O를 예측하는 모듈, (3) data placement를 수행하는 모듈로 구성됨.\n\n\n* 본 기술의 특징 1: I/O를 triggering하는 indicator (전조) 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Run-time에 저비용으로 감지될 수 있는 Indicator 정보와, 이에 의해 access될 data 간의 mapping은 Bayesian Inference 이용\n\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n\n----\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n----\n\n=== Tiering Method Avoiding I/O Bottleneck ===\n\n* I/O Prediction에 의해 예측된 미래의 Hot Data를 Proactive하게 Placement한다고 할 때, 예상되는 access pattern으로 인해 발생 가능한 I/O Bottleneck을 최소화할 수 있도록 배치하는 방법\n\n* 예를 들어 미래의 hot data block 1 (200MB), hot data block 2 (100MB), hot data block 3 (400MB)이 1분 후에 access 될 것으로 예측되었다고 가정. data block 1에 대해서는 4KB 단위로 random read가 dominant한 access pattern이 예상되고, data block 2에 대해서는 1024KB 단위의 sequential read가 dominant한 access pattern이 예상되고, data block 3에 대해서는 4KB 단위의 random read와 512KB 단위의 sequential write이 각각 80:20의 비율로 mix된 형태의 access pattern이 예상된다고 했을 때, 각 data block을 어디에 어떤 형태로 placement시킬 것인가?\n\n: machine learning 혹은 pattern mining에 기반한 base workload pattern data를 기반으로 incremental하게 update될 수 있는 data access의 spatial locality와 periodicity\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n\n* 기존 기술과의 대비 (naive I/O prediction vs. full-fledged I/O prediction)\n: 기존 기술은 누적 hit count 정보에 의지하는 naive I/O prediction으로 볼 수 있음. 이러한 방식은 workload의 변화를 제대로 반영하지 못하기 때문에, static한 workload에 대해서는 잘 동작하지만, moving target처럼 dynamic한 workload에 대해서는 오히려 과거의 누적 hit count metric이 현재의 workload 패턴을 왜곡하는 문제점을 가지고 있음. (temporal locality와 spatial locality가 깨지는 순간이 여러번 존재할 수록 기존의 naive I/O prediction 방식으로 성능 향상은 어려워짐)\n\n\n\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== _BPT-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(288,'== _BPT-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== _BPT-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== _BPT-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n[[Bnote patidea 2013-003]]\n\n\n== _BPT-004 ==\n\n=== Event-based I/O Prediction (for Proactive Data Placement) ===\n\n=== 요약 (4) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n=== 기술 상세 (4) ===\n\n* I/O 발생 메커니즘의 모델링\n: I/O prediction을 위해, type-I event (indirect I/O trigger)와 type-II event (direct I/O trigger)를 관찰 및 분석함. Type-I event의 예로서, user의 log-in, 기 log-in되어 있던 user session의 상태 변경 (idle-to-busy), system service에 대한 invocation from local or remote node 등이 포함될 수 있으며, type-II event의 예로서, user application 의 실행, system service의 실행 등이 포함될 수 있음. Type-I event는 type-II event의 발생에 영향을 미치고, type-II event는 I/O request event (data access)의 발생에 영향을 미치므로, \'type-I event\' -> \'type-II event\' -> \'I/O request event\' 까지 이어지는 인과사슬 (cause-effect chain) 기반으로 I/O 발생 메커니즘을 모델링하였음.\n\n\n* Event 정보 수집: type-I event 정보 및 type-II event 정보 정의 및 수집 방법.\n\n\n* I/O 패턴 학습 (Offline Learning): 수집된 Event 정보를 기반으로 I/O 패턴을 learning하는 방법\n\n\n* Run-time I/O 예측 (Online Matching): 시스템 모니터링에서 얻어지는 최근 및 현재의 event들과 learning으로 얻어진 I/O 패턴 DB에 기반하여 Run-time으로 미래 I/O 예측\n\n\n* 시스템 구조: 정보 수집 및 패턴 학습 모듈의 시스템 내 위치 (I/O 패턴 DB 등의 위치 및 실시간 예측 모듈과의 관계)\n\n\n* 순서도: 수집부터 학습까지 전체적인 task의 흐름\n\n\n* 예상 효과: 성능적 측면에서 사용자가 체감하는 lower latency, higher throughput 효과, 이미 발생해버린 congestion time을 피해 미리 적절한 곳에 data를 배치시킴으로써 얻어지는 bottleneck prevention 효과.\n\n=== Data Collection (4) ===\n\n* 역할: I/O 패턴 학습/발굴에 필요한 정보 수집\n\n* 수집 정보 종류\n: 전조 기반 예측에 필요한 간접 I/O 유발자, 직접 I/O 유발자, I/O 발생 이벤트 등 I/O 예측에 도움이 될 수 있는 다양한 정보를 수집. 다음 정보들이 포함될 수 있음.\n:- indirect I/O trigger trace: {(user-id | user-type), status-type, before-status, after-status, event-time}\n:- direct I/O trigger trace: {(program-id | program-type), status-type, before-status, after-status, process-owner, event-time [, process-current-working-directory, files-opened]}\n:- I/O trace: {access-time, address-accessed, read/write-flag, io-size, file-path, process, co-accessed-addresses}\n\n\n* 수집 모듈의 위치\n: storage-server가 분리되어 있는 구조인 SAN/NAS 타입의 architecture인 경우에는 application이 구동되는 server-side에 정보 수집 모듈이 위치한다. storage-server가 결합되어 있는 converged-architecture 기반의 system인 경우에는 해당 system 내에 정보 수집 모듈이 위치한다.\n\n\n* 정보 수집 방식\n: 운영체체가 제공하는 시스템 상태 정보 제공 interface를 이용하거나, blktrace, lsof, debug file system 등 이용 (e.g., /sys/kernel/debug/ 등) 혹은 /proc file system 등을 이용 가능.\n\n== _BPT-005 ==\n\n=== Coaccess-based I/O Prediction (for Proactive Data Placement) ===\n\n* (영문) Coaccess-based I/O Prediction (for Proactive Data Placement)\n* (한글) Coaccess 기반의 I/O 예측 방법 및 시스템 구조\n\n=== 요약 (5) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 coaccess-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n\n\n* 소정의 Confidence 값 이상인 X=>Y 관계를 활용하여, signature X (Y에 대한 precursor)가 access되면 어느 정도 확률로 이후에\n\n=== 발명의 이용 분야 (5) ===\n\n데이터를 저장하고 처리하는 일반적인 컴퓨팅 시스템으로서,\n* 프론트엔드/백엔드 서버\n* 네트워크 스토리지 시스템\n* 컨버지드 서버-스토리지 시스템\n* 엔터프라이즈 혹은 데이터 센터 내의 IT 인프라\n등에서 활용 가능.\n\n\n=== 발명의 목적 (5) ===\n\n\n* 기존 I/O 예측 방식의 한계: Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵게 됨.\n\n\n* Data의 이해에 기반한 I/O 예측의 필요성\n: Big Data 등의 트렌드로 인해 data access range가 더욱 넓어지고, Cloud, Virtualization 등의 트렌드로 인해 Workload의 다양화 및 역동성이 더욱 커지고있음. 이러한 상황에서 기존의 naive prediction 기반의 data placement 방식으로 I/O 성능을 향상시키기에는 근본적인 한계가 있음. 따라서 workload analysis를 통해 data access pattern 속에 숨어있는 을 이해하고, 이에 기반한 I/O prediction\n\n\n* 본 발명의 필요성/의미\n: 본 발명은 data access 정보 기반의 I/O prediction 기술로서, client-side의 rich information을 확보가 제한적인 경우에도 periodicity 및 coaccessness 특성을 기반으로 미래에 access될 data의 위치와 시점을 예측할 수 있도록 하는 기술임.\n\n=== 기술 상세 (5) ===\n\n* Data의 coaccess 특성에 기반한 proactive data placement 기법\n: 소정의 time window 내에 같이 access되는 확률이 높은 data 그룹 (coaccess group)이 있을 때, 그 그룹 내의 일부 data가 access되면 나머지 data들도 미리 fast media에 가져다 놓음으로써, 성능 향상을 얻을 수 있게 하는 방법임.\n\n\n* 개본 개념 정의 (coaccess property, coaccessness, coaccess group)\n: 지정된 시간 동안의 measurement window 내에서, 어떤 data가 access될 때 함께 access되는 경향을 가지는 다른 data들이 존재할 때 그 data들 간에 coaccess property 혹은 coaccessness가 있다고 정의한다.  이때, coaccess property를 가지는 data 그룹을 coaccess group이라고 한다. frequent item set mining 관점에서 보면 기준이 되는 support 값 이상의 확률로 관측되는 transaction들을 coaccess group으로 정의할 수 있다.\n\n\n* 추가 개념 정의 (merged coaccess group)\n: 서로 다른 coaccess group들을 개별적으로 다루기보다 이들을 묶어서 하나의 coaccess group으로 만들어 다룸으로써 meta data management의 효율성 및 proactive data placement의 효과 측면에서 장점이 발생하는 경우가 있다. 이렇게 만들어지는 coaccess group을 merged coaccess group이라 정의한다. \n: 서로 다른 coaccess group CG_A와 coaccess group CG_B가 있는데, CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합) 시켜서 새롭게 얻어진 CG_C = {CG_A, CG_B}를 하나의 coaccess group으로 다룬다.\n: computing resource의 한계로 인한 현실적인 문제로 인해 모든 경우의 frequent item set을 발굴하기 어려운 경우가 있음 (Frequent item set mining 시 실제로 maximum size of transaction을 한계 값으로 지정). merged/unionized coaccess group은 이러한 한계적인 상황을 극복할 수 있는 방법으로서 제안되는 개념임. (실제로 관측되었던 실험 data 및 graph를 이용하여 실재 근거를 제시할 것)\n\n<!--\nmerged coaccess group 관련해서.\n\n: CG_A = {a1, a2, ..., aM |supp=s1}, CG_B = {b1, b2, ..., bN |supp=s1}, CG_C = {union of CG_A and CG_B |supp=s1} 이 존재하고 발생 시기가 일치하는 것을 확인하였다면, CG_A, CG_B, CG_C 를 모두 개별적인 coaccess group으로 관리할 것이 아니라, CG_C 만을 coaccess group으로 관리한다. (조건 i) supp(CG_A) == supp(CG_B) == supp(CG_C), (조건 ii) union of CG_A and CG_B is identical to CG_C, (조건 iii) occurrence_time(CG_A) == occurrence_time(CG_B) == occurrence_time(CG_C) 이 있다고 하자. 상기 조건 i, ii, iii이 모두 만족되는 경우에는 CG_C 만을 coaccess group으로 관리하고 지정함으로써, coaccess group의 meta data management overhead를 감소시키고 coaccess-based proactive data placement의 operation efficiency를 증대시킴. /* 여기서, (조건 i)과 (조건 ii)가 만족되면 굳이 (조건 iii)까지 확인할 필요는 없을 수도 있음. 확인 필요 */\n* GCD (Greatest Common Divisor) / LCM(Least Common Multiple) / largest coaccess group 개념과 관련.\n\n* 유사 coaccess group 간 merging?\n: 예를 들어 coaccess group CG_A = {\'a\', \'b\', \'c\', \'d\'} 와 coaccess group CG_B = {\'b\', \'c\', \'d\', \'e\'} 가 있다고 했을 때, \'b\', \'c\', \'d\' data 가 겹치기는 하지만, CG_A와 CG_B는 동일한 coaccess group으로 볼 수는 없다. 그러나 만약 CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 (기준이 되는 확률 값은 시스템과 workload의 상황에 따라서 적절히 조절 가능) CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합)화 하여 단일 coaccess group으로 다루는 방식. (말이 되는 소리인지 확인 필요)\n-->\n\n\n* frequent item set mining을 활용한 coaccess group 발굴 방법\n: Coaccess group 발굴을 위해 frequent item set mining을 이용할 수 있으며, 구체적인 방법은 다음과 같다.\n: (1) 시스템의 resource 상황에 따라 적절한 support 값을 설정한다. 예를 들어 fast media (cache 혹은 tier 1 storage)의 저장 가능 용량에 따라, 얼마나 높은 등급의 hot data까지 fast media에 placement 시킬 수 있을지가 결정될 수 있다. 즉 fast media의 용량이 매우 넉넉하여 10번 중 2번의 빈도로 (support=0.2) 관찰되는 coaccess group 까지 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있고, 혹은 fast media의 용량이 여유롭지 않아서 10번 중 5번 이상의 빈도로 (support=0.5) 관찰되는 coaccess group까지만 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있다. support 값은 상황에 따라 dynamic하게 설정될 수 있으며, 시스템 관리자 혹은 data scientist등 사람에 의해 수동적으로 설정될 수도 있지만 algorithm에 의해 automatic하게 설정되도록 구현될 수도 있다.\n: (2) coaccess group에 대한 단위 관측 시간이 되는 transaction measurement window (TMW) 크기를 설정한다. transaction measurement window의 크기에 따라 coaccess group에 포함될 수 있는 data의 갯수가 달라질 수 있다. transaction measurement window 크기가 너무 작으면 함께 access되는 것으로 판단될 대상이 되는 data 갯수 자체가 감소하게 되므로 coaccessness 분석이 어렵게 된다. 반대로, transaction measurement window 크기가 너무 크면 coaccess group의 종류는 감소하고 각 coaccess group의 크기는 커지는 경향이 생기는데, 이 경우의 문제점은 data placement 시 fast media를 효율적으로 사용하는 것이 어려워질 수 있다는 것이다.\n: (3) access되는 data address 정보를 기반으로 frequent item set mining을 시작한다 (이를 위해서는 support 값과 transaction measurement window size 값이 설정되어 있어야 함).  transaction measurement window 동안 access 되는 address들의 집합을 하나의 transaction (basket)으로 묶는다. 전체 관찰 기간 동안 N 번의 transaction measurement window가 있었다면, N 개 만큼의 transaction이 만들어진다. 이 때 N 개의 transaction을 통틀어 support 값 확률 이상으로 빈번하게 등장하는 item set이 있다면 그것을 coaccess group으로 지정하고 coaccess group management ID를 부여, 관리한다.\n: (4) frequent item set mining에서의 confidence 개념을 도입하여 precursor의 존재 감지 시, 미래에 access되게 될 나머지 data들을 신속하게  (일부 데이터가 access되고 나면 나머지 데이터들도 지정된 시간 내에 함께 access될 확률을 의미하게 됨)\n: (4) run-time 시, coaccess group의 빠른 속도로 찾아낼 수 있도록 하기 위해, coaccess group \n\n\n : coaccess group 발굴이 되었다면, 어떻게 저장하고, 어떻게 불러내서(매칭시켜서) proactive data placement에 활용할 수 있게 할 것인가?\n\n\n* O(1)으로, 매칭되는 패턴을 찾을 수 있게 하려면?\n\n\n* proactive data placement를 위해 \n\n\n* Coaccess group pattern matching 방법\n\n\n* Fig{association graph, temporal occurance graph}\n: Frequent item set mining 분석 결과로 얻어진 association graph 및 시간에 따른 frequent item 발생 추이 graph를 이용하여 coaccess pattern의 실재함을 설명\n\n\n* Coaccess property는 다음과 같은 방식으로 활용될 수 있음\n:- 특정 coaccess pattern이 특정 workload에 대한 precursor가 되는 경우가 있을 수 있음. 이를 이용하여 다가올 workload의 prediction에 활용 가능 (예를 들어, coaccess group A가 access된 후 T1 시간 경과 후에 address range B를 random read하는 workload가 발생하고, T2 시간 경과 후에 address range C를 sequential write하는 workload가 오는 경향이 있다는 것을 알고 있다면, 이 정보를 기반으로 proactive하게 address range B를 fast media에 placement 시키고, address range C에 대해서는 caching  \n:- coaccess group 내의 일부 data가 access되면 그 coaccess group 내의 다른 data들을 미리 fast media에 가져다 놓음으로써 성능 향상 가능\n\n<!--\n* (for event-based I/O prediction) coaccess group이 access되는 순서를 event-based I/O prediction에 이용할 수도 있음. 예를 들어, coaccess group A가 access되고 나서 T1 시간 내로 coaccess group B가 access될 확률이 80% 이상이라고 하고, coaccess group B가 access되고 나서 T2 시간 내로 coaccess group C와 coaccess group D가 access될 확률이 90% 이상이라고 한다면, 이러한 정보를 기반으로 ... 무엇을 할 수 있나?\n-->\n\n\n* 이를 위해서 기존의 storage system 구조에 (1) coaccess pattern analysis engine, (2) coacccess pattern aware data placement controller가 추가되는 것이 필요하다.\n\n\n* coaccess pattern analyzer의 역할: workload 별로(???) coaccess pattern list 구축\n\n\n* coaccess pattern analyzer의 구조 및 동작 방식:\n\n\n* coaccess pattern aware data placement controller의 역할:\n\n\n* coaccess pattern aware data placement controller의 구조 및 동작 방식: coaccess pattern 정보를 받아들이기 위한 interface, interface로 받아들인 coaccess pattern 정보의 queuing? data placement의 I/O 단위 (granularity), data placement의 주기 (static? or dynamic?) 등 coaccess pattern 정보를 data placement 시 어떤 식으로 활용하는지\n\n=== Coaccess-based I/O Prediction의 성능 향상 효과 Estimation ===\n\n* 시뮬레이션 기반의 성능 향상 효과 비교\n:- Fast media의 4KB random read latency는 96.47 usec (SSD 830 128GB 기준), slow media의 4KB random read latency는 1,884.03 usec (HDD 7,200rpm 2TB 기준)으로 설정 (iozone에 의한 실측값 사용)\n:- 실험 시작 전에는 4GB 크기의 target data가 slow media (HDD)에 저장되어 있음.\n:- 여기에 1,000,000 개의 4KB random read request로 구성되는 workload를 인가. (각 read request들은 서로 중복되는 data를 가리키지 않으며, 4GB 크기의 target data를 한 번씩 다 request하게 된다고 가정)\n:- Workload 패턴 학습 결과, 4GB의 전체 data 중, \'가\' type의 coaccess 특성을 가지는 10MB 크기의 data 그룹 (data set 당 2,500개의 4KB data 포함)이 200개가 있으며, \'나\' type의 coaccess 특성을 가지는 1MB 크기의 data 그룹 (data set 당 250개의 4KB data 포함)이 1,000개 있는 것을 발견한 것으로 가정.\n:- \'가\' type의 coaccess 패턴은 해당 data 그룹 내 100개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 80%인 것으로 정의.\n:- \'나\' type의 coaccess 패턴을 해당 data 그룹 내 50개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 100%인 것으로 정의.\n\n\n\n\n본 기법을 적용한 경우와 그렇지 않은 경우의 성능 차이를 IOPS로 비교해보면 다음과 같다. \n\n: 예를 들어 설명하면 다음과 같다.\n: data 1, data 2, ..., data 10000으로 구성된 data set이 있으며, 각 data들의 크기는 4KB이다. 학습을 통해서, (1) 이 중 어느 한 data가 access되었을 때, 1분 이내에 나머지 data들도 같이 access되게 될 확률은 55%, (2) 이 중 2개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 80%, (3) 이 중 3개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 95%라는 것을 알고 있다고 가정한다.\n: fast media의 크기는 4GB 이며, 기존에 \n\n이라는 것을 알고 있다면, data 1, 2, 3이 access되었을 때 data 4~128까지도 미리 proactive하게 fast tier에 배치시킴으로써, 그로 인한 성능 이득을 얻을 수 있게 된다. (!!!실제 실험 사례?!!!)\n\n=== Real Data Analysis (MSN File Server) (5) ===\n\n* Analysis Target Workload Specification\n:- MSN FileServer Tracelog (2008-03-10, 약 5GB 데이터)\n:- 21720977703 microseconds (약 6시간)\n:- 29345085 IO operations / 21720 sec => 1351 IOPS\n\n\n* 첫 16.5 minutes (990 seconds) 동안의 I/O에 대해 10 seconds 단위의 Measurement Window로 측정\n\n\n* Frequent Item Sets Mining (Eclat algorithm)\n: support=0.3, minlen=1, maxlen=15\n <pre>\n> fi_s3 <- eclat(ciop_data, parameter=list(supp=0.3, maxlen=15, minlen=1, tidLists=T))\n\nparameter specification:\n tidLists support minlen maxlen            target   ext\n     TRUE     0.3      1     15 frequent itemsets FALSE\n\nalgorithmic control:\n sparse sort verbose\n      7   -2    TRUE\n\neclat - find frequent item sets with the eclat algorithm\nversion 2.6 (2004.08.16)         (c) 2002-2004   Christian Borgelt\ncreate itemset ...\nset transactions ...[1146629 item(s), 100 transaction(s)] done [1.31s].\nsorting and recoding items ... [21 item(s)] done [0.12s].\ncreating bit matrix ... [21 row(s), 100 column(s)] done [0.00s].\nwriting  ... [28761 set(s)] done [0.01s].\nCreating S4 object  ... done [0.25s].\n\n\n\n\n> summary(fi_s3)\nset of 28761 itemsets\n\nmost frequent items:\n  22159360   22155264 1504849920 3170566144 4086710272    (Other)\n     14372      14370      14336      14336      14336     123094\n\nelement (itemset/transaction) length distribution:sizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13\n  21  168  696 1951 3933 5808 6336 5115 3025 1276  364   63    5\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n  1.000   6.000   7.000   6.775   8.000  13.000\n\nsummary of quality measures:\n    support\n Min.   :0.300\n 1st Qu.:0.330\n Median :0.340\n Mean   :0.388\n 3rd Qu.:0.430\n Max.   :0.920\n\nincludes transaction ID lists: TRUE\n\nmining info:\n      data ntransactions support\n ciop_data           100     0.3\n> dim(tidLists(fi_s3))\n[1] 28761   100\n\n</pre>\n\n\n=== Brief Description of the Drawings (5) ===\n\n# [Concept] Coaccess-based I/O Prediction Concepts\n# [Architecture] Storage System Architecture with Coaccess-based Proactive Data Placement\n# [Concept] Coaccessness Analysis using Frequent Itemset Mining\n# [Flowchart] Proactive Data Placement based on Coaccessness\n# [Auxiliary] Experiment Results 1\n# [Auxiliary] Experiment Results 2\n# [Auxiliary] Performance Gain Estimation\n# [Auxiliary] Comparison with prior arts\n\n== Memo for {_BPT-004, _BPT-005} ==\n\n\n=== Think outside the box // Questions ===\n\n* 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n\n* Tiering을 좀 더 다양한 각도에서 바라보자\n:- 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 것이 기존의 tiering 개념임.\n:- SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n:- SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n:- access pattern-aware optimal placement (APOP) - 이런 것은 어떨까?\n:- Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n\n* write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반의 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지?\n\n\n* 만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? SSD-internal memory buffer를 이용해서 SSD를 NVRAM처럼 쓰도록 하는 것이 실제로 말이 되는 이야기일까?\n\n\n\n\n\n=== 기존 기술의 한계점 (Intelligent I/O Prediction 기술의 필요성) ===\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: Fast media로서 SSD가 사용되는 경우에 다음과 같은 부정적인 효과가 발생할 수 있음. 과거에는 자주 access되었던 data가 앞으로도 hot할 것으로 판단하고 tiering 시점에 fast media로 placement 시켰는데 tiering되고 난 후에 자주 access되지 않게 되면, 성능 향상 이득 없이 SSD의 wear-out만 초래하게 됨.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n\n\n\n\n <pre>\n실제 tracelog 사례를 보여주고 hot area가 변화하고 있음을 보일 것.\n</pre>\n\n* Data Placement (Tiering) 관점에서 본 EMC의 FAST, FAST-VP 기술의 한계점\n:- Coarse-grained Data Migration (768KB 까지 작게 나눌 수 있다고는 하지만, 실제로 그렇게 해서 성능이 얼마나 잘 나오는지는 확인 필요. 귀중한 자원인 DRAM을 관리를 위한 metadata로 가득 채우게 될 위험도 있음)\n:- Flexible하지 않은 Tiering 주기\n:- Busy-hour 지난 후 Idle-time에 Tiering 실시\n:- 낮은 지능의 미래 I/O 예측 능력\n:: data access count를 기반으로 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두도록 하는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n\n=== 본 발명의 차별화 포인트 ===\n\n* 본 기술의 특징 1: I/O를 triggering하는 전조 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Indicator는 Run-time에 저비용으로 감지될 수 있는 형태의 정보들로 구성\n:- 미래에 access될 data를 예측케 하는 Indicator간의 mapping은 Bayesian Inference 이용\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n=== 특허 검색/분석 결과 {storage I/O workload prediction} ===\n\n* 검색식: storage I/O workload prediction\n\n* http://portal.uspto.gov/pair/PublicPair 에서 특허 상태 확인 가능.\n\n* I/O prediction을 기반으로 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n:- 기존엔 proactive tiering이 없었나? -> IBM 특허가 나름 유사함. (US7930505, US7512766)\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n==== 1. 비교가 될만한 특허들 ====\n\n예측에 대한 언급이 되어 있는 스토리지 시스템 관련 특허들. 크게 두 가지로서, 그 중의 하나는 가상 volume에 대해 peak I/O가 발생하는 주기를 분석하여, peak I/O 발생시점이 닥치기 전에 미리 가상 volume의 data를 migration 시키는 방법에 대한 것이나, 어느 위치의 data가 언제 access될지에 대한 예측은 아님 (IBM 특허). 다른 하나는 I/O 예측을 기반으로 미래에 자주 access될만한 data를 미리 여러 노드에 분산시키는 방법에 대한 것이나, 예측 방법 자체에 대한 내용은 다루고 있지 않음 (HP 특허)\n\n\n; US7930505, US7512766 (IBM)\n\n* US7930505 (Application Number: US 12/053582), \"Controlling preemptive work balancing in data storage,\" IBM, 특허일 2011-04-19\n: storage network의 I/O 관찰을 통해 peak I/O activity의 repeating instance에 대한 signature를 VLU (virtual logical unit) 단위로 생성하고, 이에 기반하여 언제 heavy I/O가 도래할 것인지를 예측하여, heavy I/O가 오기 전에 미래 data를 faster media로 migration하도록 하는 tiering 기술에 대한 특허. (이미 I/O peak time이 온 후에 hot data를 옮기는 것은 오히려 시스템의 IO성능을 악화시킬 수도 있기 때문에 미리 migration을 하고자 하는 것임)\n: VLU 단위로, peak I/O activity가 어느 주기로 발생할지에 대한 예측임. 지능 수준과 Fine-granularity 측면에서 초보적인 수준임.\n\n* US7512766 (Application Number: US 11/213494), \"Controlling preemptive work balancing in data storage\", IBM, 특허일 2009-03-31\n: 위의 US7930505 특허와 동일 건으로 보임\n\n\n; US6912635 (HP)\n\n* US6912635 (Application Number: US 10/140968), \"Distributing workload evenly across storage media in a storage array\", HP, 특허일 2005-06-28\n: 미래에 자주 access될만한 data를 여러 arrayed storage device에 분산시키겠다는 기술. 그러나 I/O에 대한 예측 방법은 다루어지고 있지 않음.\n\n==== 2. 검색은 되었으나 관련은 없는 특허들 ====\n\n무언가를 예측한다는 측면, Storage System에 대한 기술이라는 측면에서는 관련이 있으나, 미래에 Access될 Data를 예측한다는 측면에서 제안 기술과 관련 없는 특허들\n\n\n; US7640231 (IBM)\n* US7640231 (Application Number: 11/280012), \"Approach based on self-evolving models for performance guarantees in a shared storage system\", IBM, 특허일 2009-12-29\n: network 기반 storage system에서, average latency 혹은 I/O request rate 등에 대한 주기적인 performance sampling을 기반으로, 주어진 objective function의 값을 최대화할 수 있도록 system resource를 제어할 수 있게 하는 기술.\n\n\n; US7774491, US7519725 (IBM)\n\n* US7774491 (Application Number: 12/359377), \"Utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2010-08-10\n: 복수의 클라이언트들을 대상으로 통계적인 성능 QoS를 보장하기 위한 informed throttling 방법\n: A system for utilizing informed throttling to guarantee quality of service to a plurality of clients\n\n* US7519725 (Application Number: US 10/444712), \"System and method for utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2009-04-14\n: resolve the problem of providing statistical performance guarantees for applications generating streams of read/write accesses (I/Os) on a shared, potentially distributed storage system of finite resources, by initiating throttling whenever an I/O stream is receiving insufficient resources\n\n\n; US7827283 (IBM)\n* US7827283 (Application Number: 10/367444), \"System for managing and controlling storage access requirements\", IBM, 2010-11-02\n: SAN 등의 환경에서, storage bandwidth를 관리하고 reserve하는 기술\n: A Resource Management and Reservation System (RMRS) for managing and reserving storage bandwidth, is a platform independent middleware layer that provides an interface to applications, their database management systems, or some other higher level data management systems like ADRS which does data management on behalf of the applications\n\n\n; US8433554, US8140319 (IBM)\n\n* US8433554, \"Predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2013-04-30\n: An approach for predicting performance and capacity of an information technology (IT) system before the IT system is built is described, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n* US8140319, \"Method and system for predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2012-03-20\n: A method and system for predicting performance and capacity of an information technology (IT) system before the IT system is built, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n\n\n; US8255915 (HP)\n* US8255915, \"Workload management for computer system with container hierarchy and workload-group policies\" HP, 특허일 2012-08-28\n\n\n; US7050956 (HP)\n* US7050956 (Application Number: US 09/843930), \"Method and apparatus for morphological modeling of complex systems to predict performance\", HP, 특허일 2006-05-23\n: I/O 성능 예측을 위한 complex storage system에서의 data flow 모델링 방법\n\n\n; US6718434 (HP)\n* US6718434 (Application Number: US 09/871043), \"Method and apparatus for assigning raid levels\", HP, 특허일 2004-04-06\n: Data를 disk array에 load하기 전에, workload 명세 및 device 명세에 기반하여, 적합한 RAID level을 판단하여 disk array에 할당 (RAID build)하는 방법\n\n\n; US8112586 (EMC)\n* US8112586 (Application Number: US 12/190071), \"Predicting and optimizing I/O performance characteristics in a multi-level caching system\", EMC, 특허일 2012-02-07\n: multi-level caching 시스템에서, cost대비 performance를 최적화할 수 있도록, workload에 따른 resource allocation design에 대한 recommendation 기술 (multi-level cache들 간의 workload propagation을 예측하는 것이 핵심)\n\n\n; US6629266 (IBM)\n* US6629266 (Application Number: US 09/442001), \"Method and system for transparent symptom-based selective software rejuvenation\", IBM, 특허일 2003-09-30\n: software가 언제 동작이 멈추는지를 예측함으로써, software의 dependability를 향상할 수 있게 하는 방법\n\n\n; US5088058 (Unisys)\n* US5088058 (Application Number: 07/237120), \"Apparatus and method for evaluating and predicting computer I/O performance using I/O workload snapshots for model input\", Unisys Corporation, 특허일 1992-02-11\n: I/O delay 등에 대한 확률적 모형을 기반으로 disk 시스템의 성능을 검증하고 예측하는 시뮬레이션 기반의 방법\n: A method of evaluating and predicting the performance of an I/O disk system comprised of one or more disk controllers and cooperating disk units using a simulation model containing a stact constructed such that delays are formulated as sets of stochastic processes\n\n=== Comparison of I/O Prediction Approaches ===\n\n미래에 access될 data의 위치와 시기를 예측하기 위한 방법으로 event-based I/O prediction, coaccess-based I/O prediction, periodicity-based I/O prediction 방식이 제안되며, 각각의 방식은 예측에 필요한 연산량 및 input data의 richness, 그리고 workload 변화에 대한 적응성 측면에서 장단점을 가지고 있음. 따라서, 상황에 따라서 적절한 방식을 이용하면 됨.\n\n{| style=\"border-collapse: collapse; border: 1px solid #000\" border=1\n| style=\"width: 25%\" | 비교 항목\n| style=\"width: 25%\" | periodicity-based I/O prediction\n| style=\"width: 25%\" | coaccess-based I/O prediction\n| style=\"width: 25%\" | event-based I/O prediction \n|-\n| context richness required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| computation overhead required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| adaptability to workload changes (lower is better)\n| high\n| high\n| high\n|-\n|}\n\n\n* 전조 기반 예측 vs. 주기성 기반 예측\n{| border=1\n| 비교 항목\n| 전조 기반 예측\n| 주기성 기반 예측\n|-\n| 기술 요약\n| 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식\n| address 별 access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식\n|-\n| 분석 오버헤드\n| High (학습/마이닝에 드는 비용임)\n| Low (주기성 파악 비용임)\n|-\n| Run-time 오버헤드\n| Mid (실시간 전조 모니터링 및 매칭 비용)\n| Low (주기성 계산)\n|-\n| Workload 변화 적응성\n| High\n| High (Workload 변경 사실 알림 hint 필요)\n|-\n| ...\n| ...\n| ...\n|-\n| Incrementality of Mining/Learning\n| ...\n| ...\n|}\n\n* 전조 기반 예측은, 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식임.  주기성 기반 예측은, data access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식임.\n\n\n=== Intelligent Data Placement ===\n\n* Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case\n: 여기에 HML이 사용될 수 있는 것이라면, 어떤 방식으로 어떤 이유로 그런 것일까?\n\n\n* I/O 예측 방식으로서 다음 방식이 제안되며, 각 방식에 따라서 prediction model, 수집되어야 하는 context data, I/O 패턴 DB에 저장되는 정보의 종류 및 형태가 달라질 수 있음. 예측에 이용할 수 있는 context 정보의 richness에 따라 적절한 방식을 사용할 수 있다. (서로 다른 발명이므로, 별도의 특허로 낸다)\n:- event-based I/O prediction\n:- coaccess-based I/O prediction\n:: 여기서, periodicity-based I/O prediction은 기존 특허에서 언급되었으므로, 본 발명에서는 크게 언급하지 않기로 한다.\n\n\n* I/O prediction 결과에 따라 적절한 위치에 data를 배치하게 되는데, 여기서 적절한 위치라 함은 SSD와 같은 fast tier가 될 수도 있겠으나, sequential read/write 같은 경우 striping된 HDD array가 될 수도 있고, 혹은 SAS HDD가 될 수도 있겠음. 또한 SSD 중에서도 SLC, MLC, TLC 로 구분되는 fine-grained fast tier로 구분될 수도 있음\n\n\n* 미래에 오게 될 I/O의 type도 알 수 있을까? 즉, {random/sequential}x{read/write} type 중 어느 것에 해당되는지를 안다면, 그리고 I/O의 intensiveness도 예상할 수 있다면 그에 맞춰서 placement 시 적절한 tier 혹은 적절한 node로 찾아가게 할 수 있을 것임.\n\n\n* context 정보는, 수집되는 위치에 따라, server-side에서 수집되는 context data와 storage-side에서 수집되는 context data로 분류될 수 있음. Event context data는 server-side에서 수집 가능하며, coaccess context data 및 periodicity context data는 storage-side에서 수집될 수 있음.\n\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n: 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n\n=== Design/Implementation Issues ===\n\n* workload type/class tagging 필요\n\n* cache와 tiering을 같이 쓸 때, DRAM cache가 아니라 Flash Cache의 경우 쓸데없이 모든 data가 Flash Cache에 다 들어가게 두기 보다는, tiering에서 cover하지 못하는 것들 위주로 caching하도록 하면 Flash Cache의 공간 낭비를 줄임으로써, 유효 cache 공간을 늘일 수 있게 되고, 그만큼의 성능 향상으로 연결될 수 있음.\n\n* DRAM 기반의 write cache를 적극 이용하여 마치 log-structured file system에서 제공하는 I/O merging and sequentialization 효과를 얻을 수 있음.\n\n* Tiering되는 data는 caching되는 data보다 상대적으로 coarse-grained하게 grouping하여 다룸으로써, 한꺼번에 많은 data를 move할 수 있도록 하고, meta data 관리에 필요한 overhead를 줄일 수 있게 한다. 당장 access되지 않더라도 일정 시간 내에 같이 access될 수 있는 data들은 함께 faster tier로 옮겨진다. 이때 필요한 정보가 coaccess 정보임.\n\n* Cache - Tiering 기법의 연동 방법: 예측이 실패했거나, target tier media의 가용 공간이 부족하여 tiering에서 놓치는 missing data가 있을 수 있음. 이러한 missing data들이 temporal locality가 있는 경우에는 당장은 faster tier에서 serve하지 못하더라도 cache에서 serve될 수 있는데, temporal locality가 적은 경우에는 cache만으로 cover되지 않는다. \n\n* Management table size optimization issue - 만약 모든 LBA 별로 event-based I/O prediction을 해야 한다면 현실적으로 사용 가능한 아키텍쳐가 아닐 수 있음. 만약 spatial locality가 있어서 LBA 1,000개 단위로 묶어서 대응할 수 있다면, 최소 1/1000 의 management table size 감소 효과를 얻을 수 있음. pattern mining 혹은 machine learning 실행 시에는 record 수가 감소함으로써 얻는 이득은 1,000배 이상이 될 수 있음. 관건은 얼마나 LBA 묶음을 크게 만들어서 예측에 사용할 수 있는지임.\n\n\n=== I/O Insight ===\n\n* ML 기반의 I/O Insight 발굴 (Data Placement를 위한 Macroscopic Guideline)\n: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:-> 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n\n* I/O Insight의 요건:\n:# indication, as simple as possible\n:# indication, as specific as possible\n:# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n:# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n\n* I/O Insight의 형태\n: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n=== X-Y Modeling ===\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n=== 예측 실패 대비 (별도 특허화 가능) ===\n\n* I/O 예측이 실패할 수 있으며, 이로 인한 성능 penalty를 최소화하기 위한 방법 필요.\n:- fast tier에 여유공간이 있다면 1타겟과 2타겟을 모두 올려놓아도 되지 않을까? 1타겟과 2타겟을 placement함으로써 예측 정확도를 95%까지 올릴 수 있다면, 꽤 쓸만할 수 있겠음. 그리고 1타겟과 2타겟의 모든 데이터를 다 fast tier에 올려두지 말고 일부만 fast tier에 올려두었다가, 둘 중에 하나의 타겟이 hit되었을 때, hit된 타겟의 나머지 data를 재빨리 fast tier에 올리도록 하는 incremental 방법을 사용할 수 있겠음.\n:- 그러나 이것 또한 예측 실패에 대한 완벽한 prevention이 될 수는 없다. 따라서 예측 실패가 발생하더라도 재빨리 그 성능 penalty를 mitigation할 수 있는 방법이 필요. \n:- cache를 이용하여 mitigation할 수 있을까?\n\n=== Data Read/Write 관련 Issues ===\n\n* read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n\n* 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n\n* write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간에, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면?\n\n* 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있음.\n\n* 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간이 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 scatter했던 I/O들을 나중에 다시 읽어들이려 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 원하는 고성능을 낼 수 있게 된다.\n\n* replica-based dynamic clustering\n: 위와 같은 write-and-then-read-again 상황에서, 해당 data를 저장했었던 HDD의 I/O busy/idle 여부에 상관 없이, 최고의 효과를 낼 수 있게 하기 위해, replica management를 적극적으로 활용하는 것은 어떨까? HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까?\n\n* data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 논의가 아님. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 논의임.\n\n* 그러나, 조심해야 할 것은 위의 논의는 상당히 case-dependency를 상당히 많이 탈 수도 있다는 것임. 책상 위의 아이디어로 끝날 수도 있기 때문에 조심스럽게 접근하는 것이 필요함.\n\n\n\n\n<br/>\n\n== _BPT-006 ==\n\n\n=== Data Lifetime (Valid-to-invalid) Interval 분석에 기반한 Data Placement (Tiering) ===\n\n\n* Data 별로, 혹은 LBA 별로, 혹은 Erase Block 별로? Data의 Valid to Invalid Time (Duration) 패턴을 분석할 수 있을까?\n: 이를 SSD 내에서 Low Overhead로 수행할 수 있나?\n: 혹은, 굳이 SSD 내에서 수행할 필요가 있나?\n: Host side에서 LBA 별로 분석 될 수 있다면, 이 정보를 기반으로 Tiering 시에 활용 할 수 있을 것임\n: 특정 Address에 대해서 W이 되고 나서, 다시 W이 되기까지의 시간을 측정하여 ARM 하면 될 듯. 그 두 W 간에 얼마나 많은 R들이 오는지에 대해서도 고려하여 placement 시 고려할 수 있을 것임.\n\nMeasurement Window 내에서 \'W 간의 Interval이 길다\'는 식으로 인식하는 것과, \'W의 횟수가 잦다\'고 인식하는 것 간에 어떤 차이가 있을 수 있을까?\n\n <pre>\n    01234567890123456789\na1: W-R-W-R-W-R-W-W-R--W : T20W06R04 : Short-Lifetime, Many-Reads : SLC\na2: W---W-R-W---W-W----W : T20W06R01 : Short-Lifetime, Few-Reads : SLC, DRAM?\na3: W-RRRR-W---RR-W-R-RW : T20W04R08 : Mid-Lifetime, Many-Reads : MLC\na4: W------W--R---W----W : T20W04R01 : Mid-Lifetime, Few-Reads : MLC\na5: W-R-R-R-RR-RRRR----R : T20W01R10 : Long-Lifetime, Many-Reads : MLC\na6: W------------R----R- : T20W01R02 : Long-Lifetime, Few-Reads : TLC\n</pre>\n\n=== Idle-time Prediction Based Free Space Management in SSD ===\n\n* Idle-time의 도래 시기와 지속 시간 패턴 분석에 기반한 SSD 내의 Free Space 확보 방법\n:- SSD 내에서 Low overhead (in computation, memory space)로 Idle-time 도래 시기 및 지속 시간 분석이 가능한가?\n:- 가능하다면, SSD 내에서 Garbage Collection 등으로 Free Space를 확보하는 데 있어서 어떤 장점을 가질 수 있는가?\n\n\n\n: 이를 기반으로 \n\n\n\n=== 요약 ===\n* \n\n\n=== 기술 상세 ===\n\n*\n\n\n\n\n\n== _BPT-007 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== _BPT-008 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== _BPT-009 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Ef?cient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== _BPT-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== _BPT-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== _BPT-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== _BPT-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== _BPT-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== _BPT-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== _BPT-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== _BPT-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n----\n=== (SmartSSD API) SSD-internal I/O pattern logging interface and mechanism ===\n\n\n==== Questions ====\n\n* SmartSSD에서 제공할 수 있는 logging 서비스로서 어떤 것들이 있을 수 있나?\n:- 어느? 정도의 free space를 필요로하는 critical (heavy) I/O가 어떤? 주기로 도래할 지 알 수 있을까?\n:- I/O prediction에 도움될 수 있는 정보를 SmartSSD가 기록했다가 필요한 때 (안전한 방법으로?) 줄 수 있을까?\n:- I/O prediction까지는 아니더라도 해당 SSD의 I/O wellness(어떻게 정의?)를 측정했다가 bottleneck을 회피하는 데 유용한 정보를 제공할 수 있을까?\n:- 어떤 SSD가 현재 주어지고 있는 I/O workload에 대해 list performance spec을 만족하는데 어려움을 겪고 있다는 것을 알려줄 수 있는 정보가 어떤 것이 있을까? (SSD 내 write buffer의 fullness 혹은 I/O queue의 유동성? amount of free space in over-provisioning area?) (-> I/O wellness 라는 metric을 이것과 연관시켜 정의할 수도 있으며, 결국 SLA 혹은 Performance QoS와 연관시킬 수 있음)\n:- 지금까지의 S.M.A.R.T.와 비교하였을 때, 어떤 차별점이 있는가? 기존의 S.M.A.R.T.가 제공하지 못하던 타입의 정보를 제공하고, 이로 인해 기존에는 불가능했던 새로운 알고리즘 구현 혹은 새로운 서비스 제공이 가능해짐을 보이면 좋겠음.\n\n\n\n:- workload의 history?\n:- 각 erase block이 erase되어야만 했던 주기?\n:- free space의 추이 (얼마나 많이 남아 돌던지)?\n:- I/O의 heaviness 패턴?\n:- SSD내 write buffer (혹은 write cache)의 utilization정보? (즉, write buffer가 넘칠 정도가 되어서 I/O wait을 해야만 했던 상황이 얼마나 자주 발생했는지, write buffer의 가득찬 정도를 백분율로 표현한다고 했을 때, 평균/표준편차 등으로 대표할 수 있는 normal distribution을 따르는지?\n:- 전체 sequential read/write \n\n\n==== 배경 ====\n\n* 각 SSD 모델별로, 또 동일한 모델의 SSD라 하더라도 어떤 workload에 노출되어왔는지에 따라, 현재 낼 수 있는 I/O performance가 동일하지 않을 수 있다 (!check! 관련 실험 결과 - SSD 830을 이용하여 같은 시간 동안 하나의 host에서 생성되는 workload들을 나누어 받았다 하더라도 내부적인 free block의 갯수 및 random write에 의한 block 내 파편화등의 상태가 상이하여 성능에 차이가 나는 것을 보여줄 것. uFLIP 혹은 filebench 활용 가능). 한 예로, SSD가 어느 호스트의 어느 file system에 매핑되었는지에 따라 그 SSD가 받는 workload의 특성이 다를 수 있다. file system mount point가 달라지거나, 기존과 다른 새로운 응용이 설치/서비스되는 상황이 되면 \n\n\n----\n=== I/O Prediction-based Proactive Data Placement ===\n\n* 미래에 access될 data를 예측하여 적소에 미리 가져다 놓는 기술로서, (1) I/O prediction을 위한 정보 수집 모듈, (2) 주어진 정보들에 기반하여 다가올 I/O를 예측하는 모듈, (3) data placement를 수행하는 모듈로 구성됨.\n\n\n* 본 기술의 특징 1: I/O를 triggering하는 indicator (전조) 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Run-time에 저비용으로 감지될 수 있는 Indicator 정보와, 이에 의해 access될 data 간의 mapping은 Bayesian Inference 이용\n\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n\n----\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n----\n\n=== Tiering Method Avoiding I/O Bottleneck ===\n\n* I/O Prediction에 의해 예측된 미래의 Hot Data를 Proactive하게 Placement한다고 할 때, 예상되는 access pattern으로 인해 발생 가능한 I/O Bottleneck을 최소화할 수 있도록 배치하는 방법\n\n* 예를 들어 미래의 hot data block 1 (200MB), hot data block 2 (100MB), hot data block 3 (400MB)이 1분 후에 access 될 것으로 예측되었다고 가정. data block 1에 대해서는 4KB 단위로 random read가 dominant한 access pattern이 예상되고, data block 2에 대해서는 1024KB 단위의 sequential read가 dominant한 access pattern이 예상되고, data block 3에 대해서는 4KB 단위의 random read와 512KB 단위의 sequential write이 각각 80:20의 비율로 mix된 형태의 access pattern이 예상된다고 했을 때, 각 data block을 어디에 어떤 형태로 placement시킬 것인가?\n\n: machine learning 혹은 pattern mining에 기반한 base workload pattern data를 기반으로 incremental하게 update될 수 있는 data access의 spatial locality와 periodicity\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n\n* 기존 기술과의 대비 (naive I/O prediction vs. full-fledged I/O prediction)\n: 기존 기술은 누적 hit count 정보에 의지하는 naive I/O prediction으로 볼 수 있음. 이러한 방식은 workload의 변화를 제대로 반영하지 못하기 때문에, static한 workload에 대해서는 잘 동작하지만, moving target처럼 dynamic한 workload에 대해서는 오히려 과거의 누적 hit count metric이 현재의 workload 패턴을 왜곡하는 문제점을 가지고 있음. (temporal locality와 spatial locality가 깨지는 순간이 여러번 존재할 수록 기존의 naive I/O prediction 방식으로 성능 향상은 어려워짐)\n\n\n\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== _BPT-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(289,'== _BPT-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== _BPT-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== _BPT-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n[[Bnote patidea 2013-003]]\n\n\n== _BPT-004 ==\n\n=== Event-based I/O Prediction (for Proactive Data Placement) ===\n\n=== 요약 (4) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n=== 기술 상세 (4) ===\n\n* I/O 발생 메커니즘의 모델링\n: I/O prediction을 위해, type-I event (indirect I/O trigger)와 type-II event (direct I/O trigger)를 관찰 및 분석함. Type-I event의 예로서, user의 log-in, 기 log-in되어 있던 user session의 상태 변경 (idle-to-busy), system service에 대한 invocation from local or remote node 등이 포함될 수 있으며, type-II event의 예로서, user application 의 실행, system service의 실행 등이 포함될 수 있음. Type-I event는 type-II event의 발생에 영향을 미치고, type-II event는 I/O request event (data access)의 발생에 영향을 미치므로, \'type-I event\' -> \'type-II event\' -> \'I/O request event\' 까지 이어지는 인과사슬 (cause-effect chain) 기반으로 I/O 발생 메커니즘을 모델링하였음.\n\n\n* Event 정보 수집: type-I event 정보 및 type-II event 정보 정의 및 수집 방법.\n\n\n* I/O 패턴 학습 (Offline Learning): 수집된 Event 정보를 기반으로 I/O 패턴을 learning하는 방법\n\n\n* Run-time I/O 예측 (Online Matching): 시스템 모니터링에서 얻어지는 최근 및 현재의 event들과 learning으로 얻어진 I/O 패턴 DB에 기반하여 Run-time으로 미래 I/O 예측\n\n\n* 시스템 구조: 정보 수집 및 패턴 학습 모듈의 시스템 내 위치 (I/O 패턴 DB 등의 위치 및 실시간 예측 모듈과의 관계)\n\n\n* 순서도: 수집부터 학습까지 전체적인 task의 흐름\n\n\n* 예상 효과: 성능적 측면에서 사용자가 체감하는 lower latency, higher throughput 효과, 이미 발생해버린 congestion time을 피해 미리 적절한 곳에 data를 배치시킴으로써 얻어지는 bottleneck prevention 효과.\n\n=== Data Collection (4) ===\n\n* 역할: I/O 패턴 학습/발굴에 필요한 정보 수집\n\n* 수집 정보 종류\n: 전조 기반 예측에 필요한 간접 I/O 유발자, 직접 I/O 유발자, I/O 발생 이벤트 등 I/O 예측에 도움이 될 수 있는 다양한 정보를 수집. 다음 정보들이 포함될 수 있음.\n:- indirect I/O trigger trace: {(user-id | user-type), status-type, before-status, after-status, event-time}\n:- direct I/O trigger trace: {(program-id | program-type), status-type, before-status, after-status, process-owner, event-time [, process-current-working-directory, files-opened]}\n:- I/O trace: {access-time, address-accessed, read/write-flag, io-size, file-path, process, co-accessed-addresses}\n\n\n* 수집 모듈의 위치\n: storage-server가 분리되어 있는 구조인 SAN/NAS 타입의 architecture인 경우에는 application이 구동되는 server-side에 정보 수집 모듈이 위치한다. storage-server가 결합되어 있는 converged-architecture 기반의 system인 경우에는 해당 system 내에 정보 수집 모듈이 위치한다.\n\n\n* 정보 수집 방식\n: 운영체체가 제공하는 시스템 상태 정보 제공 interface를 이용하거나, blktrace, lsof, debug file system 등 이용 (e.g., /sys/kernel/debug/ 등) 혹은 /proc file system 등을 이용 가능.\n\n== _BPT-005 ==\n\n=== Coaccess-based I/O Prediction (for Proactive Data Placement) ===\n\n* (영문) Coaccess-based I/O Prediction (for Proactive Data Placement)\n* (한글) Coaccess 기반의 I/O 예측 방법 및 시스템 구조\n\n=== 요약 (5) ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 coaccess-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n----\n\n==== Hope ====\n\n* \n\n\n*  X=>Y 관계를 활용하여, signature X (Y에 대한 precursor)가 access되면 어느 정도 확률로 이후에\n\n=== 발명의 이용 분야 (5) ===\n\n데이터를 저장하고 처리하는 일반적인 컴퓨팅 시스템으로서,\n* 프론트엔드/백엔드 서버\n* 네트워크 스토리지 시스템\n* 컨버지드 서버-스토리지 시스템\n* 엔터프라이즈 혹은 데이터 센터 내의 IT 인프라\n등에서 활용 가능.\n\n\n=== 발명의 목적 (5) ===\n\n\n* 기존 I/O 예측 방식의 한계: Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵게 됨.\n\n\n* Data의 이해에 기반한 I/O 예측의 필요성\n: Big Data 등의 트렌드로 인해 data access range가 더욱 넓어지고, Cloud, Virtualization 등의 트렌드로 인해 Workload의 다양화 및 역동성이 더욱 커지고있음. 이러한 상황에서 기존의 naive prediction 기반의 data placement 방식으로 I/O 성능을 향상시키기에는 근본적인 한계가 있음. 따라서 workload analysis를 통해 data access pattern 속에 숨어있는 을 이해하고, 이에 기반한 I/O prediction\n\n\n* 본 발명의 필요성/의미\n: 본 발명은 data access 정보 기반의 I/O prediction 기술로서, client-side의 rich information을 확보가 제한적인 경우에도 periodicity 및 coaccessness 특성을 기반으로 미래에 access될 data의 위치와 시점을 예측할 수 있도록 하는 기술임.\n\n=== 기술 상세 (5) ===\n\n* Data의 coaccess 특성에 기반한 proactive data placement 기법\n: 소정의 time window 내에 같이 access되는 확률이 높은 data 그룹 (coaccess group)이 있을 때, 그 그룹 내의 일부 data가 access되면 나머지 data들도 미리 fast media에 가져다 놓음으로써, 성능 향상을 얻을 수 있게 하는 방법임.\n\n\n* 개본 개념 정의 (coaccess property, coaccessness, coaccess group)\n: 지정된 시간 동안의 measurement window 내에서, 어떤 data가 access될 때 함께 access되는 경향을 가지는 다른 data들이 존재할 때 그 data들 간에 coaccess property 혹은 coaccessness가 있다고 정의한다.  이때, coaccess property를 가지는 data 그룹을 coaccess group이라고 한다. frequent item set mining 관점에서 보면 기준이 되는 support 값 이상의 확률로 관측되는 transaction들을 coaccess group으로 정의할 수 있다.\n\n\n* 추가 개념 정의 (merged coaccess group)\n: 서로 다른 coaccess group들을 개별적으로 다루기보다 이들을 묶어서 하나의 coaccess group으로 만들어 다룸으로써 meta data management의 효율성 및 proactive data placement의 효과 측면에서 장점이 발생하는 경우가 있다. 이렇게 만들어지는 coaccess group을 merged coaccess group이라 정의한다. \n: 서로 다른 coaccess group CG_A와 coaccess group CG_B가 있는데, CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합) 시켜서 새롭게 얻어진 CG_C = {CG_A, CG_B}를 하나의 coaccess group으로 다룬다.\n: computing resource의 한계로 인한 현실적인 문제로 인해 모든 경우의 frequent item set을 발굴하기 어려운 경우가 있음 (Frequent item set mining 시 실제로 maximum size of transaction을 한계 값으로 지정). merged/unionized coaccess group은 이러한 한계적인 상황을 극복할 수 있는 방법으로서 제안되는 개념임. (실제로 관측되었던 실험 data 및 graph를 이용하여 실재 근거를 제시할 것)\n\n<!--\nmerged coaccess group 관련해서.\n\n: CG_A = {a1, a2, ..., aM |supp=s1}, CG_B = {b1, b2, ..., bN |supp=s1}, CG_C = {union of CG_A and CG_B |supp=s1} 이 존재하고 발생 시기가 일치하는 것을 확인하였다면, CG_A, CG_B, CG_C 를 모두 개별적인 coaccess group으로 관리할 것이 아니라, CG_C 만을 coaccess group으로 관리한다. (조건 i) supp(CG_A) == supp(CG_B) == supp(CG_C), (조건 ii) union of CG_A and CG_B is identical to CG_C, (조건 iii) occurrence_time(CG_A) == occurrence_time(CG_B) == occurrence_time(CG_C) 이 있다고 하자. 상기 조건 i, ii, iii이 모두 만족되는 경우에는 CG_C 만을 coaccess group으로 관리하고 지정함으로써, coaccess group의 meta data management overhead를 감소시키고 coaccess-based proactive data placement의 operation efficiency를 증대시킴. /* 여기서, (조건 i)과 (조건 ii)가 만족되면 굳이 (조건 iii)까지 확인할 필요는 없을 수도 있음. 확인 필요 */\n* GCD (Greatest Common Divisor) / LCM(Least Common Multiple) / largest coaccess group 개념과 관련.\n\n* 유사 coaccess group 간 merging?\n: 예를 들어 coaccess group CG_A = {\'a\', \'b\', \'c\', \'d\'} 와 coaccess group CG_B = {\'b\', \'c\', \'d\', \'e\'} 가 있다고 했을 때, \'b\', \'c\', \'d\' data 가 겹치기는 하지만, CG_A와 CG_B는 동일한 coaccess group으로 볼 수는 없다. 그러나 만약 CG_A가 발생하는 시기와 CG_B가 발생하는 시기가 상당히 높은 확률로 일치한다면 (기준이 되는 확률 값은 시스템과 workload의 상황에 따라서 적절히 조절 가능) CG_A와 CG_B를 개별적으로 다룰 것이 아니라 union(합집합)화 하여 단일 coaccess group으로 다루는 방식. (말이 되는 소리인지 확인 필요)\n-->\n\n\n* frequent item set mining을 활용한 coaccess group 발굴 방법\n: Coaccess group 발굴을 위해 frequent item set mining을 이용할 수 있으며, 구체적인 방법은 다음과 같다.\n: (1) 시스템의 resource 상황에 따라 적절한 support 값을 설정한다. 예를 들어 fast media (cache 혹은 tier 1 storage)의 저장 가능 용량에 따라, 얼마나 높은 등급의 hot data까지 fast media에 placement 시킬 수 있을지가 결정될 수 있다. 즉 fast media의 용량이 매우 넉넉하여 10번 중 2번의 빈도로 (support=0.2) 관찰되는 coaccess group 까지 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있고, 혹은 fast media의 용량이 여유롭지 않아서 10번 중 5번 이상의 빈도로 (support=0.5) 관찰되는 coaccess group까지만 fast media에 proactive하게 placement 시킬 수 있는 경우가 있을 수 있다. support 값은 상황에 따라 dynamic하게 설정될 수 있으며, 시스템 관리자 혹은 data scientist등 사람에 의해 수동적으로 설정될 수도 있지만 algorithm에 의해 automatic하게 설정되도록 구현될 수도 있다.\n: (2) coaccess group에 대한 단위 관측 시간이 되는 transaction measurement window (TMW) 크기를 설정한다. transaction measurement window의 크기에 따라 coaccess group에 포함될 수 있는 data의 갯수가 달라질 수 있다. transaction measurement window 크기가 너무 작으면 함께 access되는 것으로 판단될 대상이 되는 data 갯수 자체가 감소하게 되므로 coaccessness 분석이 어렵게 된다. 반대로, transaction measurement window 크기가 너무 크면 coaccess group의 종류는 감소하고 각 coaccess group의 크기는 커지는 경향이 생기는데, 이 경우의 문제점은 data placement 시 fast media를 효율적으로 사용하는 것이 어려워질 수 있다는 것이다.\n: (3) access되는 data address 정보를 기반으로 frequent item set mining을 시작한다 (이를 위해서는 support 값과 transaction measurement window size 값이 설정되어 있어야 함).  transaction measurement window 동안 access 되는 address들의 집합을 하나의 transaction (basket)으로 묶는다. 전체 관찰 기간 동안 N 번의 transaction measurement window가 있었다면, N 개 만큼의 transaction이 만들어진다. 이 때 N 개의 transaction을 통틀어 support 값 확률 이상으로 빈번하게 등장하는 item set이 있다면 그것을 coaccess group으로 지정하고 coaccess group management ID를 부여, 관리한다.\n: (4) frequent item set mining에서의 confidence 개념을 도입하여 precursor의 존재 감지 시, 미래에 access되게 될 나머지 data들을 신속하게  (일부 데이터가 access되고 나면 나머지 데이터들도 지정된 시간 내에 함께 access될 확률을 의미하게 됨)\n: (4) run-time 시, coaccess group의 빠른 속도로 찾아낼 수 있도록 하기 위해, coaccess group \n\n\n : coaccess group 발굴이 되었다면, 어떻게 저장하고, 어떻게 불러내서(매칭시켜서) proactive data placement에 활용할 수 있게 할 것인가?\n\n\n* O(1)으로, 매칭되는 패턴을 찾을 수 있게 하려면?\n\n\n* proactive data placement를 위해 \n\n\n* Coaccess group pattern matching 방법\n\n\n* Fig{association graph, temporal occurance graph}\n: Frequent item set mining 분석 결과로 얻어진 association graph 및 시간에 따른 frequent item 발생 추이 graph를 이용하여 coaccess pattern의 실재함을 설명\n\n\n* Coaccess property는 다음과 같은 방식으로 활용될 수 있음\n:- 특정 coaccess pattern이 특정 workload에 대한 precursor가 되는 경우가 있을 수 있음. 이를 이용하여 다가올 workload의 prediction에 활용 가능 (예를 들어, coaccess group A가 access된 후 T1 시간 경과 후에 address range B를 random read하는 workload가 발생하고, T2 시간 경과 후에 address range C를 sequential write하는 workload가 오는 경향이 있다는 것을 알고 있다면, 이 정보를 기반으로 proactive하게 address range B를 fast media에 placement 시키고, address range C에 대해서는 caching  \n:- coaccess group 내의 일부 data가 access되면 그 coaccess group 내의 다른 data들을 미리 fast media에 가져다 놓음으로써 성능 향상 가능\n\n<!--\n* (for event-based I/O prediction) coaccess group이 access되는 순서를 event-based I/O prediction에 이용할 수도 있음. 예를 들어, coaccess group A가 access되고 나서 T1 시간 내로 coaccess group B가 access될 확률이 80% 이상이라고 하고, coaccess group B가 access되고 나서 T2 시간 내로 coaccess group C와 coaccess group D가 access될 확률이 90% 이상이라고 한다면, 이러한 정보를 기반으로 ... 무엇을 할 수 있나?\n-->\n\n\n* 이를 위해서 기존의 storage system 구조에 (1) coaccess pattern analysis engine, (2) coacccess pattern aware data placement controller가 추가되는 것이 필요하다.\n\n\n* coaccess pattern analyzer의 역할: workload 별로(???) coaccess pattern list 구축\n\n\n* coaccess pattern analyzer의 구조 및 동작 방식:\n\n\n* coaccess pattern aware data placement controller의 역할:\n\n\n* coaccess pattern aware data placement controller의 구조 및 동작 방식: coaccess pattern 정보를 받아들이기 위한 interface, interface로 받아들인 coaccess pattern 정보의 queuing? data placement의 I/O 단위 (granularity), data placement의 주기 (static? or dynamic?) 등 coaccess pattern 정보를 data placement 시 어떤 식으로 활용하는지\n\n=== Coaccess-based I/O Prediction의 성능 향상 효과 Estimation ===\n\n* 시뮬레이션 기반의 성능 향상 효과 비교\n:- Fast media의 4KB random read latency는 96.47 usec (SSD 830 128GB 기준), slow media의 4KB random read latency는 1,884.03 usec (HDD 7,200rpm 2TB 기준)으로 설정 (iozone에 의한 실측값 사용)\n:- 실험 시작 전에는 4GB 크기의 target data가 slow media (HDD)에 저장되어 있음.\n:- 여기에 1,000,000 개의 4KB random read request로 구성되는 workload를 인가. (각 read request들은 서로 중복되는 data를 가리키지 않으며, 4GB 크기의 target data를 한 번씩 다 request하게 된다고 가정)\n:- Workload 패턴 학습 결과, 4GB의 전체 data 중, \'가\' type의 coaccess 특성을 가지는 10MB 크기의 data 그룹 (data set 당 2,500개의 4KB data 포함)이 200개가 있으며, \'나\' type의 coaccess 특성을 가지는 1MB 크기의 data 그룹 (data set 당 250개의 4KB data 포함)이 1,000개 있는 것을 발견한 것으로 가정.\n:- \'가\' type의 coaccess 패턴은 해당 data 그룹 내 100개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 80%인 것으로 정의.\n:- \'나\' type의 coaccess 패턴을 해당 data 그룹 내 50개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 100%인 것으로 정의.\n\n\n\n\n본 기법을 적용한 경우와 그렇지 않은 경우의 성능 차이를 IOPS로 비교해보면 다음과 같다. \n\n: 예를 들어 설명하면 다음과 같다.\n: data 1, data 2, ..., data 10000으로 구성된 data set이 있으며, 각 data들의 크기는 4KB이다. 학습을 통해서, (1) 이 중 어느 한 data가 access되었을 때, 1분 이내에 나머지 data들도 같이 access되게 될 확률은 55%, (2) 이 중 2개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 80%, (3) 이 중 3개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 95%라는 것을 알고 있다고 가정한다.\n: fast media의 크기는 4GB 이며, 기존에 \n\n이라는 것을 알고 있다면, data 1, 2, 3이 access되었을 때 data 4~128까지도 미리 proactive하게 fast tier에 배치시킴으로써, 그로 인한 성능 이득을 얻을 수 있게 된다. (!!!실제 실험 사례?!!!)\n\n=== Real Data Analysis (MSN File Server) (5) ===\n\n* Analysis Target Workload Specification\n:- MSN FileServer Tracelog (2008-03-10, 약 5GB 데이터)\n:- 21720977703 microseconds (약 6시간)\n:- 29345085 IO operations / 21720 sec => 1351 IOPS\n\n\n* 첫 16.5 minutes (990 seconds) 동안의 I/O에 대해 10 seconds 단위의 Measurement Window로 측정\n\n\n* Frequent Item Sets Mining (Eclat algorithm)\n: support=0.3, minlen=1, maxlen=15\n <pre>\n> fi_s3 <- eclat(ciop_data, parameter=list(supp=0.3, maxlen=15, minlen=1, tidLists=T))\n\nparameter specification:\n tidLists support minlen maxlen            target   ext\n     TRUE     0.3      1     15 frequent itemsets FALSE\n\nalgorithmic control:\n sparse sort verbose\n      7   -2    TRUE\n\neclat - find frequent item sets with the eclat algorithm\nversion 2.6 (2004.08.16)         (c) 2002-2004   Christian Borgelt\ncreate itemset ...\nset transactions ...[1146629 item(s), 100 transaction(s)] done [1.31s].\nsorting and recoding items ... [21 item(s)] done [0.12s].\ncreating bit matrix ... [21 row(s), 100 column(s)] done [0.00s].\nwriting  ... [28761 set(s)] done [0.01s].\nCreating S4 object  ... done [0.25s].\n\n\n\n\n> summary(fi_s3)\nset of 28761 itemsets\n\nmost frequent items:\n  22159360   22155264 1504849920 3170566144 4086710272    (Other)\n     14372      14370      14336      14336      14336     123094\n\nelement (itemset/transaction) length distribution:sizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13\n  21  168  696 1951 3933 5808 6336 5115 3025 1276  364   63    5\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n  1.000   6.000   7.000   6.775   8.000  13.000\n\nsummary of quality measures:\n    support\n Min.   :0.300\n 1st Qu.:0.330\n Median :0.340\n Mean   :0.388\n 3rd Qu.:0.430\n Max.   :0.920\n\nincludes transaction ID lists: TRUE\n\nmining info:\n      data ntransactions support\n ciop_data           100     0.3\n> dim(tidLists(fi_s3))\n[1] 28761   100\n\n</pre>\n\n\n=== Brief Description of the Drawings (5) ===\n\n# [Concept] Coaccess-based I/O Prediction Concepts\n# [Architecture] Storage System Architecture with Coaccess-based Proactive Data Placement\n# [Concept] Coaccessness Analysis using Frequent Itemset Mining\n# [Flowchart] Proactive Data Placement based on Coaccessness\n# [Auxiliary] Experiment Results 1\n# [Auxiliary] Experiment Results 2\n# [Auxiliary] Performance Gain Estimation\n# [Auxiliary] Comparison with prior arts\n\n== Memo for {_BPT-004, _BPT-005} ==\n\n\n=== Think outside the box // Questions ===\n\n* 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n\n* Tiering을 좀 더 다양한 각도에서 바라보자\n:- 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 것이 기존의 tiering 개념임.\n:- SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n:- SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n:- access pattern-aware optimal placement (APOP) - 이런 것은 어떨까?\n:- Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n\n* write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반의 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지?\n\n\n* 만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? SSD-internal memory buffer를 이용해서 SSD를 NVRAM처럼 쓰도록 하는 것이 실제로 말이 되는 이야기일까?\n\n\n\n\n\n=== 기존 기술의 한계점 (Intelligent I/O Prediction 기술의 필요성) ===\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: Fast media로서 SSD가 사용되는 경우에 다음과 같은 부정적인 효과가 발생할 수 있음. 과거에는 자주 access되었던 data가 앞으로도 hot할 것으로 판단하고 tiering 시점에 fast media로 placement 시켰는데 tiering되고 난 후에 자주 access되지 않게 되면, 성능 향상 이득 없이 SSD의 wear-out만 초래하게 됨.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache에 의한 성능 향상에 한계가 있음.\n\n\n\n\n\n <pre>\n실제 tracelog 사례를 보여주고 hot area가 변화하고 있음을 보일 것.\n</pre>\n\n* Data Placement (Tiering) 관점에서 본 EMC의 FAST, FAST-VP 기술의 한계점\n:- Coarse-grained Data Migration (768KB 까지 작게 나눌 수 있다고는 하지만, 실제로 그렇게 해서 성능이 얼마나 잘 나오는지는 확인 필요. 귀중한 자원인 DRAM을 관리를 위한 metadata로 가득 채우게 될 위험도 있음)\n:- Flexible하지 않은 Tiering 주기\n:- Busy-hour 지난 후 Idle-time에 Tiering 실시\n:- 낮은 지능의 미래 I/O 예측 능력\n:: data access count를 기반으로 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두도록 하는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n\n=== 본 발명의 차별화 포인트 ===\n\n* 본 기술의 특징 1: I/O를 triggering하는 전조 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Indicator는 Run-time에 저비용으로 감지될 수 있는 형태의 정보들로 구성\n:- 미래에 access될 data를 예측케 하는 Indicator간의 mapping은 Bayesian Inference 이용\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n=== 특허 검색/분석 결과 {storage I/O workload prediction} ===\n\n* 검색식: storage I/O workload prediction\n\n* http://portal.uspto.gov/pair/PublicPair 에서 특허 상태 확인 가능.\n\n* I/O prediction을 기반으로 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n:- 기존엔 proactive tiering이 없었나? -> IBM 특허가 나름 유사함. (US7930505, US7512766)\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n==== 1. 비교가 될만한 특허들 ====\n\n예측에 대한 언급이 되어 있는 스토리지 시스템 관련 특허들. 크게 두 가지로서, 그 중의 하나는 가상 volume에 대해 peak I/O가 발생하는 주기를 분석하여, peak I/O 발생시점이 닥치기 전에 미리 가상 volume의 data를 migration 시키는 방법에 대한 것이나, 어느 위치의 data가 언제 access될지에 대한 예측은 아님 (IBM 특허). 다른 하나는 I/O 예측을 기반으로 미래에 자주 access될만한 data를 미리 여러 노드에 분산시키는 방법에 대한 것이나, 예측 방법 자체에 대한 내용은 다루고 있지 않음 (HP 특허)\n\n\n; US7930505, US7512766 (IBM)\n\n* US7930505 (Application Number: US 12/053582), \"Controlling preemptive work balancing in data storage,\" IBM, 특허일 2011-04-19\n: storage network의 I/O 관찰을 통해 peak I/O activity의 repeating instance에 대한 signature를 VLU (virtual logical unit) 단위로 생성하고, 이에 기반하여 언제 heavy I/O가 도래할 것인지를 예측하여, heavy I/O가 오기 전에 미래 data를 faster media로 migration하도록 하는 tiering 기술에 대한 특허. (이미 I/O peak time이 온 후에 hot data를 옮기는 것은 오히려 시스템의 IO성능을 악화시킬 수도 있기 때문에 미리 migration을 하고자 하는 것임)\n: VLU 단위로, peak I/O activity가 어느 주기로 발생할지에 대한 예측임. 지능 수준과 Fine-granularity 측면에서 초보적인 수준임.\n\n* US7512766 (Application Number: US 11/213494), \"Controlling preemptive work balancing in data storage\", IBM, 특허일 2009-03-31\n: 위의 US7930505 특허와 동일 건으로 보임\n\n\n; US6912635 (HP)\n\n* US6912635 (Application Number: US 10/140968), \"Distributing workload evenly across storage media in a storage array\", HP, 특허일 2005-06-28\n: 미래에 자주 access될만한 data를 여러 arrayed storage device에 분산시키겠다는 기술. 그러나 I/O에 대한 예측 방법은 다루어지고 있지 않음.\n\n==== 2. 검색은 되었으나 관련은 없는 특허들 ====\n\n무언가를 예측한다는 측면, Storage System에 대한 기술이라는 측면에서는 관련이 있으나, 미래에 Access될 Data를 예측한다는 측면에서 제안 기술과 관련 없는 특허들\n\n\n; US7640231 (IBM)\n* US7640231 (Application Number: 11/280012), \"Approach based on self-evolving models for performance guarantees in a shared storage system\", IBM, 특허일 2009-12-29\n: network 기반 storage system에서, average latency 혹은 I/O request rate 등에 대한 주기적인 performance sampling을 기반으로, 주어진 objective function의 값을 최대화할 수 있도록 system resource를 제어할 수 있게 하는 기술.\n\n\n; US7774491, US7519725 (IBM)\n\n* US7774491 (Application Number: 12/359377), \"Utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2010-08-10\n: 복수의 클라이언트들을 대상으로 통계적인 성능 QoS를 보장하기 위한 informed throttling 방법\n: A system for utilizing informed throttling to guarantee quality of service to a plurality of clients\n\n* US7519725 (Application Number: US 10/444712), \"System and method for utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2009-04-14\n: resolve the problem of providing statistical performance guarantees for applications generating streams of read/write accesses (I/Os) on a shared, potentially distributed storage system of finite resources, by initiating throttling whenever an I/O stream is receiving insufficient resources\n\n\n; US7827283 (IBM)\n* US7827283 (Application Number: 10/367444), \"System for managing and controlling storage access requirements\", IBM, 2010-11-02\n: SAN 등의 환경에서, storage bandwidth를 관리하고 reserve하는 기술\n: A Resource Management and Reservation System (RMRS) for managing and reserving storage bandwidth, is a platform independent middleware layer that provides an interface to applications, their database management systems, or some other higher level data management systems like ADRS which does data management on behalf of the applications\n\n\n; US8433554, US8140319 (IBM)\n\n* US8433554, \"Predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2013-04-30\n: An approach for predicting performance and capacity of an information technology (IT) system before the IT system is built is described, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n* US8140319, \"Method and system for predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2012-03-20\n: A method and system for predicting performance and capacity of an information technology (IT) system before the IT system is built, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n\n\n; US8255915 (HP)\n* US8255915, \"Workload management for computer system with container hierarchy and workload-group policies\" HP, 특허일 2012-08-28\n\n\n; US7050956 (HP)\n* US7050956 (Application Number: US 09/843930), \"Method and apparatus for morphological modeling of complex systems to predict performance\", HP, 특허일 2006-05-23\n: I/O 성능 예측을 위한 complex storage system에서의 data flow 모델링 방법\n\n\n; US6718434 (HP)\n* US6718434 (Application Number: US 09/871043), \"Method and apparatus for assigning raid levels\", HP, 특허일 2004-04-06\n: Data를 disk array에 load하기 전에, workload 명세 및 device 명세에 기반하여, 적합한 RAID level을 판단하여 disk array에 할당 (RAID build)하는 방법\n\n\n; US8112586 (EMC)\n* US8112586 (Application Number: US 12/190071), \"Predicting and optimizing I/O performance characteristics in a multi-level caching system\", EMC, 특허일 2012-02-07\n: multi-level caching 시스템에서, cost대비 performance를 최적화할 수 있도록, workload에 따른 resource allocation design에 대한 recommendation 기술 (multi-level cache들 간의 workload propagation을 예측하는 것이 핵심)\n\n\n; US6629266 (IBM)\n* US6629266 (Application Number: US 09/442001), \"Method and system for transparent symptom-based selective software rejuvenation\", IBM, 특허일 2003-09-30\n: software가 언제 동작이 멈추는지를 예측함으로써, software의 dependability를 향상할 수 있게 하는 방법\n\n\n; US5088058 (Unisys)\n* US5088058 (Application Number: 07/237120), \"Apparatus and method for evaluating and predicting computer I/O performance using I/O workload snapshots for model input\", Unisys Corporation, 특허일 1992-02-11\n: I/O delay 등에 대한 확률적 모형을 기반으로 disk 시스템의 성능을 검증하고 예측하는 시뮬레이션 기반의 방법\n: A method of evaluating and predicting the performance of an I/O disk system comprised of one or more disk controllers and cooperating disk units using a simulation model containing a stact constructed such that delays are formulated as sets of stochastic processes\n\n=== Comparison of I/O Prediction Approaches ===\n\n미래에 access될 data의 위치와 시기를 예측하기 위한 방법으로 event-based I/O prediction, coaccess-based I/O prediction, periodicity-based I/O prediction 방식이 제안되며, 각각의 방식은 예측에 필요한 연산량 및 input data의 richness, 그리고 workload 변화에 대한 적응성 측면에서 장단점을 가지고 있음. 따라서, 상황에 따라서 적절한 방식을 이용하면 됨.\n\n{| style=\"border-collapse: collapse; border: 1px solid #000\" border=1\n| style=\"width: 25%\" | 비교 항목\n| style=\"width: 25%\" | periodicity-based I/O prediction\n| style=\"width: 25%\" | coaccess-based I/O prediction\n| style=\"width: 25%\" | event-based I/O prediction \n|-\n| context richness required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| computation overhead required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| adaptability to workload changes (lower is better)\n| high\n| high\n| high\n|-\n|}\n\n\n* 전조 기반 예측 vs. 주기성 기반 예측\n{| border=1\n| 비교 항목\n| 전조 기반 예측\n| 주기성 기반 예측\n|-\n| 기술 요약\n| 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식\n| address 별 access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식\n|-\n| 분석 오버헤드\n| High (학습/마이닝에 드는 비용임)\n| Low (주기성 파악 비용임)\n|-\n| Run-time 오버헤드\n| Mid (실시간 전조 모니터링 및 매칭 비용)\n| Low (주기성 계산)\n|-\n| Workload 변화 적응성\n| High\n| High (Workload 변경 사실 알림 hint 필요)\n|-\n| ...\n| ...\n| ...\n|-\n| Incrementality of Mining/Learning\n| ...\n| ...\n|}\n\n* 전조 기반 예측은, 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식임.  주기성 기반 예측은, data access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식임.\n\n\n=== Intelligent Data Placement ===\n\n* Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case\n: 여기에 HML이 사용될 수 있는 것이라면, 어떤 방식으로 어떤 이유로 그런 것일까?\n\n\n* I/O 예측 방식으로서 다음 방식이 제안되며, 각 방식에 따라서 prediction model, 수집되어야 하는 context data, I/O 패턴 DB에 저장되는 정보의 종류 및 형태가 달라질 수 있음. 예측에 이용할 수 있는 context 정보의 richness에 따라 적절한 방식을 사용할 수 있다. (서로 다른 발명이므로, 별도의 특허로 낸다)\n:- event-based I/O prediction\n:- coaccess-based I/O prediction\n:: 여기서, periodicity-based I/O prediction은 기존 특허에서 언급되었으므로, 본 발명에서는 크게 언급하지 않기로 한다.\n\n\n* I/O prediction 결과에 따라 적절한 위치에 data를 배치하게 되는데, 여기서 적절한 위치라 함은 SSD와 같은 fast tier가 될 수도 있겠으나, sequential read/write 같은 경우 striping된 HDD array가 될 수도 있고, 혹은 SAS HDD가 될 수도 있겠음. 또한 SSD 중에서도 SLC, MLC, TLC 로 구분되는 fine-grained fast tier로 구분될 수도 있음\n\n\n* 미래에 오게 될 I/O의 type도 알 수 있을까? 즉, {random/sequential}x{read/write} type 중 어느 것에 해당되는지를 안다면, 그리고 I/O의 intensiveness도 예상할 수 있다면 그에 맞춰서 placement 시 적절한 tier 혹은 적절한 node로 찾아가게 할 수 있을 것임.\n\n\n* context 정보는, 수집되는 위치에 따라, server-side에서 수집되는 context data와 storage-side에서 수집되는 context data로 분류될 수 있음. Event context data는 server-side에서 수집 가능하며, coaccess context data 및 periodicity context data는 storage-side에서 수집될 수 있음.\n\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n: 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n\n=== Design/Implementation Issues ===\n\n* workload type/class tagging 필요\n\n* cache와 tiering을 같이 쓸 때, DRAM cache가 아니라 Flash Cache의 경우 쓸데없이 모든 data가 Flash Cache에 다 들어가게 두기 보다는, tiering에서 cover하지 못하는 것들 위주로 caching하도록 하면 Flash Cache의 공간 낭비를 줄임으로써, 유효 cache 공간을 늘일 수 있게 되고, 그만큼의 성능 향상으로 연결될 수 있음.\n\n* DRAM 기반의 write cache를 적극 이용하여 마치 log-structured file system에서 제공하는 I/O merging and sequentialization 효과를 얻을 수 있음.\n\n* Tiering되는 data는 caching되는 data보다 상대적으로 coarse-grained하게 grouping하여 다룸으로써, 한꺼번에 많은 data를 move할 수 있도록 하고, meta data 관리에 필요한 overhead를 줄일 수 있게 한다. 당장 access되지 않더라도 일정 시간 내에 같이 access될 수 있는 data들은 함께 faster tier로 옮겨진다. 이때 필요한 정보가 coaccess 정보임.\n\n* Cache - Tiering 기법의 연동 방법: 예측이 실패했거나, target tier media의 가용 공간이 부족하여 tiering에서 놓치는 missing data가 있을 수 있음. 이러한 missing data들이 temporal locality가 있는 경우에는 당장은 faster tier에서 serve하지 못하더라도 cache에서 serve될 수 있는데, temporal locality가 적은 경우에는 cache만으로 cover되지 않는다. \n\n* Management table size optimization issue - 만약 모든 LBA 별로 event-based I/O prediction을 해야 한다면 현실적으로 사용 가능한 아키텍쳐가 아닐 수 있음. 만약 spatial locality가 있어서 LBA 1,000개 단위로 묶어서 대응할 수 있다면, 최소 1/1000 의 management table size 감소 효과를 얻을 수 있음. pattern mining 혹은 machine learning 실행 시에는 record 수가 감소함으로써 얻는 이득은 1,000배 이상이 될 수 있음. 관건은 얼마나 LBA 묶음을 크게 만들어서 예측에 사용할 수 있는지임.\n\n\n=== I/O Insight ===\n\n* ML 기반의 I/O Insight 발굴 (Data Placement를 위한 Macroscopic Guideline)\n: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:-> 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n\n* I/O Insight의 요건:\n:# indication, as simple as possible\n:# indication, as specific as possible\n:# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n:# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n\n* I/O Insight의 형태\n: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n=== X-Y Modeling ===\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n=== 예측 실패 대비 (별도 특허화 가능) ===\n\n* I/O 예측이 실패할 수 있으며, 이로 인한 성능 penalty를 최소화하기 위한 방법 필요.\n:- fast tier에 여유공간이 있다면 1타겟과 2타겟을 모두 올려놓아도 되지 않을까? 1타겟과 2타겟을 placement함으로써 예측 정확도를 95%까지 올릴 수 있다면, 꽤 쓸만할 수 있겠음. 그리고 1타겟과 2타겟의 모든 데이터를 다 fast tier에 올려두지 말고 일부만 fast tier에 올려두었다가, 둘 중에 하나의 타겟이 hit되었을 때, hit된 타겟의 나머지 data를 재빨리 fast tier에 올리도록 하는 incremental 방법을 사용할 수 있겠음.\n:- 그러나 이것 또한 예측 실패에 대한 완벽한 prevention이 될 수는 없다. 따라서 예측 실패가 발생하더라도 재빨리 그 성능 penalty를 mitigation할 수 있는 방법이 필요. \n:- cache를 이용하여 mitigation할 수 있을까?\n\n=== Data Read/Write 관련 Issues ===\n\n* read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n\n* 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n\n* write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간에, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면?\n\n* 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있음.\n\n* 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간이 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 scatter했던 I/O들을 나중에 다시 읽어들이려 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 원하는 고성능을 낼 수 있게 된다.\n\n* replica-based dynamic clustering\n: 위와 같은 write-and-then-read-again 상황에서, 해당 data를 저장했었던 HDD의 I/O busy/idle 여부에 상관 없이, 최고의 효과를 낼 수 있게 하기 위해, replica management를 적극적으로 활용하는 것은 어떨까? HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까?\n\n* data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 논의가 아님. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 논의임.\n\n* 그러나, 조심해야 할 것은 위의 논의는 상당히 case-dependency를 상당히 많이 탈 수도 있다는 것임. 책상 위의 아이디어로 끝날 수도 있기 때문에 조심스럽게 접근하는 것이 필요함.\n\n\n\n\n<br/>\n\n== _BPT-006 ==\n\n\n=== Data Lifetime (Valid-to-invalid) Interval 분석에 기반한 Data Placement (Tiering) ===\n\n\n* Data 별로, 혹은 LBA 별로, 혹은 Erase Block 별로? Data의 Valid to Invalid Time (Duration) 패턴을 분석할 수 있을까?\n: 이를 SSD 내에서 Low Overhead로 수행할 수 있나?\n: 혹은, 굳이 SSD 내에서 수행할 필요가 있나?\n: Host side에서 LBA 별로 분석 될 수 있다면, 이 정보를 기반으로 Tiering 시에 활용 할 수 있을 것임\n: 특정 Address에 대해서 W이 되고 나서, 다시 W이 되기까지의 시간을 측정하여 ARM 하면 될 듯. 그 두 W 간에 얼마나 많은 R들이 오는지에 대해서도 고려하여 placement 시 고려할 수 있을 것임.\n\nMeasurement Window 내에서 \'W 간의 Interval이 길다\'는 식으로 인식하는 것과, \'W의 횟수가 잦다\'고 인식하는 것 간에 어떤 차이가 있을 수 있을까?\n\n <pre>\n    01234567890123456789\na1: W-R-W-R-W-R-W-W-R--W : T20W06R04 : Short-Lifetime, Many-Reads : SLC\na2: W---W-R-W---W-W----W : T20W06R01 : Short-Lifetime, Few-Reads : SLC, DRAM?\na3: W-RRRR-W---RR-W-R-RW : T20W04R08 : Mid-Lifetime, Many-Reads : MLC\na4: W------W--R---W----W : T20W04R01 : Mid-Lifetime, Few-Reads : MLC\na5: W-R-R-R-RR-RRRR----R : T20W01R10 : Long-Lifetime, Many-Reads : MLC\na6: W------------R----R- : T20W01R02 : Long-Lifetime, Few-Reads : TLC\n</pre>\n\n=== Idle-time Prediction Based Free Space Management in SSD ===\n\n* Idle-time의 도래 시기와 지속 시간 패턴 분석에 기반한 SSD 내의 Free Space 확보 방법\n:- SSD 내에서 Low overhead (in computation, memory space)로 Idle-time 도래 시기 및 지속 시간 분석이 가능한가?\n:- 가능하다면, SSD 내에서 Garbage Collection 등으로 Free Space를 확보하는 데 있어서 어떤 장점을 가질 수 있는가?\n\n\n\n: 이를 기반으로 \n\n\n\n=== 요약 ===\n* \n\n\n=== 기술 상세 ===\n\n*\n\n\n\n\n\n== _BPT-007 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== _BPT-008 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== _BPT-009 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Ef?cient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== _BPT-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== _BPT-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== _BPT-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== _BPT-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== _BPT-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== _BPT-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== _BPT-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== _BPT-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n----\n=== (SmartSSD API) SSD-internal I/O pattern logging interface and mechanism ===\n\n\n==== Questions ====\n\n* SmartSSD에서 제공할 수 있는 logging 서비스로서 어떤 것들이 있을 수 있나?\n:- 어느? 정도의 free space를 필요로하는 critical (heavy) I/O가 어떤? 주기로 도래할 지 알 수 있을까?\n:- I/O prediction에 도움될 수 있는 정보를 SmartSSD가 기록했다가 필요한 때 (안전한 방법으로?) 줄 수 있을까?\n:- I/O prediction까지는 아니더라도 해당 SSD의 I/O wellness(어떻게 정의?)를 측정했다가 bottleneck을 회피하는 데 유용한 정보를 제공할 수 있을까?\n:- 어떤 SSD가 현재 주어지고 있는 I/O workload에 대해 list performance spec을 만족하는데 어려움을 겪고 있다는 것을 알려줄 수 있는 정보가 어떤 것이 있을까? (SSD 내 write buffer의 fullness 혹은 I/O queue의 유동성? amount of free space in over-provisioning area?) (-> I/O wellness 라는 metric을 이것과 연관시켜 정의할 수도 있으며, 결국 SLA 혹은 Performance QoS와 연관시킬 수 있음)\n:- 지금까지의 S.M.A.R.T.와 비교하였을 때, 어떤 차별점이 있는가? 기존의 S.M.A.R.T.가 제공하지 못하던 타입의 정보를 제공하고, 이로 인해 기존에는 불가능했던 새로운 알고리즘 구현 혹은 새로운 서비스 제공이 가능해짐을 보이면 좋겠음.\n\n\n\n:- workload의 history?\n:- 각 erase block이 erase되어야만 했던 주기?\n:- free space의 추이 (얼마나 많이 남아 돌던지)?\n:- I/O의 heaviness 패턴?\n:- SSD내 write buffer (혹은 write cache)의 utilization정보? (즉, write buffer가 넘칠 정도가 되어서 I/O wait을 해야만 했던 상황이 얼마나 자주 발생했는지, write buffer의 가득찬 정도를 백분율로 표현한다고 했을 때, 평균/표준편차 등으로 대표할 수 있는 normal distribution을 따르는지?\n:- 전체 sequential read/write \n\n\n==== 배경 ====\n\n* 각 SSD 모델별로, 또 동일한 모델의 SSD라 하더라도 어떤 workload에 노출되어왔는지에 따라, 현재 낼 수 있는 I/O performance가 동일하지 않을 수 있다 (!check! 관련 실험 결과 - SSD 830을 이용하여 같은 시간 동안 하나의 host에서 생성되는 workload들을 나누어 받았다 하더라도 내부적인 free block의 갯수 및 random write에 의한 block 내 파편화등의 상태가 상이하여 성능에 차이가 나는 것을 보여줄 것. uFLIP 혹은 filebench 활용 가능). 한 예로, SSD가 어느 호스트의 어느 file system에 매핑되었는지에 따라 그 SSD가 받는 workload의 특성이 다를 수 있다. file system mount point가 달라지거나, 기존과 다른 새로운 응용이 설치/서비스되는 상황이 되면 \n\n\n----\n=== I/O Prediction-based Proactive Data Placement ===\n\n* 미래에 access될 data를 예측하여 적소에 미리 가져다 놓는 기술로서, (1) I/O prediction을 위한 정보 수집 모듈, (2) 주어진 정보들에 기반하여 다가올 I/O를 예측하는 모듈, (3) data placement를 수행하는 모듈로 구성됨.\n\n\n* 본 기술의 특징 1: I/O를 triggering하는 indicator (전조) 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Run-time에 저비용으로 감지될 수 있는 Indicator 정보와, 이에 의해 access될 data 간의 mapping은 Bayesian Inference 이용\n\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n\n----\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n----\n\n=== Tiering Method Avoiding I/O Bottleneck ===\n\n* I/O Prediction에 의해 예측된 미래의 Hot Data를 Proactive하게 Placement한다고 할 때, 예상되는 access pattern으로 인해 발생 가능한 I/O Bottleneck을 최소화할 수 있도록 배치하는 방법\n\n* 예를 들어 미래의 hot data block 1 (200MB), hot data block 2 (100MB), hot data block 3 (400MB)이 1분 후에 access 될 것으로 예측되었다고 가정. data block 1에 대해서는 4KB 단위로 random read가 dominant한 access pattern이 예상되고, data block 2에 대해서는 1024KB 단위의 sequential read가 dominant한 access pattern이 예상되고, data block 3에 대해서는 4KB 단위의 random read와 512KB 단위의 sequential write이 각각 80:20의 비율로 mix된 형태의 access pattern이 예상된다고 했을 때, 각 data block을 어디에 어떤 형태로 placement시킬 것인가?\n\n: machine learning 혹은 pattern mining에 기반한 base workload pattern data를 기반으로 incremental하게 update될 수 있는 data access의 spatial locality와 periodicity\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n\n* 기존 기술과의 대비 (naive I/O prediction vs. full-fledged I/O prediction)\n: 기존 기술은 누적 hit count 정보에 의지하는 naive I/O prediction으로 볼 수 있음. 이러한 방식은 workload의 변화를 제대로 반영하지 못하기 때문에, static한 workload에 대해서는 잘 동작하지만, moving target처럼 dynamic한 workload에 대해서는 오히려 과거의 누적 hit count metric이 현재의 workload 패턴을 왜곡하는 문제점을 가지고 있음. (temporal locality와 spatial locality가 깨지는 순간이 여러번 존재할 수록 기존의 naive I/O prediction 방식으로 성능 향상은 어려워짐)\n\n\n\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== _BPT-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(290,'== Wikini (radiohead) Hot Pages ==\n\n* [[Bnote 2013]] | [[Bnote patidea 2013]] | [[Bnote - References ( Papers / Patents / Articles )]]\n* [http://kandinsky/wikini/index.php/Special:AllPages AllPages] | [http://kandinsky/wikini/index.php/Special:ListFiles ListFiles] | [http://kandinsky/wikini/index.php?title=Special%3ASearch&search=bnote&go=Go Bnote Search]\n\n----\n* [https://mail.google.com/mail Gmail] [https://www.google.com/calendar/ Gcal] | [http://www.naver.com/ Naver] | [http://github.com/ GitHub] | [http://mobile.olleh.com/index.asp?code=A000000 Olleh_SMS]\n* [http://klimt/ Klimt (Rolling-Stones)]\n\n== Tasks ==\n\n# (2013-06-04) patent submit\n# (2013-06-05) <strike>HMM script</strike>\n# (2013-06-08) check [http://www.rstudio.com/shiny/ Shiny], [http://plyr.had.co.nz/ plyr] - [http://www.rstudio.com/projects/ R projects from RStudio team]\n# (2013-06-08) add CDN related 연구보고서 (slides, in Korean)\n# (2013-06-08) <strike>김혁호책임에게 Data to be traced 정보 전달</strike>\n# (2013-06-09) 폐 종이컵 (used) 모아오기','utf-8'),(291,'== 20130612_195019 ==\n\n=== R and GPGPU computing ===\n\n* http://cran.r-project.org/web/packages/gputools/\n: gputools - a few GPU enabled functions // CRAN Packages\n: This package provides R interfaces to a handful of common functions implemented using the Nvidia CUDA toolkit. Some of the functions require at least GPU Compute Capability 1.3. Thanks to Craig Stark at UC Irvine for donating time on his lab\'s Mac.\n: 0.28 (R >= 3.0.0)\n\n\n* http://www.rinfinance.com/agenda/2010/MarkSeligman_Tutorial.pdf\n: GPU Programming with R // Slides for introduction to gputools // 2010-04-15\n\n\n* http://cran.r-project.org/web/views/HighPerformanceComputing.html\n: Article // CRAN Task View: High-performance and Parallel Computing with R\n: Maintainer: Dirk Eddelbuettel // 2013-05-21\n\n\n* http://www.r-tutor.com/gpu-computing\n: GPU Computing with R\n\n\n==== Parallel Computing in R: GPUs ====\n\n* gputools (http://cran.r-project.org/web/packages/gputools/index.html)\n* cudaBayesreg (http://cran.r-project.org/web/packages/cudaBayesreg/index.html)\n* rgpu (https://trac.nbic.nl/rgpu)\n* magma (http://cran.r-project.org/web/packages/magma/index.html)\n* gcbd (http://cran.r-project.org/web/packages/gcbd/index.html)\n* OpenCL (http://cran.r-project.org/web/packages/OpenCL/index.html)\n* WideLM (http://cran.r-project.org/web/packages/WideLM/index.html)\n* HiPLARM (http://cran.r-project.org/web/packages/HiPLARM/index.html)\n* permGPU (http://cran.r-project.org/web/packages/permGPU/index.html)\n\n\n\n\n== 20130530_144254 ==\n\n=== Introductory Articles to R Statistical Computing Software ===\n\n* http://www.cran.r-project.org/doc/manuals/R-intro.pdf\n: An Introduction to R -- Notes on R: A Programming Environment for Data Analysis and Graphics (Version 3.0.1 (2013-05-16))\n\n\n\n\n== 20130528_213910 ==\n\n=== suppress the command output in R ===\n\n* use sink() function, please!\n <pre>\nsink(file=\"arm_inspect.650k-support_0.012-225x488.log\") # enabling sink operation \n# (to redirect all the stdout to the file specified)\n\ninspect(f_650k_0012)\n\nsink() # disabling sink operation\n# (after this command, you can see the output message to stdout)\n</pre>\n\n\n* References\n\n:* https://stat.ethz.ch/pipermail/r-help/2007-August/138070.html\n:: R sink behavior (stat.ethz.ch)\n\n:* http://stat.ethz.ch/R-manual/R-patched/library/base/html/sink.html\n:: Send R Output to a File\n\n\n\n\n=== defining function in R (user-defined function in R) ===\n\n* use allocation operator \'<-\' to define my custom function\n <pre>\niowa_arm_f <- function (opt_dsrc, opt_support) {\n	print(\">> IOWA ARM: started\");\n	sink(file=\"/dev/null\");\n	t <- as(opt_dsrc, \"transactions\");\n	f <- eclat(opt_dsrc, parameter=list(support=opt_support, tidLists=T));\n	sink();\n	print(\">> IOWA ARM: finished\");\n	dim(tidLists(f));\n	return(f);\n}\n</pre>\n\n* use the function as usual\n <pre>\nf <- iowa_arm_f(ciop_d010, 0.012);\ndim(tidLists(f));\n</pre>\n\n* References\n\n:* http://www.statmethods.net/management/userfunctions.html\n:: User-written Functions -- Quick-R ((B.GOOD))\n\n\n\n== 20130524_103604 ==\n\n\n=== source() ===\n\n <pre>\nsource(\'data_set.R\');\n</pre>\n\n=== arules eclat() algorithm for association rules mining ===\n\n <pre>\nlibrary(arules);\ndata <- list(\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_1187\", \"_addr_23813\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\n			 c(\"_addr_23813\", \"_addr_51276758\"),\n			 c(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\n			 c(\"_addr_51276758\", \"_addr_319275\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\n			 c(\"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\n			 c(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\n			 c(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\n			 c(\"_addr_319275\", \"_addr_51276758\")\n			 );\n\nt <- as(data, \"transactions\");\nf <- eclat(data, parameter=list(tidLists=T, support=0.25))\ndim(tidLists(f))\nas(tidLists(f), \"list\")\nimage(tidLists(f))\ninspect(f)\n</pre>\n\n\n\n\n=== read.table(), write.table(), unlist() ===\n\n <pre>\nblusjune@jimi-hendrix:[basics] $ cat filein.L20 \n\n248425 , 100276969472\n248772 , 78847959040\n249197 , 139913195520\n251828 , 75536334848\n253182 , 76534030336\n254083 , 143595069440\n254961 , 143598755840\n255185 , 150857949184\n255393 , 118433374208\n255755 , 100324941824\n256025 , 85407301632\n258666 , 95264866304\n259078 , 142196629504\n261133 , 88597774336\n263287 , 97312505856\n264135 , 112585678848\n267323 , 96259047424\n267351 , 140665122816\n267634 , 139540049920\n268982 , 117314224128\n</pre>\n\n <pre>\n> datain;\n\n   timestamp      address\n   1     248425 100276969472\n   2     248772  78847959040\n   3     249197 139913195520\n   4     251828  75536334848\n   5     253182  76534030336\n   6     254083 143595069440\n   7     254961 143598755840\n   8     255185 150857949184\n   9     255393 118433374208\n   10    255755 100324941824\n   11    256025  85407301632\n   12    258666  95264866304\n   13    259078 142196629504\n   14    261133  88597774336\n   15    263287  97312505856\n   16    264135 112585678848\n   17    267323  96259047424\n   18    267351 140665122816\n   19    267634 139540049920\n   20    268982 117314224128\n</pre>\n\n <pre>\ndatain <- read.table(\'filein.L20\', col.names=c(\"timestamp\", \"address\"), sep=\",\", header=F);\ndataout <- datain %% 10;\ndataout_ul <- unlist(dataout, use.names=F)\nwrite.table(dataout, file=\"fileout.L20\", append=F, quote=T, sep=\" , \", row.names=F, col.names=T);\n</pre>\n\n\n\n\n=== scan() ===\n\n <pre>\ndatain_double <- scan(file=\"d010\", what=double(), sep=\"\", strip.white=T);\ndatain_char <- scan(file=\"d010\", what=\"character\", sep=\"\\n\", strip.white=T);\ndatain_char <- scan(file=\"d010\", what=\"char\", sep=\";\", strip.white=T);\n</pre>\n\n\n\n\n* Case 1: \'d010\' - data file of wrong format\n\n <pre>\nblusjune@jimi-hendrix:[basics] $ cat d010\n8 4 0 5 7;\n11 5 8;\n8 6 3 5 1;\n2 11 1;\n4 6 3 12 4;\n6 7 0 10 4 7;\n7 6 3;\n3 10 7 6 7 6;\n7 10;\n11 10\n</pre>\n\n </pre>\n> scan(file=\'d010\', what=\"numeric\", sep=\";\\n\", strip.white=T)\nError in scan(file = \"d010\", what = \"numeric\", sep = \";\\n\", strip.white = T) : \n  invalid \'sep\' value: must be one byte\n\n> scan(file=\'d010\', what=\"numeric\", sep=\"\\n\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7;\"    \"11 5 8;\"       \"8 6 3 5 1;\"    \"2 11 1;\"      \n [5] \"4 6 3 12 4;\"   \"6 7 0 10 4 7;\" \"7 6 3;\"        \"3 10 7 6 7 6;\"\n [9] \"7 10;\"         \"11 10\"        \n\n> scan(file=\'d010\', what=\"numeric\", sep=\";\", strip.white=T)\nRead 19 items\n [1] \"8 4 0 5 7\"    \"\"             \"11 5 8\"       \"\"             \"8 6 3 5 1\"   \n [6] \"\"             \"2 11 1\"       \"\"             \"4 6 3 12 4\"   \"\"            \n[11] \"6 7 0 10 4 7\" \"\"             \"7 6 3\"        \"\"             \"3 10 7 6 7 6\"\n[16] \"\"             \"7 10\"         \"\"             \"11 10\"       \n\n> scan(file=\'d010\', what=\"numeric\", sep=\"\", strip.white=T)\nRead 40 items\n [1] \"8\"   \"4\"   \"0\"   \"5\"   \"7;\"  \"11\"  \"5\"   \"8;\"  \"8\"   \"6\"   \"3\"   \"5\"  \n[13] \"1;\"  \"2\"   \"11\"  \"1;\"  \"4\"   \"6\"   \"3\"   \"12\"  \"4;\"  \"6\"   \"7\"   \"0\"  \n[25] \"10\"  \"4\"   \"7;\"  \"7\"   \"6\"   \"3;\"  \"3\"   \"10\"  \"7\"   \"6\"   \"7\"   \"6;\" \n[37] \"7\"   \"10;\" \"11\"  \"10\" \n\n</pre>\n\n\n\n\n* Case 2: \'d020\' - data file of good format\n\n <pre>\nblusjune@jimi-hendrix:[basics] $ cat d020\n8 4 0 5 7; 11 5 8; 8 6 3 5 1; 2 11 1; 4 6 3 12 4; 6 7 0 10 4 7; 7 6 3; 3 10 7 6 7 6; 7 10; 11 10\n</pre>\n\n <pre>\n> scan(file=\'d020\', what=\"numeric\", sep=\"\", strip.white=T)\nRead 40 items\n [1] \"8\"   \"4\"   \"0\"   \"5\"   \"7;\"  \"11\"  \"5\"   \"8;\"  \"8\"   \"6\"   \"3\"   \"5\"  \n[13] \"1;\"  \"2\"   \"11\"  \"1;\"  \"4\"   \"6\"   \"3\"   \"12\"  \"4;\"  \"6\"   \"7\"   \"0\"  \n[25] \"10\"  \"4\"   \"7;\"  \"7\"   \"6\"   \"3;\"  \"3\"   \"10\"  \"7\"   \"6\"   \"7\"   \"6;\" \n[37] \"7\"   \"10;\" \"11\"  \"10\" \n\n> scan(file=\'d020\', what=\"numeric\", sep=\";\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7\"    \"11 5 8\"       \"8 6 3 5 1\"    \"2 11 1\"       \"4 6 3 12 4\"  \n [6] \"6 7 0 10 4 7\" \"7 6 3\"        \"3 10 7 6 7 6\" \"7 10\"         \"11 10\"     \n</pre>\n\n\n\n\n* Case 3: \'d030\' - data file of good format\n\n <pre>\n8 4 0 5 7\n11 5 8\n8 6 3 5 1\n2 11 1\n4 6 3 12 4\n6 7 0 10 4 7\n7 6 3\n3 10 7 6 7 6\n7 10\n11 10\n</pre>\n\n <pre>\n> scan(file=\'d030\', what=\'numeric\', sep=\"\", strip.white=T)\nRead 40 items\n [1] \"8\"  \"4\"  \"0\"  \"5\"  \"7\"  \"11\" \"5\"  \"8\"  \"8\"  \"6\"  \"3\"  \"5\"  \"1\"  \"2\"  \"11\"\n[16] \"1\"  \"4\"  \"6\"  \"3\"  \"12\" \"4\"  \"6\"  \"7\"  \"0\"  \"10\" \"4\"  \"7\"  \"7\"  \"6\"  \"3\" \n[31] \"3\"  \"10\" \"7\"  \"6\"  \"7\"  \"6\"  \"7\"  \"10\" \"11\" \"10\"\n\n> scan(file=\'d030\', what=\'numeric\', sep=\";\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7\"    \"11 5 8\"       \"8 6 3 5 1\"    \"2 11 1\"       \"4 6 3 12 4\"  \n [6] \"6 7 0 10 4 7\" \"7 6 3\"        \"3 10 7 6 7 6\" \"7 10\"         \"11 10\"       \n\n> scan(file=\'d030\', what=\'numeric\', sep=\"\\n\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7\"    \"11 5 8\"       \"8 6 3 5 1\"    \"2 11 1\"       \"4 6 3 12 4\"  \n [6] \"6 7 0 10 4 7\" \"7 6 3\"        \"3 10 7 6 7 6\" \"7 10\"         \"11 10\"  \n</pre>\n\n\n\n\n== 20130502_032415 ==\n\n=== R script ===\n\n <pre>\na1mjjung@secm:[r_stat] $ cat > iowa_anal_1010.R << EOF\n#iowa_anal_1010.R\n#20130430_140506\n\n\nlibrary(e1071);\nlibrary(arules);\nlibrary(scatterplot3d);\n\n\nrm(list=ls());\n\n\nmyd <- read.table(\'r_stat_infile\', col.names=c(\"_hitcnt_\", \"_addr_\", \"_mwid_\"));\nmyd_mwid <- unlist(myd[3]);\nmyd_addr <- unlist(myd[2]);\nmyd_hitcnt <- unlist(myd[1]);\n\n\nmymat <- cbind(myd_mwid, myd_addr, myd_hitcnt);\ncolnames(mymat) <- c(\"_mwid_\", \"_addr_\", \"_hcnt_\");\nnocl <- 22; itmax <- 100; (cl1 <- kmeans(mymat, nocl, iter.max=itmax));\n\n\npng(\'output_col_by_hitcnt.png\', width=1024, height=2048);\nplot(myd_mwid, myd_addr, col=myd_hitcnt);\ndev.off();\npng(\'output_col_by_cluster.png\', width=1024, height=2048);\nplot(myd_mwid, myd_addr, col=cl1$cluster);\ndev.off();\nEOF\n\n</pre>\n\n\n\n== 20130428_233708 ==\n\n\n\n=== R (r_stat) references ===\n\n\n* manual/introduction to R\n:- [http://cran.r-project.org/doc/manuals/R-intro.pdf R introduction]\n:- [http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf arules (Association Rules Mining)]\n:- [http://www.cl.cam.ac.uk/~av308/vlachos_msc_thesis.pdf using SVM]\n\n\n* \'%in%\' and example() function\n\n <pre>\nOn 8 May 2011 21:18, Berwin A Turlach <[hidden email]> wrote:\n> G\'day Dan,\n>\n> On Sun, 8 May 2011 05:06:27 -0400\n> Dan Abner <[hidden email]> wrote:\n>\n>> Hello everyone,\n>>\n>> I am attempting to use the %in% operator with the ! to produce a NOT\n>> IN type of operation. Why does this not work? Suggestions?\n\nAlternatively,\n\nexample(`%in%`)\n\nor\n\n`%ni%` = Negate(`%in%`)\n\nHTH,\n\nbaptiste\n</pre>\n\n\n\n\n=== kmeans-svm #2 :: applying the kmeans result to svm (as a guideline for supervising) ((B.GOOD)) ===\n\n <pre>\n\n# combining the columns xy (time_t1, file_id) and cl4 (cluster_id)\nxy_cl4 <- cbind(xy, cl4$cluster);\n\n# assign the column names\ncolnames(xy_cl4) <- c(\"time_t1\", \"file_id\", \"cluster_id\");\n\n# prepare the x\'s and y for svm\nsmv_x <- subset(xy_cl4, select= -cluster_id);\nsmv_y <- subset(xy_cl4, select= cluster_id);\n\n# load the library \'e1071\'\nlibrary(e1071);\n\n# execute svm analysis to get model \'m\'\nm <- svm(svm_x, svm_y);\n\n# predict a new value with the model \'m\' and x\'s data\npred <- predict(m, svm_x, decision.values = T);\n\n# check the prediction result by comparing the predicted value \'pred\' with original value \'svm_y\'\ntable(pred, svm_y);\n\n# plot the result\nplot(cmdscale(dist(xy_cl4[, -3])), col=pred, pch=c(\"o\", \"+\")[1:120 %in% m$index + 1]);\n\n# write the resultant xy_cl4 table to the file(.csv)\nwrite.csv(xy_cl4, file=\"xy_cl4.csv\", row.names=T, col.names=T);\n\n</pre>\n\n\n\n\n=== kmeans-svm #1 :: kmeans example ((B.GOOD)) ===\n\n <pre>\n\n# clear all the data in memory\nrm(list=ls());\n\n# read the data from file (.csv format)\nmyd <- read.csv(\'traces/iowa_v3.csv\', header=T);\n\n# unlist() to convert \'list\' type data to \'vector\' type data for further numerical calculation\nmyd_c1 <- unlist(myd[1]); # field: time_t0\nmyd_c2 <- unlist(myd[2]); # field: time_t1\nmyd_c3 <- unlist(myd[3]); # field: prog\nmyd_c4 <- unlist(myd[4]); # field: file\n\n# create 2-D matrix (x-y) for 2-D kmeans analysis\nxy <- cbind(myd_c2, myd_c4); # combining columns: \'time_t1\' and \'file\'\n\n# assign the column names for x-label and y-label for better look of plot()\ncolnames(xy) <- c(\"time_t1\", \"file_id\");\n\n# load the library \'e1071\'\nlibrary(e1071);\n\n# execute kmeans analysis until we get satisfactory clustering performance value (value = between_SS / total_SS)\n# number of clusters is set to \'6\' huristically after plotting data xy to see the outlook\n# number of maximum iterations is set to \'100\' huristically after executing kmeans analysis several times\ncl1 <- kmeans(xy, 6, iter.max=100); # initial trial, the clustering performance value is ... (cannot evaluate)\ncl2 <- kmeans(xy, 6, iter.max=100); # second trial, the clustering performance value is higher than the first trial\ncl3 <- kmeans(xy, 6, iter.max=100); # third trial, the clustering performance value is higher than the second trial\n(cl4 <- kmeans(xy, 6, iter.max=100)); # fourth trial, the clustering performance value is not going higher than the last trial\n\n# 2-D plotting\nplot(xy, col=cl4$cluster);\n\n</pre>\n\n\n\n\n* most outer parentheses are used to see the result of execution\n\n <pre>\n\n> (cl4 <- kmeans(xy, 6, iter.max=100))\nK-means clustering with 6 clusters of sizes 19, 22, 19, 15, 20, 25\n\nCluster means:\n   time_t1    file_id\n1 10.52632 32.2105263\n2 14.40909 43.3181818\n3 18.57895 33.5263158\n4 22.00000  0.2666667\n5  6.50000 11.7000000\n6  2.00000  1.0000000\n\nClustering vector:\n  time_t11   time_t12   time_t13   time_t14   time_t15   time_t16   time_t17 \n         6          6          6          6          5          5          5 \n  time_t18   time_t19  time_t110  time_t111  time_t112  time_t113  time_t114 \n         5          1          2          1          1          2          2 \n time_t115  time_t116  time_t117  time_t118  time_t119  time_t120  time_t121 \n         2          2          3          3          3          3          4 \n time_t122  time_t123  time_t124  time_t125  time_t126  time_t127  time_t128 \n         4          4          6          6          6          6          6 \n time_t129  time_t130  time_t131  time_t132  time_t133  time_t134  time_t135 \n         5          5          5          5          1          1          1 \n time_t136  time_t137  time_t138  time_t139  time_t140  time_t141  time_t142 \n         1          2          2          2          2          2          3 \n time_t143  time_t144  time_t145  time_t146  time_t147  time_t148  time_t149 \n         3          3          4          4          4          6          6 \n time_t150  time_t151  time_t152  time_t153  time_t154  time_t155  time_t156 \n         6          6          6          5          5          5          5 \n time_t157  time_t158  time_t159  time_t160  time_t161  time_t162  time_t163 \n         1          1          1          1          2          2          2 \n time_t164  time_t165  time_t166  time_t167  time_t168  time_t169  time_t170 \n         2          3          3          3          3          4          4 \n time_t171  time_t172  time_t173  time_t174  time_t175  time_t176  time_t177 \n         4          6          6          6          6          6          5 \n time_t178  time_t179  time_t180  time_t181  time_t182  time_t183  time_t184 \n         5          5          5          1          1          1          1 \n time_t185  time_t186  time_t187  time_t188  time_t189  time_t190  time_t191 \n         2          2          2          2          3          3          3 \n time_t192  time_t193  time_t194  time_t195  time_t196  time_t197  time_t198 \n         3          4          4          4          6          6          6 \n time_t199 time_t1100 time_t1101 time_t1102 time_t1103 time_t1104 time_t1105 \n         6          6          5          5          5          5          1 \ntime_t1106 time_t1107 time_t1108 time_t1109 time_t1110 time_t1111 time_t1112 \n         1          1          1          2          2          2          2 \ntime_t1113 time_t1114 time_t1115 time_t1116 time_t1117 time_t1118 time_t1119 \n         3          3          3          3          4          4          4 \ntime_t1120 \n         6 \n\nWithin cluster sum of squares by cluster:\n[1]  51.89474 212.09091 117.36842  14.93333  53.20000 122.00000\n (between_SS / total_SS =  98.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"        \n> \n\n</pre>\n\n\n\n\n=== example() function in R ((B.GOOD)) ===\n\nexample() function is very very very useful to grasp the quick overview of some \'concept\' or \'function\' in R\n\n <pre>\n\nexample(svm)\nexample(kmeans)\nexample(\'%in%\')\nexample(rm)\nexample(plot)\n\n</pre>\n\n\n\n\n=== convert list to vector ===\n\nNote: the first line of \'iowa_v3.csv\' file should be the header information line (not the empty line containing just \',\')\n\n <pre>\nx_as_list_type <- read.csv(\'traces/iowa_v3.csv\', header=T)\nx_as_int_vector <- unlist(x_as_list_type, use.name=F)\n</pre>\n\n\n\n\n== 20130329_111903 ==\n\n=== R script file execution from command line ===\n\n$ cat > ex1.R << EOF\n <pre>\nlibrary(\"arules\")\ndata(\"Epub\")\nEpub\nsummary(Epub)\nquit(save=\"no\")\n\n## NOTE:\n# the last line \'quit(save=\"no\")\' is very important\n# to avoid the hang-like situation of R\nEOF\n</pre>\n\n$ R < ex1.R\n <pre>\nJob <13833365> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura009>>\n\nR version 2.15.2 (2012-10-26) -- \"Trick or Treat\"\nCopyright (C) 2012 The R Foundation for Statistical Computing\nISBN 3-900051-07-0\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType \'license()\' or \'licence()\' for distribution details.\n\nR is a collaborative project with many contributors.\nType \'contributors()\' for more information and\n\'citation()\' on how to cite R or R packages in publications.\n\nType \'demo()\' for some demos, \'help()\' for on-line help, or\n\'help.start()\' for an HTML browser interface to help.\nType \'q()\' to quit R.\n\n[Previously saved workspace restored]\n\n> Loading required package: Matrix\nLoading required package: lattice\n\nAttaching package: \'arules\'\n\nThe following object(s) are masked from \'package:base\':\n\n    %in%, write\n\n> > transactions in sparse format with\n 15729 transactions (rows) and\n 936 items (columns)\n> transactions as itemMatrix in sparse format with\n 15729 rows (elements/itemsets/transactions) and\n 936 columns (items) and a density of 0.001758755 \n\nmost frequent items:\ndoc_11d doc_813 doc_4c6 doc_955 doc_698 (Other) \n    356     329     288     282     245   24393 \n\nelement (itemset/transaction) length distribution:\nsizes\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n11615  2189   854   409   198   121    93    50    42    34    26    12    10 \n   14    15    16    17    18    19    20    21    22    23    24    25    26 \n   10     6     8     6     5     8     2     2     3     2     3     4     5 \n   27    28    30    34    36    38    41    43    52    58 \n    1     1     1     2     1     2     1     1     1     1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.646   2.000  58.000 \n\nincludes extended item information - examples:\n   labels\n1 doc_11d\n2 doc_13d\n3 doc_14c\n\nincludes extended transaction information - examples:\n      transactionID           TimeStamp\n10792  session_4795 2003-01-02 10:59:00\n10793  session_4797 2003-01-02 21:46:01\n10794  session_479a 2003-01-03 00:50:38\n> \n</pre>\n\n\n=== paste() example ===\n\n* example 1\n <pre>\n> 1:6\n[1] 1 2 3 4 5 6\n> paste(1:6)\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n> paste(\"A\", 1:6)\n[1] \"A 1\" \"A 2\" \"A 3\" \"A 4\" \"A 5\" \"A 6\"\n> paste(\"A\", 1:6, sep=\"\")\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\" \"A6\"\n> 2:7\n[1] 2 3 4 5 6 7\n> seq(8,3,by=-1)\n[1] 8 7 6 5 4 3\n> paste(\"A\", 1:6, 2:7, seq(8,3,by=-1), sep=\"\")\n[1] \"A128\" \"A237\" \"A346\" \"A455\" \"A564\" \"A673\"\n> paste(\"A\", 1:6, 2:7, seq(8,3,by=-1), sep=\"-\")\n[1] \"A-1-2-8\" \"A-2-3-7\" \"A-3-4-6\" \"A-4-5-5\" \"A-5-6-4\" \"A-6-7-3\"\n> paste(\"A\", 1:6, 2:7, seq(8,3,by=-1), sep=\"/\")\n[1] \"A/1/2/8\" \"A/2/3/7\" \"A/3/4/6\" \"A/4/5/5\" \"A/5/6/4\" \"A/6/7/3\"\n</pre>\n\n* example 2\n <pre>\n> stopifnot(identical(str1 <- paste(\"A\", 1:6, sep=\"\"), str2 <- paste0(\"A\", 1:6)))\n> str1\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\" \"A6\"\n> str2\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\" \"A6\"\n\n> paste(\"Today is\", date())\n[1] \"Today is Fri Mar 29 11:22:15 2013\"\n</pre>\n\n\n\n\n== 20130327_183541 ==\n\n=== list() examples ===\n\n <pre>\n> myl = list(apple=1, banana=2, cherry=3, durian=4, elderberry=5)\n> myl\n$apple\n[1] 1\n\n$banana\n[1] 2\n\n$cherry\n[1] 3\n\n$durian\n[1] 4\n\n$elderberry\n[1] 5\n\n> myl$apple\n[1] 1\n> myl$banana\n[1] 2\n> myl$cherry\n[1] 3\n> myl$durian\n[1] 4\n> myl$elderberry\n[1] 5\n> \n</pre>\n\n\n=== read-from/save-to a file ===\n\n <pre>\n# create a formatted transaction data\n> myd <- paste(\"apple,banana\", \"apple\", \"banana,cherry\", \"banana,cherry,durian\", sep=\"\\n\")> cat(myd)\napple,banana\napple\nbanana,cherry\n\n# write the transaction data to the file\n> write(myd, file = \"myd_basket_2\") \n\n# read the transaction data from the file, and save it to a variable\n> myt <- read.transactions(\"myd_basket_2\", format = \"basket\", sep=\",\")\n\n# inspect the transaction variable\n> inspect(myt)\n  items   \n1 {apple, \n   banana}\n2 {apple} \n3 {banana,\n   cherry}\n4 {banana,\n   cherry,\n   durian}\n</pre>\n\n\n\n\n=== clear workspace (delete data) ===\n\n* References\n* [https://stat.ethz.ch/pipermail/r-help/2007-August/137694.html Clear Workspace in R]\n* [http://stackoverflow.com/questions/11761992/remove-data-from-workspace Advanced method to clear data in R]\n\n* simply, remove three data \'data_1\', \'data_2\', \'data_3\'\n <pre>\nrm(data_1, data_2, data_3)\n</pre>\n\n* remove all the data searchable by ls()\n <pre>\nrm(list = ls())\n# \'list\' is a name of parameter to be passed into \'rm()\' function,\n# so it cannot be changed, it should be \"list\" literally.\n</pre>\n\n* remove all objects whose name begins with the string \"tmp\"\n <pre>\nrm(list = ls()[grep(\"^tmp\", ls())])\nrm(list = ls(pattern=\"^tmp\"))	# making use of the \'pattern\' argument\n</pre>\n\n== 20130313_172639 ==\n\n\n=== SVM example ===\n\n <pre>\n     data(iris)\n     attach(iris)\n     \n     ## classification mode\n     # default with factor response:\n     model <- svm(Species ~ ., data = iris)\n     \n     # alternatively the traditional interface:\n     x <- subset(iris, select = -Species)\n     y <- Species\n     model <- svm(x, y) \n     \n     print(model)\n     summary(model)\n     \n     # test with train data\n     pred <- predict(model, x)\n     # (same as:)\n     pred <- fitted(model)\n     \n     # Check accuracy:\n     table(pred, y)\n     \n     # compute decision values and probabilities:\n     pred <- predict(model, x, decision.values = TRUE)\n     attr(pred, \"decision.values\")[1:4,]\n     \n     # visualize (classes by color, SV by crosses):\n     plot(cmdscale(dist(iris[,-5])),\n          col = as.integer(iris[,5]),\n          pch = c(\"o\",\"+\")[1:150 %in% model$index + 1])\n</pre>\n\n\n=== cmdscale (Classical MultiDimensional Scaling) ===\n\n* Description\n: Classical multidimensional scaling of a data matrix. (a.k.a. principal coordinates analysis (Gower, 1966)\n\n* Usage\n: cmdscale(d, k=2, eig=FALSE, add=FALSE, x.ret=FALSE)\n\n* Arguments\n: \'\'\'d [mandatory]\'\'\': a distance structure such as that returned by \'dist\' or a full symmetric matrix containing the dissimilarities\n: k [optional]: the maximum dimension of the space which the data are to be represented in; must be in {1, 2, ..., n-1}\n: eig [optional]: indicates whether eigenvalues should be returned\n: add [optional]: logical indicating if an additive constant c* should be computed, and added to the non-diagonal dissimilarities such that the modified dissimilarities are Euclidean\n: x.ret [optional]: indicates whether the doubly centered symmetric distance matrix should be returned\n\n* Details\n: Multidimensional scaling takes a set of dissimilarities and returns a set of points such that the distances between the points are approximately equal to the dissimilarities\n\n\n=== dist (Distance matrix computation) ===\n\nThis computes and returns the distance matrix computed by using the specified distance measure to compute the distances between the rows of a data matrix\n\n* Usage\n: dist(x, method = \"euclidean\", diag = FALSE, upper = FALSE, p = 2)\n\n* Arguments\n: x\n:: a numeric matrix, data frame or \'dist\' object\n: method\n:: the distance measure to be used. this must be one of \"euclidean\", \"maximum\", \"manhattan\", \"canberra\", \"binary\", or \"minkowski\" (any unambiguous substring can be given)\n: diag\n\n* Examples (by blusjune)\n\n <pre>\n> mat_a\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    1    1\n[2,]    2    2    2    2\n[3,]    3    3    3    3\n[4,]    0    0    0    0\n> dist(mat_a)\n  1 2 3\n2 2    \n3 4 2  \n4 2 4 6\n> cmdscale(dist(mat_a))\n     [,1]          [,2]\n[1,]    1  5.809542e-08\n[2,]   -1  3.057654e-09\n[3,]   -3  9.172961e-09\n[4,]    3 -9.172961e-09\n> dist(cmdscale(dist(mat_a)))\n  1 2 3\n2 2    \n3 4 2  \n4 2 4 6\n</pre>\n\n <pre>\n> mat_b\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    2    2    2\n[3,]    0    0    0\n> dist(mat_b)\n         1        2\n2 1.732051         \n3 1.732051 3.464102\n> cmdscale(dist(mat_b))\n              [,1]         [,2]\n[1,]  7.412908e-33 1.564993e-08\n[2,] -1.732051e+00 2.477578e-09\n[3,]  1.732051e+00 2.477578e-09\n> dist(cmdscale(dist(mat_b)))\n         1        2\n2 1.732051         \n3 1.732051 3.464102\n\n> ((1-2)**2 + (1-2)**2 + (1-2)**2) ** 0.5\n[1] 1.732051\n> ((2-0)**2 + (2-0)**2 + (2-0)**2) ** 0.5\n[1] 3.464102\n</pre>\n\n\n== 20130306_185022 ==\n\n=== R package (\'e1071\') installation from command line ===\n\n: Assumes that already downloaded and unpacked properly the \'e1071_1.6-1.tar.gz\' file\n\n <pre>\n\n1mjjung@secm:[e1071] $ R CMD INSTALL e1071\nJob <13412785> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura007>>\nWarning: invalid package \'e1071\'\nError: ERROR: no packages specified\n\n\n\n\na1mjjung@secm:[e1071] $ cd ..\nla1mjjung@secm:[e1071_build] $ l\ntotal 280\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:45 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\n\n\n\n\na1mjjung@secm:[e1071_build] $ R CMD INSTALL e1071\nJob <13412786> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n* installing to library \'/home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15\'\n* installing *source* package \'e1071\' ...\n** package \'e1071\' successfully unpacked and MD5 sums checked\nchecking for C++ compiler default output file name... a.out\nchecking whether the C++ compiler works... yes\nchecking whether we are cross compiling... no\nchecking for suffix of executables... \nchecking for suffix of object files... o\nchecking whether we are using the GNU C++ compiler... yes\nchecking whether g++ accepts -g... yes\n** libs\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c Rsvm.c -o Rsvm.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cmeans.c -o cmeans.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cshell.c -o cshell.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c floyd.c -o floyd.o\ng++ -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c svm.cpp -o svm.o\ng++ -shared -L/usr/local/lib64 -o e1071.so Rsvm.o cmeans.o cshell.o floyd.o svm.o\ninstalling to /home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15/e1071/libs\n** R\n** inst\n** preparing package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n   \'svmdoc.Rnw\' \n** testing if installed package can be loaded\n\n* DONE (e1071)\n\n\n\n\na1mjjung@secm:[e1071_build] $ R\nJob <13412787> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n\nR version 2.15.2 (2012-10-26) -- \"Trick or Treat\"\nCopyright (C) 2012 The R Foundation for Statistical Computing\nISBN 3-900051-07-0\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType \'license()\' or \'licence()\' for distribution details.\n\nR is a collaborative project with many contributors.\nType \'contributors()\' for more information and\n\'citation()\' on how to cite R or R packages in publications.\n\nType \'demo()\' for some demos, \'help()\' for on-line help, or\n\'help.start()\' for an HTML browser interface to help.\nType \'q()\' to quit R.\n\n> ?svm\nNo documentation for \'svm\' in specified packages and libraries:\nyou could try \'??svm\'\n> library(e1071) \nLoading required package: class\n> ?svm\n> \nSave workspace image? [y/n/c]: y\n\n\n\n\na1mjjung@secm:[e1071_build] $ pwd\n/home/X0101/a1mjjung/x/R/e1071_build\n\na1mjjung@secm:[e1071_build] $ ls -alF\ntotal 284\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:47 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\n-rw------- 1 a1mjjung X0101     25 Mar  6 18:47 .Rhistory\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:46 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\n\n</pre>\n\n\n\n== 20130304_190007 ==\n\n\n\n=== test type of some object ===\n\n <pre>\n> x <- scan(\"/tmp/scan.txt\", what=list(NULL, name=character()))\n> x <- x[sapply(x, length) > 0]\n> is.vector(x)\n\n\n> x = mat.or.vec(100,1)\n> if (is.integer(x) == TRUE) { print (\"YES\") } else { print (\"NO\") }\n[1] \"NO\"\n> if (is.vector(x) == TRUE) { print (\"YES, vector\") } else { print (\"NO, NOT vector\") }\n[1] \"YES, vector\"\n</pre>\n\n\n\n\n=== Data import (load data from a file) ===\n\n* scan()\n <pre>\n > x1 <- scan(\"/etc/hosts\", what=character())\n\n > x1     \n [1] \"127.0.0.1\"       \"localhost\"       \"#127.0.1.1\"      \"stones\"         \n [5] \"#\"               \"The\"             \"following\"       \"lines\"          \n [9] \"are\"             \"desirable\"       \"for\"             \"IPv6\"           \n[13] \"capable\"         \"hosts\"           \"::1\"             \"ip6-localhost\"  \n[17] \"ip6-loopback\"    \"fe00::0\"         \"ip6-localnet\"    \"ff00::0\"        \n[21] \"ip6-mcastprefix\" \"ff02::1\"         \"ip6-allnodes\"    \"ff02::2\"        \n[25] \"ip6-allrouters\"  \"10.0.2.15\"       \"stones-eth0\"     \"#192.168.1.109\" \n[29] \"stones\"          \"hd-master-01\"    \"#192.168.1.110\"  \"pavement\"       \n[33] \"hd-slave-0001\"   \"192.168.1.112\"   \"stones\"          \"hd-master-01\"   \n[37] \"hd-slave-0001\"   \"kandinsky\"       \"192.168.1.110\"   \"pavement\"       \n[41] \"hd-slave-0002\"  \n</pre>\n\n* read.table()\n <pre>\n> iot_r <- read.table(\'tracelog.msn_filesrvr.R\')  \n</pre>\n\n\n\n=== function defintion, for loop in R ===\n\n <pre>\n> avg_smoothing <- function(src, srcl, sf) {\n    tgtl = srcl + 1 - sf\n    tgt <- mat.or.vec(tgtl, 1)\n    for (i in 1:tgtl) {\n        tgt[i] = mean(src[i:(i+sf-1)])\n    }\n    return (tgt)\n}\n\n> vec1 <- rnorm(100, mean=10, sd=1)\n> vec1_sf2 <- avg_smoothing(vec1, 100, 2)\n> vec1_sf4 <- avg_smoothing(vec1, 100, 4)\n> vec1_sf8 <- avg_smoothing(vec1, 100, 8)\n\n> plot(vec1, col=\"gray\", type=\"l\")\n> points(vec1_sf2, col=\"red\", type=\"l\")\n> points(vec1_sf4, col=\"blue\", type=\"l\")\n> points(vec1_sf8, col=\"green\", type=\"l\")\n</pre>\n\n\n\n\n== 20130127_225539 ==\n\n=== R Installation ===\n\n* to install R statistical computing software\n** me@matrix$ sudo apt-get install r-base\n\n* to start R from command line, just type \'R\' in your terminal\n** me@matrix$ R\n\n* RStudio download [http://www.rstudio.com/ide/download/]\n** RStudio Desktop [http://www.rstudio.com/ide/download/desktop]\n**: If you run R on your desktop\n** RStudio Server [http://www.rstudio.com/ide/download/server]\n**: If you run R on a Linux server and want to enable users to remotely access RStudio using a web browser\n*** RStudio Server Getting Started [http://www.rstudio.com/ide/docs/server/getting_started]\n\n <pre>\nsudo apt-get install r-base\nwget http://download2.rstudio.org/rstudio-server-0.97.312-amd64.deb\nsudo gdebi rstudio-server-0.97.312-amd64.deb \nbsc.adm.create_me\ngoogle-chrome http://kandinsky:8787 # type ID/PW (me?!)\n</pre>\n\n\n* Debian Packages of R Software [http://cran.r-project.org/bin/linux/debian/README.html]\n\n=== R Guide/Tutorial/Example ===\n\n* R Tutorial [http://www.r-tutor.com/]\n** R Tutorial:: Data Import [http://www.r-tutor.com/r-introduction/data-frame/data-import]\n\n\n* Example R scripts [http://people.eng.unimelb.edu.au/aturpin/R/index.html]]\n\n\n* R by example [http://www.mayin.org/ajayshah/KB/R/index.html]\n\n\n* R example:: Kmeans [http://stat.ethz.ch/R-manual/R-patched/library/stats/html/kmeans.html]\n\n\n* R package install howto\n; e1071\n: Misc Functions of the Department of Statistics (e1071), TU Wien\n:* package-installation and loading\n install.packages(\"e1071\") # installing the package \'e1071\'\n library(\"e1071\") # loading the installed package \'e1071\'\n\n\n* Importing SAS, SPSS and Stata files into R [http://staff.washington.edu/glynn/r_import.pdf]\n\n\n=== R Troubleshooting ===\n\n* Problems importing csv file/converting from integer to double in R [http://stackoverflow.com/questions/8381839/problems-importing-csv-file-converting-from-integer-to-double-in-r]\n\n\n=== Misc. ===\n\n* WEKA Tutorial [http://www.cs.utexas.edu/users/ml/tutorials/Weka-tut/]\n* weka is a metric prefix for 10^30','utf-8'),(292,'== ## bNote-2013-06-13 ==\n\n=== Microsoft BuildServer07 Workload Analysis ===\n\n <pre>\n\nblusjune@jimi-hendrix:[data.out] $ cat A/f040.msbuild.anal_s0010.__valu__sig__.out \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  3\n__valu__sig__ _n_o_sigaddrs :  75405\n__valu__sig__ _sigioc_acc :  311328\n__valu__sig__ _sigaddrs_efficiency :  4.1287447782\n__valu__sig__ _n_o_addr_total :  396735\n__valu__sig__ _ioc_total :  714151\n\nblusjune@jimi-hendrix:[data.out] $ cat R/f040.msbuild.anal_s0010.__valu__sig__.out \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  3\n__valu__sig__ _n_o_sigaddrs :  72103\n__valu__sig__ _sigioc_acc :  275949\n__valu__sig__ _sigaddrs_efficiency :  3.82715004924\n__valu__sig__ _n_o_addr_total :  364572\n__valu__sig__ _ioc_total :  645921\n\nblusjune@jimi-hendrix:[data.out] $ cat W/f040.msbuild.anal_s0010.__valu__sig__.out \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  56\n__valu__sig__ _n_o_sigaddrs :  66\n__valu__sig__ _sigioc_acc :  17064\n__valu__sig__ _sigaddrs_efficiency :  258.545454545\n__valu__sig__ _n_o_addr_total :  37104\n__valu__sig__ _ioc_total :  68230\n\n</pre>\n\n\n== ## bNote-2013-06-05 ==\n\n=== Patentization (CIOP) ===\n\n\n=== Python to binary executable information ===\n\n=== Sensing: RStudio Project ===\n\n* RStudio team contributes code to many R packages and projects [http://www.rstudio.com/projects/]\n:* Shiny: Easy web applications in R [http://www.rstudio.com/shiny/]\n:* plyr: the split-apply-combine strategy for R [http://plyr.had.co.nz/]\n\n== ## bNote-2013-06-07 ==\n\n=== I/O Tracing on Microsoft Windows Using Xperf ===\n\n* [[I/O tracing on Microsoft Windows using xperf]]\n\n* [[Windows Programming on Linux with MinGW]]\n\n\n== ## bNote-2013-06-05 ==\n\n=== I/O Tracing on Microsoft Windows Using Xperf ===\n\n* [[I/O tracing on Microsoft Windows using xperf]]\n\n== ## bNote-2013-06-03 ==\n\n=== R HMM ===','utf-8'),(293,'== ## bNote-2013-06-13 ==\n\n=== EMC 장비 구매 ===\n\n==== Question Lists ====\n\n* DAE (Disk Array Enclosure) 내에도 Processor와 DRAM이 들어가는가? 만약 그렇다면 OS는 어떤 OS가 구동되는가?\n\n* DPE (Disk Processor Enclosure)와 DAE 간의 연결은 구체적으로 어떻게 되는가? DPE와 DAE 간에 SAS cable로 직접 연결되는가? 몇 개의 SAS cable로 연결되는가? 결국, DPE와 DAE 간 연결 Bandwidth는 얼마인가? 혹시 DPE와 DAE간 연결 케이블이 성능 병목의 원인이 될 수 있지는 않는가?\n\n* DPE에 들어가는 Disk들은 어떤 종류가 들어가며, 각각 몇 개가 들어가서 총 몇개인가? (즉, SAS SSD, SAS HDD, SATA HDD 각각 몇개?)\n\n* DAE에 들어가는 Disk들은 어떤 종류가 들어가며, 각각 몇 개가 들어가서 총 몇개인가? (즉, SAS SSD, SAS HDD, SATA HDD 각각 몇개?)\n\n* DPE에서 Server들로 연결되는 SAN Interface는 실제로 10GbE인가? 그 위에 iSCSI 프로토콜이 구동되는 것인가?\n\n* DPE에서 X-blade (NAS Processor)로의 연결은 구체적으로 어떻게 되는가? SAS cable인지? 아니면 10GbE인지? 아니면 FC인지?\n \n\n\n=== Microsoft BuildServer07 Workload Analysis ===\n\n <pre>\n\nblusjune@jimi-hendrix:[data.out] $ cat A/f040.msbuild.anal_s0010.__valu__sig__.out \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  3\n__valu__sig__ _n_o_sigaddrs :  75405\n__valu__sig__ _sigioc_acc :  311328\n__valu__sig__ _sigaddrs_efficiency :  4.1287447782\n__valu__sig__ _n_o_addr_total :  396735\n__valu__sig__ _ioc_total :  714151\n\nblusjune@jimi-hendrix:[data.out] $ cat R/f040.msbuild.anal_s0010.__valu__sig__.out \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  3\n__valu__sig__ _n_o_sigaddrs :  72103\n__valu__sig__ _sigioc_acc :  275949\n__valu__sig__ _sigaddrs_efficiency :  3.82715004924\n__valu__sig__ _n_o_addr_total :  364572\n__valu__sig__ _ioc_total :  645921\n\nblusjune@jimi-hendrix:[data.out] $ cat W/f040.msbuild.anal_s0010.__valu__sig__.out \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  56\n__valu__sig__ _n_o_sigaddrs :  66\n__valu__sig__ _sigioc_acc :  17064\n__valu__sig__ _sigaddrs_efficiency :  258.545454545\n__valu__sig__ _n_o_addr_total :  37104\n__valu__sig__ _ioc_total :  68230\n\n</pre>\n\n== ## bNote-2013-06-05 ==\n\n=== Patentization (CIOP) ===\n\n\n=== Python to binary executable information ===\n\n=== Sensing: RStudio Project ===\n\n* RStudio team contributes code to many R packages and projects [http://www.rstudio.com/projects/]\n:* Shiny: Easy web applications in R [http://www.rstudio.com/shiny/]\n:* plyr: the split-apply-combine strategy for R [http://plyr.had.co.nz/]\n\n== ## bNote-2013-06-07 ==\n\n=== I/O Tracing on Microsoft Windows Using Xperf ===\n\n* [[I/O tracing on Microsoft Windows using xperf]]\n\n* [[Windows Programming on Linux with MinGW]]\n\n\n== ## bNote-2013-06-05 ==\n\n=== I/O Tracing on Microsoft Windows Using Xperf ===\n\n* [[I/O tracing on Microsoft Windows using xperf]]\n\n== ## bNote-2013-06-03 ==\n\n=== R HMM ===','utf-8'),(294,'== ## bNote-2013-06-13 ==\n\n=== EMC 장비 구매 ===\n\n==== EMC VNX 5300 관련 Question Lists ====\n\n* DAE (Disk Array Enclosure) 내에도 Processor와 DRAM이 들어가는가? 만약 그렇다면 OS는 어떤 OS가 구동되는가?\n\n* DPE (Disk Processor Enclosure)와 DAE 간의 연결은 구체적으로 어떻게 되는가? DPE와 DAE 간에 SAS cable로 직접 연결되는가? 몇 개의 SAS cable로 연결되는가? 결국, DPE와 DAE 간 연결 Bandwidth는 얼마인가? 혹시 DPE와 DAE간 연결 케이블이 성능 병목의 원인이 될 수 있지는 않는가?\n\n* DPE에 들어가는 Disk들은 어떤 종류가 들어가며, 각각 몇 개가 들어가서 총 몇개인가? (즉, SAS SSD, SAS HDD, SATA HDD 각각 몇개?)\n\n* DAE에 들어가는 Disk들은 어떤 종류가 들어가며, 각각 몇 개가 들어가서 총 몇개인가? (즉, SAS SSD, SAS HDD, SATA HDD 각각 몇개?)\n\n* DPE에서 Server들로 연결되는 SAN Interface는 실제로 10GbE인가? 그 위에 iSCSI 프로토콜이 구동되는 것인가?\n\n* DPE에서 X-blade (NAS Processor)로의 연결은 구체적으로 어떻게 되는가? SAS cable인지? 아니면 10GbE인지? 아니면 FC인지?\n\n\n==== Supermicro Commodity Storage Server 관련 Question Lists ====\n\n* 우리가 NAS interface를 올린다고 하면, NAS node를 DPE node와 별개로 구축하는 것이 적절한가? 아니면 NAS 기능을 수행하는 SW가 DPE node에서 구동되는 것이 적절한가?\n\n=== Microsoft BuildServer07 Workload Analysis ===\n\n <pre>\n\nblusjune@jimi-hendrix:[data.out] $ cat A/f040.msbuild.anal_s0010.__valu__sig__.out \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  3\n__valu__sig__ _n_o_sigaddrs :  75405\n__valu__sig__ _sigioc_acc :  311328\n__valu__sig__ _sigaddrs_efficiency :  4.1287447782\n__valu__sig__ _n_o_addr_total :  396735\n__valu__sig__ _ioc_total :  714151\n\nblusjune@jimi-hendrix:[data.out] $ cat R/f040.msbuild.anal_s0010.__valu__sig__.out \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  3\n__valu__sig__ _n_o_sigaddrs :  72103\n__valu__sig__ _sigioc_acc :  275949\n__valu__sig__ _sigaddrs_efficiency :  3.82715004924\n__valu__sig__ _n_o_addr_total :  364572\n__valu__sig__ _ioc_total :  645921\n\nblusjune@jimi-hendrix:[data.out] $ cat W/f040.msbuild.anal_s0010.__valu__sig__.out \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  56\n__valu__sig__ _n_o_sigaddrs :  66\n__valu__sig__ _sigioc_acc :  17064\n__valu__sig__ _sigaddrs_efficiency :  258.545454545\n__valu__sig__ _n_o_addr_total :  37104\n__valu__sig__ _ioc_total :  68230\n\n</pre>\n\n== ## bNote-2013-06-05 ==\n\n=== Patentization (CIOP) ===\n\n\n=== Python to binary executable information ===\n\n=== Sensing: RStudio Project ===\n\n* RStudio team contributes code to many R packages and projects [http://www.rstudio.com/projects/]\n:* Shiny: Easy web applications in R [http://www.rstudio.com/shiny/]\n:* plyr: the split-apply-combine strategy for R [http://plyr.had.co.nz/]\n\n== ## bNote-2013-06-07 ==\n\n=== I/O Tracing on Microsoft Windows Using Xperf ===\n\n* [[I/O tracing on Microsoft Windows using xperf]]\n\n* [[Windows Programming on Linux with MinGW]]\n\n\n== ## bNote-2013-06-05 ==\n\n=== I/O Tracing on Microsoft Windows Using Xperf ===\n\n* [[I/O tracing on Microsoft Windows using xperf]]\n\n== ## bNote-2013-06-03 ==\n\n=== R HMM ===','utf-8'),(295,'== ## bNote-2013-06-13 ==\n\n=== EMC 장비 구매 ===\n\n==== EMC VNX 5300 관련 Question Lists ====\n\n* DAE (Disk Array Enclosure) 내에도 Processor와 DRAM이 들어가는가? DAE내의 OS는 어떤 OS가 구동되는가?\n\n* DPE (Disk Processor Enclosure)와 DAE 간의 연결은 구체적으로 어떻게 되는가? DPE와 DAE 간에 SAS cable로 직접 연결되는가? 몇 개의 SAS cable로 연결되는가? 결국, DPE와 DAE 간 연결 Bandwidth는 얼마인가? 혹시 DPE와 DAE간 연결 케이블이 성능 병목의 원인이 될 수 있지는 않는가? (DPE 내의 OS는 ?)\n\n* DAE에 들어가는 Disk들은 어떤 종류가 들어가며, 각각 몇 개가 들어가서 총 몇개인가? (즉, SAS SSD, SAS HDD, SATA HDD 각각 몇개?)\n\n* DPE에도 Disk Bay가 있는데, Vault Disk 외에 Storage Capacity로 잡히는 Disk들은 어떤 종류가 들어가며, 각각 몇 개가 들어가서 총 몇개인가? (Vault 제외하고, SAS SSD, SAS HDD, SATA HDD 각각 몇개? ) (그리고, Vault Disk들은 8개가 들어가는 것 맞는지?)\n\n* DPE에서 Server들로 연결되는 SAN Interface는 실제로 10GbE port인가? 그 위에 iSCSI 프로토콜이 구동되는 것인가? 그러면 일반 Server들은 EMC의 DPE에 연결될 때, 10GbE switch에 연결이 되어야 하는것인지?  \n\n* DPE에서 X-blade (NAS Processor)로의 연결은 구체적으로 어떻게 되는가? SAS cable인지? 아니면 10GbE인지? 아니면 FC인지?\n\n==== Supermicro Commodity Storage Server 관련 Question Lists ====\n\n* 우리가 NAS interface를 올린다고 하면, NAS node를 DPE node와 별개로 구축하는 것이 적절한가? 아니면 NAS 기능을 수행하는 SW가 DPE node에서 구동되는 것이 적절한가?\n\n=== Microsoft BuildServer07 Workload Analysis ===\n\n <pre>\n\nblusjune@jimi-hendrix:[data.out] $ cat A/f040.msbuild.anal_s0010.__valu__sig__.out \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  3\n__valu__sig__ _n_o_sigaddrs :  75405\n__valu__sig__ _sigioc_acc :  311328\n__valu__sig__ _sigaddrs_efficiency :  4.1287447782\n__valu__sig__ _n_o_addr_total :  396735\n__valu__sig__ _ioc_total :  714151\n\nblusjune@jimi-hendrix:[data.out] $ cat R/f040.msbuild.anal_s0010.__valu__sig__.out \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  3\n__valu__sig__ _n_o_sigaddrs :  72103\n__valu__sig__ _sigioc_acc :  275949\n__valu__sig__ _sigaddrs_efficiency :  3.82715004924\n__valu__sig__ _n_o_addr_total :  364572\n__valu__sig__ _ioc_total :  645921\n\nblusjune@jimi-hendrix:[data.out] $ cat W/f040.msbuild.anal_s0010.__valu__sig__.out \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  56\n__valu__sig__ _n_o_sigaddrs :  66\n__valu__sig__ _sigioc_acc :  17064\n__valu__sig__ _sigaddrs_efficiency :  258.545454545\n__valu__sig__ _n_o_addr_total :  37104\n__valu__sig__ _ioc_total :  68230\n\n</pre>\n\n== ## bNote-2013-06-05 ==\n\n=== Patentization (CIOP) ===\n\n\n=== Python to binary executable information ===\n\n=== Sensing: RStudio Project ===\n\n* RStudio team contributes code to many R packages and projects [http://www.rstudio.com/projects/]\n:* Shiny: Easy web applications in R [http://www.rstudio.com/shiny/]\n:* plyr: the split-apply-combine strategy for R [http://plyr.had.co.nz/]\n\n== ## bNote-2013-06-07 ==\n\n=== I/O Tracing on Microsoft Windows Using Xperf ===\n\n* [[I/O tracing on Microsoft Windows using xperf]]\n\n* [[Windows Programming on Linux with MinGW]]\n\n\n== ## bNote-2013-06-05 ==\n\n=== I/O Tracing on Microsoft Windows Using Xperf ===\n\n* [[I/O tracing on Microsoft Windows using xperf]]\n\n== ## bNote-2013-06-03 ==\n\n=== R HMM ===','utf-8'),(296,'\n* http://info.nimblestorage.com/rs/nimblestorage/images/nimble_storage_feature_primer.pdf\n: Nimble Storage Feature Primer\n\n* Special Features of Nimble Storage\n: Cache Accelerated Sequential Layout (CASL) Architecture\n: Dynamic Caching\n: Write-optimized Data Layout\n: Application-Tuned Block Size\n: Universal Compression\n: Efficient, Instant Snapshots\n: Thin Provisioning\n: Efficient Replication\n: Zero-Copy Clones\n: Application-Integrated Backups and Profiles\n: Custom Application Profiles\n: Scale-Up Performance\n: Scale Capacity\n: InfoSight and Proactive Wellness\n: Non-Disruptive Upgrades \n: NimbleConnect Online Community\n: Certified Storage','utf-8'),(297,'\n* http://www.purestorage.com/flash-array/tech-specs.html\n: Flash Array Technical Specifications - Pure Storage','utf-8'),(298,'\n\n <pre>\n   3705 +C_/Windows/system32/config/software+Wr+Rand\n   4347 +C_/Users/pharoah/AppData/Roaming/Microsoft/Windows/Recent/AutomaticDestinations/1b4dd67f29cb1962.automaticDestinations-ms+Wr+Rand\n   7445 +C_/_Mft+Rd+Rand\n   8217 +C_/pagefile.sys+Rd+Seqn\n   8513 +C_/_BitMap+Wr+Rand\n  14955 +C_/Users/pharoah/NTUSER.DAT.LOG1+Wr+Seqn\n  17079 +C_/Users/pharoah/NTUSER.DAT+Wr+Rand\n  37313 +C_/_LogFile+Wr+Rand\n  37544 +C_/_Mft+Wr+Rand\n 106240 +C_/pagefile.sys+Rd+Rand\n</pre>\n\n== ## bNote-2013-06-13 ==\n\n=== EMC 장비 구매 ===\n\n==== EMC VNX 5300 관련 Question Lists ====\n\n* DAE (Disk Array Enclosure) 내에도 Processor와 DRAM이 들어가는가? DAE내의 OS는 어떤 OS가 구동되는가?\n\n* DPE (Disk Processor Enclosure)와 DAE 간의 연결은 구체적으로 어떻게 되는가? DPE와 DAE 간에 SAS cable로 직접 연결되는가? 몇 개의 SAS cable로 연결되는가? 결국, DPE와 DAE 간 연결 Bandwidth는 얼마인가? 혹시 DPE와 DAE간 연결 케이블이 성능 병목의 원인이 될 수 있지는 않는가? (DPE 내의 OS는 ?)\n\n* DAE에 들어가는 Disk들은 어떤 종류가 들어가며, 각각 몇 개가 들어가서 총 몇개인가? (즉, SAS SSD, SAS HDD, SATA HDD 각각 몇개?)\n\n* DPE에도 Disk Bay가 있는데, Vault Disk 외에 Storage Capacity로 잡히는 Disk들은 어떤 종류가 들어가며, 각각 몇 개가 들어가서 총 몇개인가? (Vault 제외하고, SAS SSD, SAS HDD, SATA HDD 각각 몇개? ) (그리고, Vault Disk들은 8개가 들어가는 것 맞는지?)\n\n* DPE에서 Server들로 연결되는 SAN Interface는 실제로 10GbE port인가? 그 위에 iSCSI 프로토콜이 구동되는 것인가? 그러면 일반 Server들은 EMC의 DPE에 연결될 때, 10GbE switch에 연결이 되어야 하는것인지?  \n\n* DPE에서 X-blade (NAS Processor)로의 연결은 구체적으로 어떻게 되는가? SAS cable인지? 아니면 10GbE인지? 아니면 FC인지?\n\n==== Supermicro Commodity Storage Server 관련 Question Lists ====\n\n* 우리가 NAS interface를 올린다고 하면, NAS node를 DPE node와 별개로 구축하는 것이 적절한가? 아니면 NAS 기능을 수행하는 SW가 DPE node에서 구동되는 것이 적절한가?\n\n=== Microsoft BuildServer07 Workload Analysis ===\n\n <pre>\n\nblusjune@jimi-hendrix:[data.out] $ cat A/f040.msbuild.anal_s0010.__valu__sig__.out \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  3\n__valu__sig__ _n_o_sigaddrs :  75405\n__valu__sig__ _sigioc_acc :  311328\n__valu__sig__ _sigaddrs_efficiency :  4.1287447782\n__valu__sig__ _n_o_addr_total :  396735\n__valu__sig__ _ioc_total :  714151\n\nblusjune@jimi-hendrix:[data.out] $ cat R/f040.msbuild.anal_s0010.__valu__sig__.out \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  3\n__valu__sig__ _n_o_sigaddrs :  72103\n__valu__sig__ _sigioc_acc :  275949\n__valu__sig__ _sigaddrs_efficiency :  3.82715004924\n__valu__sig__ _n_o_addr_total :  364572\n__valu__sig__ _ioc_total :  645921\n\nblusjune@jimi-hendrix:[data.out] $ cat W/f040.msbuild.anal_s0010.__valu__sig__.out \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  56\n__valu__sig__ _n_o_sigaddrs :  66\n__valu__sig__ _sigioc_acc :  17064\n__valu__sig__ _sigaddrs_efficiency :  258.545454545\n__valu__sig__ _n_o_addr_total :  37104\n__valu__sig__ _ioc_total :  68230\n\n</pre>\n\n== ## bNote-2013-06-05 ==\n\n=== Patentization (CIOP) ===\n\n\n=== Python to binary executable information ===\n\n=== Sensing: RStudio Project ===\n\n* RStudio team contributes code to many R packages and projects [http://www.rstudio.com/projects/]\n:* Shiny: Easy web applications in R [http://www.rstudio.com/shiny/]\n:* plyr: the split-apply-combine strategy for R [http://plyr.had.co.nz/]\n\n== ## bNote-2013-06-07 ==\n\n=== I/O Tracing on Microsoft Windows Using Xperf ===\n\n* [[I/O tracing on Microsoft Windows using xperf]]\n\n* [[Windows Programming on Linux with MinGW]]\n\n\n== ## bNote-2013-06-05 ==\n\n=== I/O Tracing on Microsoft Windows Using Xperf ===\n\n* [[I/O tracing on Microsoft Windows using xperf]]\n\n== ## bNote-2013-06-03 ==\n\n=== R HMM ===','utf-8'),(299,'','utf-8'),(300,'','utf-8'),(301,'','utf-8'),(302,'','utf-8'),(303,'','utf-8'),(304,'','utf-8');
/*!40000 ALTER TABLE `radiohead_text` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `radiohead_transcache`
--

DROP TABLE IF EXISTS `radiohead_transcache`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
