--
-- Dumping data for table `mw_text`
--

LOCK TABLES `mw_text` WRITE;
/*!40000 ALTER TABLE `mw_text` DISABLE KEYS */;
INSERT INTO `mw_text` VALUES (1,'\'\'\'MediaWiki has been successfully installed.\'\'\'\n\nConsult the [http://meta.wikimedia.org/wiki/Help:Contents User\'s Guide] for information on using the wiki software.\n\n== Getting started ==\n* [http://www.mediawiki.org/wiki/Manual:Configuration_settings Configuration settings list]\n* [http://www.mediawiki.org/wiki/Manual:FAQ MediaWiki FAQ]\n* [https://lists.wikimedia.org/mailman/listinfo/mediawiki-announce MediaWiki release mailing list]','utf-8'),(2,'2013-01-08\n\ncache media로 동작하는 SSD에 요청되는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\ndm-cache에 대해서 검증해볼것.','utf-8'),(3,'== 2013-01-08 ==\n\ncache media로 동작하는 SSD에 요청되는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\ndm-cache에 대해서 검증해볼것.','utf-8'),(4,'==2013-01-08==\n\ncache media로 동작하는 SSD에 요청되는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\ndm-cache에 대해서 검증해볼것.','utf-8'),(5,'==2013-01-08==\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.','utf-8'),(6,'== DM-cache ==\n\nreference article: [http://domino.watson.ibm.com/library/cyberdig.nsf/papers/BA52BEF8B940E7438525723C006BAFEA/$File/rc24123.pdf DM-cache - Dynamic Policy Disk Caching for Storage Networking]','utf-8'),(7,'== DM-cache ==\n\n=== Reference === [http://domino.watson.ibm.com/library/cyberdig.nsf/papers/BA52BEF8B940E7438525723C006BAFEA/$File/rc24123.pdf DM-cache - Dynamic Policy Disk Caching for Storage Networking]\n\n=== Mechanism ===','utf-8'),(8,'== Bnote patidea 2013 ==\n\npatidea\n\n\n\n\n\n\n\n\n\n===\n\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf\n\n\n\n===\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-001 ]]\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n[Notation]\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n[배경 / 기존 기술의 문제점]\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n[ 본 발명의 특징 / 효과 ]\n\n  1. page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n  2. page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n  3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n\n\n[대표 청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n[대표 도면]\n\n\n\n\n\n[선행 기술]\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n[침해 적발]\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n[기술 상세]\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\nRAM page cache:\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 당장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n\nSSD page cache:\n\n- page cache media로 사용될 SSD 공간은 meta 정보 영역과 page data 영역으로 slice될 수 있음. meta 정보 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering의 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log structured manner로 page chunk를 저장함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta 정보 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta 정보 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta 정보 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta 정보 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음. \n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n\n\ntiering page pool:\n\n- 필요에 따라 하나 혹은 복수 개의 page selection 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n\n\npage cache tiering manager:\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n[청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-002 ]]\nTwo-Sieve Based Hot/Cold data Identification Algorithm for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-003 ]]\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-004 ]]\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-005 ]]\nSSD Retention Time Controlling for Caching\n\n\n[청구항]\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-006 ]]\n\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-007 ]]\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-008 ]]\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-----\nbig data를 저장 관리하는데 있어 스토리지 이슈 (ex. 수십 테라바이트를 디스크로 읽어 메모리로 올리는데 걸리는 시간 - 전송속도의 문제)\n\n\n\n\n\n-----\nhttp://en.wikipedia.org/wiki/Page_replacement_algorithm\n\n\nNot recently used\n\nThe not recently used (NRU), sometimes known as the Least Recently Used (LRU), page replacement algorithm is an algorithm that favours keeping pages in memory that have been recently used. This algorithm works on the following principle: when a page is referenced, a referenced bit is set for that page, marking it as referenced. Similarly, when a page is modified (written to), a modified bit is set. The setting of the bits is usually done by the hardware, although it is possible to do so on the software level as well.\n\nAt a certain fixed time interval, the clock interrupt triggers and clears the referenced bit of all the pages, so only pages referenced within the current clock interval are marked with a referenced bit. When a page needs to be replaced, the operating system divides the pages into four classes:\n\n3. referenced, modified\n2. referenced, not modified\n1. not referenced, modified\n0. not referenced, not modified\n\nAlthough it does not seem possible for a page to be not referenced yet modified, this happens when a class 3 page has its referenced bit cleared by the clock interrupt. The NRU algorithm picks a random page from the lowest category for removal. So out of the above four pages, the NRU algorithm will replace the not referenced, not modified.\n\nNote that this algorithm implies that a modified (within clock interval) but not referenced page is less important than a not modified page that is intensely referenced.\n\nNRU is a marking algorithm, so it is (k / (k - h + 1))-competitive.\n\n\n\n\n\n\n\n[] h, k paging related references:\n\n\"Competitive Paging Algorithms\"\nhttp://www.cs.cmu.edu/~sleator/papers/competitive-paging.pdf\n\n\"A Simple Analysis for Randomized Online Weighted Paging\"\nhttp://www.win.tue.nl/~nikhil/pubs/pot-wt2.pdf','utf-8'),(9,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n=== 본 발명의 특징 / 효과 ===\n\n  1. page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n  2. page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n  3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n\n\n=== 대표 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 당장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta 정보 영역과 page data 영역으로 slice될 수 있음. meta 정보 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering의 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log structured manner로 page chunk를 저장함.\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta 정보 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta 정보 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta 정보 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta 정보 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음. \n\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page selection 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n===\n\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf\n\n\n\n===\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-001 ]]\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n[Notation]\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n[배경 / 기존 기술의 문제점]\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n[ 본 발명의 특징 / 효과 ]\n\n  1. page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n  2. page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n  3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n\n\n[대표 청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n[대표 도면]\n\n\n\n\n\n[선행 기술]\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n[침해 적발]\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n[기술 상세]\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\nRAM page cache:\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 당장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n\nSSD page cache:\n\n- page cache media로 사용될 SSD 공간은 meta 정보 영역과 page data 영역으로 slice될 수 있음. meta 정보 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering의 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log structured manner로 page chunk를 저장함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta 정보 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta 정보 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta 정보 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta 정보 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음. \n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n\n\ntiering page pool:\n\n- 필요에 따라 하나 혹은 복수 개의 page selection 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n\n\npage cache tiering manager:\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n[청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-002 ]]\nTwo-Sieve Based Hot/Cold data Identification Algorithm for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-003 ]]\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-004 ]]\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-005 ]]\nSSD Retention Time Controlling for Caching\n\n\n[청구항]\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-006 ]]\n\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-007 ]]\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-008 ]]\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-----\nbig data를 저장 관리하는데 있어 스토리지 이슈 (ex. 수십 테라바이트를 디스크로 읽어 메모리로 올리는데 걸리는 시간 - 전송속도의 문제)\n\n\n\n\n\n-----\nhttp://en.wikipedia.org/wiki/Page_replacement_algorithm\n\n\nNot recently used\n\nThe not recently used (NRU), sometimes known as the Least Recently Used (LRU), page replacement algorithm is an algorithm that favours keeping pages in memory that have been recently used. This algorithm works on the following principle: when a page is referenced, a referenced bit is set for that page, marking it as referenced. Similarly, when a page is modified (written to), a modified bit is set. The setting of the bits is usually done by the hardware, although it is possible to do so on the software level as well.\n\nAt a certain fixed time interval, the clock interrupt triggers and clears the referenced bit of all the pages, so only pages referenced within the current clock interval are marked with a referenced bit. When a page needs to be replaced, the operating system divides the pages into four classes:\n\n3. referenced, modified\n2. referenced, not modified\n1. not referenced, modified\n0. not referenced, not modified\n\nAlthough it does not seem possible for a page to be not referenced yet modified, this happens when a class 3 page has its referenced bit cleared by the clock interrupt. The NRU algorithm picks a random page from the lowest category for removal. So out of the above four pages, the NRU algorithm will replace the not referenced, not modified.\n\nNote that this algorithm implies that a modified (within clock interval) but not referenced page is less important than a not modified page that is intensely referenced.\n\nNRU is a marking algorithm, so it is (k / (k - h + 1))-competitive.\n\n\n\n\n\n\n\n[] h, k paging related references:\n\n\"Competitive Paging Algorithms\"\nhttp://www.cs.cmu.edu/~sleator/papers/competitive-paging.pdf\n\n\"A Simple Analysis for Randomized Online Weighted Paging\"\nhttp://www.win.tue.nl/~nikhil/pubs/pot-wt2.pdf','utf-8'),(10,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n=== 대표 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 당장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta 정보 영역과 page data 영역으로 slice될 수 있음. meta 정보 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering의 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log structured manner로 page chunk를 저장함.\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta 정보 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta 정보 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta 정보 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta 정보 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음. \n\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page selection 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n===\n\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf\n\n\n\n===\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-001 ]]\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n[Notation]\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n[배경 / 기존 기술의 문제점]\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n[ 본 발명의 특징 / 효과 ]\n\n  1. page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n  2. page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n  3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n\n\n[대표 청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n[대표 도면]\n\n\n\n\n\n[선행 기술]\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n[침해 적발]\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n[기술 상세]\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\nRAM page cache:\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 당장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n\nSSD page cache:\n\n- page cache media로 사용될 SSD 공간은 meta 정보 영역과 page data 영역으로 slice될 수 있음. meta 정보 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering의 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log structured manner로 page chunk를 저장함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta 정보 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta 정보 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta 정보 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta 정보 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음. \n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n\n\ntiering page pool:\n\n- 필요에 따라 하나 혹은 복수 개의 page selection 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n\n\npage cache tiering manager:\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n[청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-002 ]]\nTwo-Sieve Based Hot/Cold data Identification Algorithm for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-003 ]]\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-004 ]]\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-005 ]]\nSSD Retention Time Controlling for Caching\n\n\n[청구항]\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-006 ]]\n\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-007 ]]\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-008 ]]\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-----\nbig data를 저장 관리하는데 있어 스토리지 이슈 (ex. 수십 테라바이트를 디스크로 읽어 메모리로 올리는데 걸리는 시간 - 전송속도의 문제)\n\n\n\n\n\n-----\nhttp://en.wikipedia.org/wiki/Page_replacement_algorithm\n\n\nNot recently used\n\nThe not recently used (NRU), sometimes known as the Least Recently Used (LRU), page replacement algorithm is an algorithm that favours keeping pages in memory that have been recently used. This algorithm works on the following principle: when a page is referenced, a referenced bit is set for that page, marking it as referenced. Similarly, when a page is modified (written to), a modified bit is set. The setting of the bits is usually done by the hardware, although it is possible to do so on the software level as well.\n\nAt a certain fixed time interval, the clock interrupt triggers and clears the referenced bit of all the pages, so only pages referenced within the current clock interval are marked with a referenced bit. When a page needs to be replaced, the operating system divides the pages into four classes:\n\n3. referenced, modified\n2. referenced, not modified\n1. not referenced, modified\n0. not referenced, not modified\n\nAlthough it does not seem possible for a page to be not referenced yet modified, this happens when a class 3 page has its referenced bit cleared by the clock interrupt. The NRU algorithm picks a random page from the lowest category for removal. So out of the above four pages, the NRU algorithm will replace the not referenced, not modified.\n\nNote that this algorithm implies that a modified (within clock interval) but not referenced page is less important than a not modified page that is intensely referenced.\n\nNRU is a marking algorithm, so it is (k / (k - h + 1))-competitive.\n\n\n\n\n\n\n\n[] h, k paging related references:\n\n\"Competitive Paging Algorithms\"\nhttp://www.cs.cmu.edu/~sleator/papers/competitive-paging.pdf\n\n\"A Simple Analysis for Randomized Online Weighted Paging\"\nhttp://www.win.tue.nl/~nikhil/pubs/pot-wt2.pdf','utf-8'),(11,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n=== 대표 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta 정보 영역과 page data 영역으로 slice될 수 있음. meta 정보 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering의 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log structured manner로 page chunk를 저장함.\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta 정보 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta 정보 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta 정보 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta 정보 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음. \n\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page selection 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n===\n\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf\n\n\n\n===\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-001 ]]\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n[Notation]\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n[배경 / 기존 기술의 문제점]\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n[ 본 발명의 특징 / 효과 ]\n\n  1. page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n  2. page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n  3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n\n\n[대표 청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n[대표 도면]\n\n\n\n\n\n[선행 기술]\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n[침해 적발]\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n[기술 상세]\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\nRAM page cache:\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 당장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n\nSSD page cache:\n\n- page cache media로 사용될 SSD 공간은 meta 정보 영역과 page data 영역으로 slice될 수 있음. meta 정보 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering의 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log structured manner로 page chunk를 저장함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta 정보 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta 정보 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta 정보 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta 정보 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음. \n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n\n\ntiering page pool:\n\n- 필요에 따라 하나 혹은 복수 개의 page selection 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n\n\npage cache tiering manager:\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n[청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-002 ]]\nTwo-Sieve Based Hot/Cold data Identification Algorithm for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-003 ]]\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-004 ]]\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-005 ]]\nSSD Retention Time Controlling for Caching\n\n\n[청구항]\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-006 ]]\n\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-007 ]]\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-008 ]]\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-----\nbig data를 저장 관리하는데 있어 스토리지 이슈 (ex. 수십 테라바이트를 디스크로 읽어 메모리로 올리는데 걸리는 시간 - 전송속도의 문제)\n\n\n\n\n\n-----\nhttp://en.wikipedia.org/wiki/Page_replacement_algorithm\n\n\nNot recently used\n\nThe not recently used (NRU), sometimes known as the Least Recently Used (LRU), page replacement algorithm is an algorithm that favours keeping pages in memory that have been recently used. This algorithm works on the following principle: when a page is referenced, a referenced bit is set for that page, marking it as referenced. Similarly, when a page is modified (written to), a modified bit is set. The setting of the bits is usually done by the hardware, although it is possible to do so on the software level as well.\n\nAt a certain fixed time interval, the clock interrupt triggers and clears the referenced bit of all the pages, so only pages referenced within the current clock interval are marked with a referenced bit. When a page needs to be replaced, the operating system divides the pages into four classes:\n\n3. referenced, modified\n2. referenced, not modified\n1. not referenced, modified\n0. not referenced, not modified\n\nAlthough it does not seem possible for a page to be not referenced yet modified, this happens when a class 3 page has its referenced bit cleared by the clock interrupt. The NRU algorithm picks a random page from the lowest category for removal. So out of the above four pages, the NRU algorithm will replace the not referenced, not modified.\n\nNote that this algorithm implies that a modified (within clock interval) but not referenced page is less important than a not modified page that is intensely referenced.\n\nNRU is a marking algorithm, so it is (k / (k - h + 1))-competitive.\n\n\n\n\n\n\n\n[] h, k paging related references:\n\n\"Competitive Paging Algorithms\"\nhttp://www.cs.cmu.edu/~sleator/papers/competitive-paging.pdf\n\n\"A Simple Analysis for Randomized Online Weighted Paging\"\nhttp://www.win.tue.nl/~nikhil/pubs/pot-wt2.pdf','utf-8'),(12,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n=== 대표 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta 정보 영역과 page data 영역으로 slice될 수 있음. meta 정보 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering의 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log structured manner로 page chunk를 저장함.\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta 정보 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta 정보 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta 정보 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta 정보 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음. \n\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n===\n\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf\n\n\n\n===\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-001 ]]\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n[Notation]\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n[배경 / 기존 기술의 문제점]\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n[ 본 발명의 특징 / 효과 ]\n\n  1. page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n  2. page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n  3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n\n\n[대표 청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n[대표 도면]\n\n\n\n\n\n[선행 기술]\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n[침해 적발]\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n[기술 상세]\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\nRAM page cache:\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 당장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n\nSSD page cache:\n\n- page cache media로 사용될 SSD 공간은 meta 정보 영역과 page data 영역으로 slice될 수 있음. meta 정보 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering의 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log structured manner로 page chunk를 저장함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta 정보 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta 정보 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta 정보 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta 정보 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음. \n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n\n\ntiering page pool:\n\n- 필요에 따라 하나 혹은 복수 개의 page selection 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n\n\npage cache tiering manager:\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n[청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-002 ]]\nTwo-Sieve Based Hot/Cold data Identification Algorithm for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-003 ]]\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-004 ]]\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-005 ]]\nSSD Retention Time Controlling for Caching\n\n\n[청구항]\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-006 ]]\n\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-007 ]]\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-008 ]]\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-----\nbig data를 저장 관리하는데 있어 스토리지 이슈 (ex. 수십 테라바이트를 디스크로 읽어 메모리로 올리는데 걸리는 시간 - 전송속도의 문제)\n\n\n\n\n\n-----\nhttp://en.wikipedia.org/wiki/Page_replacement_algorithm\n\n\nNot recently used\n\nThe not recently used (NRU), sometimes known as the Least Recently Used (LRU), page replacement algorithm is an algorithm that favours keeping pages in memory that have been recently used. This algorithm works on the following principle: when a page is referenced, a referenced bit is set for that page, marking it as referenced. Similarly, when a page is modified (written to), a modified bit is set. The setting of the bits is usually done by the hardware, although it is possible to do so on the software level as well.\n\nAt a certain fixed time interval, the clock interrupt triggers and clears the referenced bit of all the pages, so only pages referenced within the current clock interval are marked with a referenced bit. When a page needs to be replaced, the operating system divides the pages into four classes:\n\n3. referenced, modified\n2. referenced, not modified\n1. not referenced, modified\n0. not referenced, not modified\n\nAlthough it does not seem possible for a page to be not referenced yet modified, this happens when a class 3 page has its referenced bit cleared by the clock interrupt. The NRU algorithm picks a random page from the lowest category for removal. So out of the above four pages, the NRU algorithm will replace the not referenced, not modified.\n\nNote that this algorithm implies that a modified (within clock interval) but not referenced page is less important than a not modified page that is intensely referenced.\n\nNRU is a marking algorithm, so it is (k / (k - h + 1))-competitive.\n\n\n\n\n\n\n\n[] h, k paging related references:\n\n\"Competitive Paging Algorithms\"\nhttp://www.cs.cmu.edu/~sleator/papers/competitive-paging.pdf\n\n\"A Simple Analysis for Randomized Online Weighted Paging\"\nhttp://www.win.tue.nl/~nikhil/pubs/pot-wt2.pdf','utf-8'),(13,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n=== 대표 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering의 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함. 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n===\n\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf\n\n\n\n===\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-001 ]]\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n[Notation]\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n[배경 / 기존 기술의 문제점]\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n[ 본 발명의 특징 / 효과 ]\n\n  1. page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n  2. page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n  3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n\n\n[대표 청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n[대표 도면]\n\n\n\n\n\n[선행 기술]\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n[침해 적발]\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n[기술 상세]\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\nRAM page cache:\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 당장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n\nSSD page cache:\n\n- page cache media로 사용될 SSD 공간은 meta 정보 영역과 page data 영역으로 slice될 수 있음. meta 정보 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering의 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log structured manner로 page chunk를 저장함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta 정보 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta 정보 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta 정보 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta 정보 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음. \n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n\n\ntiering page pool:\n\n- 필요에 따라 하나 혹은 복수 개의 page selection 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n\n\npage cache tiering manager:\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n[청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-002 ]]\nTwo-Sieve Based Hot/Cold data Identification Algorithm for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-003 ]]\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-004 ]]\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-005 ]]\nSSD Retention Time Controlling for Caching\n\n\n[청구항]\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-006 ]]\n\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-007 ]]\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-008 ]]\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-----\nbig data를 저장 관리하는데 있어 스토리지 이슈 (ex. 수십 테라바이트를 디스크로 읽어 메모리로 올리는데 걸리는 시간 - 전송속도의 문제)\n\n\n\n\n\n-----\nhttp://en.wikipedia.org/wiki/Page_replacement_algorithm\n\n\nNot recently used\n\nThe not recently used (NRU), sometimes known as the Least Recently Used (LRU), page replacement algorithm is an algorithm that favours keeping pages in memory that have been recently used. This algorithm works on the following principle: when a page is referenced, a referenced bit is set for that page, marking it as referenced. Similarly, when a page is modified (written to), a modified bit is set. The setting of the bits is usually done by the hardware, although it is possible to do so on the software level as well.\n\nAt a certain fixed time interval, the clock interrupt triggers and clears the referenced bit of all the pages, so only pages referenced within the current clock interval are marked with a referenced bit. When a page needs to be replaced, the operating system divides the pages into four classes:\n\n3. referenced, modified\n2. referenced, not modified\n1. not referenced, modified\n0. not referenced, not modified\n\nAlthough it does not seem possible for a page to be not referenced yet modified, this happens when a class 3 page has its referenced bit cleared by the clock interrupt. The NRU algorithm picks a random page from the lowest category for removal. So out of the above four pages, the NRU algorithm will replace the not referenced, not modified.\n\nNote that this algorithm implies that a modified (within clock interval) but not referenced page is less important than a not modified page that is intensely referenced.\n\nNRU is a marking algorithm, so it is (k / (k - h + 1))-competitive.\n\n\n\n\n\n\n\n[] h, k paging related references:\n\n\"Competitive Paging Algorithms\"\nhttp://www.cs.cmu.edu/~sleator/papers/competitive-paging.pdf\n\n\"A Simple Analysis for Randomized Online Weighted Paging\"\nhttp://www.win.tue.nl/~nikhil/pubs/pot-wt2.pdf','utf-8'),(14,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n=== 대표 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- RAM page cache에 있던 page가 \n- SSD에 저장된 page를 찾는 방법은 다음과 같음. 응용에 의한 disk access 요청이 왔을 때 kernel에서는 기존 방식과 마찬가지로 page cache 내에서 해당되는 page를 검색한다. 이때, class-3에 있다가 discard된 page들의 경우와는 달리, class-2로 관리되다가 SSD로 tiering된 page들에 대해서는 page cache media로 사용되는 SSD의 몇 번째 chunk인지에 대한 \n\n여기서 발견되지 않았을 때 해당 page는 SSD page cache의 어떤 page chunk에 속하는지에 대한 정보를 관리하게 됨. \n\n\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n===\n\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf\n\n\n\n===\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-001 ]]\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n[Notation]\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n[배경 / 기존 기술의 문제점]\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n[ 본 발명의 특징 / 효과 ]\n\n  1. page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n  2. page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n  3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n\n\n[대표 청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n[대표 도면]\n\n\n\n\n\n[선행 기술]\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n[침해 적발]\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n[기술 상세]\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\nRAM page cache:\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 당장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n\nSSD page cache:\n\n- page cache media로 사용될 SSD 공간은 meta 정보 영역과 page data 영역으로 slice될 수 있음. meta 정보 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering의 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log structured manner로 page chunk를 저장함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta 정보 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta 정보 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta 정보 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta 정보 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음. \n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n\n\ntiering page pool:\n\n- 필요에 따라 하나 혹은 복수 개의 page selection 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n\n\npage cache tiering manager:\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n[청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-002 ]]\nTwo-Sieve Based Hot/Cold data Identification Algorithm for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-003 ]]\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-004 ]]\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-005 ]]\nSSD Retention Time Controlling for Caching\n\n\n[청구항]\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-006 ]]\n\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-007 ]]\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-008 ]]\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-----\nbig data를 저장 관리하는데 있어 스토리지 이슈 (ex. 수십 테라바이트를 디스크로 읽어 메모리로 올리는데 걸리는 시간 - 전송속도의 문제)\n\n\n\n\n\n-----\nhttp://en.wikipedia.org/wiki/Page_replacement_algorithm\n\n\nNot recently used\n\nThe not recently used (NRU), sometimes known as the Least Recently Used (LRU), page replacement algorithm is an algorithm that favours keeping pages in memory that have been recently used. This algorithm works on the following principle: when a page is referenced, a referenced bit is set for that page, marking it as referenced. Similarly, when a page is modified (written to), a modified bit is set. The setting of the bits is usually done by the hardware, although it is possible to do so on the software level as well.\n\nAt a certain fixed time interval, the clock interrupt triggers and clears the referenced bit of all the pages, so only pages referenced within the current clock interval are marked with a referenced bit. When a page needs to be replaced, the operating system divides the pages into four classes:\n\n3. referenced, modified\n2. referenced, not modified\n1. not referenced, modified\n0. not referenced, not modified\n\nAlthough it does not seem possible for a page to be not referenced yet modified, this happens when a class 3 page has its referenced bit cleared by the clock interrupt. The NRU algorithm picks a random page from the lowest category for removal. So out of the above four pages, the NRU algorithm will replace the not referenced, not modified.\n\nNote that this algorithm implies that a modified (within clock interval) but not referenced page is less important than a not modified page that is intensely referenced.\n\nNRU is a marking algorithm, so it is (k / (k - h + 1))-competitive.\n\n\n\n\n\n\n\n[] h, k paging related references:\n\n\"Competitive Paging Algorithms\"\nhttp://www.cs.cmu.edu/~sleator/papers/competitive-paging.pdf\n\n\"A Simple Analysis for Randomized Online Weighted Paging\"\nhttp://www.win.tue.nl/~nikhil/pubs/pot-wt2.pdf','utf-8'),(15,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n=== 대표 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n===\n\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf\n\n\n\n===\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-001 ]]\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n[Notation]\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n[배경 / 기존 기술의 문제점]\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n[ 본 발명의 특징 / 효과 ]\n\n  1. page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n  2. page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n  3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n\n\n[대표 청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n[대표 도면]\n\n\n\n\n\n[선행 기술]\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n[침해 적발]\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n[기술 상세]\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\nRAM page cache:\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 당장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n\nSSD page cache:\n\n- page cache media로 사용될 SSD 공간은 meta 정보 영역과 page data 영역으로 slice될 수 있음. meta 정보 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering의 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log structured manner로 page chunk를 저장함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta 정보 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta 정보 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta 정보 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta 정보 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음. \n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n\n\ntiering page pool:\n\n- 필요에 따라 하나 혹은 복수 개의 page selection 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n\n\npage cache tiering manager:\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n[청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-002 ]]\nTwo-Sieve Based Hot/Cold data Identification Algorithm for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-003 ]]\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-004 ]]\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-005 ]]\nSSD Retention Time Controlling for Caching\n\n\n[청구항]\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-006 ]]\n\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-007 ]]\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-008 ]]\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-----\nbig data를 저장 관리하는데 있어 스토리지 이슈 (ex. 수십 테라바이트를 디스크로 읽어 메모리로 올리는데 걸리는 시간 - 전송속도의 문제)\n\n\n\n\n\n-----\nhttp://en.wikipedia.org/wiki/Page_replacement_algorithm\n\n\nNot recently used\n\nThe not recently used (NRU), sometimes known as the Least Recently Used (LRU), page replacement algorithm is an algorithm that favours keeping pages in memory that have been recently used. This algorithm works on the following principle: when a page is referenced, a referenced bit is set for that page, marking it as referenced. Similarly, when a page is modified (written to), a modified bit is set. The setting of the bits is usually done by the hardware, although it is possible to do so on the software level as well.\n\nAt a certain fixed time interval, the clock interrupt triggers and clears the referenced bit of all the pages, so only pages referenced within the current clock interval are marked with a referenced bit. When a page needs to be replaced, the operating system divides the pages into four classes:\n\n3. referenced, modified\n2. referenced, not modified\n1. not referenced, modified\n0. not referenced, not modified\n\nAlthough it does not seem possible for a page to be not referenced yet modified, this happens when a class 3 page has its referenced bit cleared by the clock interrupt. The NRU algorithm picks a random page from the lowest category for removal. So out of the above four pages, the NRU algorithm will replace the not referenced, not modified.\n\nNote that this algorithm implies that a modified (within clock interval) but not referenced page is less important than a not modified page that is intensely referenced.\n\nNRU is a marking algorithm, so it is (k / (k - h + 1))-competitive.\n\n\n\n\n\n\n\n[] h, k paging related references:\n\n\"Competitive Paging Algorithms\"\nhttp://www.cs.cmu.edu/~sleator/papers/competitive-paging.pdf\n\n\"A Simple Analysis for Randomized Online Weighted Paging\"\nhttp://www.win.tue.nl/~nikhil/pubs/pot-wt2.pdf','utf-8'),(16,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n=== 대표 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n===\n\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf\n\n\n\n===\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-001 ]]\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n[Notation]\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n[배경 / 기존 기술의 문제점]\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n[ 본 발명의 특징 / 효과 ]\n\n  1. page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n  2. page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n  3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n\n\n[대표 청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n[대표 도면]\n\n\n\n\n\n[선행 기술]\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n[침해 적발]\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n[기술 상세]\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\nRAM page cache:\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 당장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n\nSSD page cache:\n\n- page cache media로 사용될 SSD 공간은 meta 정보 영역과 page data 영역으로 slice될 수 있음. meta 정보 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering의 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log structured manner로 page chunk를 저장함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta 정보 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta 정보 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta 정보 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta 정보 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음. \n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n\n\ntiering page pool:\n\n- 필요에 따라 하나 혹은 복수 개의 page selection 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n\n\npage cache tiering manager:\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n[청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-002 ]]\nTwo-Sieve Based Hot/Cold data Identification Algorithm for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-003 ]]\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-004 ]]\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-005 ]]\nSSD Retention Time Controlling for Caching\n\n\n[청구항]\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-006 ]]\n\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-007 ]]\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-008 ]]\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-----\nbig data를 저장 관리하는데 있어 스토리지 이슈 (ex. 수십 테라바이트를 디스크로 읽어 메모리로 올리는데 걸리는 시간 - 전송속도의 문제)\n\n\n\n\n\n-----\nhttp://en.wikipedia.org/wiki/Page_replacement_algorithm\n\n\nNot recently used\n\nThe not recently used (NRU), sometimes known as the Least Recently Used (LRU), page replacement algorithm is an algorithm that favours keeping pages in memory that have been recently used. This algorithm works on the following principle: when a page is referenced, a referenced bit is set for that page, marking it as referenced. Similarly, when a page is modified (written to), a modified bit is set. The setting of the bits is usually done by the hardware, although it is possible to do so on the software level as well.\n\nAt a certain fixed time interval, the clock interrupt triggers and clears the referenced bit of all the pages, so only pages referenced within the current clock interval are marked with a referenced bit. When a page needs to be replaced, the operating system divides the pages into four classes:\n\n3. referenced, modified\n2. referenced, not modified\n1. not referenced, modified\n0. not referenced, not modified\n\nAlthough it does not seem possible for a page to be not referenced yet modified, this happens when a class 3 page has its referenced bit cleared by the clock interrupt. The NRU algorithm picks a random page from the lowest category for removal. So out of the above four pages, the NRU algorithm will replace the not referenced, not modified.\n\nNote that this algorithm implies that a modified (within clock interval) but not referenced page is less important than a not modified page that is intensely referenced.\n\nNRU is a marking algorithm, so it is (k / (k - h + 1))-competitive.\n\n\n\n\n\n\n\n[] h, k paging related references:\n\n\"Competitive Paging Algorithms\"\nhttp://www.cs.cmu.edu/~sleator/papers/competitive-paging.pdf\n\n\"A Simple Analysis for Randomized Online Weighted Paging\"\nhttp://www.win.tue.nl/~nikhil/pubs/pot-wt2.pdf','utf-8'),(17,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n===\n\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf\n\n\n\n===\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-001 ]]\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n[Notation]\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n[배경 / 기존 기술의 문제점]\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n[ 본 발명의 특징 / 효과 ]\n\n  1. page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n\n  2. page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n\n  3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n\n\n[대표 청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n2. page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n3. N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n\n\n[대표 도면]\n\n\n\n\n\n[선행 기술]\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n[침해 적발]\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n[기술 상세]\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\nRAM page cache:\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 당장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n\nSSD page cache:\n\n- page cache media로 사용될 SSD 공간은 meta 정보 영역과 page data 영역으로 slice될 수 있음. meta 정보 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering의 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log structured manner로 page chunk를 저장함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta 정보 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta 정보 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta 정보 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta 정보 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음. \n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n\n\ntiering page pool:\n\n- 필요에 따라 하나 혹은 복수 개의 page selection 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n\n\npage cache tiering manager:\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n[청구항]\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-002 ]]\nTwo-Sieve Based Hot/Cold data Identification Algorithm for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-003 ]]\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-004 ]]\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-005 ]]\nSSD Retention Time Controlling for Caching\n\n\n[청구항]\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-006 ]]\n\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-007 ]]\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-008 ]]\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-----\nbig data를 저장 관리하는데 있어 스토리지 이슈 (ex. 수십 테라바이트를 디스크로 읽어 메모리로 올리는데 걸리는 시간 - 전송속도의 문제)\n\n\n\n\n\n-----\nhttp://en.wikipedia.org/wiki/Page_replacement_algorithm\n\n\nNot recently used\n\nThe not recently used (NRU), sometimes known as the Least Recently Used (LRU), page replacement algorithm is an algorithm that favours keeping pages in memory that have been recently used. This algorithm works on the following principle: when a page is referenced, a referenced bit is set for that page, marking it as referenced. Similarly, when a page is modified (written to), a modified bit is set. The setting of the bits is usually done by the hardware, although it is possible to do so on the software level as well.\n\nAt a certain fixed time interval, the clock interrupt triggers and clears the referenced bit of all the pages, so only pages referenced within the current clock interval are marked with a referenced bit. When a page needs to be replaced, the operating system divides the pages into four classes:\n\n3. referenced, modified\n2. referenced, not modified\n1. not referenced, modified\n0. not referenced, not modified\n\nAlthough it does not seem possible for a page to be not referenced yet modified, this happens when a class 3 page has its referenced bit cleared by the clock interrupt. The NRU algorithm picks a random page from the lowest category for removal. So out of the above four pages, the NRU algorithm will replace the not referenced, not modified.\n\nNote that this algorithm implies that a modified (within clock interval) but not referenced page is less important than a not modified page that is intensely referenced.\n\nNRU is a marking algorithm, so it is (k / (k - h + 1))-competitive.\n\n\n\n\n\n\n\n[] h, k paging related references:\n\n\"Competitive Paging Algorithms\"\nhttp://www.cs.cmu.edu/~sleator/papers/competitive-paging.pdf\n\n\"A Simple Analysis for Randomized Online Weighted Paging\"\nhttp://www.win.tue.nl/~nikhil/pubs/pot-wt2.pdf','utf-8'),(18,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n===\n\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(19,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\nDRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음.\n기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음. \n\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것','utf-8'),(20,'== 2013-01-08 ==\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n== 2013-01-03 ==\n\n=== Hadoop and (distributed) Cache ===\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(21,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것','utf-8'),(22,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함*.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n(*) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것','utf-8'),(23,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것','utf-8'),(24,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것','utf-8'),(25,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n(RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것','utf-8'),(26,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것','utf-8'),(27,'== { patent-brian-2013-001 } ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것','utf-8'),(28,'== {{ patent-brian-2013-001 }} ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것','utf-8'),(29,'== | patent-brian-2013-001 | ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것','utf-8'),(30,'== patent-brian-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것','utf-8'),(31,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것','utf-8'),(32,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것','utf-8'),(33,'== PATENT-BRIAN-2013-Template ==\n\n\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n\n\n\n\n=== 본 발명의 특징 / 효과 ===\n\n\n\n\n\n=== 대표 청구항 ===\n\n\n\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n\n\n\n\n=== 침해 적발 ===\n\n\n\n\n\n=== 기술 상세 ===\n\n\n\n\n\n=== 청구항 ===\n\n\n\n\n\n=== Notation ===\n\n\n\n\n\n=== Misc. ===','utf-8'),(34,'== Python Code ==\n\n<nowiki>\n\nclass LRU_Cache(object):\n\n    def __init__(self, original_function, maxsize=1000):\n        self.original_function = original_function\n        self.maxsize = maxsize\n        self.mapping = {}\n\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        self.head = [None, None, None, None]        # oldest\n        self.tail = [self.head, None, None, None]   # newest\n        self.head[NEXT] = self.tail\n\n    def __call__(self, *key):\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        mapping, head, tail = self.mapping, self.head, self.tail\n        sentinel = object()\n\n        link = mapping.get(key, sentinel)\n        if link is sentinel:\n            value = self.original_function(*key)\n            if len(mapping) >= self.maxsize:\n                oldest = head[NEXT]\n                next_oldest = oldest[NEXT]\n                head[NEXT] = next_oldest\n                next_oldest[PREV] = head\n                del mapping[oldest[KEY]]\n            last = tail[PREV]\n            link = [last, tail, key, value]\n            mapping[key] = last[NEXT] = tail[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = tail[PREV]\n            last[NEXT] = tail[PREV] = link\n            link[PREV] = last\n            link[NEXT] = tail\n        return value\n\nif __name__ == \'__main__\':\n    p = LRU_Cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n</nowiki>','utf-8'),(35,'== Python Code ==\n\n <nowiki>\n\nclass LRU_Cache(object):\n\n    def __init__(self, original_function, maxsize=1000):\n        self.original_function = original_function\n        self.maxsize = maxsize\n        self.mapping = {}\n\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        self.head = [None, None, None, None]        # oldest\n        self.tail = [self.head, None, None, None]   # newest\n        self.head[NEXT] = self.tail\n\n    def __call__(self, *key):\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        mapping, head, tail = self.mapping, self.head, self.tail\n        sentinel = object()\n\n        link = mapping.get(key, sentinel)\n        if link is sentinel:\n            value = self.original_function(*key)\n            if len(mapping) >= self.maxsize:\n                oldest = head[NEXT]\n                next_oldest = oldest[NEXT]\n                head[NEXT] = next_oldest\n                next_oldest[PREV] = head\n                del mapping[oldest[KEY]]\n            last = tail[PREV]\n            link = [last, tail, key, value]\n            mapping[key] = last[NEXT] = tail[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = tail[PREV]\n            last[NEXT] = tail[PREV] = link\n            link[PREV] = last\n            link[NEXT] = tail\n        return value\n\nif __name__ == \'__main__\':\n    p = LRU_Cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n</nowiki>','utf-8'),(36,'== Class Based Implementation ==\n\n <nowiki>\n\nclass LRU_Cache(object):\n\n    def __init__(self, original_function, maxsize=1000):\n        self.original_function = original_function\n        self.maxsize = maxsize\n        self.mapping = {}\n\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        self.head = [None, None, None, None]        # oldest\n        self.tail = [self.head, None, None, None]   # newest\n        self.head[NEXT] = self.tail\n\n    def __call__(self, *key):\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        mapping, head, tail = self.mapping, self.head, self.tail\n        sentinel = object()\n\n        link = mapping.get(key, sentinel)\n        if link is sentinel:\n            value = self.original_function(*key)\n            if len(mapping) >= self.maxsize:\n                oldest = head[NEXT]\n                next_oldest = oldest[NEXT]\n                head[NEXT] = next_oldest\n                next_oldest[PREV] = head\n                del mapping[oldest[KEY]]\n            last = tail[PREV]\n            link = [last, tail, key, value]\n            mapping[key] = last[NEXT] = tail[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = tail[PREV]\n            last[NEXT] = tail[PREV] = link\n            link[PREV] = last\n            link[NEXT] = tail\n        return value\n\nif __name__ == \'__main__\':\n    p = LRU_Cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n</nowiki>','utf-8'),(37,'=== Class-based Implementation ===\n\n <nowiki>\n\nclass LRU_Cache(object):\n\n    def __init__(self, original_function, maxsize=1000):\n        self.original_function = original_function\n        self.maxsize = maxsize\n        self.mapping = {}\n\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        self.head = [None, None, None, None]        # oldest\n        self.tail = [self.head, None, None, None]   # newest\n        self.head[NEXT] = self.tail\n\n    def __call__(self, *key):\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        mapping, head, tail = self.mapping, self.head, self.tail\n        sentinel = object()\n\n        link = mapping.get(key, sentinel)\n        if link is sentinel:\n            value = self.original_function(*key)\n            if len(mapping) >= self.maxsize:\n                oldest = head[NEXT]\n                next_oldest = oldest[NEXT]\n                head[NEXT] = next_oldest\n                next_oldest[PREV] = head\n                del mapping[oldest[KEY]]\n            last = tail[PREV]\n            link = [last, tail, key, value]\n            mapping[key] = last[NEXT] = tail[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = tail[PREV]\n            last[NEXT] = tail[PREV] = link\n            link[PREV] = last\n            link[NEXT] = tail\n        return value\n\nif __name__ == \'__main__\':\n    p = LRU_Cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n</nowiki>\n\n\n=== Function-based Implementation ===\n[http://code.activestate.com/recipes/577969-simplified-highly-optimized-lru-cache/ LRU cache]\n\n <nowiki>\n\ndef cache_generator(original_function, maxsize):\n    mapping = {}\n    mapping_get = mapping.get\n    root = [None, None]\n    root[:] = [root, root]\n    value = None\n    size = 0\n\n    PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n    while 1:\n        key = yield value\n        link = mapping_get(key, root)\n        if link is root:\n            value = original_function(key)\n            if size < maxsize:\n                size += 1\n            else:\n                old_prev, old_next, old_key, old_value = root[NEXT]\n                root[NEXT] = old_next\n                old_next[PREV] = root\n                del mapping[old_key]\n            last = root[PREV]\n            link = [last, root, key, value]\n            mapping[key] = last[NEXT] = root[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = root[PREV]\n            last[NEXT] = root[PREV] = link\n            link[PREV] = last\n            link[NEXT] = root\n\ndef make_cache(original_function, maxsize=100):\n    \'Create a cache around a function that takes a single argument\'\n    c = cache_generator(original_function, maxsize)\n    next(c)\n    return c.send\n\n\nif __name__ == \'__main__\':\n    p = make_cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n\n</nowiki>','utf-8'),(38,'=== Class-based Implementation ===\n\n[http://stackoverflow.com/questions/4443920/python-building-a-lru-cache LRU Cache Class-based approach]\n\n <nowiki>\n\nclass LRU_Cache(object):\n\n    def __init__(self, original_function, maxsize=1000):\n        self.original_function = original_function\n        self.maxsize = maxsize\n        self.mapping = {}\n\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        self.head = [None, None, None, None]        # oldest\n        self.tail = [self.head, None, None, None]   # newest\n        self.head[NEXT] = self.tail\n\n    def __call__(self, *key):\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        mapping, head, tail = self.mapping, self.head, self.tail\n        sentinel = object()\n\n        link = mapping.get(key, sentinel)\n        if link is sentinel:\n            value = self.original_function(*key)\n            if len(mapping) >= self.maxsize:\n                oldest = head[NEXT]\n                next_oldest = oldest[NEXT]\n                head[NEXT] = next_oldest\n                next_oldest[PREV] = head\n                del mapping[oldest[KEY]]\n            last = tail[PREV]\n            link = [last, tail, key, value]\n            mapping[key] = last[NEXT] = tail[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = tail[PREV]\n            last[NEXT] = tail[PREV] = link\n            link[PREV] = last\n            link[NEXT] = tail\n        return value\n\nif __name__ == \'__main__\':\n    p = LRU_Cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n</nowiki>\n\n=== Function-based Implementation ===\n[http://code.activestate.com/recipes/577969-simplified-highly-optimized-lru-cache/ LRU cache]\n\n <nowiki>\n\ndef cache_generator(original_function, maxsize):\n    mapping = {}\n    mapping_get = mapping.get\n    root = [None, None]\n    root[:] = [root, root]\n    value = None\n    size = 0\n\n    PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n    while 1:\n        key = yield value\n        link = mapping_get(key, root)\n        if link is root:\n            value = original_function(key)\n            if size < maxsize:\n                size += 1\n            else:\n                old_prev, old_next, old_key, old_value = root[NEXT]\n                root[NEXT] = old_next\n                old_next[PREV] = root\n                del mapping[old_key]\n            last = root[PREV]\n            link = [last, root, key, value]\n            mapping[key] = last[NEXT] = root[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = root[PREV]\n            last[NEXT] = root[PREV] = link\n            link[PREV] = last\n            link[NEXT] = root\n\ndef make_cache(original_function, maxsize=100):\n    \'Create a cache around a function that takes a single argument\'\n    c = cache_generator(original_function, maxsize)\n    next(c)\n    return c.send\n\n\nif __name__ == \'__main__\':\n    p = make_cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n\n</nowiki>','utf-8'),(39,'=== Class-based Implementation ===\n\n[http://stackoverflow.com/questions/4443920/python-building-a-lru-cache LRU Cache Class-based approach]\n\n <nowiki>\n\nclass LRU_Cache(object):\n\n    def __init__(self, original_function, maxsize=1000):\n        self.original_function = original_function\n        self.maxsize = maxsize\n        self.mapping = {}\n\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        self.head = [None, None, None, None]        # oldest\n        self.tail = [self.head, None, None, None]   # newest\n        self.head[NEXT] = self.tail\n\n    def __call__(self, *key):\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        mapping, head, tail = self.mapping, self.head, self.tail\n        sentinel = object()\n\n        link = mapping.get(key, sentinel)\n        if link is sentinel:\n            value = self.original_function(*key)\n            if len(mapping) >= self.maxsize:\n                oldest = head[NEXT]\n                next_oldest = oldest[NEXT]\n                head[NEXT] = next_oldest\n                next_oldest[PREV] = head\n                del mapping[oldest[KEY]]\n            last = tail[PREV]\n            link = [last, tail, key, value]\n            mapping[key] = last[NEXT] = tail[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = tail[PREV]\n            last[NEXT] = tail[PREV] = link\n            link[PREV] = last\n            link[NEXT] = tail\n        return value\n\nif __name__ == \'__main__\':\n    p = LRU_Cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n</nowiki>\n\n=== Function-based Implementation ===\n\n* References\n[http://code.activestate.com/recipes/577969-simplified-highly-optimized-lru-cache/ LRU cache - Function-based Approach]\n\n <nowiki>\n\ndef cache_generator(original_function, maxsize):\n    mapping = {}\n    mapping_get = mapping.get\n    root = [None, None]\n    root[:] = [root, root]\n    value = None\n    size = 0\n\n    PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n    while 1:\n        key = yield value\n        link = mapping_get(key, root)\n        if link is root:\n            value = original_function(key)\n            if size < maxsize:\n                size += 1\n            else:\n                old_prev, old_next, old_key, old_value = root[NEXT]\n                root[NEXT] = old_next\n                old_next[PREV] = root\n                del mapping[old_key]\n            last = root[PREV]\n            link = [last, root, key, value]\n            mapping[key] = last[NEXT] = root[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = root[PREV]\n            last[NEXT] = root[PREV] = link\n            link[PREV] = last\n            link[NEXT] = root\n\ndef make_cache(original_function, maxsize=100):\n    \'Create a cache around a function that takes a single argument\'\n    c = cache_generator(original_function, maxsize)\n    next(c)\n    return c.send\n\n\nif __name__ == \'__main__\':\n    p = make_cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n\n</nowiki>','utf-8'),(40,'=== Class-based Implementation ===\n\n* References\n[http://stackoverflow.com/questions/4443920/python-building-a-lru-cache LRU Cache - Class-based approach]\n\n <nowiki>\n\nclass LRU_Cache(object):\n\n    def __init__(self, original_function, maxsize=1000):\n        self.original_function = original_function\n        self.maxsize = maxsize\n        self.mapping = {}\n\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        self.head = [None, None, None, None]        # oldest\n        self.tail = [self.head, None, None, None]   # newest\n        self.head[NEXT] = self.tail\n\n    def __call__(self, *key):\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        mapping, head, tail = self.mapping, self.head, self.tail\n        sentinel = object()\n\n        link = mapping.get(key, sentinel)\n        if link is sentinel:\n            value = self.original_function(*key)\n            if len(mapping) >= self.maxsize:\n                oldest = head[NEXT]\n                next_oldest = oldest[NEXT]\n                head[NEXT] = next_oldest\n                next_oldest[PREV] = head\n                del mapping[oldest[KEY]]\n            last = tail[PREV]\n            link = [last, tail, key, value]\n            mapping[key] = last[NEXT] = tail[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = tail[PREV]\n            last[NEXT] = tail[PREV] = link\n            link[PREV] = last\n            link[NEXT] = tail\n        return value\n\nif __name__ == \'__main__\':\n    p = LRU_Cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n</nowiki>\n\n=== Function-based Implementation ===\n\n* References\n[http://code.activestate.com/recipes/577969-simplified-highly-optimized-lru-cache/ LRU cache - Function-based Approach]\n\n <nowiki>\n\ndef cache_generator(original_function, maxsize):\n    mapping = {}\n    mapping_get = mapping.get\n    root = [None, None]\n    root[:] = [root, root]\n    value = None\n    size = 0\n\n    PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n    while 1:\n        key = yield value\n        link = mapping_get(key, root)\n        if link is root:\n            value = original_function(key)\n            if size < maxsize:\n                size += 1\n            else:\n                old_prev, old_next, old_key, old_value = root[NEXT]\n                root[NEXT] = old_next\n                old_next[PREV] = root\n                del mapping[old_key]\n            last = root[PREV]\n            link = [last, root, key, value]\n            mapping[key] = last[NEXT] = root[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = root[PREV]\n            last[NEXT] = root[PREV] = link\n            link[PREV] = last\n            link[NEXT] = root\n\ndef make_cache(original_function, maxsize=100):\n    \'Create a cache around a function that takes a single argument\'\n    c = cache_generator(original_function, maxsize)\n    next(c)\n    return c.send\n\n\nif __name__ == \'__main__\':\n    p = make_cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n\n</nowiki>','utf-8'),(41,'=== Class-based Implementation ===\n\n* References\n[http://stackoverflow.com/questions/4443920/python-building-a-lru-cache LRU Cache - Class-based approach]\n\n <nowiki>\n\nclass LRU_Cache(object):\n\n    def __init__(self, original_function, maxsize=1000):\n        self.original_function = original_function\n        self.maxsize = maxsize\n        self.mapping = {}\n\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        self.head = [None, None, None, None]        # oldest\n        self.tail = [self.head, None, None, None]   # newest\n        self.head[NEXT] = self.tail\n\n    def __call__(self, *key):\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        mapping, head, tail = self.mapping, self.head, self.tail\n        sentinel = object()\n\n        link = mapping.get(key, sentinel)\n        if link is sentinel:\n            value = self.original_function(*key)\n            if len(mapping) >= self.maxsize:\n                oldest = head[NEXT]\n                next_oldest = oldest[NEXT]\n                head[NEXT] = next_oldest\n                next_oldest[PREV] = head\n                del mapping[oldest[KEY]]\n            last = tail[PREV]\n            link = [last, tail, key, value]\n            mapping[key] = last[NEXT] = tail[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = tail[PREV]\n            last[NEXT] = tail[PREV] = link\n            link[PREV] = last\n            link[NEXT] = tail\n        return value\n\nif __name__ == \'__main__\':\n    p = LRU_Cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n</nowiki>\n\n=== Function-based Implementation ===\n\n* References\n[http://code.activestate.com/recipes/577969-simplified-highly-optimized-lru-cache/ LRU Cache - Function-based Approach]\n\n <nowiki>\n\ndef cache_generator(original_function, maxsize):\n    mapping = {}\n    mapping_get = mapping.get\n    root = [None, None]\n    root[:] = [root, root]\n    value = None\n    size = 0\n\n    PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n    while 1:\n        key = yield value\n        link = mapping_get(key, root)\n        if link is root:\n            value = original_function(key)\n            if size < maxsize:\n                size += 1\n            else:\n                old_prev, old_next, old_key, old_value = root[NEXT]\n                root[NEXT] = old_next\n                old_next[PREV] = root\n                del mapping[old_key]\n            last = root[PREV]\n            link = [last, root, key, value]\n            mapping[key] = last[NEXT] = root[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = root[PREV]\n            last[NEXT] = root[PREV] = link\n            link[PREV] = last\n            link[NEXT] = root\n\ndef make_cache(original_function, maxsize=100):\n    \'Create a cache around a function that takes a single argument\'\n    c = cache_generator(original_function, maxsize)\n    next(c)\n    return c.send\n\n\nif __name__ == \'__main__\':\n    p = make_cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n\n</nowiki>','utf-8'),(42,'=== Class-based Implementation ===\n\n* References\n** [http://stackoverflow.com/questions/4443920/python-building-a-lru-cache LRU Cache - Class-based approach]\n\n <nowiki>\n\nclass LRU_Cache(object):\n\n    def __init__(self, original_function, maxsize=1000):\n        self.original_function = original_function\n        self.maxsize = maxsize\n        self.mapping = {}\n\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        self.head = [None, None, None, None]        # oldest\n        self.tail = [self.head, None, None, None]   # newest\n        self.head[NEXT] = self.tail\n\n    def __call__(self, *key):\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        mapping, head, tail = self.mapping, self.head, self.tail\n        sentinel = object()\n\n        link = mapping.get(key, sentinel)\n        if link is sentinel:\n            value = self.original_function(*key)\n            if len(mapping) >= self.maxsize:\n                oldest = head[NEXT]\n                next_oldest = oldest[NEXT]\n                head[NEXT] = next_oldest\n                next_oldest[PREV] = head\n                del mapping[oldest[KEY]]\n            last = tail[PREV]\n            link = [last, tail, key, value]\n            mapping[key] = last[NEXT] = tail[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = tail[PREV]\n            last[NEXT] = tail[PREV] = link\n            link[PREV] = last\n            link[NEXT] = tail\n        return value\n\nif __name__ == \'__main__\':\n    p = LRU_Cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n</nowiki>\n\n=== Function-based Implementation ===\n\n* References\n[http://code.activestate.com/recipes/577969-simplified-highly-optimized-lru-cache/ LRU Cache - Function-based Approach]\n\n <nowiki>\n\ndef cache_generator(original_function, maxsize):\n    mapping = {}\n    mapping_get = mapping.get\n    root = [None, None]\n    root[:] = [root, root]\n    value = None\n    size = 0\n\n    PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n    while 1:\n        key = yield value\n        link = mapping_get(key, root)\n        if link is root:\n            value = original_function(key)\n            if size < maxsize:\n                size += 1\n            else:\n                old_prev, old_next, old_key, old_value = root[NEXT]\n                root[NEXT] = old_next\n                old_next[PREV] = root\n                del mapping[old_key]\n            last = root[PREV]\n            link = [last, root, key, value]\n            mapping[key] = last[NEXT] = root[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = root[PREV]\n            last[NEXT] = root[PREV] = link\n            link[PREV] = last\n            link[NEXT] = root\n\ndef make_cache(original_function, maxsize=100):\n    \'Create a cache around a function that takes a single argument\'\n    c = cache_generator(original_function, maxsize)\n    next(c)\n    return c.send\n\n\nif __name__ == \'__main__\':\n    p = make_cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n\n</nowiki>','utf-8'),(43,'=== Class-based Implementation ===\n\n* References\n*# [http://stackoverflow.com/questions/4443920/python-building-a-lru-cache LRU Cache - Class-based approach]\n\n <nowiki>\n\nclass LRU_Cache(object):\n\n    def __init__(self, original_function, maxsize=1000):\n        self.original_function = original_function\n        self.maxsize = maxsize\n        self.mapping = {}\n\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        self.head = [None, None, None, None]        # oldest\n        self.tail = [self.head, None, None, None]   # newest\n        self.head[NEXT] = self.tail\n\n    def __call__(self, *key):\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        mapping, head, tail = self.mapping, self.head, self.tail\n        sentinel = object()\n\n        link = mapping.get(key, sentinel)\n        if link is sentinel:\n            value = self.original_function(*key)\n            if len(mapping) >= self.maxsize:\n                oldest = head[NEXT]\n                next_oldest = oldest[NEXT]\n                head[NEXT] = next_oldest\n                next_oldest[PREV] = head\n                del mapping[oldest[KEY]]\n            last = tail[PREV]\n            link = [last, tail, key, value]\n            mapping[key] = last[NEXT] = tail[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = tail[PREV]\n            last[NEXT] = tail[PREV] = link\n            link[PREV] = last\n            link[NEXT] = tail\n        return value\n\nif __name__ == \'__main__\':\n    p = LRU_Cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n</nowiki>\n\n=== Function-based Implementation ===\n\n* References\n[http://code.activestate.com/recipes/577969-simplified-highly-optimized-lru-cache/ LRU Cache - Function-based Approach]\n\n <nowiki>\n\ndef cache_generator(original_function, maxsize):\n    mapping = {}\n    mapping_get = mapping.get\n    root = [None, None]\n    root[:] = [root, root]\n    value = None\n    size = 0\n\n    PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n    while 1:\n        key = yield value\n        link = mapping_get(key, root)\n        if link is root:\n            value = original_function(key)\n            if size < maxsize:\n                size += 1\n            else:\n                old_prev, old_next, old_key, old_value = root[NEXT]\n                root[NEXT] = old_next\n                old_next[PREV] = root\n                del mapping[old_key]\n            last = root[PREV]\n            link = [last, root, key, value]\n            mapping[key] = last[NEXT] = root[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = root[PREV]\n            last[NEXT] = root[PREV] = link\n            link[PREV] = last\n            link[NEXT] = root\n\ndef make_cache(original_function, maxsize=100):\n    \'Create a cache around a function that takes a single argument\'\n    c = cache_generator(original_function, maxsize)\n    next(c)\n    return c.send\n\n\nif __name__ == \'__main__\':\n    p = make_cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n\n</nowiki>','utf-8'),(44,'=== Class-based Implementation ===\n\n* References\n*# [http://stackoverflow.com/questions/4443920/python-building-a-lru-cache LRU Cache - Class-based approach]\n\n <nowiki>\n\nclass LRU_Cache(object):\n\n    def __init__(self, original_function, maxsize=1000):\n        self.original_function = original_function\n        self.maxsize = maxsize\n        self.mapping = {}\n\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        self.head = [None, None, None, None]        # oldest\n        self.tail = [self.head, None, None, None]   # newest\n        self.head[NEXT] = self.tail\n\n    def __call__(self, *key):\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        mapping, head, tail = self.mapping, self.head, self.tail\n        sentinel = object()\n\n        link = mapping.get(key, sentinel)\n        if link is sentinel:\n            value = self.original_function(*key)\n            if len(mapping) >= self.maxsize:\n                oldest = head[NEXT]\n                next_oldest = oldest[NEXT]\n                head[NEXT] = next_oldest\n                next_oldest[PREV] = head\n                del mapping[oldest[KEY]]\n            last = tail[PREV]\n            link = [last, tail, key, value]\n            mapping[key] = last[NEXT] = tail[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = tail[PREV]\n            last[NEXT] = tail[PREV] = link\n            link[PREV] = last\n            link[NEXT] = tail\n        return value\n\nif __name__ == \'__main__\':\n    p = LRU_Cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n</nowiki>\n\n=== Function-based Implementation ===\n\n* References\n*# [http://code.activestate.com/recipes/577969-simplified-highly-optimized-lru-cache/ LRU Cache - Function-based Approach]\n\n <nowiki>\n\ndef cache_generator(original_function, maxsize):\n    mapping = {}\n    mapping_get = mapping.get\n    root = [None, None]\n    root[:] = [root, root]\n    value = None\n    size = 0\n\n    PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n    while 1:\n        key = yield value\n        link = mapping_get(key, root)\n        if link is root:\n            value = original_function(key)\n            if size < maxsize:\n                size += 1\n            else:\n                old_prev, old_next, old_key, old_value = root[NEXT]\n                root[NEXT] = old_next\n                old_next[PREV] = root\n                del mapping[old_key]\n            last = root[PREV]\n            link = [last, root, key, value]\n            mapping[key] = last[NEXT] = root[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = root[PREV]\n            last[NEXT] = root[PREV] = link\n            link[PREV] = last\n            link[NEXT] = root\n\ndef make_cache(original_function, maxsize=100):\n    \'Create a cache around a function that takes a single argument\'\n    c = cache_generator(original_function, maxsize)\n    next(c)\n    return c.send\n\n\nif __name__ == \'__main__\':\n    p = make_cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n\n</nowiki>','utf-8'),(45,'== Bnote 2013 ==\n\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n== 2013-01-03 ==\n\n=== Hadoop and (distributed) Cache ===\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(46,'== Bnote 2013 ==\n\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(47,'== Bnote 2013 ==\n\n\n\n=== 2013-01-09 ===\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n\n\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(48,'== Bnote 2013 ==\n\n\n\n=== 2013-01-09 ===\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm Hardware Recommendations for Apache Hadoop, Hortonworks]\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(49,'== Bnote 2013 ==\n\n\n\n=== 2013-01-09 ===\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(50,'== Bnote 2013 ==\n\n\n\n=== 2013-01-09 ===\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf Benchmarking Cloud Serving Systems with YCSB]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(51,'== Bnote 2013 ==\n\n\n\n=== 2013-01-09 ===\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(52,'== Bnote 2013 ==\n\n\n\n=== 2013-01-09 ===\n\n==== References: Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(53,'== Bnote 2013 ==\n\n\n\n=== 2013-01-09 ===\n\n==== References: Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n==== References:\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(54,'== Bnote 2013 ==\n\n\n\n=== 2013-01-09 ===\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern]\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(55,'== Bnote 2013 ==\n\n\n\n=== 2013-01-09 ===\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(56,'== Bnote 2013 ==\n\n\n\n=== 2013-01-09 ===\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(57,'== Bnote 2013 ==\n\n\n\n=== 2013-01-09 ===\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(58,'== Bnote 2013 ==\n\n\n\n=== 2013-01-09 ===\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(59,'== Bnote 2013 ==\n\n\n\n=== 2013-01-09 ===\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(60,'== Bnote 2013 ==\n\n\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"]\n\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(61,'== Bnote 2013 ==\n\n\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"]\n\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(62,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"]\n\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(63,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"]\n\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(64,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] # Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(65,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(66,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n\n* wordcount case\n\n\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\n\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nHDFS layer \n\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(67,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n\n* wordcount case\n\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행)\n\n(70개의 Map 태스크와 1개의 Reduce 태스크로 구성)\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O간의 Gap 존재함을 확인.\n단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐.\n(Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등)\n분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n\n\n\n\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(68,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(69,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(70,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n<nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(71,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(72,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n==== 미팅 일정 ====\n1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정 (수퍼컴 기반 Hadoop 클러스터 관련) // VDI 되는 미팅룸 하나 예약해둘것\n\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(73,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n== PATENT-BRIAN-2013-002 ==\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-003 ]]\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-004 ]]\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-005 ]]\nSSD Retention Time Controlling for Caching\n\n\n[청구항]\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-006 ]]\n\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-007 ]]\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n[[ 발명 2013-008 ]]\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8');
INSERT INTO `mw_text` VALUES (74,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n== PATENT-BRIAN-2013-002 ==\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n발명 2013-003\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n발명 2013-004\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n발명 2013-005\nSSD Retention Time Controlling for Caching\n\n\n[청구항]\n\n\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n발명 2013-006\n\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n발명 2013-007\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n\n#_________________________________________________________________________#\n\n발명 2013-008\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(75,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\nSSD Retention Time Controlling for Caching\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(76,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\nSSD Retention Time Controlling for Caching\nOR\nCaching-optimal SSD Retention Time Control\n\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(77,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* SSD Retention Time Controlling for Caching\n* Caching-optimal SSD Retention Time Control\n\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(78,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n\n=== 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM 기반의 page cache로서, 디스크에서 access된 data를 cache하는 역할을 수행함. 현재 (recently) 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- data를 page cache pool에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 방식은 직접적으로 수정될 필요 없음. page access 상태 정보를 update 하는 모듈에 hybrid page cache의 page cache tiering에 필요한 메타 정보를 추가적으로 관리하는 모듈이 추가됨.\n\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되어 있는지에 대한 tiered-place 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, tiered-place 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n\n==== tiering page pool ====\n\n- 필요에 따라 하나 혹은 복수 개의 page classification 알고리즘에 의해 선택된 page들을 담아두는 장소로서, 별도로 명시한 방식에 의해 지정된 크기의 tiering buffer 단위로 tiering I/O를 수행함. (참고로 tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. 다음은 한 실시예임. SSD I/O의 병렬성을 극대화 하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 base로 하여, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 단위 tiering I/O 크기, 즉 tiering buffer의 크기로 결정할 수 있음. 이 값들은 관리자에 의해서 명시적으로 지정되거나, 혹은 해당 사이트에서의 performance profiling 등의 작업을 통해 adaptive하게 결정될 수도 있음).\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n\n\n\n\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(79,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n** hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n : can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n** hadoop over baremetal enviornment\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n==== 미팅 일정 ====\n1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정 (수퍼컴 기반 Hadoop 클러스터 관련) // VDI 되는 미팅룸 하나 예약해둘것\n\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(80,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n** hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*: can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n** hadoop over baremetal enviornment\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n==== 미팅 일정 ====\n1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정 (수퍼컴 기반 Hadoop 클러스터 관련) // VDI 되는 미팅룸 하나 예약해둘것\n\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(81,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n** hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*: -> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n** hadoop over baremetal enviornment\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n==== 미팅 일정 ====\n1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정 (수퍼컴 기반 Hadoop 클러스터 관련) // VDI 되는 미팅룸 하나 예약해둘것\n\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(82,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n** hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*:-> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n** hadoop over baremetal enviornment\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n==== 미팅 일정 ====\n1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정 (수퍼컴 기반 Hadoop 클러스터 관련) // VDI 되는 미팅룸 하나 예약해둘것\n\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(83,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*:-> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n==== 미팅 일정 ====\n1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정 (수퍼컴 기반 Hadoop 클러스터 관련) // VDI 되는 미팅룸 하나 예약해둘것\n\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(84,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n==== 미팅 일정 ====\n1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정 (수퍼컴 기반 Hadoop 클러스터 관련) // VDI 되는 미팅룸 하나 예약해둘것\n\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(85,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n==== 미팅 일정 ====\n1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정 (수퍼컴 기반 Hadoop 클러스터 관련) // VDI 되는 미팅룸 하나 예약해둘것\n\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(86,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(87,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n--\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(88,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n----\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(89,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n---\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(90,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n----\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(91,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n----\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n----\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(92,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n----\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n----\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n----\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(93,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n----\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n----\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n----\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n----\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(94,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n----\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n----\n\n==== 미팅 일정 ====\n 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n- 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n- VDI 되는 미팅룸 하나 예약해둘것\n----\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n----\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(95,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n----\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n----\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n----\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(96,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n----\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n----\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(97,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n----\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n----\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(98,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n----\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(99,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n----\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(100,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n----\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(101,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n----\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(102,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(103,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(104,'== Bnote 2013 ==\n\n\n=== 2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(105,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== 2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(106,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== 2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(107,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== 2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(108,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(109,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n*: [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n\n\n\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(110,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(111,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: SOLUTION: you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(112,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'SOLUTION\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(113,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(114,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n* Web UI (available by default)\n** http://localhost:50070/ – web UI of the NameNode daemon\n** http://localhost:50030/ – web UI of the JobTracker daemon\n** http://localhost:50060/ – web UI of the TaskTracker daemon\n\n* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n* Chukwa: Hadoop Subproject for Large-scale Log Collection and Analysis]\n* [http://ganglia.info/ Ganglia]\n* Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and Grids]\n* [http://www.nagios.org/ Nagios]\n* Nagios: Industry Standard In IT Infrastructure Monitoring\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(115,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n# Chukwa: Hadoop Subproject for Large-scale Log Collection and Analysis]\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(116,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n# Chukwa: Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n;Term\n:Definition\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(117,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n# Chukwa: Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n#;Term\n#:Definition\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(118,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n#; Chukwa\n#: Hadoop Subproject for Large-scale Log Collection and Analysis\n#: [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(119,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n# ;Chukwa\n# : Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(120,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n# ;Chukwa\n# :Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(121,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n#;Chukwa\n#:Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(122,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n# Chukwa: Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(123,'== /etc/hosts ==\n\n\n##blusjune@jimi-hendrix:[~] $ cat /etc/hosts\n##_ver=20130110_231232\n\n### Beee\'s /etc/hosts\n### 2012-02-21 09:43 24h\n\n127.0.0.1       localhost\n#127.0.1.1      woodstock\n\n# The following lines are desirable for IPv6 capable hosts\n::1     ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n\n\n\n\n### Beee Research Cluster\n###\n\n# beee network \'A\'\n75.2.93.160     a160 kandinsky          # jimi-hendrix\n75.2.93.158     a158 klimt              # rolling-stones\n75.2.93.142     a142                    ## not assigned yet\n75.2.93.89      a89                     ## not assigned yet\n75.2.93.88      a88 bt beatles klee     ##\n75.2.93.87      a87                     ## not assigned yet\n75.2.99.141     a141 rh radiohead gogh\n\n# beee network \'B\'\n10.123.1.1      b01 jh jimi-hendrix hd-master-01 # a160\n10.123.1.2      b02 rs rolling-stones hd-slave-0001 # a158\n10.123.1.3      b03 md miles-davis\n10.123.1.4      b04 bg buddy-guy\n10.123.1.5      b05 jq jamiroquai\n10.123.1.6      b06 jm john-mayer\n10.123.1.7      b07 px prince hd-slave-0002\n10.123.1.8      b08 wm wes-montgomery\n10.123.1.9      b09 as aerosmith\n10.123.1.10     b10 nv nirvana\n10.123.1.11     b11 lz led-zeppelin\n10.123.1.12     b12 sr stoneroses\n10.123.1.13     b13 gr guns-n-roses\n10.123.1.14     b14 ma massive-attack\n10.123.1.15     b15 mj michael-jackson\n10.123.1.16     b16 kb kenny-burrell\n10.123.1.17     b17 gm george-michael\n10.123.1.21     b21 dd duran-duran\n#10.123.1.99     b99 rh radiohead\n10.123.1.99     b99 jb jeff-beck\n\n# beee network \'C\'\n75.2.252.39     c39 rj ronny-jordan glastonbury gb monet\n\n# beee network \'D\'\n75.2.99.89      d89 be bill-evans\n75.2.99.90      d90 tt toots-thielemans\n\n# beee network \'E\'\n10.0.1.1        e01 iu01\n10.0.1.2        e02 iu02\n10.0.1.3        e03 iu03\n10.0.1.4        e04 iu04\n10.0.1.5        e05 iu05\n10.0.1.6        e06 iu06\n10.0.1.7        e07 iu07\n10.0.1.8        e08 iu08\n10.0.1.9        e09 iu09\n10.0.1.10       e10 iu10\n10.0.1.11       e11 iu11\n10.0.1.12       e12 iu12\n10.0.1.13       e13 iu13\n10.0.1.14       e14 iu14\n10.0.1.15       e15 iu15\n10.0.1.16       e16 iu16\n\n# SAIT research infra network\n202.20.183.10   secm\n202.20.185.10   secs\n202.20.185.201  eod1\n202.20.185.202  eod2\n202.20.184.11   saiteod1\n202.20.184.12   saiteod2\n202.20.183.100  sait1 superftp\n202.20.185.100  sec1\n202.20.183.12   saitlic1\n202.20.183.14   saitlic2\n202.20.185.12   seclic1\n202.20.185.203  eodssr1\n202.20.185.204  eodssr2\n202.20.185.205  eodssr3\n202.20.185.206  eodssr4\n202.20.185.207  eodssr5\n202.20.185.208  eodssr6\n202.20.185.209  eodssr7\n202.20.185.210  eodssr8\n\n\n# special purpose hosts\n75.2.93.1       gw\n202.20.142.13   ns1\n75.1.142.13     ns2\n10.41.128.98    ns3\n\n\n# temporary hosts\n75.2.253.70     nutanix ntnx','utf-8'),(124,'== /etc/hosts ==\n\n <nowiki>\n##blusjune@jimi-hendrix:[~] $ cat /etc/hosts\n##_ver=20130110_231232\n\n### Beee\'s /etc/hosts\n### 2012-02-21 09:43 24h\n\n127.0.0.1       localhost\n#127.0.1.1      woodstock\n\n# The following lines are desirable for IPv6 capable hosts\n::1     ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n\n\n\n\n### Beee Research Cluster\n###\n\n# beee network \'A\'\n75.2.93.160     a160 kandinsky          # jimi-hendrix\n75.2.93.158     a158 klimt              # rolling-stones\n75.2.93.142     a142                    ## not assigned yet\n75.2.93.89      a89                     ## not assigned yet\n75.2.93.88      a88 bt beatles klee     ##\n75.2.93.87      a87                     ## not assigned yet\n75.2.99.141     a141 rh radiohead gogh\n\n# beee network \'B\'\n10.123.1.1      b01 jh jimi-hendrix hd-master-01 # a160\n10.123.1.2      b02 rs rolling-stones hd-slave-0001 # a158\n10.123.1.3      b03 md miles-davis\n10.123.1.4      b04 bg buddy-guy\n10.123.1.5      b05 jq jamiroquai\n10.123.1.6      b06 jm john-mayer\n10.123.1.7      b07 px prince hd-slave-0002\n10.123.1.8      b08 wm wes-montgomery\n10.123.1.9      b09 as aerosmith\n10.123.1.10     b10 nv nirvana\n10.123.1.11     b11 lz led-zeppelin\n10.123.1.12     b12 sr stoneroses\n10.123.1.13     b13 gr guns-n-roses\n10.123.1.14     b14 ma massive-attack\n10.123.1.15     b15 mj michael-jackson\n10.123.1.16     b16 kb kenny-burrell\n10.123.1.17     b17 gm george-michael\n10.123.1.21     b21 dd duran-duran\n#10.123.1.99     b99 rh radiohead\n10.123.1.99     b99 jb jeff-beck\n\n# beee network \'C\'\n75.2.252.39     c39 rj ronny-jordan glastonbury gb monet\n\n# beee network \'D\'\n75.2.99.89      d89 be bill-evans\n75.2.99.90      d90 tt toots-thielemans\n\n# beee network \'E\'\n10.0.1.1        e01 iu01\n10.0.1.2        e02 iu02\n10.0.1.3        e03 iu03\n10.0.1.4        e04 iu04\n10.0.1.5        e05 iu05\n10.0.1.6        e06 iu06\n10.0.1.7        e07 iu07\n10.0.1.8        e08 iu08\n10.0.1.9        e09 iu09\n10.0.1.10       e10 iu10\n10.0.1.11       e11 iu11\n10.0.1.12       e12 iu12\n10.0.1.13       e13 iu13\n10.0.1.14       e14 iu14\n10.0.1.15       e15 iu15\n10.0.1.16       e16 iu16\n\n# SAIT research infra network\n202.20.183.10   secm\n202.20.185.10   secs\n202.20.185.201  eod1\n202.20.185.202  eod2\n202.20.184.11   saiteod1\n202.20.184.12   saiteod2\n202.20.183.100  sait1 superftp\n202.20.185.100  sec1\n202.20.183.12   saitlic1\n202.20.183.14   saitlic2\n202.20.185.12   seclic1\n202.20.185.203  eodssr1\n202.20.185.204  eodssr2\n202.20.185.205  eodssr3\n202.20.185.206  eodssr4\n202.20.185.207  eodssr5\n202.20.185.208  eodssr6\n202.20.185.209  eodssr7\n202.20.185.210  eodssr8\n\n\n# special purpose hosts\n75.2.93.1       gw\n202.20.142.13   ns1\n75.1.142.13     ns2\n10.41.128.98    ns3\n\n\n# temporary hosts\n75.2.253.70     nutanix ntnx\n\n</nowiki>','utf-8'),(125,'\'\'\'MediaWiki has been successfully installed.\'\'\'\n\nConsult the [http://meta.wikimedia.org/wiki/Help:Contents User\'s Guide] for information on using the wiki software.\n\n\n== Hot Links ==\n* [https://mail.google.com/mail Gmail]\n* [http://www.naver.com/ Naver]\n* [http://github.com/ GitHub]\n*\n* [http://klimt/ Klimt (Rolling-Stones)]\n*\n* [http://mobile.olleh.com/index.asp?code=A000000 Olleh SMS]\n\n\n\n== Getting started ==\n* [http://www.mediawiki.org/wiki/Manual:Configuration_settings Configuration settings list]\n* [http://www.mediawiki.org/wiki/Manual:FAQ MediaWiki FAQ]\n* [https://lists.wikimedia.org/mailman/listinfo/mediawiki-announce MediaWiki release mailing list]','utf-8'),(126,'\'\'\'MediaWiki has been successfully installed.\'\'\'\n\nConsult the [http://meta.wikimedia.org/wiki/Help:Contents User\'s Guide] for information on using the wiki software.\n\n\n== Hot Links ==\n* [https://mail.google.com/mail Gmail]\n* [http://www.naver.com/ Naver]\n* [http://github.com/ GitHub]\n*\n* [http://klimt/ Klimt (Rolling-Stones)]\n*\n* [http://mobile.olleh.com/index.asp?code=A000000 Olleh SMS]\n\n== Getting started ==\n* [http://www.mediawiki.org/wiki/Manual:Configuration_settings Configuration settings list]\n* [http://www.mediawiki.org/wiki/Manual:FAQ MediaWiki FAQ]\n* [https://lists.wikimedia.org/mailman/listinfo/mediawiki-announce MediaWiki release mailing list]','utf-8'),(127,'Bwiki Wikini\n\n== Hot Links ==\n* [https://mail.google.com/mail Gmail]\n* [http://www.naver.com/ Naver]\n* [http://github.com/ GitHub]\n*\n* [http://klimt/ Klimt (Rolling-Stones)]\n*\n* [http://mobile.olleh.com/index.asp?code=A000000 Olleh SMS]\n\n== Getting started ==\n* [http://www.mediawiki.org/wiki/Manual:Configuration_settings Configuration settings list]\n* [http://www.mediawiki.org/wiki/Manual:FAQ MediaWiki FAQ]\n* [https://lists.wikimedia.org/mailman/listinfo/mediawiki-announce MediaWiki release mailing list]\n* [http://meta.wikimedia.org/wiki/Help:Contents User\'s Guide]','utf-8'),(128,'\'Bwiki Wikini\'\n\n== Hot Links ==\n* [https://mail.google.com/mail Gmail]\n* [http://www.naver.com/ Naver]\n* [http://github.com/ GitHub]\n*\n* [http://klimt/ Klimt (Rolling-Stones)]\n*\n* [http://mobile.olleh.com/index.asp?code=A000000 Olleh SMS]\n\n== Getting started ==\n* [http://www.mediawiki.org/wiki/Manual:Configuration_settings Configuration settings list]\n* [http://www.mediawiki.org/wiki/Manual:FAQ MediaWiki FAQ]\n* [https://lists.wikimedia.org/mailman/listinfo/mediawiki-announce MediaWiki release mailing list]\n* [http://meta.wikimedia.org/wiki/Help:Contents User\'s Guide]','utf-8'),(129,'\'\'\' Bwiki Wikini \'\'\'\n\n== Hot Links ==\n* [https://mail.google.com/mail Gmail]\n* [http://www.naver.com/ Naver]\n* [http://github.com/ GitHub]\n*\n* [http://klimt/ Klimt (Rolling-Stones)]\n*\n* [http://mobile.olleh.com/index.asp?code=A000000 Olleh SMS]\n\n== Getting started ==\n* [http://www.mediawiki.org/wiki/Manual:Configuration_settings Configuration settings list]\n* [http://www.mediawiki.org/wiki/Manual:FAQ MediaWiki FAQ]\n* [https://lists.wikimedia.org/mailman/listinfo/mediawiki-announce MediaWiki release mailing list]\n* [http://meta.wikimedia.org/wiki/Help:Contents User\'s Guide]','utf-8'),(130,'\'\'\' Bwiki Wikini \'\'\'\n\n== Hot Links ==\n* [https://mail.google.com/mail Gmail]\n* [http://www.naver.com/ Naver]\n* [http://github.com/ GitHub]\n* [http://mobile.olleh.com/index.asp?code=A000000 Olleh SMS]\n\n\n* [http://klimt/ Klimt (Rolling-Stones)]\n\n== Getting started ==\n* [http://www.mediawiki.org/wiki/Manual:Configuration_settings Configuration settings list]\n* [http://www.mediawiki.org/wiki/Manual:FAQ MediaWiki FAQ]\n* [https://lists.wikimedia.org/mailman/listinfo/mediawiki-announce MediaWiki release mailing list]\n* [http://meta.wikimedia.org/wiki/Help:Contents User\'s Guide]','utf-8'),(131,'\'\'\' Bwiki Wikini \'\'\'\n\n== Hot Links ==\n* [https://mail.google.com/mail Gmail]\n* [http://www.naver.com/ Naver]\n* [http://github.com/ GitHub]\n* [http://mobile.olleh.com/index.asp?code=A000000 Olleh SMS]\n* [http://klimt/ Klimt (Rolling-Stones)]\n\n== Getting started ==\n* [http://www.mediawiki.org/wiki/Manual:Configuration_settings Configuration settings list]\n* [http://www.mediawiki.org/wiki/Manual:FAQ MediaWiki FAQ]\n* [https://lists.wikimedia.org/mailman/listinfo/mediawiki-announce MediaWiki release mailing list]\n* [http://meta.wikimedia.org/wiki/Help:Contents User\'s Guide]','utf-8'),(132,'\n\n=== ifconfig ===\n* 2013-01-11\n\n <nowiki>\nb@ub01:[~] $ ifconfig\neth0      Link encap:Ethernet  HWaddr 00:25:90:63:4c:fc\n          inet addr:75.2.99.89  Bcast:75.2.99.255  Mask:255.255.255.0\n          inet6 addr: fe80::225:90ff:fe63:4cfc/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:10530 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:123 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000\n          RX bytes:1520516 (1.5 MB)  TX bytes:21039 (21.0 KB)\n          Interrupt:16 Memory:fb5e0000-fb600000\n\neth1      Link encap:Ethernet  HWaddr 00:25:90:63:4c:fd\n          inet addr:10.0.3.1  Bcast:10.0.3.255  Mask:255.255.255.0\n          inet6 addr: fe80::225:90ff:fe63:4cfd/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:788 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:74 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000\n          RX bytes:420059 (420.0 KB)  TX bytes:7632 (7.6 KB)\n          Interrupt:17 Memory:fb6e0000-fb700000\n\neth2      Link encap:Ethernet  HWaddr 00:02:c9:52:c5:59\n          inet addr:10.0.1.1  Bcast:10.0.1.255  Mask:255.255.255.0\n          inet6 addr: fe80::202:c9ff:fe52:c559/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:785 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:461 errors:1 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000\n          RX bytes:71059 (71.0 KB)  TX bytes:37913 (37.9 KB)\n\nib0       Link encap:UNSPEC  HWaddr 80-00-00-59-FE-80-00-00-00-00-00-00-00-00-00-00\n          inet addr:10.0.2.1  Bcast:10.0.2.255  Mask:255.255.255.0\n          inet6 addr: fe80::202:c903:52:c559/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:2044  Metric:1\n          RX packets:4 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:17 errors:0 dropped:6 overruns:0 carrier:0\n          collisions:0 txqueuelen:256\n          RX bytes:978 (978.0 B)  TX bytes:2659 (2.6 KB)\n\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:16436  Metric:1\n          RX packets:420 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:420 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:53273 (53.2 KB)  TX bytes:53273 (53.2 KB)\n</nowiki>','utf-8'),(133,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-11 ===\n\n==== Misc. info (각종 업무 담당자 연락처) ====\n\n;기술원 에어컨 안나올때\n: #9120 기술원 통합 방재 센터 (과장) (지원팀 > 환경안전그룹)\n\n;VDI (SBC) 문제 있을 떄\n: #8272 (VDI HelpDesk)\n\n; 네트워크 안될 때 (방화벽 등)\n: 어디로? \n\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n# Chukwa: Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(134,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-11 ===\n\n==== Misc. info (각종 업무 담당자 연락처) ====\n\n;기술원 에어컨 안나올때\n: #9120 기술원 통합 방재 센터 (과장) (지원팀 > 환경안전그룹)\n\n;VDI (SBC) 문제 있을 떄\n: #8272 (VDI HelpDesk)\n\n; 네트워크 안될 때 (방화벽 등)\n: 어디로? \n\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50010/ – web UI of the DataNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n# Chukwa: Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(135,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-11 ===\n\n==== Misc. info (각종 업무 담당자 연락처) ====\n\n;기술원 에어컨 안나올때\n: #9120 기술원 통합 방재 센터 (과장) (지원팀 > 환경안전그룹)\n\n;VDI (SBC) 문제 있을 떄\n: #8272 (VDI HelpDesk)\n\n; 네트워크 안될 때 (방화벽 등)\n: 어디로? \n\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n\\#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n# Chukwa: Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(136,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-11 ===\n\n==== Misc. info (각종 업무 담당자 연락처) ====\n\n;기술원 에어컨 안나올때\n: #9120 기술원 통합 방재 센터 (과장) (지원팀 > 환경안전그룹)\n\n;VDI (SBC) 문제 있을 떄\n: #8272 (VDI HelpDesk)\n\n; 네트워크 안될 때 (방화벽 등)\n: 어디로? \n\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n# Chukwa: Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(137,'\'\'\' Bwiki Wikini \'\'\'\n\n== Hot Links ==\n* [https://mail.google.com/mail Gmail]\n* [http://www.naver.com/ Naver]\n* [http://github.com/ GitHub]\n* [http://mobile.olleh.com/index.asp?code=A000000 Olleh SMS]\n* [http://klimt/ Klimt (Rolling-Stones)]\n* [http://bill-evans/hdm/ \"pleiades: \'1+4\'-node Hadoop Cluster\"]\n\n== Getting started ==\n* [http://www.mediawiki.org/wiki/Manual:Configuration_settings Configuration settings list]\n* [http://www.mediawiki.org/wiki/Manual:FAQ MediaWiki FAQ]\n* [https://lists.wikimedia.org/mailman/listinfo/mediawiki-announce MediaWiki release mailing list]\n* [http://meta.wikimedia.org/wiki/Help:Contents User\'s Guide]','utf-8'),(138,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-11 ===\n\n==== Misc. info (각종 업무 담당자 연락처) ====\n\n;기술원 에어컨 안나올때\n: #9120 기술원 통합 방재 센터 (과장) (지원팀 > 환경안전그룹)\n\n;VDI (SBC) 문제 있을 떄\n: #8272 (VDI HelpDesk)\n\n; 네트워크 안될 때 (방화벽 등)\n: 어디로?\n\n\n==== Hadoop Cluster Problem Solved ====\n\n\n* [http://wiki.apache.org/hadoop/CouldOnlyBeReplicatedTo Could Only Be Replicated To ...]\n <nowiki>\nCould Only Be Replicated To ...\n\nA common message people see is \"could only be replicated to 0 nodes, instead of ...\".\n\nWhat does this mean? It means that the Block Replication mechanism of HDFS could not make any copies of a file it wanted to create. This can be caused by\n\nNo DataNode instances being up and running. Action: look at the servers, see if the processes are running.\nThe DataNode instances cannot talk to the server, through networking or Hadoop configuration problems. Action: look at the logs of one of the DataNodes.\nYour DataNode instances have no hard disk space in their configured data directories. Action: look at the dfs.data.dir list in the node configurations, verify that at least one of the directories exists, and is writeable by the user running the Hadoop processes. Then look at the logs.\nYour DataNode instances have run out of space. Look at the disk capacity via the Namenode web pages. Delete old files. Compress under-used files. Buy more disks for existing servers (if there is room), upgrade the existing servers to bigger drives, or add some more servers.\nThe reserved space for a DN (as set in dfs.datanode.du.reserved is greater than the remaining free space, so the DN thinks it has no free space\nYou may also get this message due to permissions, eg if JT can not create jobtracker.info on startup.\nThis is not a problem in Hadoop, it is a problem in your cluster that you are going to have to fix on your own. Sorry.\n</nowiki>\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n# Chukwa: Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(139,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-11 ===\n\n==== Misc. info (각종 업무 담당자 연락처) ====\n\n;기술원 에어컨 안나올때\n: #9120 기술원 통합 방재 센터 (과장) (지원팀 > 환경안전그룹)\n\n;VDI (SBC) 문제 있을 떄\n: #8272 (VDI HelpDesk)\n\n; 네트워크 안될 때 (방화벽 등)\n: 어디로?\n\n\n\n\n\n==== Python for ML ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n\n\n==== Hadoop Cluster Problem Solved ====\n\n* [http://wiki.apache.org/hadoop/CouldOnlyBeReplicatedTo Could Only Be Replicated To ...]\n <nowiki>\nCould Only Be Replicated To ...\n\nA common message people see is \"could only be replicated to 0 nodes, instead of ...\".\n\nWhat does this mean? It means that the Block Replication mechanism of HDFS could not make any copies of a file it wanted to create. This can be caused by\n\nNo DataNode instances being up and running. Action: look at the servers, see if the processes are running.\nThe DataNode instances cannot talk to the server, through networking or Hadoop configuration problems. Action: look at the logs of one of the DataNodes.\nYour DataNode instances have no hard disk space in their configured data directories. Action: look at the dfs.data.dir list in the node configurations, verify that at least one of the directories exists, and is writeable by the user running the Hadoop processes. Then look at the logs.\nYour DataNode instances have run out of space. Look at the disk capacity via the Namenode web pages. Delete old files. Compress under-used files. Buy more disks for existing servers (if there is room), upgrade the existing servers to bigger drives, or add some more servers.\nThe reserved space for a DN (as set in dfs.datanode.du.reserved is greater than the remaining free space, so the DN thinks it has no free space\nYou may also get this message due to permissions, eg if JT can not create jobtracker.info on startup.\nThis is not a problem in Hadoop, it is a problem in your cluster that you are going to have to fix on your own. Sorry.\n</nowiki>\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n# Chukwa: Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(140,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-11 ===\n\n==== Misc. info (각종 업무 담당자 연락처) ====\n\n;기술원 에어컨 안나올때\n: #9120 기술원 통합 방재 센터 (과장) (지원팀 > 환경안전그룹)\n\n;VDI (SBC) 문제 있을 떄\n: #8272 (VDI HelpDesk)\n\n; 네트워크 안될 때 (방화벽 등)\n: 어디로?\n\n\n\n\n\n==== Hadoop Cluster Problem Solved ====\n\n* [http://wiki.apache.org/hadoop/CouldOnlyBeReplicatedTo Could Only Be Replicated To ...]\n <nowiki>\nCould Only Be Replicated To ...\n\nA common message people see is \"could only be replicated to 0 nodes, instead of ...\".\n\nWhat does this mean? It means that the Block Replication mechanism of HDFS could not make any copies of a file it wanted to create. This can be caused by\n\nNo DataNode instances being up and running. Action: look at the servers, see if the processes are running.\nThe DataNode instances cannot talk to the server, through networking or Hadoop configuration problems. Action: look at the logs of one of the DataNodes.\nYour DataNode instances have no hard disk space in their configured data directories. Action: look at the dfs.data.dir list in the node configurations, verify that at least one of the directories exists, and is writeable by the user running the Hadoop processes. Then look at the logs.\nYour DataNode instances have run out of space. Look at the disk capacity via the Namenode web pages. Delete old files. Compress under-used files. Buy more disks for existing servers (if there is room), upgrade the existing servers to bigger drives, or add some more servers.\nThe reserved space for a DN (as set in dfs.datanode.du.reserved is greater than the remaining free space, so the DN thinks it has no free space\nYou may also get this message due to permissions, eg if JT can not create jobtracker.info on startup.\nThis is not a problem in Hadoop, it is a problem in your cluster that you are going to have to fix on your own. Sorry.\n</nowiki>\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n# Chukwa: Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(141,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-11 ===\n\n==== Misc. info (각종 업무 담당자 연락처) ====\n\n;기술원 에어컨 안나올때\n: #9120 기술원 통합 방재 센터 (과장) (지원팀 > 환경안전그룹)\n\n;VDI (SBC) 문제 있을 떄\n: #8272 (VDI HelpDesk)\n\n; 네트워크 안될 때 (방화벽 등)\n: 어디로?\n\n\n==== Hadoop Cluster Problem Solved ====\n\n* [http://wiki.apache.org/hadoop/CouldOnlyBeReplicatedTo Could Only Be Replicated To ...]\n <nowiki>\nCould Only Be Replicated To ...\n\nA common message people see is \"could only be replicated to 0 nodes, instead of ...\".\n\nWhat does this mean? It means that the Block Replication mechanism of HDFS could not make any copies of a file it wanted to create. This can be caused by\n\nNo DataNode instances being up and running. Action: look at the servers, see if the processes are running.\nThe DataNode instances cannot talk to the server, through networking or Hadoop configuration problems. Action: look at the logs of one of the DataNodes.\nYour DataNode instances have no hard disk space in their configured data directories. Action: look at the dfs.data.dir list in the node configurations, verify that at least one of the directories exists, and is writeable by the user running the Hadoop processes. Then look at the logs.\nYour DataNode instances have run out of space. Look at the disk capacity via the Namenode web pages. Delete old files. Compress under-used files. Buy more disks for existing servers (if there is room), upgrade the existing servers to bigger drives, or add some more servers.\nThe reserved space for a DN (as set in dfs.datanode.du.reserved is greater than the remaining free space, so the DN thinks it has no free space\nYou may also get this message due to permissions, eg if JT can not create jobtracker.info on startup.\nThis is not a problem in Hadoop, it is a problem in your cluster that you are going to have to fix on your own. Sorry.\n</nowiki>\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n# Chukwa: Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(142,'== Bnote 2013 ==\n\n\n=== bNote-2013-01-11 ===\n\n==== Misc. info (각종 업무 담당자 연락처) ====\n\n;기술원 에어컨 안나올때\n: #9120 기술원 통합 방재 센터 (과장) (지원팀 > 환경안전그룹)\n\n;VDI (SBC) 문제 있을 떄\n: #8272 (VDI HelpDesk)\n\n; 네트워크 안될 때 (방화벽 등)\n: 어디로?\n\n\n==== HDM (Hadoop Monitoring) ====\n\n <nowiki>\nroot@ub01:[hdmon] # pwd\n/usr/local/home/www/hdmon\nroot@ub01:[hdmon] # cat index.html \n<head>\n	<title>Hadoop Cluster Monitor</title>\n</head>\n<body bgcolor=\"#ffffff\">\n	<table width=\"100%\" height=\"100%\">\n		<tr>\n			<td>\n				<iframe width=\"1200\" height=\"700\" src=\"http://hd-master-01:50070/\">Namenode Monitor @hd-master-01</iframe>\n				<iframe width=\"1200\" height=\"700\" src=\"http://hd-master-01:50030/\">JobTracker Monitor @hd-master-01</iframe>\n			</td>\n		</tr>\n		<tr>\n			<td>\n				<iframe width=\"600\" height=\"600\" src=\"http://hd-slave-0001:50060/\">TaskTracker Monitor @hd-slave-0001</iframe>\n				<iframe width=\"600\" height=\"600\" src=\"http://hd-slave-0002:50060/\">TaskTracker Monitor @hd-slave-0002</iframe>\n				<iframe width=\"600\" height=\"600\" src=\"http://hd-slave-0003:50060/\">TaskTracker Monitor @hd-slave-0003</iframe>\n				<iframe width=\"600\" height=\"600\" src=\"http://hd-slave-0004:50060/\">TaskTracker Monitor @hd-slave-0004</iframe>\n			</td>\n		</tr>\n	</table>\n</body>\n</nowiki>\n\n==== Hadoop Cluster Problem Solved ====\n\n* [http://wiki.apache.org/hadoop/CouldOnlyBeReplicatedTo Could Only Be Replicated To ...]\n <nowiki>\nCould Only Be Replicated To ...\n\nA common message people see is \"could only be replicated to 0 nodes, instead of ...\".\n\nWhat does this mean? It means that the Block Replication mechanism of HDFS could not make any copies of a file it wanted to create. This can be caused by\n\nNo DataNode instances being up and running. Action: look at the servers, see if the processes are running.\nThe DataNode instances cannot talk to the server, through networking or Hadoop configuration problems. Action: look at the logs of one of the DataNodes.\nYour DataNode instances have no hard disk space in their configured data directories. Action: look at the dfs.data.dir list in the node configurations, verify that at least one of the directories exists, and is writeable by the user running the Hadoop processes. Then look at the logs.\nYour DataNode instances have run out of space. Look at the disk capacity via the Namenode web pages. Delete old files. Compress under-used files. Buy more disks for existing servers (if there is room), upgrade the existing servers to bigger drives, or add some more servers.\nThe reserved space for a DN (as set in dfs.datanode.du.reserved is greater than the remaining free space, so the DN thinks it has no free space\nYou may also get this message due to permissions, eg if JT can not create jobtracker.info on startup.\nThis is not a problem in Hadoop, it is a problem in your cluster that you are going to have to fix on your own. Sorry.\n</nowiki>\n\n=== bNote-2013-01-10 ===\n\n\n==== Hadoop Monitoring ====\n\n# Web UI (available by default)\n#* http://localhost:50070/ – web UI of the NameNode daemon\n#* http://localhost:50030/ – web UI of the JobTracker daemon\n#* http://localhost:50060/ – web UI of the TaskTracker daemon\n# Chukwa: Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n\n<br/>\n\n==== What If? ====\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n==== Hadoop Error ====\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n==== Python Implementation for Bayesian Inference ====\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n==== 미팅 일정 ====\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n==== Hadoop Distributed Grep Case ====\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n=== bNote-2013-01-09 ===\n\n\n==== Misc. ====\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n==== Hadoop Workload Analysis ====\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n==== Hadoop Tutorial ====\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n==== Server Workload Patterns ====\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n==== Supermicro 16-node Server ====\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n=== bNote-2013-01-08 ===\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n=== bNote-2013-01-03 ===\n\n==== Hadoop and (distributed) Cache ====\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(143,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM과 SSD를 같은 방식으로 사용할 경우 야기될 수 있는 cache speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location map 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨. \n\n- SSD page cache에 page data가 tieirng될 때에는 log-structured 방식으로 write됨. 즉 기존 page data는 \n\n\n tiering\n즉 RAM page cache의 tiering page pool에서 tiering buffer 단위로 챠읽혀져 나가지만, SSD page cache 입장에서는 page chunk 단위로ㅕ write됨. 예를 들어 SSD page cache의 page chunk의 크기가 1024KB인데, tiering buffer의 크기가 256KB이라면 \n\n   tiering I/O는 tiering buffer 단위로 이루어짐.\n\n- SSD page cache에 page 데이터가 write될 때는 log-structure 방식으로 이루어지는데, 특히 tiering buffer\n\nRAM page cache 입장에서는 tiering buffer 단위로 tiering page pool에서 page들이 SSD page cache로 tiering되log-structured 방식으로 해당 데이터가 SSD에 write됨. \n\n\ntiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n\n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의 \n\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(144,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정함. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 또한 page cache tiering을 위해 N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk를 저장함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location map 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 반드시 page chunk 단위로 I/O가 일어날 필요는 없음. SSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨. \n\n- SSD page cache에 page data가 tieirng될 때에는 log-structured 방식으로 write됨. 즉 기존 page data는 \n\n\n tiering\n즉 RAM page cache의 tiering page pool에서 tiering buffer 단위로 챠읽혀져 나가지만, SSD page cache 입장에서는 page chunk 단위로ㅕ write됨. 예를 들어 SSD page cache의 page chunk의 크기가 1024KB인데, tiering buffer의 크기가 256KB이라면 \n\n   tiering I/O는 tiering buffer 단위로 이루어짐.\n\n- SSD page cache에 page 데이터가 write될 때는 log-structure 방식으로 이루어지는데, 특히 tiering buffer\n\nRAM page cache 입장에서는 tiering buffer 단위로 tiering page pool에서 page들이 SSD page cache로 tiering되log-structured 방식으로 해당 데이터가 SSD에 write됨. \n\n\ntiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n\n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의 \n\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(145,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역에는 tiering된 page cache data가 저장됨.\n\n- page cache media로 사용되는 SSD에서는 log-structured manner로 page chunk write이 처리됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- N_tb 크기의 page chunk에 대한 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location map 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page chunk 단위로 read될 수도 있고 page 단위로 read될 수도 있음. \n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨. \n\n- SSD page cache에 page data가 tieirng될 때에는 log-structured 방식으로 write됨. 즉 기존 page data는 \n\n\n tiering\n즉 RAM page cache의 tiering page pool에서 tiering buffer 단위로 챠읽혀져 나가지만, SSD page cache 입장에서는 page chunk 단위로ㅕ write됨. 예를 들어 SSD page cache의 page chunk의 크기가 1024KB인데, tiering buffer의 크기가 256KB이라면 \n\n   tiering I/O는 tiering buffer 단위로 이루어짐.\n\n- SSD page cache에 page 데이터가 write될 때는 log-structure 방식으로 이루어지는데, 특히 tiering buffer\n\nRAM page cache 입장에서는 tiering buffer 단위로 tiering page pool에서 page들이 SSD page cache로 tiering되log-structured 방식으로 해당 데이터가 SSD에 write됨. \n\n\ntiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n\n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의 \n\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(146,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk location lookup table이 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page chunk 단위로 read될 수도 있지만 page 단위로 read될 수도 있음.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음.\n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- secondary page cache media로부터의 page data read가 page chunk 단위로 일어날 수도 있고 page 단위로 일어날 수도 있음을 고려하여 page chunk data write 시 SSD 내부에 in-page-chunk page lookup table을 작성해둔다. 예를 들어 page chunk가 \n\n\n\n    \n- secondary page cache media로 사용되는 SSD에서 data read 시, N_tb 크기의 page chunk 단위로 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해당하는 page chunk의 위치를 파악하고, 각 channel에서 동시에 해당 data를 읽어와서 host로 data를 전달함.\n\n- secondary page cache media로 사용되는 SSD에서 data read 시, page 단위로 read 요청이 SSD에 들어오면, SSD는 요청에 해당하는 page data를 찾기 위해 page chunk location lookup table을 참고하여 해\n\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location map 정보가 기록됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page chunk 단위로 read될 수도 있고 page 단위로 read될 수도 있음. \n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk location lookup table을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨. \n\n- SSD page cache에 page data가 tieirng될 때에는 log-structured 방식으로 write됨. 즉 기존 page data는 \n\n\n tiering\n즉 RAM page cache의 tiering page pool에서 tiering buffer 단위로 챠읽혀져 나가지만, SSD page cache 입장에서는 page chunk 단위로ㅕ write됨. 예를 들어 SSD page cache의 page chunk의 크기가 1024KB인데, tiering buffer의 크기가 256KB이라면 \n\n   tiering I/O는 tiering buffer 단위로 이루어짐.\n\n- SSD page cache에 page 데이터가 write될 때는 log-structure 방식으로 이루어지는데, 특히 tiering buffer\n\nRAM page cache 입장에서는 tiering buffer 단위로 tiering page pool에서 page들이 SSD page cache로 tiering되log-structured 방식으로 해당 데이터가 SSD에 write됨. \n\n\ntiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n\n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의 \n\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(147,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD page cache에서 \n\n\n\n\n==== hybrid page cache meta information management ====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음.\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨. \n\n- SSD page cache에 page data가 tieirng될 때에는 log-structured 방식으로 write됨. 즉 기존 page data는 \n\n\n tiering\n즉 RAM page cache의 tiering page pool에서 tiering buffer 단위로 챠읽혀져 나가지만, SSD page cache 입장에서는 page chunk 단위로ㅕ write됨. 예를 들어 SSD page cache의 page chunk의 크기가 1024KB인데, tiering buffer의 크기가 256KB이라면 \n\n   tiering I/O는 tiering buffer 단위로 이루어짐.\n\n- SSD page cache에 page 데이터가 write될 때는 log-structure 방식으로 이루어지는데, 특히 tiering buffer\n\nRAM page cache 입장에서는 tiering buffer 단위로 tiering page pool에서 page들이 SSD page cache로 tiering되log-structured 방식으로 해당 데이터가 SSD에 write됨. \n\n\ntiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n\n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의 \n\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(148,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk가 log-structured 방식으로 write되고 난 다음에는 다시 page chunk 0의 위치에 write 되게 된다. 이때 erase 연산으로 인한 I/O 속도 저하를 방지하기 위해 SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. 이때 erase될 page chunk를 선택하는 기준으로서, 일반적으로 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old--first 방법을 사용할 수도 있겠지만, page cache의 page find/get 메커니즘에 의해 이미 SSD page cache로부터 RAM에 다시 load되어 사용되고 있는 page들이 많이 존재하는 page chunk부터 erase를 시키는 already-read-page-chunk-first\n\n==== hybrid page cache meta information management ====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음.\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨. \n\n- SSD page cache에 page data가 tieirng될 때에는 log-structured 방식으로 write됨. 즉 기존 page data는 \n\n\n tiering\n즉 RAM page cache의 tiering page pool에서 tiering buffer 단위로 챠읽혀져 나가지만, SSD page cache 입장에서는 page chunk 단위로ㅕ write됨. 예를 들어 SSD page cache의 page chunk의 크기가 1024KB인데, tiering buffer의 크기가 256KB이라면 \n\n   tiering I/O는 tiering buffer 단위로 이루어짐.\n\n- SSD page cache에 page 데이터가 write될 때는 log-structure 방식으로 이루어지는데, 특히 tiering buffer\n\nRAM page cache 입장에서는 tiering buffer 단위로 tiering page pool에서 page들이 SSD page cache로 tiering되log-structured 방식으로 해당 데이터가 SSD에 write됨. \n\n\ntiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n\n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의 \n\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(149,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache에서는 tiering buffer에 의해\n\n==== hybrid page cache meta information management ====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음.\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨. \n\n- SSD page cache에 page data가 tieirng될 때에는 log-structured 방식으로 write됨. 즉 기존 page data는 \n\n\n tiering\n즉 RAM page cache의 tiering page pool에서 tiering buffer 단위로 챠읽혀져 나가지만, SSD page cache 입장에서는 page chunk 단위로ㅕ write됨. 예를 들어 SSD page cache의 page chunk의 크기가 1024KB인데, tiering buffer의 크기가 256KB이라면 \n\n   tiering I/O는 tiering buffer 단위로 이루어짐.\n\n- SSD page cache에 page 데이터가 write될 때는 log-structure 방식으로 이루어지는데, 특히 tiering buffer\n\nRAM page cache 입장에서는 tiering buffer 단위로 tiering page pool에서 page들이 SSD page cache로 tiering되log-structured 방식으로 해당 데이터가 SSD에 write됨. \n\n\ntiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n\n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의 \n\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(150,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n==== hybrid page cache meta information management ====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음.\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨. \n\n- SSD page cache에 page data가 tieirng될 때에는 log-structured 방식으로 write됨. 즉 기존 page data는 \n\n\n tiering\n즉 RAM page cache의 tiering page pool에서 tiering buffer 단위로 챠읽혀져 나가지만, SSD page cache 입장에서는 page chunk 단위로ㅕ write됨. 예를 들어 SSD page cache의 page chunk의 크기가 1024KB인데, tiering buffer의 크기가 256KB이라면 \n\n   tiering I/O는 tiering buffer 단위로 이루어짐.\n\n- SSD page cache에 page 데이터가 write될 때는 log-structure 방식으로 이루어지는데, 특히 tiering buffer\n\nRAM page cache 입장에서는 tiering buffer 단위로 tiering page pool에서 page들이 SSD page cache로 tiering되log-structured 방식으로 해당 데이터가 SSD에 write됨. \n\n\ntiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n\n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의 \n\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(151,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨. \n\n- SSD page cache에 page data가 tieirng될 때에는 log-structured 방식으로 write됨. 즉 기존 page data는 \n\n\n tiering\n즉 RAM page cache의 tiering page pool에서 tiering buffer 단위로 챠읽혀져 나가지만, SSD page cache 입장에서는 page chunk 단위로ㅕ write됨. 예를 들어 SSD page cache의 page chunk의 크기가 1024KB인데, tiering buffer의 크기가 256KB이라면 \n\n   tiering I/O는 tiering buffer 단위로 이루어짐.\n\n- SSD page cache에 page 데이터가 write될 때는 log-structure 방식으로 이루어지는데, 특히 tiering buffer\n\nRAM page cache 입장에서는 tiering buffer 단위로 tiering page pool에서 page들이 SSD page cache로 tiering되log-structured 방식으로 해당 데이터가 SSD에 write됨. \n\n\ntiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n\n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의 \n\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(152,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨. \n\n- SSD page cache에 page data가 tieirng될 때에는 log-structured 방식으로 write됨. 즉 기존 page data는 \n\n\n tiering\n즉 RAM page cache의 tiering page pool에서 tiering buffer 단위로 챠읽혀져 나가지만, SSD page cache 입장에서는 page chunk 단위로ㅕ write됨. 예를 들어 SSD page cache의 page chunk의 크기가 1024KB인데, tiering buffer의 크기가 256KB이라면 \n\n   tiering I/O는 tiering buffer 단위로 이루어짐.\n\n- SSD page cache에 page 데이터가 write될 때는 log-structure 방식으로 이루어지는데, 특히 tiering buffer\n\nRAM page cache 입장에서는 tiering buffer 단위로 tiering page pool에서 page들이 SSD page cache로 tiering되log-structured 방식으로 해당 데이터가 SSD에 write됨. \n\n\ntiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n\n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의 \n\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n* checkpoint\n- workload 상황에 따라 각 page들에 대한 I/O access 패턴이 변경될 수 있으며, 그 결과로 class-2에 속했던 page들이 class-1으로 상향 조정되는 경우가 있을 수 있다. 물론 반대로 class-2에 속했던 page들이 class-3으로 하향 조정되는 경우도 있을 수 있지 않을까? (이 경우는 class-2에 속하던 page들이 SSD로 tiering 되고 나서, 너무 실적이 없을때? 아니면 실적이 없는 상태로 RAM에서 오래 동안 머물고 있을 수도 있을까? 그것 자체가 class-3을 의미하는 것일텐데.....)\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(153,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨. \n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8');
INSERT INTO `mw_text` VALUES (154,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(155,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(156,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n## 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n## server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(157,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(158,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(159,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(160,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(161,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== #_ 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(162,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== #_ 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== #_ 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(163,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== #_ 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== #_ 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== #_ 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(164,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== \'\'\'배경 / 기존 기술의 문제점\'\'\' ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== #_ 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== #_ 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(165,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== #_ 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== #_ 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(166,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== #_ 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(167,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# N-window 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 monitoring window를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(168,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- page class 분류를 위한 access 패턴 모니터링은 recency, frequency, repeatability를 측정하는 복수 개의 windows를 활용하는 multi-window 방식을 기반으로 이루어짐.\n\n- multi-window 방식들 중 짧은 기간 동안 관찰하는 window 1과 긴 기간동안 access 패턴을 관찰하는 window 2을 함께 이용하는 Two-window 모델이 있음.\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(169,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(170,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 (periodicity) 발견을 위해서 어떤 방식을 이용할 수 있을까? [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(171,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([periodicity]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [periodicity transform] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(172,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 공개/등록 문건을 검색할 수 있었으나, 본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(173,'\n\n\n \n\n\nWhat is a periodicity transform?\n\n\n\nIt is a way to automatically detect periodicities.\n\nWhat does it do?\n\nTo give you an idea of what the Periodicity Transform is good at, let\'s look at a simple example. Here is a data record that I constructed by adding together two periodic sequences, one with period 13 and the other with period 19. For good measure, I added about 25% noise. The result looks like this:\n\n[http://eceserv0.ece.wisc.edu/~sethares/images/xyz.gif]\n\n\nIt\'s not so easy to \"see\" the two periodic sequences that are buried inside, is it? If we take the Fourier transform (DFT), then we get:\n\n\n\nAgain, its pretty difficult to see any pattern here. The 13-periodic and the 19-periodic sequences are still hidden. Now let\'s apply the periodicity transform called the \"M-Best\" algorithm, which searches for the M largest periodicities. With M=10, we get:\n\n\n\nNow that\'s more like it! The two periods (at 13 and 19) are clearly visible. The other eight small periods reflect minor accidental patterns that happen to occur in the noise. The Periodicity Transforms are good at finding periodicities in data.\n\nHow does it work?\n\nMost standard transforms can be interpreted as projections onto suitable subspaces, and in most cases (such as the Fourier and Wavelet transforms), the subspaces are orthogonal. Such orthogonality implies that the projection onto one subspace is independent of the projection onto others. Thus a projection onto one sinusoidal basis function (in the Fourier Transform) is independent of the projections onto others, and the Fourier decomposition can proceed by projecting onto one subspace, subtracting out the projection, and repeating. Orthogonality guarantees that the order of projection is irrelevant. This is not true for projection onto nonorthogonal subspaces such as the periodic subspaces Sp. Thus the order in which the projections occur effects the decomposition, and the Periodicity Transform does not in general provide a unique representation. Once the succession of the projections is specified, however, then the answer is unique.\n\nThe Periodicity Transform searches for the best periodic characterization of the length N signal x. The underlying technique is to project x onto some periodic subspace Sp. This periodicity is then removed from x leaving the residual r stripped of its p-periodicities. Both the projection x and the residual r may contain other periodicities, and so may be decomposed into other q-periodic components by further projection onto Sq. The trick in designing a useful algorithm is to provide a sensible criterion for choosing the order in which the successive p\'s and q\'s are chosen. The intended goal of the decomposition, the amount of computational resources available, and the measure of \"goodness-of-fit\" all influence the algorithm. Our paper discusses four ways to mind our p\'s and q\'s.\n\nFour flavors\n\n(1) The \"small to large\" algorithm assumes a threshold T and calculates the projections onto Sp beginning with p=1 and progressing through p=N/2. Whenever the projection contains at least T percent of the energy in x, then the corresponding projection is chosen as a basis element.\n\n(2) The \"M-best\" algorithm maintains a list of the M best periodicities and the corresponding basis elements. When a new (sub)periodicity is detected that removes more power from the signal than one currently on the list, the new one replaces the old, and the algorithm iterates.\n\n(3) The \"best-correlation\" algorithm projects x onto all the periodic basis elements, essentially measuring the correlation between x and the individual periodic basis elements. The p with the largest (in absolute value) correlation is then used for the projection.\n\n(4) The \"best-frequency\" algorithm determines p by Fourier methods and then projects onto Sp.\n\nMATLAB routines to calculate all of these variations are available by clicking here.\n\nFor more details, see our paper:\n\nW. A. Sethares and T. W. Staley, \"Periodicity Transforms\", IEEE Transactions on Signal Processing, Nov 1999. You can download a slightly raw pdf version here.\n\nA companion paper explores the application of Periodicity Transforms to the automatic detection of rhythm in musical performance.\n\n\n\nTo get to my homepage, click here.','utf-8'),(174,'[http://sethares.engr.wisc.edu/downper.html \"What is a periodicity transform?\"]\n\n \n\n\nWhat is a periodicity transform?\n\n\n\nIt is a way to automatically detect periodicities.\n\nWhat does it do?\n\nTo give you an idea of what the Periodicity Transform is good at, let\'s look at a simple example. Here is a data record that I constructed by adding together two periodic sequences, one with period 13 and the other with period 19. For good measure, I added about 25% noise. The result looks like this:\n\n[http://eceserv0.ece.wisc.edu/~sethares/images/xyz.gif]\n\n\nIt\'s not so easy to \"see\" the two periodic sequences that are buried inside, is it? If we take the Fourier transform (DFT), then we get:\n\n\n\nAgain, its pretty difficult to see any pattern here. The 13-periodic and the 19-periodic sequences are still hidden. Now let\'s apply the periodicity transform called the \"M-Best\" algorithm, which searches for the M largest periodicities. With M=10, we get:\n\n\n\nNow that\'s more like it! The two periods (at 13 and 19) are clearly visible. The other eight small periods reflect minor accidental patterns that happen to occur in the noise. The Periodicity Transforms are good at finding periodicities in data.\n\nHow does it work?\n\nMost standard transforms can be interpreted as projections onto suitable subspaces, and in most cases (such as the Fourier and Wavelet transforms), the subspaces are orthogonal. Such orthogonality implies that the projection onto one subspace is independent of the projection onto others. Thus a projection onto one sinusoidal basis function (in the Fourier Transform) is independent of the projections onto others, and the Fourier decomposition can proceed by projecting onto one subspace, subtracting out the projection, and repeating. Orthogonality guarantees that the order of projection is irrelevant. This is not true for projection onto nonorthogonal subspaces such as the periodic subspaces Sp. Thus the order in which the projections occur effects the decomposition, and the Periodicity Transform does not in general provide a unique representation. Once the succession of the projections is specified, however, then the answer is unique.\n\nThe Periodicity Transform searches for the best periodic characterization of the length N signal x. The underlying technique is to project x onto some periodic subspace Sp. This periodicity is then removed from x leaving the residual r stripped of its p-periodicities. Both the projection x and the residual r may contain other periodicities, and so may be decomposed into other q-periodic components by further projection onto Sq. The trick in designing a useful algorithm is to provide a sensible criterion for choosing the order in which the successive p\'s and q\'s are chosen. The intended goal of the decomposition, the amount of computational resources available, and the measure of \"goodness-of-fit\" all influence the algorithm. Our paper discusses four ways to mind our p\'s and q\'s.\n\nFour flavors\n\n(1) The \"small to large\" algorithm assumes a threshold T and calculates the projections onto Sp beginning with p=1 and progressing through p=N/2. Whenever the projection contains at least T percent of the energy in x, then the corresponding projection is chosen as a basis element.\n\n(2) The \"M-best\" algorithm maintains a list of the M best periodicities and the corresponding basis elements. When a new (sub)periodicity is detected that removes more power from the signal than one currently on the list, the new one replaces the old, and the algorithm iterates.\n\n(3) The \"best-correlation\" algorithm projects x onto all the periodic basis elements, essentially measuring the correlation between x and the individual periodic basis elements. The p with the largest (in absolute value) correlation is then used for the projection.\n\n(4) The \"best-frequency\" algorithm determines p by Fourier methods and then projects onto Sp.\n\nMATLAB routines to calculate all of these variations are available by clicking here.\n\nFor more details, see our paper:\n\nW. A. Sethares and T. W. Staley, \"Periodicity Transforms\", IEEE Transactions on Signal Processing, Nov 1999. You can download a slightly raw pdf version here.\n\nA companion paper explores the application of Periodicity Transforms to the automatic detection of rhythm in musical performance.\n\n\n\nTo get to my homepage, click here.','utf-8'),(175,'== What is a periodicity transform? ==\n\nIt is a way to automatically detect periodicities.\n\n[http://sethares.engr.wisc.edu/downper.html \"What is a periodicity transform?\"]\n\n\n=== What does it do? ===\n\nTo give you an idea of what the Periodicity Transform is good at, let\'s look at a simple example. Here is a data record that I constructed by adding together two periodic sequences, one with period 13 and the other with period 19. For good measure, I added about 25% noise. The result looks like this:\n\n[http://eceserv0.ece.wisc.edu/~sethares/images/xyz.gif]\n\nIt\'s not so easy to \"see\" the two periodic sequences that are buried inside, is it? If we take the Fourier transform (DFT), then we get:\n\nAgain, its pretty difficult to see any pattern here. The 13-periodic and the 19-periodic sequences are still hidden. Now let\'s apply the periodicity transform called the \"M-Best\" algorithm, which searches for the M largest periodicities. With M=10, we get:\n\nNow that\'s more like it! The two periods (at 13 and 19) are clearly visible. The other eight small periods reflect minor accidental patterns that happen to occur in the noise. The Periodicity Transforms are good at finding periodicities in data.\n\n=== How does it work? ===\n\nMost standard transforms can be interpreted as projections onto suitable subspaces, and in most cases (such as the Fourier and Wavelet transforms), the subspaces are orthogonal. Such orthogonality implies that the projection onto one subspace is independent of the projection onto others. Thus a projection onto one sinusoidal basis function (in the Fourier Transform) is independent of the projections onto others, and the Fourier decomposition can proceed by projecting onto one subspace, subtracting out the projection, and repeating. Orthogonality guarantees that the order of projection is irrelevant. This is not true for projection onto nonorthogonal subspaces such as the periodic subspaces Sp. Thus the order in which the projections occur effects the decomposition, and the Periodicity Transform does not in general provide a unique representation. Once the succession of the projections is specified, however, then the answer is unique.\n\nThe Periodicity Transform searches for the best periodic characterization of the length N signal x. The underlying technique is to project x onto some periodic subspace Sp. This periodicity is then removed from x leaving the residual r stripped of its p-periodicities. Both the projection x and the residual r may contain other periodicities, and so may be decomposed into other q-periodic components by further projection onto Sq. The trick in designing a useful algorithm is to provide a sensible criterion for choosing the order in which the successive p\'s and q\'s are chosen. The intended goal of the decomposition, the amount of computational resources available, and the measure of \"goodness-of-fit\" all influence the algorithm. Our paper discusses four ways to mind our p\'s and q\'s.\n\n=== Four flavors ===\n\n(1) The \"small to large\" algorithm assumes a threshold T and calculates the projections onto Sp beginning with p=1 and progressing through p=N/2. Whenever the projection contains at least T percent of the energy in x, then the corresponding projection is chosen as a basis element.\n\n(2) The \"M-best\" algorithm maintains a list of the M best periodicities and the corresponding basis elements. When a new (sub)periodicity is detected that removes more power from the signal than one currently on the list, the new one replaces the old, and the algorithm iterates.\n\n(3) The \"best-correlation\" algorithm projects x onto all the periodic basis elements, essentially measuring the correlation between x and the individual periodic basis elements. The p with the largest (in absolute value) correlation is then used for the projection.\n\n(4) The \"best-frequency\" algorithm determines p by Fourier methods and then projects onto Sp.\n\nMATLAB routines to calculate all of these variations are available by clicking here.\n\nFor more details, see our paper:\n\nW. A. Sethares and T. W. Staley, \"Periodicity Transforms\", IEEE Transactions on Signal Processing, Nov 1999. You can download a slightly raw pdf version here.\n\nA companion paper explores the application of Periodicity Transforms to the automatic detection of rhythm in musical performance.\n\n\n\nTo get to my homepage, click here.','utf-8'),(176,'== What is a periodicity transform? ==\n\nIt is a way to automatically detect periodicities.\n\n[http://sethares.engr.wisc.edu/downper.html \"What is a periodicity transform?\"]\n\n\n=== What does it do? ===\n\nTo give you an idea of what the Periodicity Transform is good at, let\'s look at a simple example. Here is a data record that I constructed by adding together two periodic sequences, one with period 13 and the other with period 19. For good measure, I added about 25% noise. The result looks like this:\n\n[http://eceserv0.ece.wisc.edu/~sethares/images/xyz.gif]\n\nIt\'s not so easy to \"see\" the two periodic sequences that are buried inside, is it? If we take the Fourier transform (DFT), then we get:\n\nAgain, its pretty difficult to see any pattern here. The 13-periodic and the 19-periodic sequences are still hidden. Now let\'s apply the periodicity transform called the \"M-Best\" algorithm, which searches for the M largest periodicities. With M=10, we get:\n\nNow that\'s more like it! The two periods (at 13 and 19) are clearly visible. The other eight small periods reflect minor accidental patterns that happen to occur in the noise. The Periodicity Transforms are good at finding periodicities in data.\n\n=== How does it work? ===\n\nMost standard transforms can be interpreted as projections onto suitable subspaces, and in most cases (such as the Fourier and Wavelet transforms), the subspaces are orthogonal. Such orthogonality implies that the projection onto one subspace is independent of the projection onto others. Thus a projection onto one sinusoidal basis function (in the Fourier Transform) is independent of the projections onto others, and the Fourier decomposition can proceed by projecting onto one subspace, subtracting out the projection, and repeating. Orthogonality guarantees that the order of projection is irrelevant. This is not true for projection onto nonorthogonal subspaces such as the periodic subspaces Sp. Thus the order in which the projections occur effects the decomposition, and the Periodicity Transform does not in general provide a unique representation. Once the succession of the projections is specified, however, then the answer is unique.\n\nThe Periodicity Transform searches for the best periodic characterization of the length N signal x. The underlying technique is to project x onto some periodic subspace Sp. This periodicity is then removed from x leaving the residual r stripped of its p-periodicities. Both the projection x and the residual r may contain other periodicities, and so may be decomposed into other q-periodic components by further projection onto Sq. The trick in designing a useful algorithm is to provide a sensible criterion for choosing the order in which the successive p\'s and q\'s are chosen. The intended goal of the decomposition, the amount of computational resources available, and the measure of \"goodness-of-fit\" all influence the algorithm. Our paper discusses four ways to mind our p\'s and q\'s.\n\n=== Four flavors ===\n\n(1) The \"small to large\" algorithm assumes a threshold T and calculates the projections onto Sp beginning with p=1 and progressing through p=N/2. Whenever the projection contains at least T percent of the energy in x, then the corresponding projection is chosen as a basis element.\n\n(2) The \"M-best\" algorithm maintains a list of the M best periodicities and the corresponding basis elements. When a new (sub)periodicity is detected that removes more power from the signal than one currently on the list, the new one replaces the old, and the algorithm iterates.\n\n(3) The \"best-correlation\" algorithm projects x onto all the periodic basis elements, essentially measuring the correlation between x and the individual periodic basis elements. The p with the largest (in absolute value) correlation is then used for the projection.\n\n(4) The \"best-frequency\" algorithm determines p by Fourier methods and then projects onto Sp.\n\nMATLAB routines to calculate all of these variations are available by clicking here.\n\n\n=== References ===\n\nFor more details, see our paper:\n\nW. A. Sethares and T. W. Staley, \"Periodicity Transforms\", IEEE Transactions on Signal Processing, Nov 1999. You can download a slightly raw pdf version here.\n\nA companion paper explores the application of Periodicity Transforms to the automatic detection of rhythm in musical performance.\n\n\n\nTo get to my homepage, click here.','utf-8'),(177,'== What is a periodicity transform? ==\n\nIt is a way to automatically detect periodicities.\n\n[http://sethares.engr.wisc.edu/downper.html \"What is a periodicity transform?\"]\n\n\n=== What does it do? ===\n\nTo give you an idea of what the Periodicity Transform is good at, let\'s look at a simple example. Here is a data record that I constructed by adding together two periodic sequences, one with period 13 and the other with period 19. For good measure, I added about 25% noise. The result looks like this:\n\n[http://eceserv0.ece.wisc.edu/~sethares/images/xyz.gif]\n\nIt\'s not so easy to \"see\" the two periodic sequences that are buried inside, is it? If we take the Fourier transform (DFT), then we get:\n\nAgain, its pretty difficult to see any pattern here. The 13-periodic and the 19-periodic sequences are still hidden. Now let\'s apply the periodicity transform called the \"M-Best\" algorithm, which searches for the M largest periodicities. With M=10, we get:\n\nNow that\'s more like it! The two periods (at 13 and 19) are clearly visible. The other eight small periods reflect minor accidental patterns that happen to occur in the noise. The Periodicity Transforms are good at finding periodicities in data.\n\n=== How does it work? ===\n\nMost standard transforms can be interpreted as projections onto suitable subspaces, and in most cases (such as the Fourier and Wavelet transforms), the subspaces are orthogonal. Such orthogonality implies that the projection onto one subspace is independent of the projection onto others. Thus a projection onto one sinusoidal basis function (in the Fourier Transform) is independent of the projections onto others, and the Fourier decomposition can proceed by projecting onto one subspace, subtracting out the projection, and repeating. Orthogonality guarantees that the order of projection is irrelevant. This is not true for projection onto nonorthogonal subspaces such as the periodic subspaces Sp. Thus the order in which the projections occur effects the decomposition, and the Periodicity Transform does not in general provide a unique representation. Once the succession of the projections is specified, however, then the answer is unique.\n\nThe Periodicity Transform searches for the best periodic characterization of the length N signal x. The underlying technique is to project x onto some periodic subspace Sp. This periodicity is then removed from x leaving the residual r stripped of its p-periodicities. Both the projection x and the residual r may contain other periodicities, and so may be decomposed into other q-periodic components by further projection onto Sq. The trick in designing a useful algorithm is to provide a sensible criterion for choosing the order in which the successive p\'s and q\'s are chosen. The intended goal of the decomposition, the amount of computational resources available, and the measure of \"goodness-of-fit\" all influence the algorithm. Our paper discusses four ways to mind our p\'s and q\'s.\n\n=== Four flavors ===\n\n(1) The \"small to large\" algorithm assumes a threshold T and calculates the projections onto Sp beginning with p=1 and progressing through p=N/2. Whenever the projection contains at least T percent of the energy in x, then the corresponding projection is chosen as a basis element.\n\n(2) The \"M-best\" algorithm maintains a list of the M best periodicities and the corresponding basis elements. When a new (sub)periodicity is detected that removes more power from the signal than one currently on the list, the new one replaces the old, and the algorithm iterates.\n\n(3) The \"best-correlation\" algorithm projects x onto all the periodic basis elements, essentially measuring the correlation between x and the individual periodic basis elements. The p with the largest (in absolute value) correlation is then used for the projection.\n\n(4) The \"best-frequency\" algorithm determines p by Fourier methods and then projects onto Sp.\n\nMATLAB routines to calculate all of these variations are available by clicking here.\n\n\n=== References ===\n\nFor more details, see our paper:\n\nW. A. Sethares and T. W. Staley, [http://eceserv0.ece.wisc.edu/~sethares/periodic.html \"Periodicity Transforms\"], IEEE Transactions on Signal Processing, Nov 1999. You can download a slightly raw pdf version here.\n\nA [http://sethares.engr.wisc.edu/papers/rhypap.html companion paper] explores the application of Periodicity Transforms to the automatic detection of rhythm in musical performance.\n\n\n\nTo get to my homepage, click here.','utf-8'),(178,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(179,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== # 대표 도면 ===\n\n=== 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(180,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== # 대표 도면 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(181,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== # 대표 도면 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(182,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n=== # 대표 도면 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(183,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n<br/>\n\n=== # 대표 도면 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(184,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(185,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8');
INSERT INTO `mw_text` VALUES (186,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(187,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 slice될 수 있음. meta info 영역에는 page chunk lookup table per SSD 정보가 저장됨. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(188,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(189,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(190,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering buffer\n- page cache tiering manager\n\n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(191,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD를 결합한 hybrid page cache는 다음과 같은 요소로 구성됨.\n- RAM page cache\n- SSD page cache\n- tiering page pool\n- page cache tiering manager\n\npage cache tiering manager는 \n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(192,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n- RAM page cache\n- SSD page cache\n- page cache tiering manager\n  - page classification module\n  - tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 page tiering up/down이 수행된다 (tiering page pool). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(193,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 page tiering up/down이 수행된다 (tiering page pool). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(194,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering page pool ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering page pool이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering page pool 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering page pool 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering page pool 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering page pool 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering page pool 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering page pool에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering page pool내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering page pool에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(195,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n- 각 class에 대한 설명:\n	- class-1: RAM page cache에 저장될 page들이 속하는 class\n	- class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n	- class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n- page class 분류 조건:\n	- 조건1: \"currently-hot\"\n	- 조건 2: \"currently-not-hot\" AND \"once-hot\"\n	- 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(196,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n* 각 class에 대한 설명:\n** class-1: RAM page cache에 저장될 page들이 속하는 class\n** class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n** class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n* page class 분류 조건:\n** 조건1: \"currently-hot\"\n** 조건 2: \"currently-not-hot\" AND \"once-hot\"\n** 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(197,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n* 각 class에 대한 설명:\n** class-1: RAM page cache에 저장될 page들이 속하는 class\n** class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n** class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n* page class 분류 조건:\n** 조건1: \"currently-hot\"\n** 조건 2: \"currently-not-hot\" AND \"once-hot\"\n** 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n# application에서 512-byte 만큼만 (혹은 1-byte만) 읽어온다고 했을 때에도 page cache에서는 4096-byte 단위로 find/get 하게 될까? (당연히 그럴 것 같지만... 확인 필요), 그리고 실제 block layer에 request가 전달될 때에는 어떻게 될까? 1-byte를 요청했어도 1 sector (512-byte)만큼 읽게 될까? 다시 정리하자면, application 수준에서 1-byte를 요청한 경우, page cache layer에서는 4096-byte를 읽어오게 되고, block layer에서는 512-byte를 읽어오게 되는 것일까?\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(198,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# page tiering 시 spatial-locality가 있는 page들끼리 묶어 같이 tiering될 수 있도록 하는 방식. (SSD page cache로 tiering-down 하는 경우 명시, 그리고 SSD page cache에서 tiering-up 하는 경우 명시)\n# 매우 고속이면서, 자원 효율적으로 spatial-locality 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# 매우 고속이면서, 자원 효율적으로 periodicity 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# periodicity 특성과 recency 특성을 함께 고려하여 tier-1 page cache (RAM page cache)와 tier-2 page cache (SSD page cache)에 cache data placement하는 방법\n\n\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n* 각 class에 대한 설명:\n** class-1: RAM page cache에 저장될 page들이 속하는 class\n** class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n** class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n* page class 분류 조건:\n** 조건1: \"currently-hot\"\n** 조건 2: \"currently-not-hot\" AND \"once-hot\"\n** 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n# application에서 512-byte 만큼만 (혹은 1-byte만) 읽어온다고 했을 때에도 page cache에서는 4096-byte 단위로 find/get 하게 될까? (당연히 그럴 것 같지만... 확인 필요), 그리고 실제 block layer에 request가 전달될 때에는 어떻게 될까? 1-byte를 요청했어도 1 sector (512-byte)만큼 읽게 될까? 다시 정리하자면, application 수준에서 1-byte를 요청한 경우, page cache layer에서는 4096-byte를 읽어오게 되고, block layer에서는 512-byte를 읽어오게 되는 것일까?\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(199,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# page tiering 시 spatial-locality가 있는 page들끼리 묶어 같이 tiering될 수 있도록 하는 방식. (SSD page cache로 tiering-down 하는 경우 명시, 그리고 SSD page cache에서 tiering-up 하는 경우 명시)\n# 매우 고속이면서, 자원 효율적으로 spatial-locality 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# 매우 고속이면서, 자원 효율적으로 periodicity 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# periodicity 특성과 recency 특성을 함께 고려하여 tier-1 page cache (RAM page cache)와 tier-2 page cache (SSD page cache)에 cache data placement하는 방법\n\n\n\n; 발명 요약\n: cache data에 대한 tier 별 placement가 가능하도록 access 패턴을 기반으로 page class를 분류하고, 이에 따라 tier-1 page cache, tier-2 page cache 간에 page chunk 단위로 page tiering을 수행하는 아키텍쳐 및 방법\n\n# page cache layer에서 동작하고, tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는 것을 특징으로 하는 RAM-SSD hybrid page cache 아키텍쳐.\n#* RAM 기반의 tier-1 page cache의 역할, layout 및 동작\n#** page chunk 단위로 page를 담아 tiering I/O를 수행하는 tiering buffer를 통해 tiering-down 하는 방법/순서\n#** Radix tree에 추가될 정보 및 사용/업데이트 방식\\\n#** 고속이며 자원 효율적인, spatial locality 패턴을 파악하는 방법\n#** spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n#** SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법  \n#** tiering-down 될 data의 양이 늘어가는 추세를 고려하여 page chunk * N 만큼 미리 TRIM하도록 SSD에 요청 - early TRIM\n#* SSD 기반의 tier-2 page cache의 역할, layout 및 동작\n#** tiering-down 시 page chunk data를 write하고 meta info를 관리하는 방법: log-structured, write 공간 확보를 위한 page chunk 단위의 victim selection 방식\n#** tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 tiering-up시키고, 해당 page chunk에 대한 status update하는 방법\n#* page cache tiering manager의 역할\n#** page에 대한 recency, periodicity를 고려하여 page를 classify 하는 방법\n#*** 고속이며 자원 효율적인, N-sieve 기반으로 recency와 periodicity 패턴을 파악하는 방법 (N-period, Short+Long)\n#** 시스템 event에 기반하는 passive-triggering 및 tiering pool size 모니터링에 기반하는 active-triggering 방식으로 tiering을 스케줄링하는 page cache tiering scheduler\n\n\n\n\nN-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\npage tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\npage tiering 시 spatial-locality가 있는 page들끼리 묶어 같이 tiering될 수 있도록 하는 방식. (SSD page cache로 tiering-down 하는 경우 명시, 그리고 SSD page cache에서 tiering-up 하는 경우 명시)\n매우 고속이면서, 자원 효율적으로 spatial-locality 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n매우 고속이면서, 자원 효율적으로 periodicity 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\nperiodicity 특성과 recency 특성을 함께 고려하여 tier-1 page cache (RAM page cache)와 tier-2 page cache (SSD page cache)에 cache data placement하는 방법\n\n\n\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n* 각 class에 대한 설명:\n** class-1: RAM page cache에 저장될 page들이 속하는 class\n** class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n** class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n* page class 분류 조건:\n** 조건1: \"currently-hot\"\n** 조건 2: \"currently-not-hot\" AND \"once-hot\"\n** 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n# application에서 512-byte 만큼만 (혹은 1-byte만) 읽어온다고 했을 때에도 page cache에서는 4096-byte 단위로 find/get 하게 될까? (당연히 그럴 것 같지만... 확인 필요), 그리고 실제 block layer에 request가 전달될 때에는 어떻게 될까? 1-byte를 요청했어도 1 sector (512-byte)만큼 읽게 될까? 다시 정리하자면, application 수준에서 1-byte를 요청한 경우, page cache layer에서는 4096-byte를 읽어오게 되고, block layer에서는 512-byte를 읽어오게 되는 것일까?\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(200,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# page tiering 시 spatial-locality가 있는 page들끼리 묶어 같이 tiering될 수 있도록 하는 방식. (SSD page cache로 tiering-down 하는 경우 명시, 그리고 SSD page cache에서 tiering-up 하는 경우 명시)\n# 매우 고속이면서, 자원 효율적으로 spatial-locality 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# 매우 고속이면서, 자원 효율적으로 periodicity 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# periodicity 특성과 recency 특성을 함께 고려하여 tier-1 page cache (RAM page cache)와 tier-2 page cache (SSD page cache)에 cache data placement하는 방법\n\n\n\n; 발명 요약\n: cache data에 대한 tier 별 placement가 가능하도록 access 패턴을 기반으로 page class를 분류하고, 이에 따라 tier-1 page cache, tier-2 page cache 간에 page chunk 단위로 page tiering을 수행하는 아키텍쳐 및 방법\n\n# page cache layer에서 동작하고, tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는 것을 특징으로 하는 RAM-SSD hybrid page cache 아키텍쳐.\n# RAM 기반의 tier-1 page cache의 역할, layout 및 동작\n#* page chunk 단위로 page를 담아 tiering I/O를 수행하는 tiering buffer를 통해 tiering-down 하는 방법/순서\n#* Radix tree에 추가될 정보 및 사용/업데이트 방식\\\n#* 고속이며 자원 효율적인, spatial locality 패턴을 파악하는 방법\n#* spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n#* SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법  \n#* tiering-down 될 data의 양이 늘어가는 추세를 고려하여 page chunk * N 만큼 미리 TRIM하도록 SSD에 요청 - early TRIM\n# SSD 기반의 tier-2 page cache의 역할, layout 및 동작\n#* tiering-down 시 page chunk data를 write하고 meta info를 관리하는 방법: log-structured, write 공간 확보를 위한 page chunk 단위의 victim selection 방식\n#* tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 tiering-up시키고, 해당 page chunk에 대한 status update하는 방법\n# page cache tiering manager의 역할\n#* page에 대한 recency, periodicity를 고려하여 page를 classify 하는 방법\n#** 고속이며 자원 효율적인, N-sieve 기반으로 recency와 periodicity 패턴을 파악하는 방법 (N-period, Short+Long)\n#* 시스템 event에 기반하는 passive-triggering 및 tiering pool size 모니터링에 기반하는 active-triggering 방식으로 tiering을 스케줄링하는 page cache tiering scheduler\n\n\n\n\nN-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\npage tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\npage tiering 시 spatial-locality가 있는 page들끼리 묶어 같이 tiering될 수 있도록 하는 방식. (SSD page cache로 tiering-down 하는 경우 명시, 그리고 SSD page cache에서 tiering-up 하는 경우 명시)\n매우 고속이면서, 자원 효율적으로 spatial-locality 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n매우 고속이면서, 자원 효율적으로 periodicity 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\nperiodicity 특성과 recency 특성을 함께 고려하여 tier-1 page cache (RAM page cache)와 tier-2 page cache (SSD page cache)에 cache data placement하는 방법\n\n\n\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n* 각 class에 대한 설명:\n** class-1: RAM page cache에 저장될 page들이 속하는 class\n** class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n** class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n* page class 분류 조건:\n** 조건1: \"currently-hot\"\n** 조건 2: \"currently-not-hot\" AND \"once-hot\"\n** 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n# application에서 512-byte 만큼만 (혹은 1-byte만) 읽어온다고 했을 때에도 page cache에서는 4096-byte 단위로 find/get 하게 될까? (당연히 그럴 것 같지만... 확인 필요), 그리고 실제 block layer에 request가 전달될 때에는 어떻게 될까? 1-byte를 요청했어도 1 sector (512-byte)만큼 읽게 될까? 다시 정리하자면, application 수준에서 1-byte를 요청한 경우, page cache layer에서는 4096-byte를 읽어오게 되고, block layer에서는 512-byte를 읽어오게 되는 것일까?\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(201,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# page tiering 시 spatial-locality가 있는 page들끼리 묶어 같이 tiering될 수 있도록 하는 방식. (SSD page cache로 tiering-down 하는 경우 명시, 그리고 SSD page cache에서 tiering-up 하는 경우 명시)\n# 매우 고속이면서, 자원 효율적으로 spatial-locality 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# 매우 고속이면서, 자원 효율적으로 periodicity 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# periodicity 특성과 recency 특성을 함께 고려하여 tier-1 page cache (RAM page cache)와 tier-2 page cache (SSD page cache)에 cache data placement하는 방법\n\n\n\n=== # 발명 요약 ===\n\ncache data에 대한 tier 별 placement가 가능하도록 access 패턴을 기반으로 page class를 분류하고, 이에 따라 tier-1 page cache, tier-2 page cache 간에 page chunk 단위로 page tiering을 수행하는 아키텍쳐 및 방법\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# page cache layer에서 동작하고, tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는 것을 특징으로 하는 RAM-SSD hybrid page cache 아키텍쳐.\n# RAM 기반의 tier-1 page cache의 역할, layout 및 동작\n#* page chunk 단위로 page를 담아 tiering I/O를 수행하는 tiering buffer를 통해 tiering-down 하는 방법/순서\n#* Radix tree에 추가될 정보 및 사용/업데이트 방식\\\n#* 고속이며 자원 효율적인, spatial locality 패턴을 파악하는 방법\n#* spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n#* SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법  \n#* tiering-down 될 data의 양이 늘어가는 추세를 고려하여 page chunk * N 만큼 미리 TRIM하도록 SSD에 요청 - early TRIM\n# SSD 기반의 tier-2 page cache의 역할, layout 및 동작\n#* tiering-down 시 page chunk data를 write하고 meta info를 관리하는 방법: log-structured, write 공간 확보를 위한 page chunk 단위의 victim selection 방식\n#* tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 tiering-up시키고, 해당 page chunk에 대한 status update하는 방법\n# page cache tiering manager의 역할\n#* page에 대한 recency, periodicity를 고려하여 page를 classify 하는 방법\n#** 고속이며 자원 효율적인, N-sieve 기반으로 recency와 periodicity 패턴을 파악하는 방법 (N-period, Short+Long)\n#* 시스템 event에 기반하는 passive-triggering 및 tiering pool size 모니터링에 기반하는 active-triggering 방식으로 tiering을 스케줄링하는 page cache tiering scheduler\n\n\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n* 각 class에 대한 설명:\n** class-1: RAM page cache에 저장될 page들이 속하는 class\n** class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n** class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n* page class 분류 조건:\n** 조건1: \"currently-hot\"\n** 조건 2: \"currently-not-hot\" AND \"once-hot\"\n** 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n# application에서 512-byte 만큼만 (혹은 1-byte만) 읽어온다고 했을 때에도 page cache에서는 4096-byte 단위로 find/get 하게 될까? (당연히 그럴 것 같지만... 확인 필요), 그리고 실제 block layer에 request가 전달될 때에는 어떻게 될까? 1-byte를 요청했어도 1 sector (512-byte)만큼 읽게 될까? 다시 정리하자면, application 수준에서 1-byte를 요청한 경우, page cache layer에서는 4096-byte를 읽어오게 되고, block layer에서는 512-byte를 읽어오게 되는 것일까?\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(202,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# page tiering 시 spatial-locality가 있는 page들끼리 묶어 같이 tiering될 수 있도록 하는 방식. (SSD page cache로 tiering-down 하는 경우 명시, 그리고 SSD page cache에서 tiering-up 하는 경우 명시)\n# 매우 고속이면서, 자원 효율적으로 spatial-locality 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# 매우 고속이면서, 자원 효율적으로 periodicity 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# periodicity 특성과 recency 특성을 함께 고려하여 tier-1 page cache (RAM page cache)와 tier-2 page cache (SSD page cache)에 cache data placement하는 방법\n\n\n\n=== # 발명 요약 ===\n\ncache data에 대한 tier 별 placement가 가능하도록 access 패턴을 기반으로 page class를 분류하고, 이에 따라 tier-1 page cache, tier-2 page cache 간에 page chunk 단위로 page tiering을 수행하는 아키텍쳐 및 방법\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# page cache layer에서 동작하고, tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는 것을 특징으로 하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n# RAM 기반의 tier-1 page cache의 역할, layout 및 동작\n#* page chunk 단위로 page를 담아 tiering I/O를 수행하는 tiering buffer를 통해 tiering-down 하는 방법/순서\n#* Radix tree에 추가될 정보 및 사용/업데이트 방식\\\n#* 고속이며 자원 효율적인, spatial locality 패턴을 파악하는 방법\n#* spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n#* SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법  \n#* tiering-down 될 data의 양이 늘어가는 추세를 고려하여 page chunk * N 만큼 미리 TRIM하도록 SSD에 요청 - early TRIM\n\n# SSD 기반의 tier-2 page cache의 역할, layout 및 동작\n#* tiering-down 시 page chunk data를 write하고 meta info를 관리하는 방법: log-structured, write 공간 확보를 위한 page chunk 단위의 victim selection 방식\n#* tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 tiering-up시키고, 해당 page chunk에 대한 status update하는 방법\n\n# page cache tiering manager의 역할\n#* page에 대한 recency, periodicity를 고려하여 page를 classify 하는 방법\n#** 고속이며 자원 효율적인, N-sieve 기반으로 recency와 periodicity 패턴을 파악하는 방법 (N-period, Short+Long)\n#* 시스템 event에 기반하는 passive-triggering 및 tiering pool size 모니터링에 기반하는 active-triggering 방식으로 tiering을 스케줄링하는 page cache tiering scheduler\n\n\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n* 각 class에 대한 설명:\n** class-1: RAM page cache에 저장될 page들이 속하는 class\n** class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n** class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n* page class 분류 조건:\n** 조건1: \"currently-hot\"\n** 조건 2: \"currently-not-hot\" AND \"once-hot\"\n** 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n# application에서 512-byte 만큼만 (혹은 1-byte만) 읽어온다고 했을 때에도 page cache에서는 4096-byte 단위로 find/get 하게 될까? (당연히 그럴 것 같지만... 확인 필요), 그리고 실제 block layer에 request가 전달될 때에는 어떻게 될까? 1-byte를 요청했어도 1 sector (512-byte)만큼 읽게 될까? 다시 정리하자면, application 수준에서 1-byte를 요청한 경우, page cache layer에서는 4096-byte를 읽어오게 되고, block layer에서는 512-byte를 읽어오게 되는 것일까?\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(203,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# page tiering 시 spatial-locality가 있는 page들끼리 묶어 같이 tiering될 수 있도록 하는 방식. (SSD page cache로 tiering-down 하는 경우 명시, 그리고 SSD page cache에서 tiering-up 하는 경우 명시)\n# 매우 고속이면서, 자원 효율적으로 spatial-locality 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# 매우 고속이면서, 자원 효율적으로 periodicity 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# periodicity 특성과 recency 특성을 함께 고려하여 tier-1 page cache (RAM page cache)와 tier-2 page cache (SSD page cache)에 cache data placement하는 방법\n\n\n\n=== # 발명 요약 ===\n\ncache data에 대한 tier 별 placement가 가능하도록 access 패턴을 기반으로 page class를 분류하고, 이에 따라 tier-1 page cache, tier-2 page cache 간에 page chunk 단위로 page tiering을 수행하는 아키텍쳐 및 방법\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# page cache layer에서 동작하고, tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는 것을 특징으로 하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n# RAM 기반의 tier-1 page cache의 역할, layout 및 동작\n#* page chunk 단위로 page를 담아 tiering I/O를 수행하는 tiering buffer를 통해 tiering-down 하는 방법/순서\n#* Radix tree에 추가될 정보 및 사용/업데이트 방식\\\n#* 고속이며 자원 효율적인, spatial locality 패턴을 파악하는 방법\n#* spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n#* SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n#* tiering-down 될 data의 양이 늘어가는 추세를 고려하여 page chunk * N 만큼 미리 TRIM하도록 SSD에 요청 - early TRIM\n# SSD 기반의 tier-2 page cache의 역할, layout 및 동작\n#* tiering-down 시 page chunk data를 write하고 meta info를 관리하는 방법: log-structured, write 공간 확보를 위한 page chunk 단위의 victim selection 방식\n#* tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 tiering-up시키고, 해당 page chunk에 대한 status update하는 방법\n# page cache tiering manager의 역할\n#* page에 대한 recency, periodicity를 고려하여 page를 classify 하는 방법\n#** 고속이며 자원 효율적인, N-sieve 기반으로 recency와 periodicity 패턴을 파악하는 방법 (N-period, Short+Long)\n#* 시스템 event에 기반하는 passive-triggering 및 tiering pool size 모니터링에 기반하는 active-triggering 방식으로 tiering을 스케줄링하는 page cache tiering scheduler\n\n\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n* 각 class에 대한 설명:\n** class-1: RAM page cache에 저장될 page들이 속하는 class\n** class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n** class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n* page class 분류 조건:\n** 조건1: \"currently-hot\"\n** 조건 2: \"currently-not-hot\" AND \"once-hot\"\n** 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n# application에서 512-byte 만큼만 (혹은 1-byte만) 읽어온다고 했을 때에도 page cache에서는 4096-byte 단위로 find/get 하게 될까? (당연히 그럴 것 같지만... 확인 필요), 그리고 실제 block layer에 request가 전달될 때에는 어떻게 될까? 1-byte를 요청했어도 1 sector (512-byte)만큼 읽게 될까? 다시 정리하자면, application 수준에서 1-byte를 요청한 경우, page cache layer에서는 4096-byte를 읽어오게 되고, block layer에서는 512-byte를 읽어오게 되는 것일까?\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(204,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# page tiering 시 spatial-locality가 있는 page들끼리 묶어 같이 tiering될 수 있도록 하는 방식. (SSD page cache로 tiering-down 하는 경우 명시, 그리고 SSD page cache에서 tiering-up 하는 경우 명시)\n# 매우 고속이면서, 자원 효율적으로 spatial-locality 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# 매우 고속이면서, 자원 효율적으로 periodicity 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# periodicity 특성과 recency 특성을 함께 고려하여 tier-1 page cache (RAM page cache)와 tier-2 page cache (SSD page cache)에 cache data placement하는 방법\n\n\n\n=== # 발명 요약 ===\n\ncache data에 대한 tier 별 placement가 가능하도록 access 패턴을 기반으로 page class를 분류하고, 이에 따라 tier-1 page cache, tier-2 page cache 간에 page chunk 단위로 page tiering을 수행하는 아키텍쳐 및 방법\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# page cache layer에서 동작하고, tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는 것을 특징으로 하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n# RAM 기반의 tier-1 page cache의 역할, layout 및 동작\n#* page chunk 단위로 page를 담아 tiering I/O를 수행하는 tiering buffer를 통해 tiering-down 하는 방법/순서\n#* Radix tree에 추가될 정보 및 사용/업데이트 방식\\\n#* 고속이며 자원 효율적인, spatial locality 패턴을 파악하는 방법\n#* spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n#* SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n#* tiering-down 될 data의 양이 늘어가는 추세를 고려하여 page chunk * N 만큼 미리 TRIM하도록 SSD에 요청 - early TRIM\n#----\n# SSD 기반의 tier-2 page cache의 역할, layout 및 동작\n#* tiering-down 시 page chunk data를 write하고 meta info를 관리하는 방법: log-structured, write 공간 확보를 위한 page chunk 단위의 victim selection 방식\n#* tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 tiering-up시키고, 해당 page chunk에 대한 status update하는 방법\n# page cache tiering manager의 역할\n#* page에 대한 recency, periodicity를 고려하여 page를 classify 하는 방법\n#** 고속이며 자원 효율적인, N-sieve 기반으로 recency와 periodicity 패턴을 파악하는 방법 (N-period, Short+Long)\n#* 시스템 event에 기반하는 passive-triggering 및 tiering pool size 모니터링에 기반하는 active-triggering 방식으로 tiering을 스케줄링하는 page cache tiering scheduler\n\n\n\n# cache-optimal SSD I/O mechanism\n\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n* 각 class에 대한 설명:\n** class-1: RAM page cache에 저장될 page들이 속하는 class\n** class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n** class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n* page class 분류 조건:\n** 조건1: \"currently-hot\"\n** 조건 2: \"currently-not-hot\" AND \"once-hot\"\n** 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n# application에서 512-byte 만큼만 (혹은 1-byte만) 읽어온다고 했을 때에도 page cache에서는 4096-byte 단위로 find/get 하게 될까? (당연히 그럴 것 같지만... 확인 필요), 그리고 실제 block layer에 request가 전달될 때에는 어떻게 될까? 1-byte를 요청했어도 1 sector (512-byte)만큼 읽게 될까? 다시 정리하자면, application 수준에서 1-byte를 요청한 경우, page cache layer에서는 4096-byte를 읽어오게 되고, block layer에서는 512-byte를 읽어오게 되는 것일까?\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(205,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# page tiering 시 spatial-locality가 있는 page들끼리 묶어 같이 tiering될 수 있도록 하는 방식. (SSD page cache로 tiering-down 하는 경우 명시, 그리고 SSD page cache에서 tiering-up 하는 경우 명시)\n# 매우 고속이면서, 자원 효율적으로 spatial-locality 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# 매우 고속이면서, 자원 효율적으로 periodicity 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# periodicity 특성과 recency 특성을 함께 고려하여 tier-1 page cache (RAM page cache)와 tier-2 page cache (SSD page cache)에 cache data placement하는 방법\n\n\n\n=== # 발명 요약 ===\n\ncache data에 대한 tier 별 placement가 가능하도록 access 패턴을 기반으로 page class를 분류하고, 이에 따라 tier-1 page cache, tier-2 page cache 간에 page chunk 단위로 page tiering을 수행하는 아키텍쳐 및 방법\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# page cache layer에서 동작하고, tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는 것을 특징으로 하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n# RAM 기반의 tier-1 page cache의 역할, layout 및 동작\n#* page chunk 단위로 page를 담아 tiering I/O를 수행하는 tiering buffer를 통해 tiering-down 하는 방법/순서\n#* Radix tree에 추가될 정보 및 사용/업데이트 방식\\\n#* 고속이며 자원 효율적인, spatial locality 패턴을 파악하는 방법\n#* spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n#* SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n#* tiering-down 될 data의 양이 늘어가는 추세를 고려하여 page chunk * N 만큼 미리 TRIM하도록 SSD에 요청 - early TRIM\n<!-- -->\n# SSD 기반의 tier-2 page cache의 역할, layout 및 동작\n#* tiering-down 시 page chunk data를 write하고 meta info를 관리하는 방법: log-structured, write 공간 확보를 위한 page chunk 단위의 victim selection 방식\n#* tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 tiering-up시키고, 해당 page chunk에 대한 status update하는 방법\n# page cache tiering manager의 역할\n#* page에 대한 recency, periodicity를 고려하여 page를 classify 하는 방법\n#** 고속이며 자원 효율적인, N-sieve 기반으로 recency와 periodicity 패턴을 파악하는 방법 (N-period, Short+Long)\n#* 시스템 event에 기반하는 passive-triggering 및 tiering pool size 모니터링에 기반하는 active-triggering 방식으로 tiering을 스케줄링하는 page cache tiering scheduler\n\n\n\n# cache-optimal SSD I/O mechanism\n\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n* 각 class에 대한 설명:\n** class-1: RAM page cache에 저장될 page들이 속하는 class\n** class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n** class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n* page class 분류 조건:\n** 조건1: \"currently-hot\"\n** 조건 2: \"currently-not-hot\" AND \"once-hot\"\n** 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n# application에서 512-byte 만큼만 (혹은 1-byte만) 읽어온다고 했을 때에도 page cache에서는 4096-byte 단위로 find/get 하게 될까? (당연히 그럴 것 같지만... 확인 필요), 그리고 실제 block layer에 request가 전달될 때에는 어떻게 될까? 1-byte를 요청했어도 1 sector (512-byte)만큼 읽게 될까? 다시 정리하자면, application 수준에서 1-byte를 요청한 경우, page cache layer에서는 4096-byte를 읽어오게 되고, block layer에서는 512-byte를 읽어오게 되는 것일까?\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(206,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# page tiering 시 spatial-locality가 있는 page들끼리 묶어 같이 tiering될 수 있도록 하는 방식. (SSD page cache로 tiering-down 하는 경우 명시, 그리고 SSD page cache에서 tiering-up 하는 경우 명시)\n# 매우 고속이면서, 자원 효율적으로 spatial-locality 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# 매우 고속이면서, 자원 효율적으로 periodicity 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# periodicity 특성과 recency 특성을 함께 고려하여 tier-1 page cache (RAM page cache)와 tier-2 page cache (SSD page cache)에 cache data placement하는 방법\n\n\n\n=== # 발명 요약 ===\n\ncache data에 대한 tier 별 placement가 가능하도록 access 패턴을 기반으로 page class를 분류하고, 이에 따라 tier-1 page cache, tier-2 page cache 간에 page chunk 단위로 page tiering을 수행하는 아키텍쳐 및 방법\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# page cache layer에서 동작하고, tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는 것을 특징으로 하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n# RAM 기반의 tier-1 page cache의 역할, layout 및 동작\n#* page chunk 단위로 page를 담아 tiering I/O를 수행하는 tiering buffer를 통해 tiering-down 하는 방법/순서\n#* Radix tree에 추가될 정보 및 사용/업데이트 방식\n#* 고속이며 자원 효율적인, spatial locality 패턴을 파악하는 방법\n#* spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n#* SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n#* tiering-down 될 data의 양이 늘어가는 추세를 고려하여 page chunk * N 만큼 미리 TRIM하도록 SSD에 요청 - early TRIM\n<!-- -->\n# SSD 기반의 tier-2 page cache의 역할, layout 및 동작\n#* tiering-down 시 page chunk data를 write하고 meta info를 관리하는 방법: log-structured, write 공간 확보를 위한 page chunk 단위의 victim selection 방식\n#* tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 tiering-up시키고, 해당 page chunk에 대한 status update하는 방법\n<!-- -->\n# page cache tiering manager의 역할\n#* page에 대한 recency, periodicity를 고려하여 page를 classify 하는 방법\n#** 고속이며 자원 효율적인, N-sieve 기반으로 recency와 periodicity 패턴을 파악하는 방법 (N-period, Short+Long)\n#* 시스템 event에 기반하는 passive-triggering 및 tiering pool size 모니터링에 기반하는 active-triggering 방식으로 tiering을 스케줄링하는 page cache tiering scheduler\n\n\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n* 각 class에 대한 설명:\n** class-1: RAM page cache에 저장될 page들이 속하는 class\n** class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n** class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n* page class 분류 조건:\n** 조건1: \"currently-hot\"\n** 조건 2: \"currently-not-hot\" AND \"once-hot\"\n** 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n# application에서 512-byte 만큼만 (혹은 1-byte만) 읽어온다고 했을 때에도 page cache에서는 4096-byte 단위로 find/get 하게 될까? (당연히 그럴 것 같지만... 확인 필요), 그리고 실제 block layer에 request가 전달될 때에는 어떻게 될까? 1-byte를 요청했어도 1 sector (512-byte)만큼 읽게 될까? 다시 정리하자면, application 수준에서 1-byte를 요청한 경우, page cache layer에서는 4096-byte를 읽어오게 되고, block layer에서는 512-byte를 읽어오게 되는 것일까?\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(207,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# page tiering 시 spatial-locality가 있는 page들끼리 묶어 같이 tiering될 수 있도록 하는 방식. (SSD page cache로 tiering-down 하는 경우 명시, 그리고 SSD page cache에서 tiering-up 하는 경우 명시)\n# 매우 고속이면서, 자원 효율적으로 spatial-locality 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# 매우 고속이면서, 자원 효율적으로 periodicity 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# periodicity 특성과 recency 특성을 함께 고려하여 tier-1 page cache (RAM page cache)와 tier-2 page cache (SSD page cache)에 cache data placement하는 방법\n\n\n\n=== # 발명 요약 ===\n\ncache data에 대한 tier 별 placement가 가능하도록 access 패턴을 기반으로 page class를 분류하고, 이에 따라 tier-1 page cache, tier-2 page cache 간에 page chunk 단위로 page tiering을 수행하는 아키텍쳐 및 방법\n\n<br/>\n\n=== # 대표 청구항 ===\n\n: page cache layer에서 동작하고, tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는 것을 특징으로 하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n# RAM 기반의 tier-1 page cache의 역할, layout 및 동작\n#* page chunk 단위로 page를 담아 tiering I/O를 수행하는 tiering buffer를 통해 tiering-down 하는 방법/순서\n#* Radix tree에 추가될 정보 및 사용/업데이트 방식\n#* 고속이며 자원 효율적인, spatial locality 패턴을 파악하는 방법\n#* spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n#* SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n#* tiering-down 될 data의 양이 늘어가는 추세를 고려하여 page chunk * N 만큼 미리 TRIM하도록 SSD에 요청 - early TRIM\n<!-- -->\n# SSD 기반의 tier-2 page cache의 역할, layout 및 동작\n#* tiering-down 시 page chunk data를 write하고 meta info를 관리하는 방법: log-structured, write 공간 확보를 위한 page chunk 단위의 victim selection 방식\n#* tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 tiering-up시키고, 해당 page chunk에 대한 status update하는 방법\n<!-- -->\n# page cache tiering manager의 역할\n#* page에 대한 recency, periodicity를 고려하여 page를 classify 하는 방법\n#** 고속이며 자원 효율적인, N-sieve 기반으로 recency와 periodicity 패턴을 파악하는 방법 (N-period, Short+Long)\n#* 시스템 event에 기반하는 passive-triggering 및 tiering pool size 모니터링에 기반하는 active-triggering 방식으로 tiering을 스케줄링하는 page cache tiering scheduler\n\n\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n* 각 class에 대한 설명:\n** class-1: RAM page cache에 저장될 page들이 속하는 class\n** class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n** class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n* page class 분류 조건:\n** 조건1: \"currently-hot\"\n** 조건 2: \"currently-not-hot\" AND \"once-hot\"\n** 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n# application에서 512-byte 만큼만 (혹은 1-byte만) 읽어온다고 했을 때에도 page cache에서는 4096-byte 단위로 find/get 하게 될까? (당연히 그럴 것 같지만... 확인 필요), 그리고 실제 block layer에 request가 전달될 때에는 어떻게 될까? 1-byte를 요청했어도 1 sector (512-byte)만큼 읽게 될까? 다시 정리하자면, application 수준에서 1-byte를 요청한 경우, page cache layer에서는 4096-byte를 읽어오게 되고, block layer에서는 512-byte를 읽어오게 되는 것일까?\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(208,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# page tiering 시 spatial-locality가 있는 page들끼리 묶어 같이 tiering될 수 있도록 하는 방식. (SSD page cache로 tiering-down 하는 경우 명시, 그리고 SSD page cache에서 tiering-up 하는 경우 명시)\n# 매우 고속이면서, 자원 효율적으로 spatial-locality 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# 매우 고속이면서, 자원 효율적으로 periodicity 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# periodicity 특성과 recency 특성을 함께 고려하여 tier-1 page cache (RAM page cache)와 tier-2 page cache (SSD page cache)에 cache data placement하는 방법\n\n\n\n=== # 발명 요약 ===\n\ncache data에 대한 tier 별 placement가 가능하도록 access 패턴을 기반으로 page class를 분류하고, 이에 따라 tier-1 page cache, tier-2 page cache 간에 page chunk 단위로 page tiering을 수행하는 아키텍쳐 및 방법\n\n<br/>\n\n=== # 대표 청구항 ===\n\n; 한 줄 요약\n: page cache layer에서 동작하고, tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는 것을 특징으로 하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n# RAM 기반의 tier-1 page cache의 역할, layout 및 동작\n#* page chunk 단위로 page를 담아 tiering I/O를 수행하는 tiering buffer를 통해 tiering-down 하는 방법/순서\n#* Radix tree에 추가될 정보 및 사용/업데이트 방식\n#* 고속이며 자원 효율적인, spatial locality 패턴을 파악하는 방법\n#* spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n#* SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n#* tiering-down 될 data의 양이 늘어가는 추세를 고려하여 page chunk * N 만큼 미리 TRIM하도록 SSD에 요청 - early TRIM\n<!-- -->\n# SSD 기반의 tier-2 page cache의 역할, layout 및 동작\n#* tiering-down 시 page chunk data를 write하고 meta info를 관리하는 방법: log-structured, write 공간 확보를 위한 page chunk 단위의 victim selection 방식\n#* tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 tiering-up시키고, 해당 page chunk에 대한 status update하는 방법\n<!-- -->\n# page cache tiering manager의 역할\n#* page에 대한 recency, periodicity를 고려하여 page를 classify 하는 방법\n#** 고속이며 자원 효율적인, N-sieve 기반으로 recency와 periodicity 패턴을 파악하는 방법 (N-period, Short+Long)\n#* 시스템 event에 기반하는 passive-triggering 및 tiering pool size 모니터링에 기반하는 active-triggering 방식으로 tiering을 스케줄링하는 page cache tiering scheduler\n\n\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n* 각 class에 대한 설명:\n** class-1: RAM page cache에 저장될 page들이 속하는 class\n** class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n** class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n* page class 분류 조건:\n** 조건1: \"currently-hot\"\n** 조건 2: \"currently-not-hot\" AND \"once-hot\"\n** 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n# application에서 512-byte 만큼만 (혹은 1-byte만) 읽어온다고 했을 때에도 page cache에서는 4096-byte 단위로 find/get 하게 될까? (당연히 그럴 것 같지만... 확인 필요), 그리고 실제 block layer에 request가 전달될 때에는 어떻게 될까? 1-byte를 요청했어도 1 sector (512-byte)만큼 읽게 될까? 다시 정리하자면, application 수준에서 1-byte를 요청한 경우, page cache layer에서는 4096-byte를 읽어오게 되고, block layer에서는 512-byte를 읽어오게 되는 것일까?\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(209,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# page tiering 시 spatial-locality가 있는 page들끼리 묶어 같이 tiering될 수 있도록 하는 방식. (SSD page cache로 tiering-down 하는 경우 명시, 그리고 SSD page cache에서 tiering-up 하는 경우 명시)\n# 매우 고속이면서, 자원 효율적으로 spatial-locality 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# 매우 고속이면서, 자원 효율적으로 periodicity 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# periodicity 특성과 recency 특성을 함께 고려하여 tier-1 page cache (RAM page cache)와 tier-2 page cache (SSD page cache)에 cache data placement하는 방법\n\n\n\n=== # 발명 요약 ===\n\ncache data에 대한 tier 별 placement가 가능하도록 access 패턴을 기반으로 page class를 분류하고, 이에 따라 tier-1 page cache, tier-2 page cache 간에 page chunk 단위로 page tiering을 수행하는 아키텍쳐 및 방법\n\n<br/>\n\n=== # 대표 청구항 ===\n\n* page cache layer에서 동작하고, tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는 것을 특징으로 하는 RAM-SSD hybrid page cache 아키텍쳐.\n\n# RAM 기반의 tier-1 page cache의 역할, layout 및 동작\n#* page chunk 단위로 page를 담아 tiering I/O를 수행하는 tiering buffer를 통해 tiering-down 하는 방법/순서\n#* Radix tree에 추가될 정보 및 사용/업데이트 방식\n#* 고속이며 자원 효율적인, spatial locality 패턴을 파악하는 방법\n#* spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n#* SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n#* tiering-down 될 data의 양이 늘어가는 추세를 고려하여 page chunk * N 만큼 미리 TRIM하도록 SSD에 요청 - early TRIM\n<!-- -->\n# SSD 기반의 tier-2 page cache의 역할, layout 및 동작\n#* tiering-down 시 page chunk data를 write하고 meta info를 관리하는 방법: log-structured, write 공간 확보를 위한 page chunk 단위의 victim selection 방식\n#* tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 tiering-up시키고, 해당 page chunk에 대한 status update하는 방법\n<!-- -->\n# page cache tiering manager의 역할\n#* page에 대한 recency, periodicity를 고려하여 page를 classify 하는 방법\n#** 고속이며 자원 효율적인, N-sieve 기반으로 recency와 periodicity 패턴을 파악하는 방법 (N-period, Short+Long)\n#* 시스템 event에 기반하는 passive-triggering 및 tiering pool size 모니터링에 기반하는 active-triggering 방식으로 tiering을 스케줄링하는 page cache tiering scheduler\n\n\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n* 각 class에 대한 설명:\n** class-1: RAM page cache에 저장될 page들이 속하는 class\n** class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n** class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n* page class 분류 조건:\n** 조건1: \"currently-hot\"\n** 조건 2: \"currently-not-hot\" AND \"once-hot\"\n** 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n# application에서 512-byte 만큼만 (혹은 1-byte만) 읽어온다고 했을 때에도 page cache에서는 4096-byte 단위로 find/get 하게 될까? (당연히 그럴 것 같지만... 확인 필요), 그리고 실제 block layer에 request가 전달될 때에는 어떻게 될까? 1-byte를 요청했어도 1 sector (512-byte)만큼 읽게 될까? 다시 정리하자면, application 수준에서 1-byte를 요청한 경우, page cache layer에서는 4096-byte를 읽어오게 되고, block layer에서는 512-byte를 읽어오게 되는 것일까?\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8'),(210,'== PATENT-BRIAN-2013-001 ==\n\nRAM-SSD Hybrid Page Cache\n:: Page Cache Layer에서 동작하는 Page tiering 기반의 RAM-SSD Hybrid Cache 기술\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n=== # 본 발명의 특징 / 효과 ===\n\n# page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n# page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O 효과의 장점을 얻을 수 있음)\n# multi-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification: N개의 sieve를 이용하여 각 page에 대한 recency, periodicity, frequency 특성을 관찰. 이를 기반으로 page의 hotness를 구분하고 re-hit potential을 잡아낼 수 있음. 이렇게 얻어진 정보들은 page tiering을 위한 page classification 시에 사용되어 RAM을 primary page cache buffer 및 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이를 통해 RAM과 SSD 미디어의 특성에 최적화된 방식으로 page caching이 수행될 수 있음.\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid page cache 아키텍쳐.\n# N-sieve 방식의 page hit 패턴 분석 및 이를 기반으로 하는 page classification.\n# page tiering 메커니즘 기반의 RAM-SSD간 cache data 이동 방식.\n# page tiering 시 spatial-locality가 있는 page들끼리 묶어 같이 tiering될 수 있도록 하는 방식. (SSD page cache로 tiering-down 하는 경우 명시, 그리고 SSD page cache에서 tiering-up 하는 경우 명시)\n# 매우 고속이면서, 자원 효율적으로 spatial-locality 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# 매우 고속이면서, 자원 효율적으로 periodicity 패턴을 찾아낼 수 있는 알고리즘. (N-sieve 기반)\n# periodicity 특성과 recency 특성을 함께 고려하여 tier-1 page cache (RAM page cache)와 tier-2 page cache (SSD page cache)에 cache data placement하는 방법\n\n\n\n=== # 발명 요약 ===\n\ncache data에 대한 tier 별 placement가 가능하도록 access 패턴을 기반으로 page class를 분류하고, 이에 따라 tier-1 page cache, tier-2 page cache 간에 page chunk 단위로 page tiering을 수행하는 아키텍쳐 및 방법\n\n<br/>\n\n=== # 대표 청구항 ===\n\n# page cache layer에서 동작하고, tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는 것을 특징으로 하는 RAM-SSD hybrid page cache 아키텍쳐.\n<!-- -->\n# RAM 기반의 tier-1 page cache의 역할, layout 및 동작\n#* page chunk 단위로 page를 담아 tiering I/O를 수행하는 tiering buffer를 통해 tiering-down 하는 방법/순서\n#* Radix tree에 추가될 정보 및 사용/업데이트 방식\n#* 고속이며 자원 효율적인, spatial locality 패턴을 파악하는 방법\n#* spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n#* SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n#* tiering-down 될 data의 양이 늘어가는 추세를 고려하여 page chunk * N 만큼 미리 TRIM하도록 SSD에 요청 - early TRIM\n<!-- -->\n# SSD 기반의 tier-2 page cache의 역할, layout 및 동작\n#* tiering-down 시 page chunk data를 write하고 meta info를 관리하는 방법: log-structured, write 공간 확보를 위한 page chunk 단위의 victim selection 방식\n#* tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 tiering-up시키고, 해당 page chunk에 대한 status update하는 방법\n<!-- -->\n# page cache tiering manager의 역할\n#* page에 대한 recency, periodicity를 고려하여 page를 classify 하는 방법\n#** 고속이며 자원 효율적인, N-sieve 기반으로 recency와 periodicity 패턴을 파악하는 방법 (N-period, Short+Long)\n#* 시스템 event에 기반하는 passive-triggering 및 tiering pool size 모니터링에 기반하는 active-triggering 방식으로 tiering을 스케줄링하는 page cache tiering scheduler\n\n\n\n<br/>\n\n=== # 대표 도면 ===\n\n<br/>\n\n=== # 선행 기술 ===\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n다음 검색식\n ()\n으로 __건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 recency 및 periodicity 를 기반으로 page classification을 수행하는 cache algorithm에 대한 선행 기술은 발견하지 못하였음.\n\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n=== # 침해 적발 ===\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n<br/>\n\n=== # 기술 상세 ===\n\nRAM과 SSD 기반의 hybrid page cache의 아키텍쳐는 다음과 같은 구성 요소를 포함한다.\n\n* RAM page cache\n* SSD page cache\n* page cache tiering manager\n** page classification module\n** tiering I/O module\n\nRAM page cache와 SSD page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page tiering up/down 작업을 수행하는 역할을 한다. page access 패턴을 분석하는 알고리즘에 따라서 page class가 결정되고 (page classification module), 이를 참고하여 RAM page cache tier와 SSD page cache tier 간에 page tiering up/down이 수행된다 (tiering I/O module). \n\n본 발명에서는 RAM-SSD hybrid page cache의 구현에 필요한 page access classification 알고리즘 및 RAM-SSD hybrid page cache 아키텍쳐에 대해 다룬다.\n\npage hit pattern을 분석하는 page classification 알고리즘에 의해 page를 몇 가지 class로 분류하는 역할을 수행하며, 이 page class에 따라 RAM page cache와 SSD page cache 중 저장될 곳이 구분됨.\n\nRAM page cache와 SSD page cache는 \n\n\n==== RAM page cache ====\n\n- RAM에 존재하는 page cache로서, 디스크에서 access된 data를 우선적으로 cache하는 역할을 수행함. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 정책으로 운영됨.\n\n- 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 이 page 상태 정보 update 모듈에 추가된다. 이 page class 관리 모듈은 이후 섹션에서 다룬다.\n\n<br/>\n\n==== SSD page cache ====\n\n- page cache media로 사용될 SSD 공간은 meta info 영역과 page data 영역으로 나뉘어 사용될 수 있음. meta info 영역에는 sub-page chunk lookup table per SSD 정보 및 in-page-chunk page table 정보를 포함함. page data 영역은 tiering된 page cache data가 저장되는 공간이며, page data write 시에는 page 크기의 정수 배에 해당되는 page chunk 단위로 log-structured 방식으로 write되고, page data read 시에는 page 단위로 read됨.\n\n- page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n- page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n- SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임. \n\n- erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 old-chunk-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있는 page chunk부터 erase 시키는 loaded-chunk-first 방법을 사용할 수 있음.\n\n- loaded-chunk-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n- erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있음.\n\n- hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n\n\n\n\nSSD page cache로부터 bulk page find/get I/O가 발생하는 경우에 필요할 수 있는 추가적인 SSD page cache I/O optimization은 별도의 발명에서 다룬다.\n\n- 따라서 page cache media에서 발생하는 I/O의 패턴은 log-structured 방식으로 sequential하게 write되는 page chunk write I/O 패턴과, chunk 단위로 random하게 read되는 page chunk read I/O 패턴, page chunk location lookup을 ....\n\n- 이렇게 함으로써 SSD 내에 존재하는 병렬성을 극대화 할 수 있게 되며, page cache tiering 시 최적의 I/O 성능을 이끌어낼 수 있음. SSD를 일반적인 block 디바이스 스토리지로 사용하는 것이 아니기 때문에 page data 영역에서는 random block addressing을 할 필요가 없으며, 이것이 page data 영역에서 발생하는 I/O에 대해서 page mapping을 하지 않아도 되는 이유임.\n\n- 물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함.\n\n<!--\n\n# further study/patents\n\n- 복수 개의 SSD를 second-tier page cache media로 사용하는 경우 page tiering I/O는 어떻게 처리되는 것이 바람직한가? 복수 개의 SSD들의 내부 병렬화 구조가 동일한 경우와 서로 다른 경우에는 page tiering buffer의 크기를 어떻게 잡는 것이 최적인가? SSD array에 대한 I/O coordination을 담당하는 SAVL과 같은 layer가 있다고 했을 때, hybrid page cache에서 이를 어떻게 활용할 수 있을까?\n\n- 연달아서 여러 개의 page chunk에 대한 read 혹은 write가 발생하게 되는 경우에... 어떤 식의 최적화된 page cache tiering I/O 방식을 쓸 수 있을까?\n\n- meta info 영역과 page data 영역에 대한 access가 동시에 발생할 수 있는가? 아니면 항상 meta info 영역을 거쳐서 page chunk lookup table per SSD을 read하거나 write한 후에 page data 영역의 page chunk data를 처리하는가? 혹은 meta info 영역과 page data 영역에 대한 access가 동시에 발생하지 않는다 하더라도, meta info 영역에 대한 update (SLC 경우 1500us, MLC의 경우 3300us 정도의 시간 필요)가 일어난 후에 page chunk data를 write하거나 read 해야 한다면 meta 영역과 page data 영역을 way 차원에서 구분하는 것이 필요할 수 있음.\n\n-->\n\n<br/>\n\n==== tiering I/O module ====\n\n- page classification 알고리즘에 의해 SSD page cache로 tiering될 것으로 선택된 page들이 (class-2) 존재하는 RAM 상의 공간이 tiering I/O module이며, tiering buffer 단위로 처리됨.\n\n- tiering tiering buffer의 크기는 cache media로 사용될 SSD의 사양과 page cache tiering 방식을 고려한 최적의 크기로 결정될 수 있음. \n\n- tiering buffer의 크기는 관리자에 의해서 명시적으로 지정될 수도 있지만, page cache media로 사용될 SSD에게 관련 정보를 요청하여 전달받은 값을 이용할 수도 있음. 다음은 한 실시예임. SSD 내의 I/O 병렬성을 극대화하여 최고의 성능을 낼 수 있도록 SSD 내 channel의 수와 erase block의 크기를 곱한 값을 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 라고 부른다고 했을 때, 이 값의 1배 혹은 그 이상의 정수배가 되는 값을 tiering buffer의 크기로 결정할 수 있음.\n\n- tiering I/O module 내에 tiering buffer로 나누어 떨어지지 않는 크기의 page들이 들어가 있는 경우에는 시스템 policy에 따라서 다음 두 가지 방식 (가, 나) 중 하나를 선택적으로 적용 가능. tiering buffer의 크기가 N_tb, tiering I/O module 내에 들어 있는 data의 크기가 N_tpp, 그리고 N_tpp를 N_tb로 나눈 나머지 값이 N_res (0 <= N_res < N_tb)라고 가정하자. N_tb 크기로 tiering buffer를 꽉 채워서 (N_tpp - N_res) / N_tb 횟수 만큼 tiering I/O 동작이 수행된 후, (가) tiering I/O module 내에 남아 있는 N_res 만큼의 data는 이번에 처리하지 않고, 추가로 data가 N_tb 크기 이상 쌓였을 때 처리, 혹은 (나) N_res 만큼의 data에 필요한 만큼 zero padding을 하여 N_tb 크기로 만든 후에 tiering buffer에 담아 처리.\n\n- tiering buffer가 tiering I/O module 내에 있는 page들을 처리하는 순서는 특별히 제한될 필요는 없으며, 한 실시예로서 먼저 쌓인 page들부터 먼저 처리하는 방식으로 구현될 수 있음.\n\n- tiering I/O module 내에 page 들을 쌓아두다가 본 발명이 명시하는 적절한 시점에 tiering buffer에 의해 RAM과 SSD 간의 tiering I/O가 이루어짐으로써 asynchronous 방식으로 tiering I/O가 이루어지게 됨. 이로 인한 효과는 asynchronous I/O가 synchronous I/O 대비 가지는 장점에 해당됨.\n\n- RAM에 있는 tiering I/O module에서 SSD로 page tiering이 일어나는 시점은 on-shrink-event triggering 방식으로 결정이 되거나, on-buffer-full-event triggering 방식으로 결정될 수 있다.\n\n- 다음은 on-shrink-event triggering에 대한 설명임. 시스템에서 주기적으로, 혹은 상황에 따라 필요에 의해 수행되는 page cache shrink 작업 시, LRU (least recently used) 알고리즘에 근거하여 evict될 page들을 찾게 된다. 이때, tiering I/O module내의 page들은 그냥 discard하는 것이 아니라 SSD로의 page tiering 작업을 수행한다. 중요한 것은 순서로서, page tiering 작업이 먼저 일어나기 전에 page cache shrink 작업이 시작되지 않도록 한다. 이를 위해서 tiering buffer pool에 있는 page들의 상태 flag에 unevictable bit을 설정하는 방식을 사용하거나, page cache shrink에 의해 page들이 evict되는 작업이 시작되기 전에 page tiering 작업이 먼저 완료될 수 있도록 실행 순서를 제어하는 방법을 사용할 수 있다.\n\n- 다음은 on-buffer-full-event triggering에 대한 설명임. 전술한 on-event triggering 방식이 다소 피동적임에 비해, 이 방식은 상대적으로 능동적인 방식이라고 볼 수 있음. tiering I/O module에 tiering buffer의 크기 만큼, 혹은 그의 정수 배에 해당하는 충분한 크기가 되었을 때, SSD로의 page tiering 작업을 수행한다.\n\n- page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다. \n\n\n\n\n\n\n\n- SSD가 SSD page cache media로 신규 사용될 때에는 tiering buffer 크기가 결정되어야 함. 한 예로서, OS 설치 시 hybrid page cache를 사용하겠다고 선택한 경우에 SSD page cache media로 사용할 하나 혹은 복수 개의 SSD를 선택하는 시점이 될 수 있음. 또 다른 한 예로서, hybrid page cache가 동작하고 있지 않던 시스템에 새로 SSD를 장착한 후 이 SSD를 SSD page cache media로 사용하려는 경우가 될 수 있음. 또 다른 한 예로서, 기존에 이미 RAM과 SSD 기반의 hybrid page cache를 사용하고 있었는데 SSD를 추가로 장착하여 SSD page cache media의 크기를 증가시키려고 하는 경우가 될 수 있음.\n\n- 한 개의 물리적인 SSD가 아니라 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE 개의 물리적인 SSD가 hybrid page cache media로 사용되는 경우에는 복수 개의 SSD를 직접적으로 이용하는 방법과 복수 개의 SSD가 하나의 가상화된 SSD로 보이게 하는 layer의 도움을 받아 이용하는 방법이 있을 수 있음. 복수 개의 SSD를 직접적으로 이용하기 위해서는 각 SSD마다 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE를 \n\n\nhardware 및 가상화된 SSD tiering buffer 크기로 사용할 최적의 I/O 크기를 결정하기 위해서 각 \n\n- 우선 SSD array에 대한 I/O를 최적으로 관리해주는 SW가 별도로 존재하지 않는 경우에는 (no-SAIC case) hybrid page cache 모듈이 tiering buffer 크기를 직접 결정하게 됨. 그러나 SSD array에 대한 I/O를 최적으로 관리해주는 모듈 (편의상 SSD Array I/O Coordination (SAIC) 모듈이라 부르겠음)이 시스템에 존재하는 경우에는 (SAIC case) page cache tiering을 위한 최적의 tiering buffer의 크기를 SAIC 모듈에게 문의하여 결정 가능.\n\n- no-SAIC case에서는 상황에 따라 다음과 같이 tiering buffer 크기를 계산할 수 있음. 만약 SSD page cache로 사용될 SSD들의 OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 가 모두 동일하고, SSD page cache media로 사용할 SSD가 낼 수 있는 bandwidth의 총합이 hybrid page cache SSD들에게 허용된 시스템 bus의 bandwidth를 넘지 않는 수준이라면, tiering buffer의 크기는 NUMBER_OF_SSDS_FOR_HYBRID_PAGE_CACHE * OPTIMAL_PAGE_CACHE_TIERING_IO_SIZE 값으로 결정되거나, 몇 번의 test를 거쳐서 최적의\n\n\n<br/>\n\n==== page cache tiering manager ====\n\n===== access pattern-based page classification =====\n\n- page에 대한 access 패턴을 모니터링하여 page class를 classify하고 update하는 역할을 수행함. page cache에 대한 tiering은 이렇게 분류된 page class 정보를 기반으로 이루어짐.\n\n* 각 class에 대한 설명:\n** class-1: RAM page cache에 저장될 page들이 속하는 class\n** class-2: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 discard하지 않고 SSD page cache에 저장될 page들이 속하는 class\n** class-3: kernel의 기존 page replacement 메커니즘에 의해서 evict될 때 SSD page cache에 저장되지 않고 discard될 page들이 속하는 class\n\n* page class 분류 조건:\n** 조건1: \"currently-hot\"\n** 조건 2: \"currently-not-hot\" AND \"once-hot\"\n** 조건 3: \"currently-not-hot\" AND \"periodic\"\n조건 1에 해당하는 page들은 class-1으로 분류되고, 조건 2 혹은 조건 3에 해당하는 page들은 class-2로 분류됨. 그리고 위의 조건들 중 어느 하나에도 해당하지 못했던 page들은 자동적으로 class-3으로 분류됨. page cache에 첫 추가되는 page들은 기본적으로 class-3에 속하게 되며, access 패턴 모니터링에 따라 class-2 혹은 class-1으로, 혹은 다시 class-3으로 상태 update가 이루어짐.\n\n- recency, frequency, repeatability를 측정하는 multi-window 기반으로 page class 분류를 위한 access 패턴 모니터링 및 분석이 이루어짐.\n\n- multi-window 방식에는 두 최근의 짧은 기간 동안의 access 패턴을 관찰하는 window-1과 긴 기간동안 access 패턴을 관찰하는 window-2을 함께 이용하는 Two-sieve 모델이 있음.\n* 여기서 small-size window와 large-size window의 크기를 어떻게 결정해야 하는가?\n\n- 짧은 기간동안 (즉, 짧은 거리의 과거 시점으로부터 현재까지의 - 이 기간을 Monitoring Period 1이라 칭한다)  access 패턴을 관찰하는 window 1을 통해서 분류해내고자 하는 data는 “currently-hot” data임. 즉 짧은 기간 관측했음에도 불구하고 hit 수가 여러 번 count되고 있는 data가 있다면, 그 data는 “currently-hot”한 것으로 간주해도 큰 무리가 없음. 이러한 data들은 당장 hot하게 access되고 는 data이므로 RAM에 cache해둠으로써 성능 이득을 얻을 수 있다. 이러한 data들은 class-1으로 분류됨.\n\n- 한편, 짧은 최근 기간동안 관찰했던 window 1에서 “currently-hot”한 것으로 보이지는 않았지만 약간만 길게 관찰했다면 의미있는 access 빈도를 보여주었을 수 있었던 data들도 있을 수 있다. 이를 window 1에 비해 긴 기간동안 관찰하는 window 2를 통해 파악할 수 있다. 예를 들어, 한 차례 access될 때 매우 hot한 정도는 아니더라도 일정 간격을 두고 꾸준하게 access되고 있는 data들이 있을 수도 있다. 이런 data들이 여유 RAM 공간이 부족한 상황에서 당장 “currently-hot” 하지 않다고 해서 그냥 버려지게 된다면, 다시 주기적으로 access될 때마다 그 만큼 HDD read로 인한 성능 저하를 겪게 될 수 있다. 따라서 이러한 data들은 evict되어야 하는 상황이 되었을 때 그냥 discard시켜버릴 것이 아니라 SSD에 저장을 해둔다면 성능 향상에 도움이 될 수 있다. 혹은 window 2 내에서 주기적인 access 패턴이 보이지는 않았다 하더라도, window 1가 커버하는 매우 최근 기간동안은 아니었지만 window 2가 커버하는 긴 최근 기간동안 hot한 access 패턴을 보인 data가 있을 수 있다. 한 번 hot한 access 패턴을 보인 data는 이후에 다시 hot한 access 패턴을 보일 가능성이 있으므로 이러한 data들도 potentially re-hit 가능성이 있다고 볼 수 있다. 이러한 data들은 class-2로 분류됨.\n\n- 그리고 긴 기간 동안 관찰하는 window 2에서조차도 hit count가 낮거나, 의미있는 access 패턴을 보여주지 못했던 data들은 potentially re-hit될 가능성이 낮다고 판단하고, 이러한 data들은 class-3으로 분류하며, evict되어야 하는 상황이 되었을 때 SSD에 저장하는 등의 별도의 처리를 하지 않고 그냥 discard시켜버린다.\n\n<br/>\n\n===== hybrid page cache meta information management =====\n\n- page tiering에 의해서 SSD page cache로 page가 이동될 때, page cache 내의 해당 page가 있던 노드 (일반적으로는 radix tree 형태로 관리되고 있으나 그에 종속적으로 구현될 필요는 없음)에는 page-tiered-out flag가 set 되며, 어느 SSD의 어느 위치에 저장되는지에 대한 page location 정보가 기록됨. 기본적으로 page location을 구성하는 정보는 SSD 식별 정보, SSD 내 page chunk 식별 정보, page chunk 내 page offset 정보로 구성됨.\n\n- 응용 혹은 kernel에 의해 disk data access 요청이 왔을 때, page find/get 루틴은 page cache에서 해당되는 page를 찾아서 돌려주는 역할을 수행함. 이를 위해 page find/get 루틴은 page-tiered-out flag를 확인하여 매칭되는 page node가 발견되지 않으면 disk로부터 data를 가져오기 위해 disk I/O 요청을 발생시킨다. 매칭되는 page node가 발견된 경우에는 우선 page-tiered-out flag가 set 되었는지를 확인한다. 만약 page-tiered-out flag가 unset인 경우라면, RAM page cache에 해당 page 데이터가 존재하는 경우로서, 바로 해당 page data를 돌려준다. 만약 page-tiered-out flag가 set인 경우라면, page location map 정보를 활용하여 SSD page cache에 저장되어 있는 page 데이터를 가져올 수 있도록 한다. 이렇게 disk로부터 가져온 page data는 해당되는 page node내에 저장되고, page hit count, access time 등 page access 패턴을 관리하는 필드들의 값도 update된다.\n\n- page tiering에 의해서 RAM page cache로부터 SSD page cache로 tiering-out될 때에는 tiering buffer의 크기에 해당하는 page chunk 단위로 I/O가 SSD write이 이루어지지만, SSD page cache에서 page find/get이 일어나는 경우에는 page 단위로 read됨. 이를 통해서 가급적 기존 방식의 page cache 동작 방식의 수정이 최소화될 수 있음. 기존 page find/get이 일어나는 방식과 달라지는 부분은 일반적인 file system hierarchy에서의 logical block address 정보를 가지고 있는 inode 대신 SSD page cache 내에서의 page location 정보를 가지고 있는 tiered page location table이 참고된다는 점임.\n\n\n<br/>\n\n=== 청구항 ===\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n\n\n=== Notation ===\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n\n=== Misc. ===\n## (done) Tiering Buffer 기반 Page Cache Tiering 방식의 RAM-SSD Hybrid Page Cache 아키텍쳐 및 방법\n## (done) ## RAM과 SSD를 어떻게 hybrid로 사용할 것인지, 즉 둘 간의 역할에 대해 설명.\n## (별도 특허화 필요) ## SAVL-like layer를 이용한다는 것도 기술할 것\n\n## Page Cache의 Class를 분류하고 관리하는 방법\n## Page Classification Diagram 관련\n## Page Class Transition Diagram 관련\n## T2에 가는 경우 중, available DRAM size가 부족하고, 그러면서도 update 연산이 많이 일어나는 page들에 대해서 SSD로 내려야 하는 경우, (SSD의 retention time 조절 등을 통해 확보되거나, 처음부터 many-erase에 성능/수명적으로 tolerant한 SSD를 이용하도록) SSD 영역 혹은 SSD unit을 선택하도록 하는 기능도 추가할 것\n\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다.\n# application에서 512-byte 만큼만 (혹은 1-byte만) 읽어온다고 했을 때에도 page cache에서는 4096-byte 단위로 find/get 하게 될까? (당연히 그럴 것 같지만... 확인 필요), 그리고 실제 block layer에 request가 전달될 때에는 어떻게 될까? 1-byte를 요청했어도 1 sector (512-byte)만큼 읽게 될까? 다시 정리하자면, application 수준에서 1-byte를 요청한 경우, page cache layer에서는 4096-byte를 읽어오게 되고, block layer에서는 512-byte를 읽어오게 되는 것일까?\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n== PATENT-BRIAN-2013-002 ==\nContent Repeatability-Aware SSD Cache Management\n\n\n[청구항]\n\n\n\n\n\n\n== PATENT-BRIAN-2013-003 ==\nLikely Zone Based Page Preplacement\n\n\nassociated-likely-zone 기반의 page pre-placement (to RAM)\n\n- page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n- 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n- associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n\n\n[청구항]\n\n\n\n\n== PATENT-BRIAN-2013-004 ==\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-005 ==\n\n:: 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n== PATENT-BRIAN-2013-006 ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n\n\n\n\n== PATENT-BRIAN-2013-007 ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]','utf-8');
