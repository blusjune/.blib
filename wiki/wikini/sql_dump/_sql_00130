INSERT INTO `radiohead_text` VALUES (64,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n[[Bnote patidea 2013-003]]\n\n\n== PATENT-BRIAN-2013-004 ==\n\n=== Event-based I/O Prediction ===\n\n=== 요약 ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n=== 기술 상세 ===\n\n* I/O 발생 메커니즘의 모델링\n: I/O prediction을 위해, type-I event (indirect I/O trigger)와 type-II event (direct I/O trigger)를 관찰 및 분석함. Type-I event의 예로서, user의 log-in, 기 log-in되어 있던 user session의 상태 변경 (idle-to-busy), system service에 대한 invocation from local or remote node 등이 포함될 수 있으며, type-II event의 예로서, user application 의 실행, system service의 실행 등이 포함될 수 있음. Type-I event는 type-II event의 발생에 영향을 미치고, type-II event는 I/O request event (data access)의 발생에 영향을 미치므로, \'type-I event\' -> \'type-II event\' -> \'I/O request event\' 까지 이어지는 인과사슬 (cause-effect chain) 기반으로 I/O 발생 메커니즘을 모델링하였음.\n\n\n* Event 정보 수집: type-I event 정보 및 type-II event 정보 정의 및 수집 방법.\n\n\n* I/O 패턴 학습 (Offline Learning): 수집된 Event 정보를 기반으로 I/O 패턴을 learning하는 방법\n\n\n* Run-time I/O 예측 (Online Matching): 시스템 모니터링에서 얻어지는 최근 및 현재의 event들과 learning으로 얻어진 I/O 패턴 DB에 기반하여 Run-time으로 미래 I/O 예측\n\n\n* 시스템 구조: 정보 수집 및 패턴 학습 모듈의 시스템 내 위치 (I/O 패턴 DB 등의 위치 및 실시간 예측 모듈과의 관계)\n\n\n* 순서도: 수집부터 학습까지 전체적인 task의 흐름\n\n\n* 예상 효과: 성능적 측면에서 사용자가 체감하는 lower latency, higher throughput 효과, 이미 발생해버린 congestion time을 피해 미리 적절한 곳에 data를 배치시킴으로써 얻어지는 bottleneck prevention 효과.\n\n\n=== Data Collection ===\n\n* 역할: I/O 패턴 학습/발굴에 필요한 정보 수집\n\n* 수집 정보 종류\n: 전조 기반 예측에 필요한 간접 I/O 유발자, 직접 I/O 유발자, I/O 발생 이벤트 등 I/O 예측에 도움이 될 수 있는 다양한 정보를 수집. 다음 정보들이 포함될 수 있음.\n:- indirect I/O trigger trace: {(user-id | user-type), status-type, before-status, after-status, event-time}\n:- direct I/O trigger trace: {(program-id | program-type), status-type, before-status, after-status, process-owner, event-time [, process-current-working-directory, files-opened]}\n:- I/O trace: {access-time, address-accessed, read/write-flag, io-size, file-path, process, co-accessed-addresses}\n\n\n* 수집 모듈의 위치\n: storage-server가 분리되어 있는 구조인 SAN/NAS 타입의 architecture인 경우에는 application이 구동되는 server-side에 정보 수집 모듈이 위치한다. storage-server가 결합되어 있는 converged-architecture 기반의 system인 경우에는 해당 system 내에 정보 수집 모듈이 위치한다.\n\n\n* 정보 수집 방식\n: 운영체체가 제공하는 시스템 상태 정보 제공 interface를 이용하거나, blktrace, lsof, debug file system 등 이용 (e.g., /sys/kernel/debug/ 등) 혹은 /proc file system 등을 이용 가능.\n\n== PATENT-BRIAN-2013-005 ==\n\n=== Coaccess-based I/O Prediction ===\n\n\n=== 요약 ===\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n\n=== 기술 상세 ===\n\n* Data의 coaccess 특성에 기반한 proactive data placement 기법\n: 소정의 time window 내에 같이 access되는 확률이 높은 data 그룹이 있을 때, 그 그룹 내의 data가 access되는 것을 알게 되면, 그 그룹 내의 나머지 data들도 미리 fast media에 가져다 놓음으로써 성능 향상을 얻을 수 있게 하는 방법임.\n\n\n\n\n* 시뮬레이션 기반의 성능 향상 효과 비교\n:- Fast media의 4KB random read latency는 96.47 usec (SSD 830 128GB 기준), slow media의 4KB random read latency는 1,884.03 usec (HDD 7,200rpm 2TB 기준)으로 설정 (iozone에 의한 실측값 사용)\n:- 실험 시작 전에는 4GB 크기의 target data가 slow media (HDD)에 저장되어 있음.\n:- 여기에 1,000,000 개의 4KB random read request로 구성되는 workload를 인가. (각 read request들은 서로 중복되는 data를 가리키지 않으며, 4GB 크기의 target data를 한 번씩 다 request하게 된다고 가정)\n:- Workload 패턴 학습 결과, 4GB의 전체 data 중, \'가\' type의 coaccess 특성을 가지는 10MB 크기의 data 그룹 (data set 당 2,500개의 4KB data 포함)이 200개가 있으며, \'나\' type의 coaccess 특성을 가지는 1MB 크기의 data 그룹 (data set 당 250개의 4KB data 포함)이 1,000개 있는 것을 발견한 것으로 가정.\n:- \'가\' type의 coaccess 패턴은 해당 data 그룹 내 100개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 80%인 것으로 정의.\n:- \'나\' type의 coaccess 패턴을 해당 data 그룹 내 50개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 100%인 것으로 정의.\n\n\n\n\n: 에 대해 본 기법을 적용한 경우와 그렇지 않은 경우의 성능 차이를 IOPS로 비교해보면 다음과 같다. 기법을 적용하기 전인 baseline의 경우에는 \n\n\n* coaccessness 분석에 필요한 parameter로서, \n\n\n\n\n\n\n\n: 예를 들어 설명하면 다음과 같다.\n: data 1, data 2, ..., data 10000으로 구성된 data set이 있으며, 각 data들의 크기는 4KB이다. 학습을 통해서, (1) 이 중 어느 한 data가 access되었을 때, 1분 이내에 나머지 data들도 같이 access되게 될 확률은 55%, (2) 이 중 2개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 80%, (3) 이 중 3개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 95%라는 것을 알고 있다고 가정한다.\n: fast media의 크기는 4GB 이며, 기존에 \n\n\n   이라는 것을 알고 있다면, data 1, 2, 3이 access되었을 때 data 4~128까지도 미리 proactive하게 fast tier에 배치시킴으로써, 그로 인한 성능 이득을 얻을 수 있게 된다. (!!!실제 실험 사례?!!!)\n\n\n\n\n\n\n\n\n\n== PATENT-BRIAN-2013-006 ==\n\n\n=== Data Lifetime (Valid-to-invalid) Interval 분석에 기반한 Data Placement (Tiering) ===\n\n\n* Data 별로, 혹은 LBA 별로, 혹은 Erase Block 별로? Data의 Valid to Invalid Time (Duration) 패턴을 분석할 수 있을까?\n: 이를 SSD 내에서 Low Overhead로 수행할 수 있나?\n: 혹은, 굳이 SSD 내에서 수행할 필요가 있나?\n: Host side에서 LBA 별로 분석 될 수 있다면, 이 정보를 기반으로 Tiering 시에 활용 할 수 있을 것임\n: 특정 Address에 대해서 W이 되고 나서, 다시 W이 되기까지의 시간을 측정하여 ARM 하면 될 듯. 그 두 W 간에 얼마나 많은 R들이 오는지에 대해서도 고려하여 placement 시 고려할 수 있을 것임.\n\nMeasurement Window 내에서 \'W 간의 Interval이 길다\'는 식으로 인식하는 것과, \'W의 횟수가 잦다\'고 인식하는 것 간에 어떤 차이가 있을 수 있을까?\n\n <pre>\n    01234567890123456789\na1: W-R-W-R-W-R-W-W-R--W : T20W06R04 : Short-Lifetime, Many-Reads : SLC\na2: W---W-R-W---W-W----W : T20W06R01 : Short-Lifetime, Few-Reads : SLC, DRAM?\na3: W-RRRR-W---RR-W-R-RW : T20W04R08 : Mid-Lifetime, Many-Reads : MLC\na4: W------W--R---W----W : T20W04R01 : Mid-Lifetime, Few-Reads : MLC\na5: W-R-R-R-RR-RRRR----R : T20W01R10 : Long-Lifetime, Many-Reads : MLC\na6: W------------R----R- : T20W01R02 : Long-Lifetime, Few-Reads : TLC\n</pre>\n\n=== Idle-time Prediction Based Free Space Management in SSD ===\n\n* Idle-time의 도래 시기와 지속 시간 패턴 분석에 기반한 SSD 내의 Free Space 확보 방법\n:- SSD 내에서 Low overhead (in computation, memory space)로 Idle-time 도래 시기 및 지속 시간 분석이 가능한가?\n:- 가능하다면, SSD 내에서 Garbage Collection 등으로 Free Space를 확보하는 데 있어서 어떤 장점을 가질 수 있는가?\n\n\n\n: 이를 기반으로 \n\n\n\n=== 요약 ===\n* \n\n\n=== 기술 상세 ===\n\n*\n\n== Memo for {PATENT-BRIAN-2013-004, PATENT-BRIAN-2013-005} ==\n\n\n=== Think outside the box // Questions ===\n\n* 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n\n* Tiering을 좀 더 다양한 각도에서 바라보자\n:- 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 것이 기존의 tiering 개념임.\n:- SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n:- SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n:- access pattern-aware optimal placement (APOP) - 이런 것은 어떨까?\n:- Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n\n* write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반의 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지?\n\n\n* 만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? SSD-internal memory buffer를 이용해서 SSD를 NVRAM처럼 쓰도록 하는 것이 실제로 말이 되는 이야기일까?\n\n\n\n\n\n=== 기존 기술의 한계점 (Intelligent I/O Prediction 기술의 필요성) ===\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache만으로는 성능 향상을 기대하기 어렵게 됨.\n\n <pre>\n실제 tracelog 사례를 보여주고 hot area가 변화하고 있음을 보일 것.\n</pre>\n\n* Data Placement (Tiering) 관점에서 본 EMC의 FAST, FAST-VP 기술의 한계점\n:- Coarse-grained Data Migration (768KB 까지 작게 나눌 수 있다고는 하지만, 실제로 그렇게 해서 성능이 얼마나 잘 나오는지는 확인 필요. 귀중한 자원인 DRAM을 관리를 위한 metadata로 가득 채우게 될 위험도 있음)\n:- Flexible하지 않은 Tiering 주기\n:- Busy-hour 지난 후 Idle-time에 Tiering 실시\n:- 낮은 지능의 미래 I/O 예측 능력\n:: data access count를 기반으로 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두도록 하는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n\n=== 본 발명의 차별화 포인트 ===\n\n* 본 기술의 특징 1: I/O를 triggering하는 전조 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Indicator는 Run-time에 저비용으로 감지될 수 있는 형태의 정보들로 구성\n:- 미래에 access될 data를 예측케 하는 Indicator간의 mapping은 Bayesian Inference 이용\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n=== 특허 검색/분석 결과 {storage I/O workload prediction} ===\n\n* 검색식: storage I/O workload prediction\n\n* http://portal.uspto.gov/pair/PublicPair 에서 특허 상태 확인 가능.\n\n* I/O prediction을 기반으로 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n:- 기존엔 proactive tiering이 없었나? -> IBM 특허가 나름 유사함. (US7930505, US7512766)\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n==== 1. 비교가 될만한 특허들 ====\n\n예측에 대한 언급이 되어 있는 스토리지 시스템 관련 특허들. 크게 두 가지로서, 그 중의 하나는 가상 volume에 대해 peak I/O가 발생하는 주기를 분석하여, peak I/O 발생시점이 닥치기 전에 미리 가상 volume의 data를 migration 시키는 방법에 대한 것이나, 어느 위치의 data가 언제 access될지에 대한 예측은 아님 (IBM 특허). 다른 하나는 I/O 예측을 기반으로 미래에 자주 access될만한 data를 미리 여러 노드에 분산시키는 방법에 대한 것이나, 예측 방법 자체에 대한 내용은 다루고 있지 않음 (HP 특허)\n\n\n; US7930505, US7512766 (IBM)\n\n* US7930505 (Application Number: US 12/053582), \"Controlling preemptive work balancing in data storage,\" IBM, 특허일 2011-04-19\n: storage network의 I/O 관찰을 통해 peak I/O activity의 repeating instance에 대한 signature를 VLU (virtual logical unit) 단위로 생성하고, 이에 기반하여 언제 heavy I/O가 도래할 것인지를 예측하여, heavy I/O가 오기 전에 미래 data를 faster media로 migration하도록 하는 tiering 기술에 대한 특허. (이미 I/O peak time이 온 후에 hot data를 옮기는 것은 오히려 시스템의 IO성능을 악화시킬 수도 있기 때문에 미리 migration을 하고자 하는 것임)\n: VLU 단위로, peak I/O activity가 어느 주기로 발생할지에 대한 예측임. 지능 수준과 Fine-granularity 측면에서 초보적인 수준임.\n\n* US7512766 (Application Number: US 11/213494), \"Controlling preemptive work balancing in data storage\", IBM, 특허일 2009-03-31\n: 위의 US7930505 특허와 동일 건으로 보임\n\n\n; US6912635 (HP)\n\n* US6912635 (Application Number: US 10/140968), \"Distributing workload evenly across storage media in a storage array\", HP, 특허일 2005-06-28\n: 미래에 자주 access될만한 data를 여러 arrayed storage device에 분산시키겠다는 기술. 그러나 I/O에 대한 예측 방법은 다루어지고 있지 않음.\n\n==== 2. 검색은 되었으나 관련은 없는 특허들 ====\n\n무언가를 예측한다는 측면, Storage System에 대한 기술이라는 측면에서는 관련이 있으나, 미래에 Access될 Data를 예측한다는 측면에서 제안 기술과 관련 없는 특허들\n\n\n; US7640231 (IBM)\n* US7640231 (Application Number: 11/280012), \"Approach based on self-evolving models for performance guarantees in a shared storage system\", IBM, 특허일 2009-12-29\n: network 기반 storage system에서, average latency 혹은 I/O request rate 등에 대한 주기적인 performance sampling을 기반으로, 주어진 objective function의 값을 최대화할 수 있도록 system resource를 제어할 수 있게 하는 기술.\n\n\n; US7774491, US7519725 (IBM)\n\n* US7774491 (Application Number: 12/359377), \"Utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2010-08-10\n: 복수의 클라이언트들을 대상으로 통계적인 성능 QoS를 보장하기 위한 informed throttling 방법\n: A system for utilizing informed throttling to guarantee quality of service to a plurality of clients\n\n* US7519725 (Application Number: US 10/444712), \"System and method for utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2009-04-14\n: resolve the problem of providing statistical performance guarantees for applications generating streams of read/write accesses (I/Os) on a shared, potentially distributed storage system of finite resources, by initiating throttling whenever an I/O stream is receiving insufficient resources\n\n\n; US7827283 (IBM)\n* US7827283 (Application Number: 10/367444), \"System for managing and controlling storage access requirements\", IBM, 2010-11-02\n: SAN 등의 환경에서, storage bandwidth를 관리하고 reserve하는 기술\n: A Resource Management and Reservation System (RMRS) for managing and reserving storage bandwidth, is a platform independent middleware layer that provides an interface to applications, their database management systems, or some other higher level data management systems like ADRS which does data management on behalf of the applications\n\n\n; US8433554, US8140319 (IBM)\n\n* US8433554, \"Predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2013-04-30\n: An approach for predicting performance and capacity of an information technology (IT) system before the IT system is built is described, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n* US8140319, \"Method and system for predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2012-03-20\n: A method and system for predicting performance and capacity of an information technology (IT) system before the IT system is built, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n\n\n; US8255915 (HP)\n* US8255915, \"Workload management for computer system with container hierarchy and workload-group policies\" HP, 특허일 2012-08-28\n\n\n; US7050956 (HP)\n* US7050956 (Application Number: US 09/843930), \"Method and apparatus for morphological modeling of complex systems to predict performance\", HP, 특허일 2006-05-23\n: I/O 성능 예측을 위한 complex storage system에서의 data flow 모델링 방법\n\n\n; US6718434 (HP)\n* US6718434 (Application Number: US 09/871043), \"Method and apparatus for assigning raid levels\", HP, 특허일 2004-04-06\n: Data를 disk array에 load하기 전에, workload 명세 및 device 명세에 기반하여, 적합한 RAID level을 판단하여 disk array에 할당 (RAID build)하는 방법\n\n\n; US8112586 (EMC)\n* US8112586 (Application Number: US 12/190071), \"Predicting and optimizing I/O performance characteristics in a multi-level caching system\", EMC, 특허일 2012-02-07\n: multi-level caching 시스템에서, cost대비 performance를 최적화할 수 있도록, workload에 따른 resource allocation design에 대한 recommendation 기술 (multi-level cache들 간의 workload propagation을 예측하는 것이 핵심)\n\n\n; US6629266 (IBM)\n* US6629266 (Application Number: US 09/442001), \"Method and system for transparent symptom-based selective software rejuvenation\", IBM, 특허일 2003-09-30\n: software가 언제 동작이 멈추는지를 예측함으로써, software의 dependability를 향상할 수 있게 하는 방법\n\n\n; US5088058 (Unisys)\n* US5088058 (Application Number: 07/237120), \"Apparatus and method for evaluating and predicting computer I/O performance using I/O workload snapshots for model input\", Unisys Corporation, 특허일 1992-02-11\n: I/O delay 등에 대한 확률적 모형을 기반으로 disk 시스템의 성능을 검증하고 예측하는 시뮬레이션 기반의 방법\n: A method of evaluating and predicting the performance of an I/O disk system comprised of one or more disk controllers and cooperating disk units using a simulation model containing a stact constructed such that delays are formulated as sets of stochastic processes\n\n=== Comparison of I/O Prediction Approaches ===\n\n미래에 access될 data의 위치와 시기를 예측하기 위한 방법으로 event-based I/O prediction, coaccess-based I/O prediction, periodicity-based I/O prediction 방식이 제안되며, 각각의 방식은 예측에 필요한 연산량 및 input data의 richness, 그리고 workload 변화에 대한 적응성 측면에서 장단점을 가지고 있음. 따라서, 상황에 따라서 적절한 방식을 이용하면 됨.\n\n{| style=\"border-collapse: collapse; border: 1px solid #000\" border=1\n| style=\"width: 25%\" | 비교 항목\n| style=\"width: 25%\" | periodicity-based I/O prediction\n| style=\"width: 25%\" | coaccess-based I/O prediction\n| style=\"width: 25%\" | event-based I/O prediction \n|-\n| context richness required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| computation overhead required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| adaptability to workload changes (lower is better)\n| high\n| high\n| high\n|-\n|}\n\n\n* 전조 기반 예측 vs. 주기성 기반 예측\n{| border=1\n| 비교 항목\n| 전조 기반 예측\n| 주기성 기반 예측\n|-\n| 기술 요약\n| 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식\n| address 별 access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식\n|-\n| 분석 오버헤드\n| High (학습/마이닝에 드는 비용임)\n| Low (주기성 파악 비용임)\n|-\n| Run-time 오버헤드\n| Mid (실시간 전조 모니터링 및 매칭 비용)\n| Low (주기성 계산)\n|-\n| Workload 변화 적응성\n| High\n| High (Workload 변경 사실 알림 hint 필요)\n|-\n| ...\n| ...\n| ...\n|-\n| Incrementality of Mining/Learning\n| ...\n| ...\n|}\n\n* 전조 기반 예측은, 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식임.  주기성 기반 예측은, data access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식임.\n\n\n=== Intelligent Data Placement ===\n\n* Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case\n: 여기에 HML이 사용될 수 있는 것이라면, 어떤 방식으로 어떤 이유로 그런 것일까?\n\n\n* I/O 예측 방식으로서 다음 방식이 제안되며, 각 방식에 따라서 prediction model, 수집되어야 하는 context data, I/O 패턴 DB에 저장되는 정보의 종류 및 형태가 달라질 수 있음. 예측에 이용할 수 있는 context 정보의 richness에 따라 적절한 방식을 사용할 수 있다. (서로 다른 발명이므로, 별도의 특허로 낸다)\n:- event-based I/O prediction\n:- coaccess-based I/O prediction\n:: 여기서, periodicity-based I/O prediction은 기존 특허에서 언급되었으므로, 본 발명에서는 크게 언급하지 않기로 한다.\n\n\n* I/O prediction 결과에 따라 적절한 위치에 data를 배치하게 되는데, 여기서 적절한 위치라 함은 SSD와 같은 fast tier가 될 수도 있겠으나, sequential read/write 같은 경우 striping된 HDD array가 될 수도 있고, 혹은 SAS HDD가 될 수도 있겠음. 또한 SSD 중에서도 SLC, MLC, TLC 로 구분되는 fine-grained fast tier로 구분될 수도 있음\n\n\n* 미래에 오게 될 I/O의 type도 알 수 있을까? 즉, {random/sequential}x{read/write} type 중 어느 것에 해당되는지를 안다면, 그리고 I/O의 intensiveness도 예상할 수 있다면 그에 맞춰서 placement 시 적절한 tier 혹은 적절한 node로 찾아가게 할 수 있을 것임.\n\n\n* context 정보는, 수집되는 위치에 따라, server-side에서 수집되는 context data와 storage-side에서 수집되는 context data로 분류될 수 있음. Event context data는 server-side에서 수집 가능하며, coaccess context data 및 periodicity context data는 storage-side에서 수집될 수 있음.\n\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n: 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n\n=== Design/Implementation Issues ===\n\n* workload type/class tagging 필요\n\n* cache와 tiering을 같이 쓸 때, DRAM cache가 아니라 Flash Cache의 경우 쓸데없이 모든 data가 Flash Cache에 다 들어가게 두기 보다는, tiering에서 cover하지 못하는 것들 위주로 caching하도록 하면 Flash Cache의 공간 낭비를 줄임으로써, 유효 cache 공간을 늘일 수 있게 되고, 그만큼의 성능 향상으로 연결될 수 있음.\n\n* DRAM 기반의 write cache를 적극 이용하여 마치 log-structured file system에서 제공하는 I/O merging and sequentialization 효과를 얻을 수 있음.\n\n* Tiering되는 data는 caching되는 data보다 상대적으로 coarse-grained하게 grouping하여 다룸으로써, 한꺼번에 많은 data를 move할 수 있도록 하고, meta data 관리에 필요한 overhead를 줄일 수 있게 한다. 당장 access되지 않더라도 일정 시간 내에 같이 access될 수 있는 data들은 함께 faster tier로 옮겨진다. 이때 필요한 정보가 coaccess 정보임.\n\n* Cache - Tiering 기법의 연동 방법: 예측이 실패했거나, target tier media의 가용 공간이 부족하여 tiering에서 놓치는 missing data가 있을 수 있음. 이러한 missing data들이 temporal locality가 있는 경우에는 당장은 faster tier에서 serve하지 못하더라도 cache에서 serve될 수 있는데, temporal locality가 적은 경우에는 cache만으로 cover되지 않는다. \n\n* Management table size optimization issue - 만약 모든 LBA 별로 event-based I/O prediction을 해야 한다면 현실적으로 사용 가능한 아키텍쳐가 아닐 수 있음. 만약 spatial locality가 있어서 LBA 1,000개 단위로 묶어서 대응할 수 있다면, 최소 1/1000 의 management table size 감소 효과를 얻을 수 있음. pattern mining 혹은 machine learning 실행 시에는 record 수가 감소함으로써 얻는 이득은 1,000배 이상이 될 수 있음. 관건은 얼마나 LBA 묶음을 크게 만들어서 예측에 사용할 수 있는지임.\n\n\n=== I/O Insight ===\n\n* ML 기반의 I/O Insight 발굴 (Data Placement를 위한 Macroscopic Guideline)\n: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:-> 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n\n* I/O Insight의 요건:\n:# indication, as simple as possible\n:# indication, as specific as possible\n:# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n:# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n\n* I/O Insight의 형태\n: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n=== X-Y Modeling ===\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n=== 예측 실패 대비 (별도 특허화 가능) ===\n\n* I/O 예측이 실패할 수 있으며, 이로 인한 성능 penalty를 최소화하기 위한 방법 필요.\n:- fast tier에 여유공간이 있다면 1타겟과 2타겟을 모두 올려놓아도 되지 않을까? 1타겟과 2타겟을 placement함으로써 예측 정확도를 95%까지 올릴 수 있다면, 꽤 쓸만할 수 있겠음. 그리고 1타겟과 2타겟의 모든 데이터를 다 fast tier에 올려두지 말고 일부만 fast tier에 올려두었다가, 둘 중에 하나의 타겟이 hit되었을 때, hit된 타겟의 나머지 data를 재빨리 fast tier에 올리도록 하는 incremental 방법을 사용할 수 있겠음.\n:- 그러나 이것 또한 예측 실패에 대한 완벽한 prevention이 될 수는 없다. 따라서 예측 실패가 발생하더라도 재빨리 그 성능 penalty를 mitigation할 수 있는 방법이 필요. \n:- cache를 이용하여 mitigation할 수 있을까?\n\n=== Data Read/Write 관련 Issues ===\n\n* read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n\n* 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n\n* write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간에, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면?\n\n* 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있음.\n\n* 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간이 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 scatter했던 I/O들을 나중에 다시 읽어들이려 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 원하는 고성능을 낼 수 있게 된다.\n\n* replica-based dynamic clustering\n: 위와 같은 write-and-then-read-again 상황에서, 해당 data를 저장했었던 HDD의 I/O busy/idle 여부에 상관 없이, 최고의 효과를 낼 수 있게 하기 위해, replica management를 적극적으로 활용하는 것은 어떨까? HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까?\n\n* data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 논의가 아님. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 논의임.\n\n* 그러나, 조심해야 할 것은 위의 논의는 상당히 case-dependency를 상당히 많이 탈 수도 있다는 것임. 책상 위의 아이디어로 끝날 수도 있기 때문에 조심스럽게 접근하는 것이 필요함.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-008 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n\n\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n----\n=== (SmartSSD API) SSD-internal I/O pattern logging interface and mechanism ===\n\n\n==== Questions ====\n\n* SmartSSD에서 제공할 수 있는 logging 서비스로서 어떤 것들이 있을 수 있나?\n:- 어느? 정도의 free space를 필요로하는 critical (heavy) I/O가 어떤? 주기로 도래할 지 알 수 있을까?\n:- I/O prediction에 도움될 수 있는 정보를 SmartSSD가 기록했다가 필요한 때 (안전한 방법으로?) 줄 수 있을까?\n:- I/O prediction까지는 아니더라도 해당 SSD의 I/O wellness(어떻게 정의?)를 측정했다가 bottleneck을 회피하는 데 유용한 정보를 제공할 수 있을까?\n:- 어떤 SSD가 현재 주어지고 있는 I/O workload에 대해 list performance spec을 만족하는데 어려움을 겪고 있다는 것을 알려줄 수 있는 정보가 어떤 것이 있을까? (SSD 내 write buffer의 fullness 혹은 I/O queue의 유동성? amount of free space in over-provisioning area?) (-> I/O wellness 라는 metric을 이것과 연관시켜 정의할 수도 있으며, 결국 SLA 혹은 Performance QoS와 연관시킬 수 있음)\n:- 지금까지의 S.M.A.R.T.와 비교하였을 때, 어떤 차별점이 있는가? 기존의 S.M.A.R.T.가 제공하지 못하던 타입의 정보를 제공하고, 이로 인해 기존에는 불가능했던 새로운 알고리즘 구현 혹은 새로운 서비스 제공이 가능해짐을 보이면 좋겠음.\n\n\n\n:- workload의 history?\n:- 각 erase block이 erase되어야만 했던 주기?\n:- free space의 추이 (얼마나 많이 남아 돌던지)?\n:- I/O의 heaviness 패턴?\n:- SSD내 write buffer (혹은 write cache)의 utilization정보? (즉, write buffer가 넘칠 정도가 되어서 I/O wait을 해야만 했던 상황이 얼마나 자주 발생했는지, write buffer의 가득찬 정도를 백분율로 표현한다고 했을 때, 평균/표준편차 등으로 대표할 수 있는 normal distribution을 따르는지?\n:- 전체 sequential read/write \n\n\n==== 배경 ====\n\n* 각 SSD 모델별로, 또 동일한 모델의 SSD라 하더라도 어떤 workload에 노출되어왔는지에 따라, 현재 낼 수 있는 I/O performance가 동일하지 않을 수 있다 (!check! 관련 실험 결과 - SSD 830을 이용하여 같은 시간 동안 하나의 host에서 생성되는 workload들을 나누어 받았다 하더라도 내부적인 free block의 갯수 및 random write에 의한 block 내 파편화등의 상태가 상이하여 성능에 차이가 나는 것을 보여줄 것. uFLIP 혹은 filebench 활용 가능). 한 예로, SSD가 어느 호스트의 어느 file system에 매핑되었는지에 따라 그 SSD가 받는 workload의 특성이 다를 수 있다. file system mount point가 달라지거나, 기존과 다른 새로운 응용이 설치/서비스되는 상황이 되면 \n\n\n----\n=== I/O Prediction-based Proactive Data Placement ===\n\n* 미래에 access될 data를 예측하여 적소에 미리 가져다 놓는 기술로서, (1) I/O prediction을 위한 정보 수집 모듈, (2) 주어진 정보들에 기반하여 다가올 I/O를 예측하는 모듈, (3) data placement를 수행하는 모듈로 구성됨.\n\n\n* 본 기술의 특징 1: I/O를 triggering하는 indicator (전조) 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Run-time에 저비용으로 감지될 수 있는 Indicator 정보와, 이에 의해 access될 data 간의 mapping은 Bayesian Inference 이용\n\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n\n----\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n----\n\n=== Tiering Method Avoiding I/O Bottleneck ===\n\n* I/O Prediction에 의해 예측된 미래의 Hot Data를 Proactive하게 Placement한다고 할 때, 예상되는 access pattern으로 인해 발생 가능한 I/O Bottleneck을 최소화할 수 있도록 배치하는 방법\n\n* 예를 들어 미래의 hot data block 1 (200MB), hot data block 2 (100MB), hot data block 3 (400MB)이 1분 후에 access 될 것으로 예측되었다고 가정. data block 1에 대해서는 4KB 단위로 random read가 dominant한 access pattern이 예상되고, data block 2에 대해서는 1024KB 단위의 sequential read가 dominant한 access pattern이 예상되고, data block 3에 대해서는 4KB 단위의 random read와 512KB 단위의 sequential write이 각각 80:20의 비율로 mix된 형태의 access pattern이 예상된다고 했을 때, 각 data block을 어디에 어떤 형태로 placement시킬 것인가?\n\n: machine learning 혹은 pattern mining에 기반한 base workload pattern data를 기반으로 incremental하게 update될 수 있는 data access의 spatial locality와 periodicity\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n\n* 기존 기술과의 대비 (naive I/O prediction vs. full-fledged I/O prediction)\n: 기존 기술은 누적 hit count 정보에 의지하는 naive I/O prediction으로 볼 수 있음. 이러한 방식은 workload의 변화를 제대로 반영하지 못하기 때문에, static한 workload에 대해서는 잘 동작하지만, moving target처럼 dynamic한 workload에 대해서는 오히려 과거의 누적 hit count metric이 현재의 workload 패턴을 왜곡하는 문제점을 가지고 있음. (temporal locality와 spatial locality가 깨지는 순간이 여러번 존재할 수록 기존의 naive I/O prediction 방식으로 성능 향상은 어려워짐)\n\n\n\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(65,'== 20130530_134618 ==\n\n=== Frequent patterns/items mining examples with R::arules ===\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n* References\n:# [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))]\n:# [http://www.analysis-of-patterns.net/files/bgoethals.pdf Frequent Pattern Mining Slides - Universiteit Antwerpen]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning - Wikipedia]\n:# [http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf Frequent Pattern Mining in Web Log Data]\n:# [http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010 Temporal Pattern Mining Tutorial KDD 2010]\n:# [http://cran.r-project.org/web/packages/arules/index.html arules: Mining Association Rules and Frequent Itemsets]\n:# [http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf Introduction to arules - A computational environment for mining association rules and frequent item sets -- http://cran.r-project.org/]\n:# [http://lyle.smu.edu/IDA/arules/ Intelligent Data Analysis Lab @ SMU]\n:# [http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))]\n:# [http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/ Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning]\n:# [http://en.wikipedia.org/wiki/Sequence_mining Sequence mining]\n:# [http://en.wikipedia.org/wiki/GSP_Algorithm GSP Algorithm]\n:# [http://en.wikipedia.org/wiki/Apriori_algorithm Apriori algorithm]\n:# [http://en.wikipedia.org/wiki/Market_basket_analysis Market basket analysis]\n:# [http://en.wikipedia.org/wiki/Time_series Time series]\n\n\n----','utf-8'),(66,'== ## bNote-2013-03-29 ==\n\n=== DailyTask ===\n\n* IOWA\n\n\n=== MBO 2013 (목표 최종 확정) ===\n\n* 정명준 MBO\n <pre>\n\n30%, ~10/31\n- I/O Workload Analysis 기술 연구\n  : Dominant I/O Pattern Mining 및 Machine Learning 기반의\n    I/O 패턴 모델링 및 예측\n  : I/O Pattern 분석/예측 모델 수립\n  : I/O Pattern Mining/Learning 엔진 구현 (Python, R, Shell-script)\n\n30%, ~10/31\n- Data Placement 기술 연구\n  : Workload Analysis 결과로 얻어진 I/O Insight/Prediction을\n    활용하여 Data를 적소에 미리 배치\n  : Linux Kernel Module 형태로 Tiering 기술 형태로 구현\n  : Proactive Data Placement를 통해 분산 스토리지의 I/O 성능\n    80% 이상 개선 검증 (시뮬레이션, 혹은 Real 시스템 기반)\n  \n20%, ~10/31\n- A급 특허 3건 작성 및 심의 통과\n\n20%, ~10/31\n- 논문 1편 (To be accepted)\n\n</pre>\n\n\n* 과제 MBO (이전문님)\n <pre>\n* 분산 플랫폼 관련 특허 15편 이상 특허심의 통과 (전략출원 2편 이상 심의 통과) (30%)\n* 분산 플랫폼 관련 논문 2편 이상 accept (20%)\n* I/O coordination과 Proactive Placement를 통해 분산 스토리지의 I/O 성능 80% 이상 개선 검증\n  (HW RAID 대비) (15%)\n* 분산 Deduplication 기술을 통해 분산 스토리지에서 데이터 제거효율 3배, Coverage 4 node 달성 (3x@4node) (15%)\n* 분산 I/O Coordination 관련 기술이전 1건 (20%)\n</pre>\n\n=== 과제 변경 ===\n\n <pre>\n\n과제명: Intelligent Large-scale Data Management\n (구과제명) Real-Time Big Data Platform\n\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n\n</pre>\n\n== ## bNote-2013-03-28 ==\n\n=== SW설계기술리더양성 교육 지원 ===\n* SW Architect 사전 교육\n\n==== 현업 프로젝트 기획서 ====\n\n* 지원과정\n: SW 설계 리더 양성과정\n\n* 과제명\n: Data-intensive Storage\n\n* 프로젝트 참여자\n <pre>\n이주평	전문 연구원	Project Leader\n정명준	전문 연구원	시스템 설계, 요소기술 연구, 기능모듈 구현\n유개원	전문 연구원	요소기술 연구, 기능모듈 구현\n이형주	SDS 차장	기능모듈 구현, 기능/성능 검증\n</pre>\n\n* 과제 담당 임원\n: 심은수 상무\n\n* 과제 개요\n <pre>\n[배경 및 현안]\n□ 데이터 폭증으로 데이터센터/기업의 클라우드 스토리지 니즈 증대\n□ 클라우드 스토리지의 핵심 경쟁력은 성능 및 용량 향상 기술에 있음\n□ H/W 수준을 높이거나 S/W 최적화 기반으로 시스템의 성능을 개선\n   하는 기존 접근 방식으로는 H/W 한계를 넘어서는 성능 향상은 어려움\n□ 데이터 I/O 속도와 데이터 저장 효율을 획기적으로 개선할 수 있게 하는\n   지능적 Data Management 기술은 클라우드 스토리지 시스템의 경쟁력을\n   혁신하는 핵심 S/W 기술임\n\n[목적]\n□ 본 Sub Task에서는 지능적 Data Management 기술 중,\n   데이터 I/O 속도 향상 기술을 연구/개발한다\n   * I/O Workload Analysis에 기반한 Proactive Data Placement 기술 확보\n     - Real trace data에 대한 I/O Workload Analysis를 통해\n       dominant workload 패턴 발굴 및 I/O 예측 모델 학습\n     - I/O 예측 모델에 기반한 multi-tier (horizontal - vertical) 간\n       proactive data 배치 수행\n</pre>\n\n* 목표\n <pre>\n[기능/성능/품질]\n□ I/O Workload Analysis에 기반한 Proactive Data Placement\n  - I/O Workload Analysis 모듈\n    - Real trace data 수집 기능\n    - Trace data parsing 및 transform 기능 (analysis를 위한 전처리)\n    - Dominant workload pattern 추출 및 I/O model 학습\n  - Proactive Data Placement 모듈\n    - Tier management 및 data move 기능\n	- I/O monitoring 및 hot/cold 판단 기능\n\n[중간 산출물]\n□ Hot/Cold Data Placement 모듈\n  - 핵심적인 automated tiering 기능 구현\n    : Data access 패턴 관찰을 통해, hot data는 고속의 storage tier에,\n      cold data는 상대적으로 느린 속도의 storage tier에, 주기적 배치\n  - 동일 수준의 시스템에서, 동일 workload에 대해,\n    경쟁사 (E사) 수준의 성능 달성 (metric: average IOPS)\n\n[최종 결과물]\n□ Proactive Data placement 모듈\n  - I/O 예측 모델에 기반한 proactive data placement\n    : Dominant workload 패턴 분석 및 I/O 예측 모델에 기반한\n	  multi-tier 간 선제적 data 배치를 통해 I/O 성능 향상\n  - 동일 수준의 시스템에서, 동일 workload에 대해,\n    경쟁사 (E사) 대비 100% 이상의 성능 향상 달성 (metric: average IOPS)\n</pre>\n\n* 기대 효과\n <pre>\n□ 지능적 Data Management 기술은 Big Data를 다루는\n   클라우드 스토리지 서비스의 핵심 기술로 활용 가능\n   - Big Data 시장에서는 특히 스토리지 분야가 연간 61.4%의 성장율로\n     전체 시장 성장을 주도\n</pre>\n\n* 과제 구성\n <pre>\n[전체 Architecture]\n□ Proactive Data Placement 시스템은\n   Workload Analysis 모듈과 Data Placement 모듈로 구성됨\n  * Workload Analysis 모듈은 Trace Log 데이터에 대한\n    Off-line I/O Analysis를 수행하여 I/O 패턴에 대한 Insight을 확보함\n    (e.g., 어느 위치의 Data가 언제쯤 Access될 것인지를 예측)\n  * Data Placement 모듈은 I/O 패턴에 대한 Insight 정보와, 실시간으로\n    모니터링되는 시스템 상태 정보를 이용하여, Data를 미리 적소에 배치함\n\n[과제 적용부 기술항목]\n□ Workload Analysis 모듈: I/O Prediction Model Optimization 이슈\n  - 응용 및 시스템 특성에 따라 Workload 특성이 다를 수 있음\n    Workload 별로 Prediction Model을 구성하는 주요 X\'s 의 최적화 필요\n□ Data Placement 모듈: Overhead 최소화 및 Tiering 구현 최적화 이슈 \n  - Real-time Monitoring으로 인해 시스템에 가해지는 Overhead 최소화 필요\n  - Tiering 기능 구현 시 I/O 특성 및 시스템 구조를 반영한 최적화 필요\n</pre>\n\n\n==== 입과 추천서 ====\n\n[본인 업무 이력]\n\n* 2004.08 ~ 2007.09 : Security & Trusted Computing 기술 연구/개발\n:- 휴대폰 Content/Right Protection 기술인 OMA DRM S/W 개발, 무선사에 기술 이전\n:- Secure MMC를 위한 Crypto Engine 개발 참여 및 MMC IOP T/F 활동, 메모리사에 기여\n:- System의 무결성 보장 기술인 Trusted Computing 기술 연구 주도, Mandatory Access Control 기술을 무선사에 이전, LiMo (Linux Mobile) Security 표준에 반영 (SubPL)\n:- A급 특허 6건 출원, 논문 2건 (ACM SACMAT \'08 등)\n\n* 2007.10 ~ 2008.05 : 전사 6시그마 MBB (Master Black Belt) 양성 과정\n:- 제 16기 6시그마 MBB 과정에 입과하여 6시그마 이론 연구 및 실습 과제를 진행하고 BB 교육 과정 강의를 진행하였음. MBB 인증 시험 통과\n\n* 2008.06 ~ 2010.10 : Virtualization 및 Operating System 기술 연구/개발\n:- H/W가상화 기술인 Xen Hypervisor의 Security 연구 참여\n:- OS가상화 기술 기반의 State Migration S/W 개발, 스토리지사업부로 기술이전(SubPL)\n:- Russia연구소와 협력, Android 부팅속도를 향상시키는 FastBoot 기술 연구 (SubPL)\n:- 본사 사업지원팀 Vision 2020 T/F에 핵심 멤버로 참여, 15개 미래 기술 테마 발굴\n:- A급 특허 6건 출원 (전략 출원 2건), 논문 1건 (MobiCom \'09)\n\n* 2010.11 ~ 2013.현재 : Data-intensive Storage 기술 연구/개발\n:- I/O Workload Analysis에 기반한 Proactive Data Placement 기술 연구 주도\n::- Workload Analysis에서 획득한 I/O에 대한 근본적인 이해를 바탕으로 Data Management 알고리즘을 혁신, 스토리지 시스템 성능을 향상시키는 기술임\n:- 본 과제는 메모리사의 사업영역 확장 및 \'클라우드 스토리지 서비스\'를 위한 스토리지 시스템 기술 확보에 기여하고 있음\n:- A급 특허 6건 출원, 논문 3건 (ICCE 등)\n\n[소속부서장 추천 사유]\n\n* (양성 후 활용계획)\n:- 스토리지 시스템 설계/구현 시 S/W Architect로 활용\n* (인물평 및 추천사유)\n:- 정명준 전문은 시스템 분야에 대한 깊은 기술적 이해와 원만한 커뮤니케이션 능력을 바탕으로한 성공적인 프로젝트 발굴/주도 경험을 가지고 있습니다.\n:- 향후 Architect로서, 해당 과제의 S/W 설계 리딩을 통해 스토리지 시스템의 차별화된 기술 경쟁력을 만들어 내는 데에 기여할 수 있을 것으로 판단되어, 금번 S/W 설계 리더 과정에 추천합니다.\n\n== ## bNote-2013-03-26 ==\n\n=== DailyTask ===\n\n* IOWA Proactive Data Placement Formulation\n* Data Representation (as a pre-processing for association rules mining)\n\n* Patentization\n:- Distributed Multi-level Caching\n:- IO Pattern-optimal Data Placement for Tiering\n:- Virtualization-aware Caching/Tiering/Placement\n\n* Study\n:- Btier\n:- Bcache\n:- Fusion IO Caching Technology (directCache, ioTurbine)\n:- EMC FAST (Fully Automated Storage Tiering)\n:- OpenStack\n:- Xen\n:- VASA, VAAI (VMware의 storage virtualization 기술들)\n:- PCIe fabric switching\n:- Software Defined Storage\n:- Virstore? (VMware가 인수?)\n\n* 심상무님께 주간보고 내용\n:- Tiering Test SW Platform 구축 건 (Open source 활용, SDS 이형주 차장님과 함께)\n:- Real Trace Log Data 확보 진행 건 (수퍼컴센터의 Analytics Workload Trace, VDI Trace)\n\n=== Patentization ===\n\n* Access Pattern Aware Tiering\n\n\n=== Memo ===\n\n* Turbine: <기계> 높은 압력의 유체를 날개바퀴의 날개에 부딪치게 함으로써 회전하는 힘을 얻는 원동기. 사용하는 유체의 종류에 따라 수력 터빈, 증기 터빈, 가스 터빈 따위가 있다.\n\n== ## bNote-2013-03-25 ==\n\n=== DailyTask ===\n\n* 업무 File 정리\n* IOWA Proactive Data Placement Formulation\n\n=== Patidea ===\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n:- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [http://www-03.ibm.com/systems/software/gpfs/][http://public.dhe.ibm.com/common/ssi/ecm/en/pos03096usen/POS03096USEN.PDF]\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n* advanced tiering: access pattern-aware optimal placement (APOP)\n:- Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n:: 예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n:- 이에 필요한 data access pattern 모니터링/분석 방법\n:: 데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\n::: NIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n:- 이를 위해 필요한 system architecture 구조\n:: 기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n=== Formulation: IOWA based Proactive Data Placement ===\n\n* Formulating [[IOWA]] or [[I/O Workload Analysis]]\n:- to clarify the total amount of work\n:- to clarify the sub-tasks (can be modularized)\n:- to clarify the area to get focused\n\n\n=== Information: S사, K사 ===\n\n* Kaminario, Solidfire\n: From 서정민 전문\n: SSIC 미팅노트로부터 FACT를 각색한 정보를 공유합니다.  제 의견은 반영하지 않았습니다.\n\n* SolidFire (SSIC 초기미팅 결과)\n:* 1. Company Overview\n::- 3 years, 82 people\n::- $37M in funding (현재 Series B 단계로, 2013년 Series C 가능성 있음)\n::- 12 customers, 4 announced: 2 private cloud enterprise customers. \n::- Multi-tenancy가 기반인 Cloud 시장을 타겟으로 제품 제작 (OpenStack, CloudStack 연동)\n:* 2. Technology: QoS, Scalability, Inline deduplication/Compression\n:: (a) QoS\n::: OS 내에서 QoS를 Volume 단위로 관리 \n::: IOPS/latency QoS support (No R/W separate QoS) \n:: (b) Scalability\n::: Full data distribution across all the nodes \n::: All the nodes contributes to rebuilds\n:* 3. Current Arch./Tech. (GA)\n::- System configuration: 5~100 nodes (they have 40 nodes in test)\n::- H/W Configuration\n:: (a) CPU: Dual 2.5GHz Sandy Bridge with 6 cores each. \n::: 10 Cores는 mostly compute intensive work including dedup and compression. \n::: 2 Core는 handles IO to SSDs\n:: (b) SSD: Viking for boot/metadata, Intel SATA SSDs (relies on supercap in SSD)\n:: (c) Network: iSCSI (FC/NFS in the future, NFS just for small filer)\n::- Performance\n::: Latency Avg is .5ms to 2ms. Worst is 20 to 30 ms. \n:* 4. 금년도 추가 개발계획 (일부)\n::- Remote replication, sync and async, coming in Q3\n::- Encryption is also on the roadmap. \n\n* Kaminario (SSIC 초기 미팅 결과)\n:* 1. Company Overview\n::- Found in 2008.3, Sequoia(VC) funded\n::- 30 patents (the engineers have 76 from the previous jobs)\n::- Target: general-purpose storage system (OLTP, OLAP, VDI)\n::: focusing Latency, Throughput, IOPS all\n::- Shipping scale-out systems for the last 2 and 1/2 years\n::- Competitors: XtremIO, SolidFire\n::- 엔터프라이즈 기능 포커스: resiliency, self-healing, automation 중심\n:* 2. Technology\n::- Core 기술에 대한 파악 결과 없음\n:* 3. Previous Arch.\n::- Dell Blade 서버 방식으로 Fusion-IO 탑재\n:* 4. Current Arch. (개발 중)\n::- 1U rack server 기반 SMART or STEC SAS SSDs 사용\n::: low cost SSDs, low end Xeon, 32GB memory 등 Cost를 줄이는 방식 채용\n::: \"They use LSI SAS controller but don’t use dual port functionality.\"\n::: No SATA SSD (SATA SSD는 신뢰성 문제 야기하는 것으로 판단)\n::: -> SSIC 전문가는 SAS Dual port 기술 개발을 실패하지 않았는가 하는 의문 제기\n::- Performance is about 100,000 IOPS/node.\n::- No Dedup/compression \n::- \"Their SPC-1 result has 20x better price performance than previous SPC results. \"\n::- Currently focus on reducing long tail numbers.  (already has good IOPS)\n::: Performance degradation: < 25% at loss of data node\n\n=== References ===\n\n* [http://www.kaseya.com/download/en-us/white_papers/KaseyaBuyersGuidePaper.pdf IT Systems Management Buyers’ Guide // Kaseya]\n* [http://www.sata-io.org/technology/6Gbdetails.asp SATA-IO Revision 3.1 Specification // Queued Trim Command]\n\n----\n\n== ## bNote-2013-03-22 ==\n\n <pre>\n(EMC (Forum OR World) VNX) ((performance OR \"iops\") AND (\"per dollar\" OR \"dollar per\" OR \"per $\" OR \"/$\" OR \"$/\")) \"vs\" (filetype:pdf OR filetype:ppt OR filetype:pptx)\n</pre>\n\n=== DailyTask ===\n\n----\n==== Books of Machine Learning / Data Mining ====\n* \"Machine Learning\" // Tom Mitchell, McGraw Hill, 1997 ((B.GOOD))\n:- [http://www.cs.cmu.edu/~tom/mlbook.html Book]\n:- [http://www.cs.cmu.edu/~tom/mlbook-chapter-slides.html Slides]\n\n* \"Mining of Massive Datasets\" // Anand Rajaraman, Jeffrey David Ullman ((B.GOOD))\n:- [http://i.stanford.edu/~ullman/mmds.html Book - Online Version]\n:- [http://i.stanford.edu/~ullman/mmds/book.pdf Download the latest book (PDF, 415 pages, approximately 2.5MB)]\n\n----\n\n==== Machine Learning / Data Mining ====\n\n* [http://en.wikipedia.org/wiki/Gradient_descent Gradient Descent]\n* [http://ko.wikipedia.org/wiki/%EC%9D%8C%ED%95%A8%EC%88%98]\n* [http://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95%EC%82%AC%EC%83%81]\n* [http://ko.wikipedia.org/wiki/%EB%AF%B8%EC%A0%81%EB%B6%84%ED%95%99]\n* [http://ko.wikipedia.org/wiki/%ED%8E%B8%EB%AF%B8%EB%B6%84]\n* [http://ko.wikipedia.org/wiki/%ED%8F%89%EA%B7%A0%EA%B0%92_%EC%A0%95%EB%A6%AC]\n* [http://www.iiswc.org/iiswc2008/Papers/012.pdf] Characterization of Storage Workload Traces from Production Windows Servers // Swaroop Kavalanekar, Bruce Worthington, Qi Zhang, Vishal Sharda\n\n----\n\n== ## bNote-2013-03-21 ==\n\n=== Official Death of ... ===\n* What to do? why?\n\n=== DailyTask ===\n\n* list of candidate tasks\n\n:- (~) IOWA DM/ML Planning\n::- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n::- Hadoop case 고려 (in-local-node vertical placement between DRAM-SSD-HDD 뿐만 아니라 horizontal placement (among distributed nodes in Hadoop cluster)도 고려)\n\n:- IOWA ML Study\n::- IOWA:: Bayesian Network study\n::- IOWA:: Neural Network study\n::- IOWA:: HMM study\n::- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n::- (~) IOWA:: Association Rule Mining, Frequent Item Sets Mining (Market Basket Problem)\n\n:- page cache to be revisited\n\n:- Real IO trace 확보 작업\n::- 김혁호 책임이 보내준 업무요청 사례 기반, 업무요청 문서 작성할 것\n::- (V) NetApp (김주영 과장)에게 연락 -> 스토리지 모델 정보 요청 -> 추가 문의 (Management Console에서 보다 자세한 정보를 볼 수는 없는지? NetApp OnCommand System Manager 같은 Tool을 사용해서라도..) -> 김주영 과장과 통화 (오전 10:20 ~ 46), 이번 주 중에 찾아뵙고 말씀 나누는 걸로 ...\n::- EMC 연락\n::- (~) Dell (지근영 대리)에게 연락\n:::- Dell EqualLogic PS6100XV iSCSI arrays [http://www.dell.com/us/enterprise/p/equallogic-ps6100xv/pd] [http://www.equallogic.com/products/default.aspx?id=10661]\n\n:- (~) Distributed Computing Problem Study\n::- CAP theorem (CAP: Consistency, Availability, Partition tolerance) [http://en.wikipedia.org/wiki/CAP_theorem]\n::- Paxos [http://en.wikipedia.org/wiki/Paxos_(computer_science)]\n::- State Machine Replication [http://en.wikipedia.org/wiki/State_machine_replication]\n\n----\n=== Formulation: IOWA Proactive Data Placement (moved to ## bNote-2013-03-25) ===\n----\n\n== ## bNote-2013-03-20 ==\n\n=== DailyTask ===\n\n* list of candidate tasks\n\n:- (~) IOWA DM/ML Planning\n::- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n::- Hadoop case 고려 (in-local-node vertical placement between DRAM-SSD-HDD 뿐만 아니라 horizontal placement (among distributed nodes in Hadoop cluster)도 고려)\n\n:- IOWA ML Study\n::- IOWA:: Bayesian Network study\n::- IOWA:: Neural Network study\n::- IOWA:: HMM study\n::- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n::- (~) IOWA:: Association Rule Mining, Frequent Item Sets Mining (Market Basket Problem)\n\n:- page cache to be revisited\n\n:- Real IO trace 확보 작업\n::- 김혁호 책임이 보내준 업무요청 사례 기반, 업무요청 문서 작성할 것\n::- (V) NetApp (김주영 과장)에게 연락 -> 스토리지 모델 정보 요청 -> 추가 문의 (Management Console에서 보다 자세한 정보를 볼 수는 없는지? NetApp OnCommand System Manager 같은 Tool을 사용해서라도..) -> 김주영 과장과 통화 (오전 10:20 ~ 46), 이번 주 중에 찾아뵙고 말씀 나누는 걸로 ...\n::- EMC 연락\n::- (~) Dell (지근영 대리)에게 연락\n:::- Dell EqualLogic PS6100XV iSCSI arrays [http://www.dell.com/us/enterprise/p/equallogic-ps6100xv/pd] [http://www.equallogic.com/products/default.aspx?id=10661]\n\n:- (V) 2013년 MBO 작성\n::- IOWA PDP (I/O Workload Analysis based Proactive Data Placement) 와 IOBA (I/O Bottleneck Analysis) 두 아이템으로 작성\n\n:- (~) Distributed Computing Problem Study\n::- CAP theorem (CAP: Consistency, Availability, Partition tolerance) [http://en.wikipedia.org/wiki/CAP_theorem]\n::- Paxos [http://en.wikipedia.org/wiki/Paxos_(computer_science)]\n::- State Machine Replication [http://en.wikipedia.org/wiki/State_machine_replication]\n\n----\n\n=== Formulation: IOWA Proactive Data Placement (moved to #bNote-2013-03-21) ===\n\n* [[http://kandinsky/wikini/index.php/Bnote_2013#Formulation:_IOWA_Proactive_Data_Placement]]\n\n=== Supercom Usage Statistics ===\n\n <pre>\nblusjune@jimi-hendrix:[~] $ ssh a1mjjung@supercom\na1mjjung@supercom\'s password:\nLast login: Tue Mar 19 19:28:27 2013 from 75.2.93.158\n----------------------------------------------------------\n| During : 20130311 ~ 20130317                            |\n| Username : a1mjjung , Application(Total jobs) : unix(3)\n----------------------------------------------------------\nTotal RUN time : 2 min 36 secs\nAverage RUN time : 52 secs\nMaximum RUN time : 1 min 14 secs\nAverage Wait time 1 secs\nMaximum Wait time 1 secs\n ---------------------------------------------------------\n</pre>\n\n----\n== ## bNote-2013-03-19 ==\n\n\n\n=== The Market-Basket Model ===\n\n\n=== IOWA::Outlook (MSN FileServer IO Trace // msnfs) ===\n\n* # of IOs (Read/Write/All)\n<pre>\na1mjjung@secm:[microsoft_msn_filesrvr_6h] $ wc -l tracelog.msn_filesrvr.[ARW]\n\n  29345085 tracelog.msn_filesrvr.A\n  19729611 tracelog.msn_filesrvr.R\n   9615474 tracelog.msn_filesrvr.W\n</pre>\n\n* Reads Outlook\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130313_154605.sigio_25.iowsz_2000.t1_2000] $ cat __simout.sigio_25.iowsz_2000.t1_2000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  17\n__valu__sig__ _n_o_sigaddrs :  173401\n__valu__sig__ _sigioc_acc :  5005517\n__valu__sig__ _sigaddrs_efficiency :  28.8667135714\n__valu__sig__ _n_o_addr_total :  5053016\n__valu__sig__ _ioc_total :  19729611\n</pre>\n\n* Writes Outlook\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130313_155325.sigio_25.iowsz_2000.t1_2000] $ cat __simout.sigio_25.iowsz_2000.t1_2000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  8\n__valu__sig__ _n_o_sigaddrs :  211100\n__valu__sig__ _sigioc_acc :  2989803\n__valu__sig__ _sigaddrs_efficiency :  14.1629701563\n__valu__sig__ _n_o_addr_total :  4506823\n__valu__sig__ _ioc_total :  9615474\n</pre>\n\n* Microsoft Production Workload Trace - Related Articles\n\n:- \"Characterization of Storage Workload Traces from Production Windows Servers\", IISWC 2008, Swaroop Kavalanekar, Bruce Worthington, Qi Zhang, Vishal Sharda, Microsoft Corporation [http://www.iiswc.org/iiswc2008/sildes/4_3.pdf Slides], [http://www.iiswc.org/iiswc2008/Papers/012.pdf Papers]\n\n:- \"Write Off-Loading: Practical Power Management for Enterprise Storage\" [http://static.usenix.org/event/fast08/tech/full_papers/narayanan/narayanan.pdf FAST 2008]\n\n=== R Tutorial (Data Frame, Preview) ===\n\n----\n==== Data Frame ====\nA data frame is used for storing data tables. It is a list of vectors of equal length. For example, the following variable df is a data frame containing three vectors n, s, b.\n\n <pre>\n> n = c(2, 3, 5) \n> s = c(\"aa\", \"bb\", \"cc\") \n> b = c(TRUE, FALSE, TRUE) \n> df = data.frame(n, s, b)       # df is a data frame\n</pre>\n\n----\n==== Built-in Data Frame ====\nWe use built-in data frames in R for our tutorials. For example, here is a built-in data frame in R, called mtcars.\n\n <pre>\n> mtcars \n               mpg cyl disp  hp drat   wt ... \nMazda RX4     21.0   6  160 110 3.90 2.62 ... \nMazda RX4 Wag 21.0   6  160 110 3.90 2.88 ... \nDatsun 710    22.8   4  108  93 3.85 2.32 ... \n               ............\n</pre>\n\nThe top line of the table, called the header, contains the column names. Each horizontal line afterward denotes a data row, which begins with the name of the row, and then followed by the actual data. Each data member of a row is called a cell.\n\nTo retrieve data in a cell, we would enter its row and column coordinates in the single square bracket \"[]\" operator. The two coordinates are separated by a comma. In other words, the coordinates begins with row position, then followed by a comma, and ends with the column position. The order is important.\n\nHere is the cell value from the first row, second column of mtcars.\n\n <pre>\n> mtcars[1, 2] \n[1] 6\n</pre>\n\nMoreover, we can use the row and column names instead of the numeric coordinates.\n\n <pre>\n> mtcars[\"Mazda RX4\", \"cyl\"] \n[1] 6\n</pre>\n\nLastly, the number of data rows in the data frame is given by the nrow function.\n\n <pre>\n> nrow(mtcars)    # number of data rows \n[1] 32\n</pre>\n\nAnd the number of columns of a data frame is given by the ncol function.\n\n <pre>\n> ncol(mtcars)    # number of columns \n[1] 11\n</pre>\n\nFurther details of the mtcars data set is available in the R documentation.\n\n <pre>\n> help(mtcars)\n</pre>\n\n----\n\n==== Preview ====\n\nPreview\nInstead of printing out the entire data frame, it is often desirable to preview it with the head function beforehand.\n\n <pre>\n> head(mtcars) \n               mpg cyl disp  hp drat   wt ... \nMazda RX4     21.0   6  160 110 3.90 2.62 ... \n               ............\n</pre>\n\n----\n\n==== Data Import ====\n\n\n\n\n\n\n\n\nIt is necessary to import the sample textbook data into R before you start working on your homework.\n\n* Excel File\n: Quite often, the sample data is in Excel format, and needs to be imported into R prior to use. For this, we use the read.xls function from the gdata package. It reads from an Excel spreadsheet and returns a data frame. The following shows how to load an Excel spreadsheet named \"mydata.xls\". As the package is not in the core R library, it has to be installed and loaded into the R workspace.\n\n <pre>\n> library(gdata)                   # load the gdata package \n> help(read.xls)                   # documentation \n> mydata = read.xls(\"mydata.xls\")  # read from first sheet\n</pre>\n\n* Minitab File\n: If the data file is in Minitab Portable Worksheet format, it can be opened with the read.mtp function from the foreign package. It returns a list of components in the Minitab worksheet.\n\n <pre>\n> library(foreign)                 # load the foreign package \n> help(read.mtp)                   # documentation \n> mydata = read.mtp(\"mydata.mtp\")  # read from .mtp file\n</pre>\n\n* SPSS File\n: For the data files in SPSS format, it can be opened with the read.spss function from the foreign package. There is a \"to.data.frame\" option for choosing whether a data frame is to be returned.\n\n <pre>\n> library(foreign)                 # load the foreign package \n> help(read.spss)                  # documentation \n> mydata = read.spss(\"myfile\", to.data.frame=TRUE)\n</pre>\n\n* Table File\n: A data table can resides in a text file. The cells inside the table are separated by blank characters. Here is an example of a table with 4 rows and 3 columns.\n\n <pre>\n100   a1   b1 \n200   a2   b2 \n300   a3   b3 \n400   a4   b4\n</pre>\n\nNow copy and paste the table above in a file named \"mydata.txt\" with a text editor. Then load the data into the workspace with the read.table function.\n\n <pre>\n> mydata = read.table(\"mydata.txt\")  # read text file \n> mydata                             # print data frame \n   V1 V2 V3 \n1 100 a1 b1 \n2 200 a2 b2 \n3 300 a3 b3 \n4 400 a4 b4\n</pre>\n\nFor further detail of the read.table function, please consult the R documentation.\n\n <pre>\n> help(read.table)\n</pre>\n\n* CSV File\nThe sample data can also be in comma separated values (CSV) format. Each cell inside such data file is separated by a special character, which usually is a comma, although other characters can be used as well.\n\nThe first row of the data file should contain the column names instead of the actual data. Here is a sample of the expected format.\n\n <pre>\nCol1,Col2,Col3 \n100,a1,b1 \n200,a2,b2 \n300,a3,b3\n</pre>\n\nAfter we copy and paste the data above in a file named \"mydata.csv\" with a text editor, we can read the data with the read.csv function.\n\n <pre>\n> mydata = read.csv(\"mydata.csv\")  # read csv file \n> mydata                           # print data frame \n  Col1 Col2 Col3 \n1  100   a1   b1 \n2  200   a2   b2 \n3  300   a3   b3\n</pre>\n\nIn various European locales, as the comma character serves as decimal point, the read.csv2 function should be used instead. For further detail of the read.csv and read.csv2 functions, please consult the R documentation.\n\n <pre>\n> help(read.csv)\n</pre>\n\n----\n\n== ## bNote-2013-03-18 ==\n\n=== DailyPlan ===\n\n* list of candidate tasks\n\n:- Real IO trace 확보 작업\n::- 김혁호 책임이 보내준 업무요청 사례 기반, 업무요청 문서 작성할 것\n::- [V://j] NetApp (김주영 과장)에게 연락 -> 스토리지 모델 정보 요청 -> 추가 문의 (Management Console에서 보다 자세한 정보를 볼 수는 없는지? NetApp OnCommand System Manager 같은 Tool을 사용해서라도..) -> 김주영 과장과 통화 (오전 10:20 ~ 46), 이번 주 중에 찾아뵙고 말씀 나누는 걸로 ...\n::- EMC 연락\n::- [~] Dell (지근영 대리)에게 연락\n:::- Dell EqualLogic PS6100XV iSCSI arrays [http://www.dell.com/us/enterprise/p/equallogic-ps6100xv/pd] [http://www.equallogic.com/products/default.aspx?id=10661]\n\n:- IOWA ML Planning\n::- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n::- Hadoop case 고려 (in-local-node vertical placement between DRAM-SSD-HDD 뿐만 아니라 horizontal placement (among distributed nodes in Hadoop cluster)도 고려)\n\n:- IOWA ML Study\n::- IOWA:: Bayesian Network study\n::- IOWA:: Neural Network study\n::- IOWA:: HMM study\n::- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n\n:- Netflix의 Cloud Computing Challenge 내용 파악\n(심상무님 지시: 거기가 우리보다 앞서 있으니, 어떤 기술들이 필요한지, 이슈가 무엇인지에 대한 힌트를 얻을 수 있을 것임)\n\n:- page cache to be revisited\n\n\n----\n\n=== Linux File Systems: Ext2 vs. Ext3 vs. Ext4 ===\n\n* [http://www.thegeekstuff.com/2011/05/ext2-ext3-ext4/]\n\next2, ext3 and ext4 are all filesystems created for Linux. This article explains the following:\n\n:- High level difference between these filesystems.\n:- How to create these filesystems.\n:- How to convert from one filesystem type to another.\n\n==== Ext2 ====\n\n* Ext2 stands for second extended file system.\n* It was introduced in 1993. Developed by Remy Card.\n* This was developed to overcome the limitation of the original ext file system.\n* Ext2 does not have journaling feature.\n* On flash drives, usb drives, ext2 is recommended, as it doesn’t need to do the over head of journaling.\n* Maximum individual file size can be from 16 GB to 2 TB\n* Overall ext2 file system size can be from 2 TB to 32 TB\n\n\n==== Ext3 ====\n\n* Ext3 stands for third extended file system.\n* It was introduced in 2001. Developed by Stephen Tweedie.\n* Starting from Linux Kernel 2.4.15 ext3 was available.\n* The main benefit of ext3 is that it allows journaling.\n* Journaling has a dedicated area in the file system, where all the changes are tracked. When the system crashes, the possibility of file system corruption is less because of journaling.\n* Maximum individual file size can be from 16 GB to 2 TB\n* Overall ext3 file system size can be from 2 TB to 32 TB\n* There are three types of journaling available in ext3 file system.\n:- Journal ? Metadata and content are saved in the journal.\n:- Ordered ? Only metadata is saved in the journal. Metadata are journaled only after writing the content to disk. This is the default.\n:- Writeback ? Only metadata is saved in the journal. Metadata might be journaled either before or after the content is written to the disk.\n* You can convert a ext2 file system to ext3 file system directly (without backup/restore).\n\n\n==== Ext4 ====\n\n* Ext4 stands for fourth extended file system.\n* It was introduced in 2008.\n* Starting from Linux Kernel 2.6.19 ext4 was available.\n* Supports huge individual file size and overall file system size.\n* Maximum individual file size can be from 16 GB to 16 TB\n* Overall maximum ext4 file system size is 1 EB (exabyte). 1 EB = 1024 PB (petabyte). 1 PB = 1024 TB (terabyte).\n* Directory can contain a maximum of 64,000 subdirectories (as opposed to 32,000 in ext3)\n* You can also mount an existing ext3 fs as ext4 fs (without having to upgrade it).\n* Several other new features are introduced in ext4: multiblock allocation, delayed allocation, journal checksum. fast fsck, etc. All you need to know is that these new features have improved the performance and reliability of the filesystem when compared to ext3.\n* In ext4, you also have the option of turning the journaling feature “off”.\n\n\n----\n\n=== Frequent patterns/items mining examples with R::arules ===\n\n[[Association rule mining]]\n\n=== R ===\n\n* png file output work-around against \'plot() error\' in R\n:- [http://www.mail-archive.com/r-help@r-project.org/msg40658.html]\n <pre>\nThe png() device does not need an X server to connect to. I think it\nused to in versions gone by, but not any more. Here I\'ve disabled X so\nthat X11() doesn\'t work, but png() still does:\n\n > x11()\n Error in X11(d$display, d$width, d$height, d$pointsize, d$gamma,\nd$colortype,  :\n   unable to start device X11cairo\n In addition: Warning message:\n In x11() : unable to open connection to X11 display \'\'\n > png(file=\"foo2.png\")\n > plot(1:10)\n > dev.off()\n null device\n          1\n\n I suspect your R was compiled without png support. What does the\n\'capabilities()\' function in R tell you?\n\n > capabilities()\n    jpeg      png     tiff    tcltk      X11     aqua http/ftp  sockets\n    TRUE     TRUE     TRUE     TRUE    FALSE    FALSE     TRUE     TRUE\n  libxml     fifo   cledit    iconv      NLS  profmem    cairo\n    TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE\n</pre>\n\n\n\n==== An Introduction to R ====\n\n* [http://cran.r-project.org/doc/manuals/R-intro.html#The-read_002etable_0028_0029-function An Introduction to R - Table of Contents]\n\n\n==== R Tutorial - (http://www.r-tutor.com/) ====\n\n* [http://www.r-tutor.com/gpu-computing/gaussian-process/rvbm Bayesian Classification with Gaussian Process]\n* [http://www.r-tutor.com/content/r-tutorial-ebook R Tutorial with Bayesian Statistics Using OpenBUGS]\n* [http://www.r-tutor.com/bayesian-statistics/openbugs Bayesian Inference Using OpenBUGS]\n* [http://www.r-tutor.com/gpu-computing/svm/rpusvm-2 Support Vector Machine with GPU, Part II]\n\n==== R: Input and output: scripts, saving and loading data ((B.GOOD)) ====\n\n* [http://egret.psychol.cam.ac.uk/statistics/R/savingloading.html Cambridge University]\n\n\n* General file-handling commands\n <pre>\nsetwd(\"c:/myfiles\") # use / or \\\\ to separate directories under Windows (\\\\ becomes \\ once processed through the escape character mechanism)\ndir() # list the contents of the current directory\n</pre>\n\n\n* Running scripts\n <pre>\nsource(\"myfile.R\") # load and execute a script of R commands\n</pre>\n\n* For a startup script\n: edit \".Rprofile\" in your home directory (for details see ?Startup). Here\'s an example\n <pre>\n# RNC ~/.Rprofile\n\n# auto width adjustment\n.adjustWidth <- function(...){\n       options(width=Sys.getenv(\"COLUMNS\"))\n       TRUE\n}\n.adjustWidthCallBack <- addTaskCallback(.adjustWidth)\n\n.First <- function() cat(\"\\n   Script ~/.Rprofile executed.\\n\\n\")\n.Last <- function()  cat(\"\\n   Goodbye!\\n\\n\")\n</pre>\n\n\n* Redirecting output\n <pre>\nsink(\"myfile.txt\") # redirect console output to a file\nsink() # restore output to the screen\n\npdf(\"mygraph.pdf\") # subsequent graphical output will go to a PDF\npng(\"mygraph.png\") # subsequent graphical output will go to a PNG\njpeg(\"mygraph.jpeg\") # subsequent graphical output will go to a JPEG\nbmp(\"mygraph.bmp\") # subsequent graphical output will go to a BMP\npostscript(\"mygraph.ps\") # subsequent graphical output will go to a PostScript file\ndev.off() # back to the screen\n</pre>\n\n\n* Text files\n <pre>\nmy.data = read.csv(filename)\nmy.data = read.csv(file.choose())\n# Note: (1) = and <- are synonymous, and are the assignment operator (while == tests for equality)\n#       (2) file.choose() pops up a live filename picker\n#       (3) The default is to assume a header row with variable names (header=TRUE),\n#           and no row names, but you can change all these defaults (e.g. row.names=1 reads\n#           row names from the first column).\n\nattach(my.data) # you might then want to attach the new data to the path, though this is optional\n\nwrite.csv(my.data, filename2) # Write the data to a new file. There are several options available; see the help (use ?write.csv)\nwrite.csv(my.data, file=\"d:/temp/newfile.csv\", row.names=FALSE) # Here\'s one: turn off row names to avoid creating a spurious additional column.\n\nread.table(...)  # } A more generic way to read/write tabular data from/to disk\nwrite.table(...) # } (read.csv and write.csv are specialized versions of read.table and write.table)\n</pre>\n\n\n* Microsoft Excel spreadsheets\n <pre>\nlibrary(RODBC)\nchannel <- odbcConnectExcel(\"Osteomalacia_data.xls\") # specify the filename\npatientdata <- sqlFetch(channel, \"Vitamin_D_levels\") # specify a sheet within the spreadsheet\nindexcasedata <- sqlFetch(channel, \"Sheet2\") # by default Excel names individual sheets Sheet1, Sheet2, ..., though you may have renamed them something more informative\nodbcClose(channel)\n</pre>\n\n\n* SPSS data\n <pre>\nlibrary(foreign)\nmydata <- data.frame(read.spss(\"filename.sav\"))\n# Remember you can also use file.choose() in place of the filename, as above.\n</pre>\n\n\n* ODBC data sources (databases)\n <pre>\n# 1. Connect\nlibrary(RODBC)\nchannel <- odbcConnect(\"my_DSN\") # specify your DSN here\n# if you need to specify a username/password, use:\n#  channel <-odbcConnect(\"mydsn\", uid=\"username\", pwd=\"password\")\n\n# 2. List all tables\nsqlTables(channel)\n\n# 3. Fetch a whole table into a data frame\nmydataframe <- sqlFetch(channel, \"my_table_name\") # fetch a table from the database in its entirety\nclose(channel)\n\n# 4. Fetch the results of a query into a data frame. Example:\nmydf2 <- sqlQuery(channel, \"SELECT * FROM MonkeyCantab_LOOKUP_TaskTypes WHERE TaskType < 6\")\n</pre>\n\nIf you\'re using MySQL, you can talk to the database directly:\n <pre>\nlibrary(RMySQL) # use install.packages(\"RMySQL\") if this produces an error\n# if the install.packages() command produces an error, under Ubuntu:\n# use \"sudo apt-get install libmysql++-dev\" (in addition to MySQL itself, i.e. the\n# \"mysql-server mysql-client mysql-navigator mysql-admin\" packages)\ncon <- dbConnect(MySQL(), host=\"localhost\", port=3306, dbname=\"mydatabase\", user=\"myuser\", password=\"mypassword\")\ndbListTables(con)\ndbListFields(con, \"table_name\")\nd <- dbReadTable(con, \"table_name\")\ne <- dbGetQuery(con, \"SELECT COUNT(*) FROM table_name\")\n# and much more possible\n</pre>\n\n\n* R native format\n <pre>\nsave(myobject1, myobject2, ..., file=\"D:/temp/mydata.rda\")\nload(file=\"D:/temp/mydata.rda\")\n# note that the load command recreates the \"mydata\" object without prompting\n# you can also use save.image() to save a whole workspace\n</pre>\n\n\n* Other data-moving techniques\nTo export the definition of an R object (which you can then re-import using \"object = THISTHING\"):\n <pre>\ndput(object, \"\")\n</pre>\n\nTo read a tabular object with a header row from the clipboard\n <pre>\nobject = read.table(\"clipboard\", header=T)\n</pre>\n\n----\n\n=== Samsung SSD 840 Series Information ===\n\n* [http://thessdreview.com/our-reviews/samsung-840-series-240gb-ssd-review-the-worlds-first-tlc-ssd-takes-the-stage/4/ Samsung 840 Series 250GB SSD Review ? The Worlds First TLC SSD Takes Center Stage]\n\n* [http://www.techspot.com/review/578-samsung-840-pro-ssd/ Samsung 840 Pro SSD Review]\n\n----\n\n=== Gnuplot Tips ===\n\n\n* How to unset key [http://people.duke.edu/~hpgavin/gnuplot.html]\n <pre>\n      Create a title:                  > set title \"Force-Deflection Data\" \n      Put a label on the x-axis:       > set xlabel \"Deflection (meters)\"\n      Put a label on the y-axis:       > set ylabel \"Force (kN)\"\n      Change the x-axis range:         > set xrange [0.001:0.005]\n      Change the y-axis range:         > set yrange [20:500]\n      Have Gnuplot determine ranges:   > set autoscale\n      Move the key:                    > set key 0.01,100\n      Delete the key:                  > unset key\n      Put a label on the plot:         > set label \"yield point\" at 0.003, 260 \n      Remove all labels:               > unset label\n      Plot using log-axes:             > set logscale\n      Plot using log-axes on y-axis:   > unset logscale; set logscale y \n      Change the tic-marks:            > set xtics (0.002,0.004,0.006,0.008)\n      Return to the default tics:      > unset xtics; set xtics auto\n</pre>\n\n* Mouse and hotkey support in interactive terminals\n\n: Interaction with the current plot via mouse and hotkeys is supported for the X11, OS/2 Presentation Manager, ggi and Windows terminals. See `mouse input` for more information on mousing. See help for bind for information on hotkeys. Also see the documentation for individual mousing terminals `ggi`, `pm`, `windows` and `x11`.\n\n: Here are briefly some useful hotkeys. Hit \'h\' in the interactive interval for help. Hit \'m\' to switch mousing on/off. Hit \'g\' for grid, \'l\' for log and \'e\' for replot. Hit \'r\' for ruler to measure peak distances (linear scale) or peak ratios (log scale), and \'5\' for polar coordinates inside a map. Zoom by mouse (MB3), and move in the zoom history by \'p\', \'u\', \'n\'; hit \'a\' for autoscale. Use other mouse buttons to put current mouse coordinates to clipboard (double click of MB1), add temporarily or permanently labels to the plot (middle mouse button MB2). Rotate a 3D surface by mouse. Hit spacebar to switch to the gnuplot command window.\n\n: Sample script: mousevariables.dem\n\n* [http://www.gnuplot.info/docs_4.0/gnuplot.html#Mouse_and_hotkey_support_in_interactive_terminals Mouse and hotkey support in interactive terminals -- Gnuplot info]\n\n=== NetApp Storage System Management Software ===\n\n* NetApp OnCommand System Manager [http://www.netapp.com/us/products/management-software/system-manager.aspx]\n:\n\n== ## bNote-2013-03-15 ==\n\n=== DailyPlanning 2013-03-15 ===\n\n* list of candidate tasks\n\n:- [V] HML basic concept study, 오늘 AP 주제에 대해 lightreading\n\n:- [V] 엄교수님께 특허 일정 전달\n\n:- Real IO trace 확보 작업\n::- [V] 김혁호 책임과 미팅 > 13:30 미팅 수행 (업무요청하기로 함)\n::- NetApp, Dell, EMC 측과 연락\n::- 지근영 대리에게 연락\n\n:- IOWA:: Bayesian Network study\n:- IOWA:: Neural Network study\n:- IOWA:: HMM study\n:- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n\n:- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n\n:- page cache to be revisited\n\n:- Data Placement 이슈: Media Difference (RAM,SSD,HDD) 외에 어떤 이슈가 있는가? Hadoop 같은 경우 노드 간 수평적 이동 이슈 있음. 매우 중요.\n\n:- 계획 외 업무들\n::- 이주평전문님의 본사팀과의 Conference Call 위해, 상무님 회의 대신 참석 (차주 화요일: 소장님께보고, 목요일: 부원장님께보고), 액션아이템 이전문님과 팀원께 전달.\n::- AP 세미나 참석 (최희열 전문)\n::- 팀 미팅: 소장님보고 자료 대응 방안 논의 -> IO Prediction 기반의 time 차원 제어로 공간적인 IO 속도 제약 극복 (마치 SS랩의 cooperative caching case처럼)\n\n\n=== HML Study:: \"Reducing the Dimensionality of Data with Neural Networks\" ===\n\n* Gradient descent [http://en.wikipedia.org/wiki/Gradient_descent]\n\n:- Gradient descent is a first-order optimization algorithm [http://en.wikipedia.org/wiki/First-order_approximation]\n\n:- Gradient descent to find the local minimum, gradient ascent to find the local maximum\n:: Gradient descent를 이용하여 function의 local \'\'\'minimum\'\'\'을 찾아내기 위해서는, 현재 지점에서의 function의 \'\'\'negative\'\'\' of the gradient (or of the approximate gradient)에 비례하는 taking steps를 한다. 만약 \'\'\'positive\'\'\' of the gradient에 비례하여 taking step한다면 그 function의 local \'\'\'maximum\'\'\'에 다가가게 된다. 이러한 절차는 gradient ascent라고 한다.\n\n\n* Gradient [http://en.wikipedia.org/wiki/Gradient]\n: Vector calculus에서, scalar field의 gradient는 다음 조건을 만족하는 vector field이다.\n:: direction은 scalar field의 증가분 (rate of increase)이 가장 최대가 되는 방향이다\n:: magnitude는 그 증가분이 된다 {{ In vector calculus, the gradient of a scalar field is a vector field that points in the direction of the greatest rate of increase of the scalar field, and whose magnitude is that rate of increase. }}\n\n\n* Orders of approximation [http://en.wikipedia.org/wiki/First-order_approximation]\n: terms for how precise an approximation is.\n: to indicate progressively more refined approximations: in increasing order of precision, a zeroth order approximation, a first order approximation, a second order approximation, and so forth\n: (Formally) an nth order of approximation\n:: one where the order of magnitude of the error is at most x^n, 혹은 big O notation으로 나타낸다면, error는 O(x^n) 이다.\n: detailed explanation with examples\n::- Zeroth-order (constant; a flat line with no slope; a polynomial of degree 0)\n::: x = [0, 1, 2]\n::: y = [3, 3, 5]\n::: y ~ f(x) = 3.67\n::- First-order (a linear approximation; straight line with a slope; a polynomial of degree 1)\n::: x = [0, 1, 2]\n::: y = [3, 3, 5]\n::: y ~ f(x) = x + 2.67\n::- Second-order (a quadratic polynomial; geometrically, a parabola; a polynomial of degree 2)\n::: x = [0, 1, 2]\n::: y = [3, 3, 5]\n::: y ~ f(x) = x^2 - x + 3\n\n=== Memo ===\n\n* 엄교수님과 연락 내용\n\n:- 엄교수님께 특허 일정 전달 (2013-03-15, 10\n:: 교수님, 안녕하세요? 기술원의 정명준전문입니다. 특허일정을 알아본 결과 3월 29일까지 직무발명서 시스템 등록을 하면 된다고 합니다. 앞으로 2주 정도 여유가 있네요 ^^ 그동안 실험결과를 정리하고 핵심아이디어 및 청구항을 잘 정리하면 될 것 같습니다. 차주 목요일 쯤에 한 번 조박사와 통화하여 기술상세/청구항/기존특허비교/침해적발등을 같이 논의해보면 어떨까합니다만 교수님 보시기에는 어떠신지요? 오늘도 멋진 하루 보내시구요, 항상 감사합니다. 정명준 드림.\n\n=== 연락처 (자주 사용하는) ===\n\n* 기술원 이주평 전문 : 01025984182, 010-2598-4182, #9956 : jupyung.lee@samsung.com\n* 기술원 신현정 전문 : 0173249294, 017-324-9294, #9747 : pharoah@samsung.com\n* 기술원 서정민 전문 : 01025441231, 010-2544-1231, #9817 : tony.seo@samsung.com\n* 기술원 구본철 전문 : 01091905907, 010-9190-5907, #9704 : bc.gu@samsung.com \n* 기술원 유개원 전문 : : gaewon.you@samsung.com\n* 기술원 최희열 전문 : 01096236578, 010-9623-6578, #9692 : heeyoul.choi@samsung.com\n* 기술원 문민영 전문 : , , #9716 :\n* 기술원 최영상 전문 : , , #9951 :\n* 기술원 박상도 전문 : , , #9586 :\n* 기술원 전바롬 전문 : , , #9547 :\n* 기술원 송인철 전문 : , , #9962 :\n* 기술원 박정현 연구원 : , , #9238 :\n\n* 기술원 심은수 상무 : 01020518077, 010-2051-8077, #9950 : eunsoo.shim@samsung.com\n* 기술원 서영완 전문 : 01030020208, 010-3002-0208, #9843 : sywpro@samsung.com\n* 기술원 유연아 사원 : 01090338452, 010-9033-8452, #9858 : yeonah78.yu@samsung.com\n\n* 삼성 SDS ESDM 인프라그룹 이형주 차장님: _ : hj001.lee@partner.samsung.com\n\n* 기술원 에어컨 안나올 때 (기술원 통합 방재 센터, 과장, 지원팀 > 환경안전그룹): #9120 :\n* VDI (SBC) 문제 있을 때 (VDI HelpDesk): #8272 : \n* 네트워크 안될 때 (방화벽 등) 어디로?: :\n* CLMS 시스템 문의 - 한지연 선임 / 서초 인사 CI 그룹: :\n\n* 서울대 컴퓨터공학부 엄현상 교수님 : 0162324667, 016-232-4667, 02-880-6755 : hseom@cse.snu.ac.kr\n* 서울대 컴퓨터공학부 조인순 박사 : 01051317886, 010-5131-7886, 02-880-9330 : insoonjo@gmail.com\n* 서울대 컴퓨터공학부 성민영 석사과정 : 01047245304, 010-4724-5304 : mysung@dcslab.snu.ac.kr\n\n=== HML (Hierarchical Machine Learning) AP (Advanced Program) ===\n\n* 세미나 일정\n\n{| border=\"1\"\n| 이름\n| 논문제목\n| 날짜\n|-\n| 최희열\n| Reducing the dimensionality of data with neural networks [http://www.cs.toronto.edu/~hinton/science.pdf]\n| 03월 15일 \n|-\n| 민윤홍	\n| A fast learning algorithm for deep belief nets [http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf]\n| 03월 22일 \n|-\n| 성재모	\n| Graphical Models \n| 03월 29일 \n|-\n| 정명준	\n| Hierarchical Temporal Memory including HTM Cortical Learning Algorithms \n| 04월 05일 \n|-\n| 박상도 	\n| How to Grow a Mind: Statistics, Structure, and Abstraction	\n| 04월 12일\n|-\n| 전바롬	\n| Learning Hierarchical Models of Scenes, Objects, and Parts\n| 04월 19일\n|-\n| 이호섭\n| Building high-level features using large scale unsupervised learning\n| 4월 26일\n|-\n| 박정현\n| High-Performance Neural Networks for Visual Object Classification\n| 05월 03일\n|-\n| 이호식\n| Deep Neural Networks for Acoustic Modeling in Speech Recognition\n| 5월 10일\n|-\n| 이예하\n| Unsupervised feature learning for audio classification using convolutioinal deep belief networks\n| 05월 24일\n|-\n| 송인철\n| Multimodal Deep Learning\n| 05월 31일\n|-\n|}\n\n== ## bNote-2013-03-14 ==\n\n\n=== Hidden Markov model (HMM) ===\n\n[[Hidden Markov model (HMM)]]\n\n== ## bNote-2013-03-13 ==\n\n\n=== Supercom Usage Statistics ===\n\n <pre>\n----------------------------------------------------------\n| During : 20130304 ~ 20130310                            |\n| Username : a1mjjung , Application(Total jobs) : matlab(1)\n----------------------------------------------------------\nTotal RUN time : 2 min 16 secs\nAverage RUN time : 2 min 16 secs\nMaximum RUN time : 2 min 16 secs\nAverage Wait time 1 secs\nMaximum Wait time 1 secs\n ---------------------------------------------------------\n|                       Application(Total jobs) : unix(14)\n----------------------------------------------------------\nTotal RUN time : 13 min 32 secs\nAverage RUN time : 58 secs\nMaximum RUN time : 3 min 23 secs\nAverage Wait time 1 secs\nMaximum Wait time 2 secs\n ---------------------------------------------------------\n\n</pre>\n\n=== ACM Transactions on Storage ===\n\n삼성 SDS 강석우 상무님 요청으로 우리 팀이 Review하게 됨.\n\n* [http://mc.manuscriptcentral.com/tos Welcome to the ACM Transactions on Storage manuscript submission site]\n\n== ## bNote-2013-03-12 ==\n\n\n=== SNIA Real IO Traces ===\n\n\n----\n==== Microsoft Production MSNStorageFileServer ( msnfs ) ====\n\n* Summary (Reads/Writes - All)\n: 2008-03-10 01:00 + 6 hours\n: Total # of IOs\n:: = 29,345,085 (total)\n:: = 19,729,611 (reads) + 9,615,474 (writes)\n:: = 29345085 = 19729611 + 9615474\n: Average IOPS\n:: = 1358.56 (= 29345085 / (6 * 3600))\n: Average interval time between IOs\n:: = 736 micro-seconds (= (6 * 3600 * 10^6 ) / 29345085)\n\n\n* Summary (Reads)\n <pre>\na1mjjung@secm:[R] $ pwd\n/home/X0101/a1mjjung/x/iowa.sidewinder/sidewinder/microsoft_msn_filesrvr_6h/MSNStorageFileServer/tdir/iowa-mw30m/R\n\na1mjjung@secm:[R] $ grep __valu__sig__ f030.infile_R.iowa.anal_s0010 \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  17\n__valu__sig__ _n_o_sigaddrs :  173401\n__valu__sig__ _sigioc_acc :  5005517\n__valu__sig__ _sigaddrs_efficiency :  28.8667135714\n__valu__sig__ _n_o_addr_total :  5053016\n__valu__sig__ _ioc_total :  19729611\n</pre>\n\n\n* Trace Log Field Information\n <pre>\n\n       1,         2,                 3,        4,      5,          6,      7,           8,       9,       10,          11,      12,       13,         14,       15\nDiskRead, TimeStamp, ProcessName (PID), ThreadID, IrpPtr, ByteOffset, IOSize, ElapsedTime, DiskNum, IrpFlags, DiskSvcTime, I/O Pri,  VolSnap, FileObject, FileName\n\nDiskWrite, TimeStamp, ProcessName (PID), ThreadID, IrpPtr, ByteOffset, IOSize, ElapsedTime, DiskNum, IrpFlags, DiskSvcTime, I/O Pri, VolSnap, FileObject, FileName\n</pre>\n\n\n* [[Trace Data Preprocessing Screenshot - Microsoft Production Trace - MSN FileServer]]\n\n\n----\n\n==== MSR Cambridge IO Traces ====\n\n\n* Summary\n: 22.4GB Trace Data from Data center servers\n:: \'\'\'13 servers, 36 volumes, 179 disks, 1 week\'\'\'\n\n\n* Backgrounds\n: Many enterprise servers are less I/O intensive than TPC benchmarks, which are specifically designed to stress the system under test. Enterprise workloads also show significant variation in usage over time, for example due to diurnal patterns.\n: In order to understand better the I/O patterns generated by standard data center servers, we instrumented the core servers in our building\'s data center to generate per volume block-level traces for one week.\n\n\n* References\n:* \"Write Off-Loading: Practical Power Management for Enterprise Storage\" - FAST 2008 [http://www.usenix.org/event/fast08/tech/narayanan.html]\n::- Messages from this paper\n::: The traditional view is that enterprise workloads make spinning disks down ineffective because idle periods are too short. We analyzed block-level traces from 36 volumes in an enterprise data center for one week and concluded that significant idle periods exist, and that they can be further increased by modifying the read/write patterns using write off-loading. Write off-loading allows write requests on spun-down disks to be temporarily redirected to persistent storage elsewhere in the data center.\n:* \"Self-organizing Storage (SOS) Project - Software\" \n::- Tools: nfsdump/nfsscan [http://www.eecs.harvard.edu/sos/software/index.html]\n::- SOS Project Traces [http://www.eecs.harvard.edu/sos/traces.html]\n\n\n* Trace Log Field Names\n: Timestamp, Hostname, DiskNumber, Type(Read/Write), Offset, Size, ResponseTime\n:- Timestamp: the time the I/O was issued in \"Windows filetime\"\n:- Hostname: the hostname (should be the same as that in the trace file name)\n:- DiskNumber: the disknumber (should be the same as in the trace file name)\n:- Type: \"Read\" or \"Write\"\n:- Offset: starting offset of the I/O in bytes (from the start of the logical disk)\n:- Size: transfer size of the I/O request in bytes\n:- ResponseTime: time taken by the I/O to complete, in \"Windows filetime\"\n\n\n* Trace Log Example\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ head -100 rsrch_0.csv \n\n128166372003061629,rsrch,0,Read,7014609920,24576,41286\n128166372016382155,rsrch,0,Write,1317441536,8192,1963\n128166372026382245,rsrch,0,Write,2436440064,4096,1835\n128166372036348580,rsrch,0,Write,3196526592,57344,35436\n128166372036379390,rsrch,0,Write,3154132992,4096,4626\n128166372036382264,rsrch,0,Write,3154124800,4096,1752\n128166372053100669,rsrch,0,Write,7609925632,10240,2053\n128166372053101032,rsrch,0,Write,15282630656,16384,1691\n128166372053101054,rsrch,0,Write,7612473344,16384,1668\n</pre>\n\n\n* IO Trace Nodes\n{| border=\"1\"\n| Node\n| Description\n| # of volumes\n| # of IOs\n|-\n| usr\n| User home directories\n| 3\n|-\n| proj\n| Project directories\n| 5\n|-\n| prn\n| Print server\n| 2\n|-\n| hm\n| Hardware monitoring\n| 2\n|-\n| rsrch\n| Research projects\n| 3\n|-\n| prxy\n| Firewall/WebProxy\n| 2\n|-\n| src1\n| Source control\n| 3\n|-\n| src2\n| Source control\n| 3\n|-\n| stg\n| Web staging\n| 2\n|-\n| ts\n| Terminal server\n| 1\n|-\n| web\n| Web/SQL server\n| 4\n|-\n| mds\n| Media server\n| 2\n|-\n| wdev\n| Test web server\n| 4\n|-\n|}\n\n\n* # of IOs\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ wc -l *.csv\n    3993316 hm_0.csv\n     609311 hm_1.csv\n    1211034 mds_0.csv\n    1637711 mds_1.csv\n    5585886 prn_0.csv\n   11233411 prn_1.csv\n    4224524 proj_0.csv\n   23639742 proj_1.csv\n   29266482 proj_2.csv\n    2244644 proj_3.csv\n    6465639 proj_4.csv\n   12518968 prxy_0.csv\n  168638964 prxy_1.csv\n    1433655 rsrch_0.csv\n      13780 rsrch_1.csv\n     207587 rsrch_2.csv\n   37415613 src1_0.csv\n   45746222 src1_1.csv\n    1907773 src1_2.csv\n    1557814 src2_0.csv\n     657774 src2_1.csv\n    1156885 src2_2.csv\n    2030915 stg_0.csv\n    2196861 stg_1.csv\n    1801734 ts_0.csv\n    2237889 usr_0.csv\n   45283980 usr_1.csv\n   10570046 usr_2.csv\n    1143261 wdev_0.csv\n       1055 wdev_1.csv\n     181266 wdev_2.csv\n        682 wdev_3.csv\n    2029945 web_0.csv\n     160891 web_1.csv\n    5175368 web_2.csv\n      31380 web_3.csv\n  434212008 total\n</pre>\n\n\n* List of traces\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ l\ntotal 22956880\ndrwxrwxr-x 2 a1mjjung X0101       4096 Mar 12 16:07 ./\ndrwxrwxr-x 3 a1mjjung X0101       4096 Mar 12 16:04 ../\n-r--r--r-- 1 a1mjjung X0101       1262 Oct 31  2008 DISCLAIMER.txt\n-r--r--r-- 1 a1mjjung X0101       1712 Oct 31  2008 MD5.txt\n-r--r--r-- 1 a1mjjung X0101       1815 Oct 31  2008 README.txt\n-r--r--r-- 1 a1mjjung X0101  202270441 Oct 30  2008 hm_0.csv\n-r--r--r-- 1 a1mjjung X0101   29765989 Oct 30  2008 hm_1.csv\n-r--r--r-- 1 a1mjjung X0101   63955502 Oct 30  2008 mds_0.csv\n-r--r--r-- 1 a1mjjung X0101   88337038 Oct 30  2008 mds_1.csv\n-r--r--r-- 1 a1mjjung X0101  291595716 Oct 30  2008 prn_0.csv\n-r--r--r-- 1 a1mjjung X0101  597136927 Oct 30  2008 prn_1.csv\n-r--r--r-- 1 a1mjjung X0101  233038754 Oct 30  2008 proj_0.csv\n-r--r--r-- 1 a1mjjung X0101 1305533029 Oct 30  2008 proj_1.csv\n-r--r--r-- 1 a1mjjung X0101 1614727432 Oct 30  2008 proj_2.csv\n-r--r--r-- 1 a1mjjung X0101  119913539 Oct 30  2008 proj_3.csv\n-r--r--r-- 1 a1mjjung X0101  350117046 Oct 30  2008 proj_4.csv\n-r--r--r-- 1 a1mjjung X0101  658840568 Oct 30  2008 prxy_0.csv\n-r--r--r-- 1 a1mjjung X0101 9043988744 Oct 30  2008 prxy_1.csv\n-r--r--r-- 1 a1mjjung X0101   77717781 Oct 31  2008 rsrch_0.csv\n-r--r--r-- 1 a1mjjung X0101     755814 Oct 31  2008 rsrch_1.csv\n-r--r--r-- 1 a1mjjung X0101   11154823 Oct 31  2008 rsrch_2.csv\n-r--r--r-- 1 a1mjjung X0101 2077380082 Oct 31  2008 src1_0.csv\n-r--r--r-- 1 a1mjjung X0101 2536095762 Oct 31  2008 src1_1.csv\n-r--r--r-- 1 a1mjjung X0101  101236500 Oct 31  2008 src1_2.csv\n-r--r--r-- 1 a1mjjung X0101   82511780 Oct 31  2008 src2_0.csv\n-r--r--r-- 1 a1mjjung X0101   35607343 Oct 31  2008 src2_1.csv\n-r--r--r-- 1 a1mjjung X0101   63026546 Oct 31  2008 src2_2.csv\n-r--r--r-- 1 a1mjjung X0101  105682669 Oct 31  2008 stg_0.csv\n-r--r--r-- 1 a1mjjung X0101  116358242 Oct 31  2008 stg_1.csv\n-r--r--r-- 1 a1mjjung X0101   93309044 Oct 31  2008 ts_0.csv\n-r--r--r-- 1 a1mjjung X0101  118478959 Oct 31  2008 usr_0.csv\n-r--r--r-- 1 a1mjjung X0101 2451360295 Oct 31  2008 usr_1.csv\n-r--r--r-- 1 a1mjjung X0101  574047026 Oct 31  2008 usr_2.csv\n-r--r--r-- 1 a1mjjung X0101   60262085 Oct 31  2008 wdev_0.csv\n-r--r--r-- 1 a1mjjung X0101      56014 Oct 31  2008 wdev_1.csv\n-r--r--r-- 1 a1mjjung X0101    9593588 Oct 31  2008 wdev_2.csv\n-r--r--r-- 1 a1mjjung X0101      35650 Oct 31  2008 wdev_3.csv\n-r--r--r-- 1 a1mjjung X0101  107571350 Oct 31  2008 web_0.csv\n-r--r--r-- 1 a1mjjung X0101    8507311 Oct 31  2008 web_1.csv\n-r--r--r-- 1 a1mjjung X0101  276121116 Oct 31  2008 web_2.csv\n-r--r--r-- 1 a1mjjung X0101    1649607 Oct 31  2008 web_3.csv\n</pre>\n\n\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ wc -l *.csv\n    3993316 hm_0.csv\n     609311 hm_1.csv\n    1211034 mds_0.csv\n    1637711 mds_1.csv\n    5585886 prn_0.csv\n   11233411 prn_1.csv\n    4224524 proj_0.csv\n   23639742 proj_1.csv\n   29266482 proj_2.csv\n    2244644 proj_3.csv\n    6465639 proj_4.csv\n   12518968 prxy_0.csv\n  168638964 prxy_1.csv\n    1433655 rsrch_0.csv\n      13780 rsrch_1.csv\n     207587 rsrch_2.csv\n   37415613 src1_0.csv\n   45746222 src1_1.csv\n    1907773 src1_2.csv\n    1557814 src2_0.csv\n     657774 src2_1.csv\n    1156885 src2_2.csv\n    2030915 stg_0.csv\n    2196861 stg_1.csv\n    1801734 ts_0.csv\n    2237889 usr_0.csv\n   45283980 usr_1.csv\n   10570046 usr_2.csv\n    1143261 wdev_0.csv\n       1055 wdev_1.csv\n     181266 wdev_2.csv\n        682 wdev_3.csv\n    2029945 web_0.csv\n     160891 web_1.csv\n    5175368 web_2.csv\n      31380 web_3.csv\n  434212008 total\n</pre>\n\n=== Real IO Trace 수집 (삼성 SDS 강석우 상무) ===\n\n* 삼성SDS 강석우 상무 (클라우드 플랫폼 팀장)\n\n* 삼성 SDS 박성록 수석보 (클라우드 플랫폼 운영그룹)\n\n* 진행 현황\n\n:* EMC\n::- 박정원 과장에게 연락함. 담당자인 이임호 부장 소개해줌.\n::- 이임호 부장은 아직 연락 못함\n\n:* Dell\n::- 지근영 대리와 통화/메일 (TraceLog 요청사항을 Dell에게 전달하겠다고 함)\n:::- Trace Log 데이터 예제 전달\n\n:* NetApp\n::- 김주영 과장과 통화/메일\n:::- Trace Log 데이터 예제 전달\n\n:* Supercom 센터\n\n\n* 스토리지 상주 지원 인력\n:* EMC\n::- 노영변부장 : 010-5384-6426 (EMC KOREA 삼성 PMO)\n::- 박정원과장 : 010-9052-7805 (EMC KOREA 센터 상주지원)(연락하였음)\n:::- Phone call, IO trace 수집에 대해 설명 -> 담당자 연결 시켜줌 (이임호 부장)\n::- 이임호 부장: 010-3203-7823 (EMC 삼성전담, 프리세일즈 기술컨설턴트)\n:::- Not yet connected (another phone call)\n:* Dell\n::- 이정민차장 : 010-2908-0759 \n::- 지근영대리 : 010-9530-2819 (한빛마이크로 센터상주지원)\n:::- DELL 기술지원 업무(수원ICT센터) 상주 : (6층 전산실 DELL EQL스토리지 기술지원)\n:::- 연락처: <dell.korea@samsung.com> (443-803  경기?수원시?영통구?매탄3동 410-1 삼성SDS 수원ICT S/W연구소 4층)\n:::- 3/12 연락하였음. (전화/메신저/메일로 상황 설명 하였으며, 현재 Dell에 요청 전달된 상태임)\n:* NetApp(상주지원 없음)\n::- 최병석이사 : 010-8998-7138(NetApp Korea)\n::- 김주영과장 : 010-9577-4272 (아리라)\n:::- Email sent, Phone call\n\n <pre>\n\n\n------- Original Message -------\n\nSender : 강석우<sukwkang@samsung.com> 상무/팀장/클라우드플랫폼팀/삼성SDS\n\nDate : 2013-03-08 14:06 (GMT+09:00)\n\nTitle : Re: Re: Fwd: 스토리지 벤더 현황입니다.\n\n \n\n아마 스토리지 벤더 별로 자체 테스트 시스템이 있기 때문에 테스트 시스템에서 로그수집이 가능할 겁니다. 아니면 본사에서 이미 가지고 있을수도 있구요.\n\n \n\n강석우 드림\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-03-08 14:02 (GMT+09:00)\n\nTitle : Re: Fwd: 스토리지 벤더 현황입니다.\n\n \n\n대단히 감사합니다, 강 상무님.\n\n \n\n심은수 드림\n\n \n\n------- Original Message -------\n\nSender : 강석우<sukwkang@samsung.com> 상무/팀장/클라우드플랫폼팀/삼성SDS\n\nDate : 2013-03-08 13:58 (GMT+09:00)\n\nTitle : Fwd: 스토리지 벤더 현황입니다.\n\n \n\n심상무님,\n\n \n\n아래의 스토리지 벤더에 연락해서 요청을 하시면 됩니다. SDS의 클라우드 팀 강석우 상무 소개로 연락했다고 말씀하시구요. 만약 협조를 잘 안하면 저에게 다시 연락주세요. 제가 협조하도록 만들겠습니다. :-)\n\n \n\n강석우 드림\n\n \n\n------- Original Message -------\n\nSender : 박성록<rocky@samsung.com> 수석보/클라우드플랫폼운영그룹/삼성SDS\n\nDate : 2013-03-08 13:48 (GMT+09:00)\n\nTitle : 스토리지 벤더 현황입니다.\n\n \n\n \n\n안녕하십니까?  클라우드플랫폼운영그룹 박성록수석보입니다.\n\n \n\n스토리지 상주 지원 인력입니다.\n\n1. EMC \n\n  - 노영변부장 : 010-5384-6426 (EMC KOREA 삼성 PMO)\n\n  - 박정원과장 : 010-9052-7805  (EMC KOREA 센터 상주지원)\n\n \n\n2. Dell\n\n  - 이정민차장 : 010-2908-0759 \n\n  - 지근영대리 : 010-9530-2819 (한빛마이크로 센터상주지원)\n\n \n\n3. NetApp(상주지원 없음)\n\n - 최병석이사 : 010-8998-7138(NetApp Korea)\n\n - 김주영과장 : 010-9577-4272 (아리라)\n  \n</pre>\n\n=== Multimedia Streaming vs. IOWA-based PDP ===\n\n* eMBMS: Evolved Multimedia Broadcast Multicast Service\n: Expway\'s eMBMS [http://blog.expway.com/ Expway\'s eMBMS Solution Allows Mobile Operators to off-Load Mobile Traffic by 20%]\n:- Key Technical Features\n::- FLUTE (File Delivery over Unidirectional Transport) protocol\n::- Forward Error Correction\n::- File Repair\n::- Service Announcement\n::- DASH Video Protocol\n:- Mobile Traffic Prediction (in 2016)\n::- 70% of mobile traffic will be video\n::- 10% of all TV viewing will be on tablets\n::- This equauls to 25 million DVDs sent every single hour\n:- Expway Company\n::- 7 years of experience focused on mobile broadcast software\n:::- Robust and Mature Products\n:::- Optimized Bandwidth Usage\n:::- Low Footprint Terminal Stack\n\n\n\n* DASH: Dynamic Adaptive Streaming over HTTP (a.k.a MPEG-DASH)\n: Internet 상으로 media content에 대한 고품질 streaming을 가능하게 하는 기술 (기존 HTTP 웹서버들로부터 deliver됨)\n: 컨텐츠를 small HTTP-based file segement들로 쪼개어 다룬다는 점에서 Apple의 HTTP Live Streaming (HLS)와 유사하다고 볼 수 있음. (컨텐츠 예: movie, sports event의 live broadcast 등)\n\n\n* HTTP Live Streaming (HLS): HTTP-based media streaming communications protocol (QuickTime과 iOS software의 일부로 Apple이 구현함)\n:- 동작원리\n:: overall stream을 작은 HTTP-based file downloads로 쪼개어 다룬다 (각각의 download는 overall potentially unbounded transport stream의 하나의 short chunk를 담당). stream 세션 시작 시에는, available한 variouis sub-stream들에 대한 metadata를 포함하고 있는 extended M2U (m3u8) playlist를 download한다.\n:- 장점\n:: 표준화된 HTTP transaction만을 사용하기 때문에, HLS는 일반 HTTP traffic을 허용하는 firewall, proxy server들은 모두 통과 가능 (RTP와 같은 UDP 기반 프로토콜은 그렇지 못함)하며, 널리 사용 가능한 CDN 인프라를 통해서 쉽게 deliver될 수 있음.\n:- 특징\n:: AES와 같은 암호화 메커니즘 및 HTTPS 기반의 secure key distribution 방법, simple DRM 시스템을 제공함\n:: HLS의 이후 버전에서는 [[trick mode]][http://en.wikipedia.org/wiki/Trick_mode] 기반의 fast-forward/rewind 및 subtitle의 통합도 지원할 예정임 (2013-03-12 현재)\n:- 표준화\n:: Apple에서는 HLS (HTTP Live Streaming)를 Internet Draft로 작성하였음. (first stage in the process of submitting it to the IETF, as an Informational Request For Comments)\n\n\n\n=== Technical Articles ===\n\n* 상무님께서 보내주신 \"Future of Cloud Storage\" 메일에 대한 신전문님 정리\n\n아래 글들을 읽은 소감 or 요약입니다.\n글들이 이것저것 다양하네요.\n\n* [http://gigaom.com/2011/10/29/the-future-beyond-the-cloud-is-in-our-hands/ The future beyond the cloud is in our hands]\n\n: \"Cloud is all\"의 관점은 mobile device가 ubiquitous, unlimited, low-cost conntection 인데요.. 이런 생각은 현재의 mobile network의 한계와 mobile device의 빠른 발전까지 고려치 않은 비전이라는 비판입니다.\n\n:# 첫째, LTE 같은 mobile network이 향후 cloud를 모두 책임지지는 못한다는 것이고요. (WAN에서의 Bandwidth란 이미 wireless network을 사용하고 있는 user가 쓰는 용량으로 계산된 것이므로)\n:# 둘째, 향후 mobile은 n-core의 multi-gigaherts procesor와 1TB 이상의 local storage 이므로 이를 cloud에서 활용하자는 얘기입니다.\n\n: 예전의 장수석님의 Mobile Cloud 과제가 생각납니다. ^^; Mobile의 능력과 wireless network를 활용하자는 겁니다.\n::- what if those devices can talk to one another in a peer-to-peer or mesh network? \n::- What’s the aggregate power and capability of billions of these things, especially if there will be ways for them to work with and talk to one another both alone and in conjunction with cloud-based services?\n\n: 예로 든 것이 Amazon Silk과 Google의 Offline Mail입니다.\n::- Silk는 클라우드를 이용해 acceleration하는 브라우저입니다. 클라우드에서 사용자의 웹 패턴을 분석하여 미리 preloading하고 mobile에 최적화하여 속도를 높이는 건데요.역으로 mobile에 맞게 웹페이지를 작게 축소해서 mobile에 가져오기 때문에 클라우드가 동작하지 않더라도 속도를 유지시킬 수 있다고 합니다. 클라우드와 mobile간에 서로 보완하는 거지요.\n::- offline Mail은 말그대로 메일서버가 되지 않아도 전송외에 모든 메일 관리가 가능하도록 mobile에 데이터를 미리 다 가져다 놓는 겁니다.\n \n\n* [http://blog.cloudhq.net/post/36559564253/the-future-of-cloud-storage The Future of Cloud Storage]\n\n: 정확히는 이런 문제점이 나올 거다라는 cloud issue에 관한 prediction이라고 볼 수 있습니다.\n\n:# End of Files and Folders : 클라우드 서비스들이 전통적인 File이나 folder 개념을 사용하지 않고 자기만의 interface, 데이터 분류 방식을 쓰므로 나중에 cloud 간의 호환성 문제가 발생함\n:# End of Free Storage: cloud storage 시장이 mature된다면 결국 free storage는 없어질 거다. 지금은 홍보용인 거다.\n:# Data ownership Troubles : 클라우드에 데이터가 올라간 순간 사용자는 data에 대한 control를 잃어버린다.\n:# Encryption will become necessary : encryption이 필수..\n\n\n* [http://www.cs.berkeley.edu/~kubitron/talks/printable/CISCO-cloudstore.pdf Lessons from the OceanStore Project - UC Berkeley]\n\n: 첫번째 글에서 좀 더 나아간 형태..Cloud 시대를 맞이한 P2P의 재조명 되겠습니다. 과거 P2P가 공짜 미디어를 훔치는 수단이었다면 새로운 P2P는 extreme scale를 제공할 수 잇는 system의 새 디자인으로 사용할 수 있다는 얘기입니다. 여러 Client뿐 아니라 여러 Cloue Storage Provider들도 같이 참여하면 서로 윈윈할 수 있다는 얘기입니다.\n\n \n* [http://www.cloudsigma.com/blog/13-the-future-of-cloud-storage The Future of Cloud Storage (and what is wrong with the present)]\n: SAN도 local Storage도 이제 끝났다.  Distributed Replicated Block Device(DRBD) 라고 얘기하고 있습니다만, converged server+storage 형태로 Server 노드에 Storage까지 합체한 형태로 죽 붙이고 replication을 다른 node에 함으로써 latency, fail over 등의 Converged architecture장점을 얘기하고 있네요. Open Solution으로는 sheepdog이 있고 상용화버전으로는 Amplidata가 있다고 합니다.\n::>> open source로서 Linux Kernel (2.6.33 version 부터) 에 구현되어 있는 DRBD도 있음. (아래 그림 참고) HA (High Availability) 제공에 초점이 맞춰져 있고, replication mode도 fully-synchronous와 asynchronous mode, 그리고, 그 사이에, protection level과 performance 간의 tradeoff를 고려한, semi-synchronous mode (memory synchronous mode 라고도 합니다)가 지원됨.  이 Linux의 DRBD는, 우리 RACS 1 의 기술이 networked storage로 확장될 때 매우 유용한 기술적 base가 될 수 있을 것 같습니다. (마치 Local Disk Array에 대한 RACS 1이 Linux MD를 활용하여 구현되고 있는 것처럼, Networked Replicated Disk Array 기술 구현 시에 Linux DRBD를 잘 활용할 수도 있을 것임) [http://www.ibm.com/developerworks/linux/library/l-drbd/index.html High availability with the Distributed Replicated Block Device]\n\n <pre>\n\n------- Original Message -------\n\nDate : 2013-03-11 10:44 (GMT+09:00)\nTitle : Fwd: future of cloud storage\n\nFYI, \n\n저희 궁극의 시나리오가 \'무한대의 local 저장용량을 제공하는\' pervasive storage 쪽으로 잡히면서,\n상무님이 cloud storage의 핵심기술에 대한 깊은 이해를 요구하고 계십니다.\n아래 메일도 참고하시고 시간을 내어 관련 기술에 대한 study를 진행하도록 하겠습니다.\n\n\n------- Original Message -------\nDate : 2013-03-08 20:55 (GMT+09:00)\nTitle : future of cloud storage\n\n이 전문,\n\n\n아마 이 전문도 구글 검색하면 금방 찾을 글들일텐데, 하여간 내가 본 것들입니다.\n바로 아래 것은 많은 시사점을 주는 것 같습니다.\n\nhttp://gigaom.com/2011/10/29/the-future-beyond-the-cloud-is-in-our-hands/\n\n \n그 외의 글들.\n\nhttp://blog.cloudhq.net/post/36559564253/the-future-of-cloud-storage\n\nhttp://www.cs.berkeley.edu/~kubitron/talks/printable/CISCO-cloudstore.pdf\n\nhttp://www.cloudsigma.com/blog/13-the-future-of-cloud-storage\n\n \n우리가 pervasive storage로 서비스 시나리오를 잡은 만큼, 그 분야의 서비스/기술 발전 전망을 할 수 있어야겠습니다.\n</pre>\n\n== ## bNote-2013-03-11 ==\n\n\n=== Akamai - CDN Acceleration ===\n\n* 관련 기사들\n:# [[아카마이, \"쌩쌩 웹사이트 만들려면\"]] [http://www.bloter.net/archives/141513 bloter.net, 2013-01-24]\n:# [[아카마이, \'블레이즈\' 인수 - 인텔리전트 플랫폼 강화]] [http://www.bloter.net/archives/95561 bloter.net, 2012-02-09]\n:# [[아카마이, \"CDN 넘어 하이퍼커넥티드로\"]] [http://www.bloter.net/archives/92711 bloter.net, 2012-01-19]\n:# [[아카마이, CDN 장악 가속화 ... 코텐도 인수설]] [http://www.bloter.net/archives/85622 bloter.net, 2011-11-28]\n:# [[아카마이, \"스트리밍/보안 서비스를 빠르고 안전하게\"]] [http://www.bloter.net/archives/67854 bloter.net, 2011-07-13]\n:# [[네트워크 업계 \"기다렸다, 런던올림픽\"]] [http://www.bloter.net/archives/92823 bloter.net, 2012-01-22]\n\n\n=== 과제 목표 설정 ===\n\n* IOWA-based PDP 과제 목표 metric\n: 경쟁사, 기술원 현수준, 기술원 목표수준, 접근방식 등\n:- 필수 고려 사항\n:: 어째서 그러한 목표 수준을 잡았는지?\n:- 점검 사항\n:: EMC FAST 등 Automatic Tiering 기술의 현수준 파악 필요\n:- 접근 방식의 독창성/진보성\n::- ProactiveDP가 MWC 2013에 언급된 eMBMS, DASH 등과 어떤 차별점을 갖는가?\n:::- eMBMS (Evolved Multimedia Broadcast Multicast Service): LTE를 이용해 수많은 사용자에게 방송 컨텐츠를 동시에 효과적으로 배포하는 기술임. 스트리밍 전송 및 비 피크타임에 전송해 단말에 저장된 형태로 있다가 사용자가 원할 때 시청. <span style=\"color:blue\">level of intelligence</span>가 중요한 비교점이 될 수 있음.\n:::- DASH (Dynamic Adaptive Streaming over HTTP)\n::- CDN (Content Delivery Network)에서 Akamai와는 어떻게 차별되나?\n\n* 기술 진화 고민\n: evolution of technology as a driving force from old-age to the pervasive storage\n\n=== IOWA: bpo_a.20130305_104633.real_whole_trace.log ===\n\n* Trace information\n:- Machine under IOTracing: radiohead (Linux 3.2.0-34)\n:- Tracing time: 72 hours\n:- Trace log file: /x/var/iowa/sidewinder/iowa/preproc/tdir/myrealtrace/bpo_a.20130305_104633.real_whole_trace.log\n\n==== further Write pattern analysis (LBA-to-name processing, for top 18 addresses) ====\n\n* IO statistical summary\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  1115\n__valu__sig__ _n_o_sigaddrs :  18\n__valu__sig__ _sigioc_acc :  40010\n__valu__sig__ _sigaddrs_efficiency :  2222.77777778\n__valu__sig__ _n_o_addr_total :  40979\n__valu__sig__ _ioc_total :  157632\n</pre>\n\n\n* top 18 addresses\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__list__hitnum_and_addrs__.log.v1 | sort -n  | tail -20\n1093 : [1266683272]\n1105 : [1267153912]\n1113 : [1661223296]\n\n---- top 18 addr starts below ----\n1115 : [12856320, 12857344]\n1237 : [2480]\n1686 : [1266681984]\n1693 : [1266683032]\n1730 : [1266683160]\n1732 : [2352]\n1941 : [1661223328]\n2077 : [2048]\n2110 : [1267095912]\n2188 : [327569400]\n2265 : [1266683048]\n2309 : [327568408]\n2342 : [327568936]\n2364 : [1661223320]\n2395 : [1661223136]\n2526 : [1401209744]\n7185 : [1661223128]\n</pre>\n\n\n* Bar Graph for Hits_per_Addr (MyRealTrace, Radiohead, 72h)\n<!-- [[File:Myrealtrace on radiohead 72h cdst hits per addr impulses.v01.png | 500px]] -->\n[[File:Myrealtrace on radiohead 72h cdst hits per addr impulses.v01.png]]\n\n\n===== analysis table =====\n\n{| border=\"1\"\n| address\n| # of hits\n| device node\n| process accessed\n| periodicity (1000 IOs)\n| corresponding file/dir\n| notes\n|-\n| 1661223128\n| 7185\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.964271213967\n| /home/hendrix/.config/google-chrome/Default/Cookies-journal\n| inode (39585621)\n|-\n| 1401209744\n| 2526\n| (8,1) /dev/sda1\n| BrowserBlocking\n| 0.926207876573\n| /home/hendrix/.config/google-chrome/Default/Local Storage/https_plus.google.com_0.localstorage-journal\n| inode (39714913)\n|-\n| 1661223136\n| 2395\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.957927324401\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 1661223320\n| 2364\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.957927324401\n| /home/hendrix/.config/google-chrome/Default/Cookies-journal\n| inode (39585621)\n|-\n| 327568936\n| 2342\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.938895655704\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 327568408\n| 2309\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.951583434836\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 1266683048\n| 2265\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.957927324401\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 327569400\n| 2188\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.894488428745\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 1267095912\n| 2110\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.951583434836\n| /home/hendrix/.config/google-chrome/Default/Cookies-journal\n| inode (39585621)\n|-\n| 2048\n| 2077\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.94523954527\n| INVALID BLOCK\n| inode (N/A)\n|-\n| 1661223328\n| 1941\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.957927324401\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 2352\n| 1732\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.957927324401\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 1266683160\n| 1730\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.938895655704\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 1266683032\n| 1693\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.964271213967\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 1266681984\n| 1686\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.957927324401\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 2480\n| 1237\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.951583434836\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 12856320\n| 1115\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.94523954527\n| /home/hendrix/.cache/google-chrome/Default/Cache/data_1\n| inode (39585855)\n|-\n| 12857344\n| 1115\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.94523954527\n| /home/hendrix/.cache/google-chrome/Default/Cache/index\n| inode (39585853)\n|-\n|}\n\n===== analysis result (processing output) =====\n\n <pre>\nblusjune@radiohead:[top_hot_18] $ l\ntotal 16\ndrwxrwxr-x 2 blusjune blusjune 4096 Mar 11 18:44 ./\ndrwxrwxr-x 3 blusjune blusjune 4096 Mar 11 14:49 ../\n-rwxr-xr-x 1 blusjune blusjune 2566 Mar 11 18:40 .bdx.0100.y.lba_to_name.sh*\n-rw-rw-r-- 1 blusjune blusjune 1768 Mar 11 18:44 .conf.lba_to_name.sh\n\nblusjune@radiohead:[top_hot_18] $ _BDX \nBDX[ /x/var/iowa/tdir/s05/w_ptrn_analysis/top_hot_18 ]# 0100 : lba_to_name\n\n\'_conf__target_lba_list\' is from:\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ tail -29 __simout.sigio_25.iowsz_100.t1_1000.__list__hitnum_and_addrs__.log.v2\n708 : [1795164168]\n840 : [12856336]\n841 : [1270876464]\n842 : [1661223968]\n913 : [1266682992]\n929 : [1270876456]\n943 : [1266681864]\n949 : [1266751536]\n956 : [1266751544]\n1093 : [1266683272]\n1105 : [1267153912]\n1113 : [1661223296]\n1115 : [12856320, 12857344]\n1237 : [2480]\n1686 : [1266681984]\n1693 : [1266683032]\n1730 : [1266683160]\n1732 : [2352]\n1941 : [1661223328]\n2077 : [2048]\n2110 : [1267095912]\n2188 : [327569400]\n2265 : [1266683048]\n2309 : [327568408]\n2342 : [327568936]\n2364 : [1661223320]\n2395 : [1661223136]\n2526 : [1401209744]\n7185 : [1661223128]\n\n#>> configuration started\n#<< _conf__target_dev (e.g., /dev/sda) : /dev/sda\n#<< _conf__target_dev_part (e.g., /dev/sda1) : /dev/sda1\n----\n/dev/sda1   *        2048  3907026943  1953512448   83  Linux\nBlock size:               4096\n----\n#<< _conf__lba_fs_start: 2048\n#<< _conf__fblk_size: 4096\n#<< _conf__sector_size [512]: \n#>> configuration completed\n\n#>> START Processing\n/dev/sda1: 1795164168 -> _EXCEPTION_ # inode for 224395265 is NOT FOUND -- Skip processing\n/dev/sda1: 12856336 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_1 # DEV:LBA:FBLK:INODE( /dev/sda1 : 12856336 : 1606786 : 39585855 )\n/dev/sda1: 1270876464 -> _EXCEPTION_ # inode for 158859302 is NOT FOUND -- Skip processing\n/dev/sda1: 1661223968 -> /home/blusjune/.config/google-chrome/Default/Local Storage/https_plus.google.com_0.localstorage # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223968 : 207652740 : 39714912 )\n/dev/sda1: 1266682992 -> _EXCEPTION_ # inode for 158335118 is NOT FOUND -- Skip processing\n/dev/sda1: 1270876456 -> _EXCEPTION_ # inode for 158859301 is NOT FOUND -- Skip processing\n/dev/sda1: 1266681864 -> _EXCEPTION_ # inode for 158334977 is NOT FOUND -- Skip processing\n/dev/sda1: 1266751536 -> /home/blusjune/.config/google-chrome # DEV:LBA:FBLK:INODE( /dev/sda1 : 1266751536 : 158343686 : 39585586 )\n/dev/sda1: 1266751544 -> /home/blusjune/.config/google-chrome/Default # DEV:LBA:FBLK:INODE( /dev/sda1 : 1266751544 : 158343687 : 39585590 )\n/dev/sda1: 1266683272 -> _EXCEPTION_ # inode for 158335153 is NOT FOUND -- Skip processing\n/dev/sda1: 1267153912 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_0 # DEV:LBA:FBLK:INODE( /dev/sda1 : 1267153912 : 158393983 : 39585854 )\n/dev/sda1: 1661223296 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_0 # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223296 : 207652656 : 39585854 )\n/dev/sda1: 12857344 -> /home/blusjune/.cache/google-chrome/Default/Cache/index # DEV:LBA:FBLK:INODE( /dev/sda1 : 12857344 : 1606912 : 39585853 )\n/dev/sda1: 12856320 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_1 # DEV:LBA:FBLK:INODE( /dev/sda1 : 12856320 : 1606784 : 39585855 )\n/dev/sda1: 2480 -> _EXCEPTION_ # inode for 54 is NOT FOUND -- Skip processing\n/dev/sda1: 1266681984 -> _EXCEPTION_ # inode for 158334992 is NOT FOUND -- Skip processing\n/dev/sda1: 1266683032 -> _EXCEPTION_ # inode for 158335123 is NOT FOUND -- Skip processing\n/dev/sda1: 1266683160 -> _EXCEPTION_ # inode for 158335139 is NOT FOUND -- Skip processing\n/dev/sda1: 2352 -> _EXCEPTION_ # inode for 38 is NOT FOUND -- Skip processing\n/dev/sda1: 1661223328 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223328 : 207652660 : 39585620 )\n/dev/sda1: 2048 -> _EXCEPTION_ # fsblock 0 seems INVALID BLOCK -- Skip processing\n/dev/sda1: 1267095912 -> /home/blusjune/.config/google-chrome/Default/Cookies-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1267095912 : 158386733 : 39585621 )\n/dev/sda1: 327569400 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 327569400 : 40945919 : 39585620 )\n/dev/sda1: 1266683048 -> _EXCEPTION_ # inode for 158335125 is NOT FOUND -- Skip processing\n/dev/sda1: 327568408 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 327568408 : 40945795 : 39585620 )\n/dev/sda1: 327568936 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 327568936 : 40945861 : 39585620 )\n/dev/sda1: 1661223320 -> /home/blusjune/.config/google-chrome/Default/Cookies-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223320 : 207652659 : 39585621 )\n/dev/sda1: 1661223136 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223136 : 207652636 : 39585620 )\n/dev/sda1: 1401209744 -> /home/blusjune/.config/google-chrome/Default/Local Storage/https_plus.google.com_0.localstorage-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1401209744 : 175150962 : 39714913 )\n/dev/sda1: 1661223128 -> /home/blusjune/.config/google-chrome/Default/Cookies-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223128 : 207652635 : 39585621 )\n\nblusjune@radiohead:[top_hot_18] $ \n\n\n</pre>\n\n\n=== LBA-to-name processing ===\n\n* .bd/x/exphist info.\n: bd/x/exphist/sidewinder/sidewinder-20130311_190748/tools/lba_to_name/\n\n <pre>\nblusjune@jimi-hendrix:[lba_to_name] $ pwd\n/home/blusjune/bd/x/exphist/sidewinder/sidewinder-20130311_190748/tools/lba_to_name\nblusjune@jimi-hendrix:[lba_to_name] $ l\ntotal 16\ndrwxrwxr-x 2 blusjune blusjune 4096 Mar 11 19:03 ./\ndrwxrwxr-x 4 blusjune blusjune 4096 Mar 11 19:07 ../\n-rwxr-xr-x 1 blusjune blusjune 3869 Mar 11 19:03 .bdx.0100.y.lba_to_name.sh*\n-rw-rw-r-- 1 blusjune blusjune  300 Mar 11 19:03 .conf.lba_to_name.sh\n</pre>\n\n\n* References\n: [http://www.expertslogin.com/general/how-to-fix-repair-bad-blocks-in-linux/ How To Fix / Repair Bad Blocks In Linux] (B.GOOD)\n\n\n* How to find the corresponding location (file/dir) for some LBA (e.g., 1661223128)?\n\n: step 1: We need to find the block size of the filesystem\n <pre>\nblusjune@radiohead:[google] $ sudo tune2fs -l /dev/sda1  | grep Block\nBlock count:              488378112\nBlock size:               4096\nBlocks per group:         32768\n</pre>\n\n: step 2: check the starting sector of the partition (file system)\n <pre>\nblusjune@radiohead:[google] $ sudo fdisk -lu /dev/sda\n\nDisk /dev/sda: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x00001eb9\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *        2048  3907026943  1953512448   83  Linux\n\n</pre>\n\n: step 3: File the filesystem block that contains this target LBA\n <pre>\nb = (int) ((L-S) * 512 / B)\n\n, where\nb = File system block number (we need this answer)\nB = File system block size in bytes (4096 - we got this in step 1)\nL = target LBA (1661223128 - given)\nS = Starting sector of the partition as shown by fdisk -lu (2048 - we got this in step 2)\n\n, so the answer b is 207652635 (207,652,635)\n\n</pre>\n\n: step 4: Check the file/dir name with given b (207652635)\n <pre>\nblusjune@radiohead:[s05] $ sudo debugfs /dev/sda1 \ndebugfs 1.42 (29-Nov-2011)\n\ndebugfs:  testb 207652635\nBlock 207652635 marked in use\n\ndebugfs:  icheck 207652635\nBlock	Inode number\n207652635	39585621\n\ndebugfs:  ncheck 39585621\nInode	Pathname\n39585621	/home/blusjune/.config/google-chrome/Default/Cookies-journal\n\ndebugfs:\n</pre>\n\n== ## bNote-2013-03-08 ==\n\n=== lsof command guide/examples ===\n\n* [http://www.ibm.com/developerworks/aix/library/au-lsof.html Finding open files with lsof]\n\n:- Sean A. Walberg, Senior Network Engineer\n:- Summary:  Learn more about your system by seeing which files are open. Knowing which files an application has open, or which application has a particular file open, enables you to make better decisions as a system administrator. For instance, you shouldn\'t unmount a file system while files on it are open. Using lsof, you can check for open files and stopped processes before unmounting, as needed. Likewise, if you find an unknown file, you can find the application holding it open.\n\n\n=== debugfs command guide/examples ===\n\n* [http://www.cs.montana.edu/courses/309/topics/4-disks/debugfs_example.html debugfs examples - original article from montana.edu]\n\n <pre>\ndebugfs Command Examples\n\n\n# Use debufs to prowl around a file system.\n\n> debugfs /dev/hda6\ndebugfs 1.19, 13-Jul-2000 for EXT2 FS 0.5b, 95/08/09\n\n# list files\n\ndebugfs:  ls\n2790777 (12) .   32641 (12) ..   2790778 (12) dir1   2790781 (16) file1\n2790782 (4044) file2\n\n#  List the files with a long listing\n\n#  Format is:\n# Field 1:  Inode number.\n# Field 2:  First one or two digits is the type of node:\n#    2 = Character device\n#    4 = Directory\n#    6 = Block device\n#    10 = Regular file\n#    12 = Symbolic link\n#  \n#    The Last four digits are the Linux permissions\n# 3. Owner uid\n# 4. Group gid\n# 5. Size in bytes.\n# 6. Date \n# 7. Time of last creation.\n# 8. Filename.\n\ndebugfs:  ls -l\n2790777  40700   2605   2601    4096  5-Nov-2001 15:30 .\n 32641   40755   2605   2601    4096  5-Nov-2001 14:25 ..\n2790778  40700   2605   2601    4096  5-Nov-2001 12:43 dir1\n2790781 100600   2605   2601      14  5-Nov-2001 15:29 file1\n2790782 100600   2605   2601      14  5-Nov-2001 15:30 file2\n\n# dump the contents of file1\n\ndebugfs: cat file1\nThis is file1 \n\n# dump an inode to a file (same as cat, but to a file) and using\n#  instead of the file name.\n\ndebugfs: dump <2790782> file1-debugfs\n\n# dump the contents of an inode\n\ndebugfs: stat file1 \nInode: 2790782   Type: regular    Mode:  0600   Flags: 0x0   Generation: 46520506\nUser:  2605   Group:  2601   Size: 14\nFile ACL: 0    Directory ACL: 0\nLinks: 1   Blockcount: 8\nFragment:  Address: 0    Number: 0    Size: 0\nctime: 0x3be712ea -- Mon Nov  5 15:30:02 2001\natime: 0x3be712ea -- Mon Nov  5 15:30:02 2001\nmtime: 0x3be712ea -- Mon Nov  5 15:30:02 2001\nBLOCKS:\n5603924\nTOTAL: 1\n\n# Dump an directory inode and look at it.\n\ndebugfs: dump dir1 dir1-debugfs\n\n# Leave debugfs or use another xterm to look at the contents\n# using od or xxd.  The format of a directory (ext2 version 2.0) is:\n\n# Field 1. Four byte inode number.\n# Field 2. Two byte directory entry length.\n# Field 3. Two byte file name length. \n# Field 5. Filename (1-255 characters).\n# Pad.     The filename is padded to be a multiple of 4 bytes long.\n\n\n# use -c to see the file names and single byte values\n# You can see the file names and identify the locatin of\n# the other fields.  Of importance, the length of the \n# entries (octal); . (4-5), .. (20-21), file3 (34-35),\n# file4 (54-55), .file4.swp (74-75),  ...\n\n> od -c dir1-dump  \n0000000   z 225   *  \\0  \\f  \\0 001 002   .  \\0  \\0  \\0   y 225   *  \\0\n0000020  \\f  \\0 002 002   .   .  \\0  \\0 202 225   *  \\0 020  \\0 005 001\n0000040   f   i   l   e   3  \\0  \\0  \\0 201 225   *  \\0   ? 017 005 001\n0000060   f   i   l   e   4  \\0  \\0  \\0 177 225   *  \\0   ? 017  \\n 001\n0000100   .   f   i   l   e   4   .   s   w   p  \\0  \\0 200 225   *  \\0\n0000120   ? 017 006 001   f   i   l   e   4   ~   .   s   w   p   x  \\0\n0000140  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0\n*\n0010000\n\n# use -d to see the two byte values\n\n> od -d dir1-dump  \n0000000 38266    42    12   513    46     0 38265    42\n0000020    12   514 11822     0 38274    42    16   261\n0000040 26982 25964    51     0 38273    42  4056   261\n0000060 26982 25964    52     0 38271    42  4040   266\n0000100 26158 27753 13413 29486 28791     0 38272    42\n0000120  4020   262 26982 25964 32308 29486 28791   120\n0000140     0     0     0     0     0     0     0     0\n*\n0010000\n\n# You can see that the lengths of the entries are:\n#    . = 12, .. = 12, file3 = 16, file4 = 4096\n# Whoa! what happened there.  The file .file4.swp\n# and any other files in the directory have been deleted,\n# so the length of the entry goes to the end of the block\n#\n# use -l to see the four byte values.  We can see the inode\n# values of the files.\n\n> od -l dir1-dump  \n0000000     2790778    33619980          46     2790777\n0000020    33685516       11822     2790786    17104912\n0000040  1701603686          51     2790785    17108952\n0000060  1701603686          52     2790783    17436616\n0000100  1818846766  1932407909       28791     2790784\n0000120    17174452  1701603686  1932426804     7893111\n0000140           0           0           0           0\n*\n0010000\n\n\n-------------------------------------------------------------------<\n\n\n\n#\n# You inadvertently delete a file you want back.  The file was named\n# /home/harkin/test/file2.  Immediately do the following.\n#\n\n\n> umount /home\n\n# so that you don\'t create a new file that overwrites the inode\n# or use one of the file blocks.\n\n# Execute df to find out what partition /home is on\n\n>df \nFilesystem           1k-blocks      Used Available Use% Mounted on\n/dev/hda1              1011928    507860    452664  53% /\n/dev/hda8             27364092   1890176  24083896   8% /home\n/dev/hda5              8064272   3492760   4161860  46% /usr\n/dev/hda7              1011928     87956    872568  10% /var\nclowns:/db/boze       17783240  10494056   7183568  60% /home/bozo/db\n\n# Get the data on the /home filesystem\n\ntune2fs -l /dev/hda8 | grep \"Block size\"\n\n   Block size:               4096\n\n# So the block size is 4096 bytes.\n\n# Create a file system to duplicate the /home file system in case\n# you screw up royally.  This disk should be exactly the same size\n# as the file system you are backing up.  Fortunately there is an\n# unused disk /dev/hdb.\n\n> fdisk /dev/hdb\nCommand (m for help): n\nCommand action\n   l   logical (5 or over)\n   p   primary partition (1-4)\np\n\n+27364092K\nw\n\n# copy /home to the backup location\n\ndd if=/dev/hda8 of=/dev/hdb1 bs=4096\n\n# Now use debugfs to try to fix things.  We need to try to\n# find the inode of the deleted file.  Use lsdel to \n# list all of the deleted inodes on the file system.\n\ndebugfs -w            # to allow writing\ndebugfs:  lsdel\n3061 deleted inodes found.\n Inode  Owner  Mode    Size    Blocks    Time deleted\n                     .\n                     .\n3296723   2605 100600    652    1/   1 Fri Nov  2 07:30:33 2001\n3296724   2605 100600   1545    1/   1 Fri Nov  2 07:30:33 2001\n3296725   2605 100600    355    1/   1 Fri Nov  2 07:30:33 2001\n3296731   2605 100600    440    1/   1 Fri Nov  2 07:30:33 2001\n3296732   2605 100600   3536    1/   1 Fri Nov  2 07:30:33 2001\n3296733   2605 100600   2365    1/   1 Fri Nov  2 07:30:33 2001\n3296734   2605 100600    443    1/   1 Fri Nov  2 07:30:33 2001\n3296850   2605 100600   2046    1/   1 Fri Nov  2 07:30:33 2001\n3296851   2605 100600    729    1/   1 Fri Nov  2 07:30:33 2001\n3296852   2605 100600    850    1/   1 Fri Nov  2 07:30:33 2001\n3296853   2605 100600   3251    1/   1 Fri Nov  2 07:30:33 2001\n3296854   2605 100600   3733    1/   1 Fri Nov  2 07:30:33 2001\n3296855   2605 100600   3109    1/   1 Fri Nov  2 07:30:33 2001\n3296856   2605 100600   3211    1/   1 Fri Nov  2 07:30:33 2001\n652818   2605 100600 171791   43/  43 Fri Nov  2 16:07:33 2001\n897613   2605 100600   2096    1/   1 Mon Nov  5 07:49:28 2001\n979218   2605 100600   3797    1/   1 Mon Nov  5 07:49:29 2001\n979219   2605 100600   4096    1/   1 Mon Nov  5 07:49:29 2001\n179573   2605 100600   9113    3/   3 Mon Nov  5 12:41:16 2001\n636513   2605 100600   1327    1/   1 Mon Nov  5 12:41:16 2001\n636520   2605 100600     20    1/   1 Mon Nov  5 12:41:16 2001\n1338319   2605 100600   6998    2/   2 Mon Nov  5 12:48:55 2001\n \n# Based on the time and date, the inode to restore is 179573, 636513 \n# or 636520.  Try to figure out which one.\n\ndebugfs:cat <179573> \n   .\n   .\n\ndebugfs:cat <636513>\n   .\n\n# This is rather inconvenient.  If the directory where the files were\n# deleted from still exists, use the cd command to get there and then\n# use ls -d  which lists the files in the directory only, including\n# those with the deleted flag set. \n\n1566721  (12) .    32641  (12) ..    1566788  (60) 530\n1566790 (48) file1   1566791 (24) file2\n<1566747>  (20) file3\n\nThe inode numbers in brackets are deleted files.  A better looking display\ncomes with ls -ld.\n\n\n# So now you know which inode you need to restore.\n# To restore the file, you need to modify the inode, not the \n# directory entry.  This can be done with the modify_inode (mi)\n# command.  Specifically, change the deletion time to zero\n# and the link count to 1.\n\ndebugfs: mi <636513>\ndebugfs:  mi <148003>\n                              Mode    [0100644] \n                           User ID    [510] \n                          Group ID    [510] \n                              Size    [8123] \n                     Creation time    [904216575] \n                 Modification time    [904234782] \n                       Access time    [904234782] \n                     Deletion time    [904236721] 0\n                        Link count    [0] 1\n                       Block count    [16] \n                        File flags    [0x0] \n                         Reserved1    [0] \n                          File acl    [0] \n                     Directory acl    [0] \n                  Fragment address    [0] \n                   Fragment number    [0] \n                     Fragment size    [0] \n                   Direct Block #0    [100321] \n                   Direct Block #1    [100322] \n                   Direct Block #2    [100323] \n                   Direct Block #3    [100324] \n                   Direct Block #4    [200456] \n                   Direct Block #5    [200457] \n                   Direct Block #6    [200675] \n                   Direct Block #7    [200675] \n                   Direct Block #8    [304568] \n                   Direct Block #9    [0] \n                  Direct Block #10    [0] \n                  Direct Block #11    [0] \n                    Indirect Block    [0] \n             Double Indirect Block    [0] \n             Triple Indirect Block    [0] \n\n# It has been recovered.\n\n# This won\'t work for files with indirect blocks and you might find that\n# one or more blocks have been reused already.  If so, you can\n# recover as much data as possible by dumping the blocks to a file.\n\ndebugfs: dump <100321> /tmp > file1.000\ndebugfs: dump <100322> /tmp >> file1.000\n\n# and so on. For files that are longer than 12 blocks, you have to \n# trace the indirect, double-indirect and triple-indirect blocks.\n\n</pre>\n\n=== blktrace advanced ===\n\n* legacy \'blktrace\' data output\n <pre>\nDev_ID CPU_ID   SN   Timestamp      PID   Phz Act Address   Offset ProcessName\n------------------------------------------------------------------------------------  \n  8,16   1      929  2200.865379372 26328  A   R 3188196112 + 8 <- (8,17) 3188194064\n  8,17   1      930  2200.865379890 26328  Q   R 3188196112 + 8 [mysqld]\n  8,17   1      931  2200.865380598 26328  G   R 3188196112 + 8 [mysqld]\n  8,17   1      932  2200.865381014 26328  P   N [mysqld]\n  8,17   1      933  2200.865381784 26328  I   R 3188196112 + 8 [mysqld]\n  8,17   1      934  2200.865382107 26328  U   N [mysqld] 1\n  8,17   1      935  2200.865382605 26328  D   R 3188196112 + 8 [mysqld]\n  8,17   1      936  2200.871162161     0  C   R 3188196112 + 8 [0]\n  8,16   1      937  2200.871189524 26328  A   R 3188589744 + 8 <- (8,17) 3188587696\n  8,17   1      938  2200.871190517 26328  Q   R 3188589744 + 8 [mysqld]\n  8,17   1      939  2200.871192023 26328  G   R 3188589744 + 8 [mysqld]\n  8,17   1      940  2200.871192992 26328  P   N [mysqld]\n  8,17   1      941  2200.871194468 26328  I   R 3188589744 + 8 [mysqld]\n  8,17   1      942  2200.871195233 26328  U   N [mysqld] 1\n</pre>\n\n* legacy \'lsof\' data output\n <pre>\nCOMMAND     PID            USER   FD      TYPE             DEVICE SIZE/OFF       NODE NAME\n-------------------------------------------------------------------------------------------------\ninit          1            root  cwd       DIR                8,1     4096          2 /\ninit          1            root  rtd       DIR                8,1     4096          2 /\ninit          1            root  txt       REG                8,1   163096   57147454 /sbin/init\ninit          1            root  mem       REG                8,1    52120   96997047 /lib/x86_64-linux-gnu/libnss_files-2.15.so\ninit          1            root  mem       REG                8,1    47680   96993415 /lib/x86_64-linux-gnu/libnss_nis-2.15.so\ninit          1            root  mem       REG                8,1    97248   96997056 /lib/x86_64-linux-gnu/libnsl-2.15.so\ninit          1            root  mem       REG                8,1    35680   96997048 /lib/x86_64-linux-gnu/libnss_compat-2.15.so\ninit          1            root  mem       REG                8,1  1811128   96993409 /lib/x86_64-linux-gnu/libc-2.15.so\ninit          1            root  mem       REG                8,1    31752   96993413 /lib/x86_64-linux-gnu/librt-2.15.so\ninit          1            root  mem       REG                8,1   135366   96993411 /lib/x86_64-linux-gnu/libpthread-2.15.so\ninit          1            root  mem       REG                8,1   276392   96996820 /lib/x86_64-linux-gnu/libdbus-1.so.3.5.8\ninit          1            root  mem       REG                8,1    38888   96996879 /lib/x86_64-linux-gnu/libnih-dbus.so.1.0.0\ninit          1            root  mem       REG                8,1    96240   96996881 /lib/x86_64-linux-gnu/libnih.so.1.0.0\ninit          1            root  mem       REG                8,1   149280   96997051 /lib/x86_64-linux-gnu/ld-2.15.so\ninit          1            root    0u      CHR                1,3      0t0       1029 /dev/null\ninit          1            root    1u      CHR                1,3      0t0       1029 /dev/null\ninit          1            root    2u      CHR                1,3      0t0       1029 /dev/null\ninit          1            root    3r     FIFO                0,8      0t0       3001 pipe\ninit          1            root    4w     FIFO                0,8      0t0       3001 pipe\ninit          1            root    5r     0000                0,9        0       6797 anon_inode\ninit          1            root    6r     0000                0,9        0       6797 anon_inode\ninit          1            root    7u     unix 0xffff88020e5cc680      0t0       7152 socket\ninit          1            root    8u     unix 0xffff8802127caa40      0t0      10936 socket\ninit          1            root    9u     unix 0xffff8802116623c0      0t0       1999 socket\ninit          1            root   10u     unix 0xffff880211663400      0t0      10692 socket\ninit          1            root   12w      REG               8,18     2664   12845346 /var/log/upstart/mysql.log.1 (deleted)\ninit          1            root   14u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   16u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   17u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   18u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   20w      REG               8,18     1342   12845172 /var/log/upstart/modemmanager.log.1 (deleted)\ninit          1            root   21u      CHR                5,2      0t0       7184 /dev/ptmx\nkthreadd      2            root  cwd       DIR                8,1     4096          2 /\nkthreadd      2            root  rtd       DIR                8,1     4096          2 /\nkthreadd      2            root  txt   unknown                                        /proc/2/exe\nksoftirqd     3            root  cwd       DIR                8,1     4096          2 /\nksoftirqd     3            root  rtd       DIR                8,1     4096          2 /\nksoftirqd     3            root  txt   unknown                                        /proc/3/exe\n...\napache2    2241            root  mem       REG                8,1  1852792   96996819 /lib/x86_64-linux-gnu/libcrypto.so.1.0.0\napache2    2241            root  mem       REG                8,1   374608   96996818 /lib/x86_64-linux-gnu/libssl.so.1.0.0\napache2    2241            root  mem       REG                8,1  1030512   96997045 /lib/x86_64-linux-gnu/libm-2.15.so\napache2    2241            root  mem       REG                8,1    66784   96996836 /lib/x86_64-linux-gnu/libbz2.so.1.0.4\napache2    2241            root  mem       REG                8,1  1518928   64756408 /usr/lib/x86_64-linux-gnu/libdb-5.1.so\napache2    2241            root  mem       REG                8,1   105288   96993414 /lib/x86_64-linux-gnu/libresolv-2.15.so\napache2    2241            root  mem       REG                8,1  8644728   65276931 /usr/lib/apache2/modules/libphp5.so\napache2    2241            root  mem       REG                8,1    34824   65276243 /usr/lib/apache2/modules/mod_negotiation.so\napache2    2241            root  mem       REG                8,1    18432   65276567 /usr/lib/apache2/modules/mod_mime.so\napache2    2241            root  mem       REG                8,1    10240   65276556 /usr/lib/apache2/modules/mod_env.so\napache2    2241            root  mem       REG                8,1    10240   65276178 /usr/lib/apache2/modules/mod_dir.so\napache2    2241            root  mem       REG                8,1    92720   96996948 /lib/x86_64-linux-gnu/libz.so.1.2.3.4\napache2    2241            root  mem       REG                8,1    22528   65276571 /usr/lib/apache2/modules/mod_deflate.so\napache2    2241            root  mem       REG                8,1    26624   65275788 /usr/lib/apache2/modules/mod_cgi.so\napache2    2241            root  mem       REG                8,1    34824   65276563 /usr/lib/apache2/modules/mod_autoindex.so\napache2    2241            root  mem       REG                8,1    10248   65275257 /usr/lib/apache2/modules/mod_authz_user.so\napache2    2241            root  mem       REG                8,1    10248   65276546 /usr/lib/apache2/modules/mod_authz_host.so\napache2    2241            root  mem       REG                8,1    10248   65275256 /usr/lib/apache2/modules/mod_authz_groupfile.so\napache2    2241            root  mem       REG                8,1     6152   65275193 /usr/lib/apache2/modules/mod_authz_default.so\napache2    2241            root  mem       REG                8,1    10248   65276547 /usr/lib/apache2/modules/mod_authn_file.so\napache2    2241            root  mem       REG                8,1    10248   65276545 /usr/lib/apache2/modules/mod_auth_basic.so\napache2    2241            root  mem       REG                8,1    14336   65275224 /usr/lib/apache2/modules/mod_alias.so\napache2    2241            root  mem       REG                8,1    14768   96993408 /lib/x86_64-linux-gnu/libdl-2.15.so\napache2    2241            root  mem       REG                8,1    18896   96996944 /lib/x86_64-linux-gnu/libuuid.so.1.3.0\napache2    2241            root  mem       REG                8,1   170024   96996871 /lib/x86_64-linux-gnu/libexpat.so.1.5.2\napache2    2241            root  mem       REG                8,1    43288   96997046 /lib/x86_64-linux-gnu/libcrypt-2.15.so\napache2    2241            root  mem       REG                8,1  1811128   96993409 /lib/x86_64-linux-gnu/libc-2.15.so\napache2    2241            root  mem       REG                8,1   135366   96993411 /lib/x86_64-linux-gnu/libpthread-2.15.so\napache2    2241            root  mem       REG                8,1   234720   64752153 /usr/lib/libapr-1.so.0.4.6\napache2    2241            root  mem       REG                8,1   142840   64752206 /usr/lib/libaprutil-1.so.0.3.12\napache2    2241            root  mem       REG                8,1   247896   96996910 /lib/x86_64-linux-gnu/libpcre.so.3.12.1\napache2    2241            root  mem       REG                8,1   149280   96997051 /lib/x86_64-linux-gnu/ld-2.15.so\napache2    2241            root  DEL       REG                0,4               11842 /dev/zero\napache2    2241            root    0r      CHR                1,3      0t0       1029 /dev/null\napache2    2241            root    1w      CHR                1,3      0t0       1029 /dev/null\napache2    2241            root    2w      REG               8,18    12640   12845390 /var/log/apache2/error.log\napache2    2241            root    3u     IPv4              14201      0t0        TCP *:http (LISTEN)\napache2    2241            root    4r     FIFO                0,8      0t0    7004026 pipe\napache2    2241            root    5w     FIFO                0,8      0t0    7004026 pipe\napache2    2241            root    6w      REG               8,18        0   12845132 /var/log/apache2/other_vhosts_access.log\napache2    2241            root    7w      REG               8,18   127987   12845338 /var/log/apache2/access.log\n\n</pre>\n\n:* [Q] what does \'node\' information mean in \'lsof\' output?\n\n\n* converged iotrace\n: join the information from the legacy blktrace data and lsof, inotify data\n\n\n\n=== SNIA IO Trace Data Files ===\n\n\n[http://iotta.snia.org/traces SNIA I/O Trace Data Files]\n\nThe categories (or types) of I/O traces include:\n\n* Application Traces [This category is currently empty]\n: Application Traces record calls made by a specific application.\n* Block I/O Traces\n: Block I/O Traces typically include block level (e.g., at the logical volume manager, disk driver, etc. level) and block protocol (e.g., SCSI, ATA, Fibre Channel) traces.\n* Historical Traces [This category is currently empty]\n: Historical Traces include all traces that 10 or more years of age.\n* NFS Traces\n: Network File System Traces are typically those for NFS and CIFS and which reflect the protocol used by such network file systems.\n* Parallel Traces\nParallel traces, generally taken from supercomputers, record the system calls made by multiple computers running in parallel.\nSSSI WIOCP Metrics\nSSSI WIOCP, the SNIA Solid State Storage Initiative (SSSI) Workload I/O Capture Program (WIOCP), collects already-summarized empirical metrics separately for both monitored devices and processes/applications.\nStatic Snapshots\nStatic Snapshots are traces taken statically of a file system rather than of system calls.\nSystem Call Traces\nSystem Call I/O Traces typically reflect operating system calls to the file system.\nTools\nHere you can find the tools used for reading the various trace files.\n\n\n==== MSR Cambridge Traces ====\n\n* [http://iotta.snia.org/traces/388 List of Traces]\n\n* MSR Cambridge Traces 1\n: 1-week block I/O Traces of enterprise servers at MSR Cambridge.\n: The citation for the MSRC traces can be found [http://static.usenix.org/event/fast08/tech/narayanan.html FAST 2008, \"Write Off-loading: Practical Power Management for Enterprise Storage\", Dushyanth Narayanan, Austin Donnelly, and Antony Rowstron, Microsoft Research Ltd.]\n\n\n\n==== Microsoft Enterprise Traces ====\n\nTraces collected at Microsoft using the event tracing for Windows framework.\n\n* [http://iotta.snia.org/traces/130 List of Traces]\n\n* TPCC Traces 1\n: TPC-C benchmark traces collected at Microsoft using the event tracing for Windows framework\n\n* TPCC Traces 2\n: TPC-C benchmark traces collected at Microsoft using the event tracing for Windows framework\n: These six 6-minute long traces were collected at various points during a TPC-C run, all of which were during periods of steady-state activity.\n\n* TPCE Traces\n: TPC-E benchmark traces collected at Microsoft using the event tracing for Windows framework\n: These 6 traces were collected during a ~ 84-minute TPC-E run which included a ~ 20-minute warm-up time.\n\n* Exchange Server Traces\n: Production traces collected at Microsoft using the event tracing for Windows framework\n: Collected for Exchange Server for a duration of 24 hours. The single tarball includes 96 trace files, each with a duration of 15 minutes.\n\n\n\n==== Microsoft Production Server Traces ====\n\n* [http://iotta.snia.org/traces/158 List of Traces]\n\n* BuildServer00 ~ BuildServer07\n: Traces of the 25 hours activity on the Microsoft Build Server\n* Development Tools Release\n: Collected for Developers Tools Release Server for a duration of 24 hours\n* Display Ads Data Server, Display Ads Payload Server\n: Collected over a period of 24 hours for Display Ads Data/Platform payload server\n* Live Maps Back End\n: Collected for LiveMaps back-end server for a duration of 24 hours\n* MSN Storage CFS\n: Collected for MSN Storage Metadata Server for a duration of 6 hours\n* MSN Storage File Server\n: Collected for MSN Storage file server for a duration of 6 hours\n* Radius Authentication\n: Collected for RADIUS authentication server\n* Radius Back End SQL Server\n: Collected for RADIUS back-end server\n\n=== 서울대 장병탁 교수님 세미나 ===\n\n* Hypernetwork ML/AI 기술\n:- [http://bi.snu.ac.kr/Courses/g-ai06_2/book-ch4-hypernetmemory-part3.pdf The Hypernetwork Model of Memory)]\n:- [http://bi.snu.ac.kr/Publications/Theses/BS12f_ChunHS.pdf 하이퍼네트워크 연상메모리 기반의 이미지-텍스트 교차검색 (Image-Text Crossmodal Retrieval via Hypernetwork Memory]\n:: 2nd wrong answer: (하이퍼네트워크 메모리 기반의 이미지-텍스트 교차검색)\n:: 1st wrong answer: (하이퍼네트워크 기반의 이미지-텍스트 연상 교차 검색)\n\n=== 상무님께서 보내주신 Storage 미래 관련 글들 ===\n\n\n* 기타\n\n: [http://wwpi.com/index.php?option=com_content&view=article&id=8158]\n\n: [http://lib.stanford.edu/files/pasig-jan2012/11B7%20Francis%20PASIG_2011_Francis_final.pdf]\n\n: [http://www.datarecoverygroup.com/articles/data-storage-history-and-future Data Storage History and Future]\n\n\n\n\n* 스토리지 미디어의 발전 전망\n: 우리가 스토리지 미디어를 개발하지는 않지만, 미디어의 발전 전망을 고려해서 소프트웨어의 미래를 전망해야겠지요.\n\n: [http://www-03.ibm.com/ibm/history/ibm100/us/en/icons/racetrack/ IBM100] \n\n: [http://static.usenix.org/events/fast02/coufal.pdf FAST 2002]\n\n\n\n\n* DNA를 데이터 스토리지로 이용하는 것에 관한 또 다른 글입니다. 장점/비용 이슈 언급됨.\n\n: [http://www.lifehacker.com.au/2013/01/is-dna-the-future-of-data-storage/ DNA를 data storage로 이용하기]\n\n \n* Storage의 미래\n\n: [http://blogs.computerworld.com/data-storage/20865/future-data-storage-revealed Future data storage revealed]\n\n: [http://blogs.computerworld.com/data-storage/21537/top-10-storage-predictions-back-future Top 10 storage predictions]\n\n: [http://blogs.computerworld.com/data-storage/21360/will-private-cloud-kill-storage-area-network Will Private Cloud Kill SAN?]\n\n== ## bNote-2013-03-07 ==\n\n\n=== Find X\'s ===\n\n==== System Event (esp., file system change) Tracing/Monitoring/Collecting ====\n\n* LTTng\n* DTrace\n* FTrace\n* Strace\n* SystemTap\n* inotify ***\n* FAM (File Alteration Monitor) [http://oss.sgi.com/projects/fam/]\n* Gamin (File and directory monitoring system defined to be a subset of the FAM system [http://people.gnome.org/~veillard/gamin/overview.html]\n\n----\n\n==== inotify ====\n\n* inotify - monitoring file system events\n: The inotify API provides a mechanism for monitoring file system events. Inotify can be used to monitor individual files, or to monitor directories. When a directory is monitored, inotify will return events for the directory itself, and for files inside the directory.\n: The following system calls are used with this API: inotify_init(2) (or inotify_init1(2)), inotify_add_watch(2), inotify_rm_watch(2), read(2), and close(2).\n\n* /proc interfaces\n: /proc/sys/fs/inotify/max_queued_events\n: /proc/sys/fs/inotify/max_user_instances\n: /proc/sys/fs/inotify/max_user_watches\n\n* Versions\n: Inotify was merged into the 2.6.13 Linux kernel. The required library interfaces were added to glibc in version 2.4. (IN_DONT_FOLLOW, IN_MASK_ADD, and IN_ONLYDIR were only added in version 2.5.)\n\n* Check whether inotify is enabled or not\n $ grep INOTIFY_USER /boot/config-$(uname -r)\n CONFIG_INOTIFY_USER=y\n\n* Installation on Ubuntu by apt-get\n: aptitude install inotify-tools python-inotifyx libinotifytools0-dev\n\n* inotify는 다음 event들에 대해서만 detection 가능함\n:* access\n:* modify\n:* attrib\n:: watched file에 대한 메타데이터가 변경되거나, watched directory 내의 file이 변경된 경우, \'attrib\' event가 발생\n:* close_write\n:* close_nowrite\n:* close\n:* open\n:* moved_to\n:* moved_from\n:* move\n:* move_self\n:: 이 event 이후에는 file or directory는 no longer being watched된다... 는데, delete event의 경우와 무엇이 다른가?\n:* create\n:* delete\n:* delete_self\n:* unmount\n\n\n\n\n* References\n# [http://www.infoq.com/articles/inotify-linux-file-system-event-monitoring InfoQ -- Inotify: Efficient, Real-time Linux File System Event Monitoring]\n# [http://www.ibm.com/developerworks/linux/library/l-ubuntu-inotify/index.html Monitor file system activity with inotify (B.GOOD)]\n# [http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=90586523eb4b349806887c62ee70685a49415124 git.kernel.org -- fsnotify: unified filesystem notification backend, 2009-05-21~2009-06-11]\n# [http://stackoverflow.com/questions/9614184/how-to-trace-per-file-io-operations-in-linux How to trace per-file IO operations in Linux? -- /proc/PID/fd/, systemtap, strace, fanotify]\n# [http://stackoverflow.com/questions/1835947/how-do-i-program-for-linuxs-new-fanotify-file-system-monitoring-feature Stackoverflow -- How do I program for Linux\'s new \'fanotify\' file system monitoring feature?]\n# [http://stackoverflow.com/questions/8381566/best-way-to-monitor-file-system-changes-in-linux Stackoverflow -- Best way to monitor file system changes in Linux]\n# [http://ubuntuforums.org/showthread.php?t=663950 python inotify example -- Ubuntu Forums]\n# [http://pyinotify.sourceforge.net/ Pyinotify: monitor filesystem events with Python under Linux - Brief Tutorial]\n# [http://github.com/seb-m/pyinotify pyinotify github]\n\n=== directory-file-addr spatial locality ===\n\n* Directory Hierarchy\n\n <pre>\nblusjune@jimi-hendrix:[dir_file_addr_spatial_locality] $ find r0\nr0\nr0/d1\nr0/d1/d13\nr0/d1/d11\nr0/d1/d12\nr0/d2\nr0/d2/d21\nr0/d2/d21/d212\nr0/d2/d21/d211\nr0/d2/d21/d213\nr0/d2/d22\nr0/d2/d22/d221\nr0/d2/d22/d222\n</pre>\n\n== ## bNote-2013-03-06 ==\n\n=== LBA-to-name processing (DELETEME) ===\n\nDone actually 2013-03-11 14:35. [http://www.expertslogin.com/general/how-to-fix-repair-bad-blocks-in-linux/ How To Fix / Repair Bad Blocks In Linux B.GOOD]\n\n\n* How to find the corresponding location (file/dir) for some LBA (e.g., 1661223128)?\n\n: step 1: We need to find the block size of the filesystem\n <pre>\nblusjune@radiohead:[google] $ sudo tune2fs -l /dev/sda1  | grep Block\nBlock count:              488378112\nBlock size:               4096\nBlocks per group:         32768\n</pre>\n\n: step 2: check the starting sector of the partition (file system)\n <pre>\nblusjune@radiohead:[google] $ sudo fdisk -lu /dev/sda\n\nDisk /dev/sda: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x00001eb9\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *        2048  3907026943  1953512448   83  Linux\n\n</pre>\n\n: step 3: File the filesystem block that contains this target LBA\n <pre>\nb = (int) ((L-S) * 512 / B)\n\n, where\nb = File system block number (we need this answer)\nB = File system block size in bytes (4096 - we got this in step 1)\nL = target LBA (1661223128 - given)\nS = Starting sector of the partition as shown by fdisk -lu (2048 - we got this in step 2)\n\n, so the answer b is 207652635 (207,652,635)\n\n</pre>\n\n: step 4: Check the file/dir name with given b (207652635)\n <pre>\nblusjune@radiohead:[s05] $ sudo debugfs /dev/sda1 \ndebugfs 1.42 (29-Nov-2011)\n\ndebugfs:  testb 207652635\nBlock 207652635 marked in use\n\ndebugfs:  icheck 207652635\nBlock	Inode number\n207652635	39585621\n\ndebugfs:  ncheck 39585621\nInode	Pathname\n39585621	/home/blusjune/.config/google-chrome/Default/Cookies-journal\n\ndebugfs:\n</pre>\n\n=== IOWA.sim.iox (myreal_72h) ===\n\n* base trace log on radiohead\n:- name: bpo_a.20130305_104633.real_whole_trace.log\n:- size: 197,372,058 bytes (197372058)\n\n <pre>\nblusjune@radiohead:[s05] $ pwd\n/x/var/iowa/sidewinder/iowa/iowa.sim.iox/tdir/s05\n\nblusjune@radiohead:[s05] $ l\ntotal 210572\ndrwxrwxr-x  2 blusjune blusjune      4096 Mar  6 11:07 ./\ndrwxrwxr-x 10 blusjune blusjune      4096 Mar  6 19:51 ../\nlrwxrwxrwx  1 blusjune blusjune        38 Mar  5 10:43 .bdx.0100.y.proc_after_trace_s10.sh -> ../.bdx.0100.y.proc_after_trace_s10.sh\n-rw-rw-r--  1 root     root     197372058 Mar  5 13:34 bpo_a.20130305_104633.real_whole_trace.log\n-rw-rw-r--  1 blusjune blusjune   4945483 Mar  6 11:06 tracelog.myrealtrace.log.A.addr\n-rw-rw-r--  1 blusjune blusjune   1081666 Mar  6 11:06 tracelog.myrealtrace.log.R.addr\n-rw-rw-r--  1 blusjune blusjune   3863817 Mar  6 11:06 tracelog.myrealtrace.log.W.addr\n-rw-rw-r--  1 blusjune blusjune   8339772 Mar  6 11:06 tracelog.myrealtrace.log.p1.out\n</pre>\n\n\n* R.addr analysis\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_200532.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  1\n__valu__sig__ _n_o_sigaddrs :  43683\n__valu__sig__ _sigioc_acc :  43683\n__valu__sig__ _sigaddrs_efficiency :  1.0\n__valu__sig__ _n_o_addr_total :  43683\n__valu__sig__ _ioc_total :  43683\n</pre>\n\n\n* W.addr analysis\n:- Used as weekly report item (2013-03-06), and lead to a patent\n::- just 18 addresses cover 25% of IO (40,010 IOs out of 157,632 IOs)\n::- x 2222.8 caching efficiency\n\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  1115\n__valu__sig__ _n_o_sigaddrs :  18\n__valu__sig__ _sigioc_acc :  40010\n__valu__sig__ _sigaddrs_efficiency :  2222.77777778\n__valu__sig__ _n_o_addr_total :  40979\n__valu__sig__ _ioc_total :  157632\n</pre>\n\n\n* Processes Contributed to the IO Workload\n\n <pre>\na1mjjung@secm:[myrealtrace.radiohead.v01] $ cat bpo_a.20130305_104633.real_whole_trace.log  | grep \'\\[\' | sed -e \'s/.*\\(\\[.*\\]\\)/\\1/g\' | sort -u\n[(null)]\n[(null)] 1\n[(null)] 3\n[(null)] 4\n[(null)] 6\n[(null)] 7\n[0]\n[BrowserBlocking]\n[BrowserBlocking] 1\n[Chrome_CacheThr]\n[Chrome_CacheThr] 1\n[Chrome_CacheThr] 10\n[Chrome_CacheThr] 2\n[Chrome_CacheThr] 3\n[Chrome_CacheThr] 6\n[Chrome_CacheThr] 9\n[Chrome_ChildIOT]\n[Chrome_DBThread]\n[Chrome_DBThread] 1\n[Chrome_DBThread] 2\n[Chrome_DBThread] 3\n[Chrome_DBThread] 4\n[Chrome_DBThread] 5\n[Chrome_DBThread] 6\n[Chrome_FileThre]\n[Chrome_FileThre] 1\n[Chrome_FileThre] 2\n[Chrome_FileThre] 3\n[Chrome_FileThre] 4\n[Chrome_HistoryT]\n[Chrome_HistoryT] 1a1mjjung@secm:[myrealtrace.radiohead.v01] $ cat bpo_a.20130305_104633.real_whole_trace.log  | grep \'\\[\' | sed -e \'s/.*\\(\\[.*\\]\\)/\\1/g\' | sort -u\n[(null)]\n[(null)] 1\n[(null)] 3\n[(null)] 4\n[(null)] 6\n[(null)] 7\n[0]\n[BrowserBlocking]\n[BrowserBlocking] 1\n[Chrome_CacheThr]\n[Chrome_CacheThr] 1\n[Chrome_CacheThr] 10\n[Chrome_CacheThr] 2\n[Chrome_CacheThr] 3\n[Chrome_CacheThr] 6\n[Chrome_CacheThr] 9\n[Chrome_ChildIOT]\n[Chrome_DBThread]\n[Chrome_DBThread] 1\n[Chrome_DBThread] 2\n[Chrome_DBThread] 3\n[Chrome_DBThread] 4\n[Chrome_DBThread] 5\n[Chrome_DBThread] 6\n[Chrome_FileThre]\n[Chrome_FileThre] 1\n[Chrome_FileThre] 2\n[Chrome_FileThre] 3\n[Chrome_FileThre] 4\n[Chrome_HistoryT]\n[Chrome_HistoryT] 1\n[Chrome_HistoryT] 10\n[Chrome_HistoryT] 11\n[Chrome_HistoryT] 12\n[Chrome_HistoryT] 14\n[Chrome_HistoryT] 15\n[Chrome_HistoryT] 16\n[Chrome_HistoryT] 2\n[Chrome_HistoryT] 3\n[Chrome_HistoryT] 4\n[Chrome_HistoryT] 5\n[Chrome_HistoryT] 6\n[Chrome_HistoryT] 7\n[Chrome_HistoryT] 8\n[Chrome_HistoryT] 9\n[Chrome_IOThread]\n[Chrome_IOThread] 1\n[Chrome_ProcessL]\n[Chrome_SafeBrow]\n[Chrome_SafeBrow] 1\n[Chrome_SafeBrow] 4\n[Chrome_SafeBrow] 5\n[Chrome_SyncThre]\n[Chrome_SyncThre] 1\n[Chrome_SyncThre] 10\n[Chrome_SyncThre] 11\n[Chrome_SyncThre] 12\n[Chrome_SyncThre] 13\n[Chrome_SyncThre] 14\n[Chrome_SyncThre] 15\n[Chrome_SyncThre] 16\n[Chrome_SyncThre] 2\n[Chrome_SyncThre] 3\n[Chrome_SyncThre] 4\n[Chrome_SyncThre] 5\n[Chrome_SyncThre] 6\n[Chrome_SyncThre] 7\n[Chrome_SyncThre] 8\n[Chrome_SyncThre] 9\n[Chrome_WebKitTh]\n[Chrome_WebKitTh] 1\n[MediaStreamDevi]\n[MediaStreamDevi] 1\n[NetworkChangeNo]\n[NetworkChangeNo] 1\n[WorkerPool/2786]\n[WorkerPool/2786] 1\n[WorkerPool/2787]\n[WorkerPool/2787] 1\n[WorkerPool/2788]\n[WorkerPool/2788] 1\n[WorkerPool/2839]\n[WorkerPool/2839] 1\n[WorkerPool/2840]\n[WorkerPool/2840] 1\n[WorkerPool/2840] 2\n[WorkerPool/2981]\n[WorkerPool/2981] 1\n[WorkerPool/2982]\n[WorkerPool/2982] 1\n[_BDX]\n[_BDX] 1\n[apache2]\n[apache2] 1\n[apache2ctl]\n[apache2ctl] 1\n[bash]\n[bash] 1\n[bash] 2\n[bash] 3\n[bash] 4\n[bash] 5\n[blktrace]\n[cat]\n[cat] 1\n[checkarray]\n[checkarray] 1\n[chrome]\n[chrome] 1\n[chrome] 2\n[chrome] 3\n[chrome] 4\n[console-kit-dae]\n[console-kit-dae] 1\n[console-kit-dae] 4\n[cp]\n[cp] 1\n[cron]\n[dbus-launch]\n[dbus-launch] 1\n[env]\n[env] 1\n[expr]\n[expr] 2\n[flush-8:0]\n[flush-8:0] 1\n[flush-8:0] 10\n[flush-8:0] 11\n[flush-8:0] 12\n[flush-8:0] 13\n[flush-8:0] 14\n[flush-8:0] 15\n[flush-8:0] 16\n[flush-8:0] 2\n[flush-8:0] 3\n[flush-8:0] 4\n[flush-8:0] 5\n[flush-8:0] 6\n[flush-8:0] 7\n[flush-8:0] 8\n[flush-8:0] 9\n[git-pull]\n[git-pull] 1\n[git]\n[git] 1\n[gnome-pty-helpe]\n[gnome-pty-helpe] 1\n[gnome-terminal]\n[gnome-terminal] 1\n[gnome-terminal] 2\n[gnome-terminal] 3\n[gnome-terminal] 4\n[gnome-terminal] 5\n[gnome-terminal] 7\n[google-chrome]\n[google-chrome] 1\n[inotify_reader]\n[inotify_reader] 1\n[jbd2/sda1-8]\n[jbd2/sda1-8] 1\n[jbd2/sda1-8] 16\n[jbd2/sda1-8] 2\n[jbd2/sda1-8] 3\n[jbd2/sda1-8] 4\n[jbd2/sda1-8] 5\n[kworker/0:1]\n[kworker/0:2]\n[kworker/2:1]\n[kworker/2:2]\n[kworker/3:0]\n[kworker/4:0]\n[kworker/4:1]\n[kworker/4:2]\n[kworker/5:0]\n[kworker/5:1]\n[kworker/5:2]\n[kworker/7:2]\n[libvirtd]\n[libvirtd] 1\n[libvirtd] 2\n[ls]\n[ls] 1\n[mysqld]\n[nmbd]\n[nmbd] 2\n[nmbd] 4\n[nmbd] 5\n[nmbd] 6\n[python]\n[python] 1\n[python] 2\n[python] 3\n[python] 4\n[python] 5\n[rserver]\n[rserver] 1\n[rserver] 2\n[rserver] 3\n[rserver] 6\n[rserver] 8\n[rsession]\n[rsession] 1\n[run-parts]\n[run-parts] 1\n[sh]\n[sh] 1\n[smbd]\n[smbd] 1\n[smbd] 2\n[smbd] 3\n[sshd]\n[sshd] 1\n[sshd] 4\n[sshd] 5\n[swapper/0]\n[udevd]\n[udevd] 1\n[update-motd-fsc]\n[update-motd-fsc] 1\n[update-motd-upd]\n[update-motd-upd] 1\n[upstart-udev-br]\n[upstart-udev-br] 1\n[xdg-settings]\n[xdg-settings] 1\n[Chrome_HistoryT] 10\n[Chrome_HistoryT] 11\n[Chrome_HistoryT] 12\n[Chrome_HistoryT] 14\n[Chrome_HistoryT] 15\n[Chrome_HistoryT] 16\n[Chrome_HistoryT] 2\n[Chrome_HistoryT] 3\n[Chrome_HistoryT] 4\n[Chrome_HistoryT] 5\n[Chrome_HistoryT] 6\n[Chrome_HistoryT] 7\n[Chrome_HistoryT] 8\n[Chrome_HistoryT] 9\n[Chrome_IOThread]\n[Chrome_IOThread] 1\n[Chrome_ProcessL]\n[Chrome_SafeBrow]\n[Chrome_SafeBrow] 1\n[Chrome_SafeBrow] 4\n[Chrome_SafeBrow] 5\n[Chrome_SyncThre]\n[Chrome_SyncThre] 1\n[Chrome_SyncThre] 10\n[Chrome_SyncThre] 11\n[Chrome_SyncThre] 12\n[Chrome_SyncThre] 13\n[Chrome_SyncThre] 14\n[Chrome_SyncThre] 15\n[Chrome_SyncThre] 16\n[Chrome_SyncThre] 2\n[Chrome_SyncThre] 3\n[Chrome_SyncThre] 4\n[Chrome_SyncThre] 5\n[Chrome_SyncThre] 6\n[Chrome_SyncThre] 7\n[Chrome_SyncThre] 8\n[Chrome_SyncThre] 9\n[Chrome_WebKitTh]\n[Chrome_WebKitTh] 1\n[MediaStreamDevi]\n[MediaStreamDevi] 1\n[NetworkChangeNo]\n[NetworkChangeNo] 1\n[WorkerPool/2786]\n[WorkerPool/2786] 1\n[WorkerPool/2787]\n[WorkerPool/2787] 1\n[WorkerPool/2788]\n[WorkerPool/2788] 1\n[WorkerPool/2839]\n[WorkerPool/2839] 1\n[WorkerPool/2840]\n[WorkerPool/2840] 1\n[WorkerPool/2840] 2\n[WorkerPool/2981]\n[WorkerPool/2981] 1\n[WorkerPool/2982]\n[WorkerPool/2982] 1\n[_BDX]\n[_BDX] 1\n[apache2]\n[apache2] 1\n[apache2ctl]\n[apache2ctl] 1\n[bash]\n[bash] 1\n[bash] 2\n[bash] 3\n[bash] 4\n[bash] 5\n[blktrace]\n[cat]\n[cat] 1\n[checkarray]\n[checkarray] 1\n[chrome]\n[chrome] 1\n[chrome] 2\n[chrome] 3\n[chrome] 4\n[console-kit-dae]\n[console-kit-dae] 1\n[console-kit-dae] 4\n[cp]\n[cp] 1\n[cron]\n[dbus-launch]\n[dbus-launch] 1\n[env]\n[env] 1\n[expr]\n[expr] 2\n[flush-8:0]\n[flush-8:0] 1\n[flush-8:0] 10\n[flush-8:0] 11\n[flush-8:0] 12\n[flush-8:0] 13\n[flush-8:0] 14\n[flush-8:0] 15\n[flush-8:0] 16\n[flush-8:0] 2\n[flush-8:0] 3\n[flush-8:0] 4\n[flush-8:0] 5\n[flush-8:0] 6\n[flush-8:0] 7\n[flush-8:0] 8\n[flush-8:0] 9\n[git-pull]\n[git-pull] 1\n[git]\n[git] 1\n[gnome-pty-helpe]\n[gnome-pty-helpe] 1\n[gnome-terminal]\n[gnome-terminal] 1\n[gnome-terminal] 2\n[gnome-terminal] 3\n[gnome-terminal] 4\n[gnome-terminal] 5\n[gnome-terminal] 7\n[google-chrome]\n[google-chrome] 1\n[inotify_reader]\n[inotify_reader] 1\n[jbd2/sda1-8]\n[jbd2/sda1-8] 1\n[jbd2/sda1-8] 16\n[jbd2/sda1-8] 2\n[jbd2/sda1-8] 3\n[jbd2/sda1-8] 4\n[jbd2/sda1-8] 5\n[kworker/0:1]\n[kworker/0:2]\n[kworker/2:1]\n[kworker/2:2]\n[kworker/3:0]\n[kworker/4:0]\n[kworker/4:1]\n[kworker/4:2]\n[kworker/5:0]\n[kworker/5:1]\n[kworker/5:2]\n[kworker/7:2]\n[libvirtd]\n[libvirtd] 1\n[libvirtd] 2\n[ls]\n[ls] 1\n[mysqld]\n[nmbd]\n[nmbd] 2\n[nmbd] 4\n[nmbd] 5\n[nmbd] 6\n[python]\n[python] 1\n[python] 2\n[python] 3\n[python] 4\n[python] 5\n[rserver]\n[rserver] 1\n[rserver] 2\n[rserver] 3\n[rserver] 6\n[rserver] 8\n[rsession]\n[rsession] 1\n[run-parts]\n[run-parts] 1\n[sh]\n[sh] 1\n[smbd]\n[smbd] 1\n[smbd] 2\n[smbd] 3\n[sshd]\n[sshd] 1\n[sshd] 4\n[sshd] 5\n[swapper/0]\n[udevd]\n[udevd] 1\n[update-motd-fsc]\n[update-motd-fsc] 1\n[update-motd-upd]\n[update-motd-upd] 1\n[upstart-udev-br]\n[upstart-udev-br] 1\n[xdg-settings]\n[xdg-settings] 1\n\n</pre>\n\n\n=== IOWA.sim.iox (tpcc_250gb_48h) ===\n\n* R.addr analysis\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_182112.sigio_25.iowsz_100.t1_10000] $ cat __simout.sigio_25.iowsz_100.t1_10000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  7\n__valu__sig__ _n_o_sigaddrs :  126606\n__valu__sig__ _sigioc_acc :  1169938\n__valu__sig__ _sigaddrs_efficiency :  9.24077847811\n__valu__sig__ _n_o_addr_total :  1691608\n__valu__sig__ _ioc_total :  4113312\n</pre>\n\n\n* W.addr analysis\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_190100.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  601\n__valu__sig__ _n_o_sigaddrs :  46304\n__valu__sig__ _sigioc_acc :  47940036\n__valu__sig__ _sigaddrs_efficiency :  1035.33249827\n__valu__sig__ _n_o_addr_total :  4910080\n__valu__sig__ _ioc_total :  191622453\n</pre>\n\n x39.02 = ( total_#_of_IOs / total_#_of_addrs_hit_actually )\n x1035.33 = ( 25%_sig_IOs / 25%_sig_addrs )\n\n=== R \'e1071\' package install (command line) ===\n\n <pre>\na1mjjung@secm:[e1071] $ R CMD INSTALL e1071\nJob <13412785> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura007>>\nWarning: invalid package \'e1071\'\nError: ERROR: no packages specified\na1mjjung@secm:[e1071] $ cd ..\nla1mjjung@secm:[e1071_build] $ l\ntotal 280\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:45 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\na1mjjung@secm:[e1071_build] $ R CMD INSTALL e1071\nJob <13412786> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n* installing to library \'/home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15\'\n* installing *source* package \'e1071\' ...\n** package \'e1071\' successfully unpacked and MD5 sums checked\nchecking for C++ compiler default output file name... a.out\nchecking whether the C++ compiler works... yes\nchecking whether we are cross compiling... no\nchecking for suffix of executables... \nchecking for suffix of object files... o\nchecking whether we are using the GNU C++ compiler... yes\nchecking whether g++ accepts -g... yes\n** libs\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c Rsvm.c -o Rsvm.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cmeans.c -o cmeans.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cshell.c -o cshell.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c floyd.c -o floyd.o\ng++ -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c svm.cpp -o svm.o\ng++ -shared -L/usr/local/lib64 -o e1071.so Rsvm.o cmeans.o cshell.o floyd.o svm.o\ninstalling to /home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15/e1071/libs\n** R\n** inst\n** preparing package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n   \'svmdoc.Rnw\' \n** testing if installed package can be loaded\n\n* DONE (e1071)\na1mjjung@secm:[e1071_build] $ R\nJob <13412787> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n\nR version 2.15.2 (2012-10-26) -- \"Trick or Treat\"\nCopyright (C) 2012 The R Foundation for Statistical Computing\nISBN 3-900051-07-0\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType \'license()\' or \'licence()\' for distribution details.\n\nR is a collaborative project with many contributors.\nType \'contributors()\' for more information and\n\'citation()\' on how to cite R or R packages in publications.\n\nType \'demo()\' for some demos, \'help()\' for on-line help, or\n\'help.start()\' for an HTML browser interface to help.\nType \'q()\' to quit R.\n\n> ?svm\nNo documentation for \'svm\' in specified packages and libraries:\nyou could try \'??svm\'\n> library(e1071) \nLoading required package: class\n> ?svm\n> \nSave workspace image? [y/n/c]: y\na1mjjung@secm:[e1071_build] $ pwd\n/home/X0101/a1mjjung/x/R/e1071_build\na1mjjung@secm:[e1071_build] $ ls -alF\ntotal 284\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:47 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\n-rw------- 1 a1mjjung X0101     25 Mar  6 18:47 .Rhistory\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:46 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\n\n</pre>\n\n\n\n\n=== 3rd meeting with Dr. CHOI ===\n\n\n\n=== 수퍼컴 (supercom) ===\n\n\n* account\n: ID: a1mjjung\n: PW: wjdaudwns\n: IP address: 202.20.183.10 (ssh)\n\n* password change\n: 한지연 사원 (jiyoun92.han@partner.samsung.com) (031-280-8147)\n\n* python 2.7.x from 2.6.x\n: [a1mjjung@login03 ~]$ /apps/Python/Python-2.7.3/bin/python\n\n== ## bNote-2013-03-05 ==\n\n=== 최희열 전문과 2차 미팅 ===\n\n* IO trace (궤적)의 변화 예측\n: IO 궤적의 변화를 신속 정확하게 예측하는 것의 중요성\n:: 언제 hot data가 fast-tier에 올라가야 할지,\n:: 현재의 hot data 말고 새로운 hot data는 언제 올라가야할지?\n:: 현재 fast-tier에 올라가 있는 hot data가 언제 unload되어야 할지?\n\ncached/tiered hot data가 계속 hot storage area에 \n: IO hot zone 궤적 변화의 전조로 사용될 수 있을만한 시스템 정보/상태 변화\n: IO hot zone 궤적이 정말로 변화하는 것일지 아니면 기존 궤적에서의 fluctuation인지?\n\n* Challenging Problems\n\n: 단위 시간 당 hot zone의 갯수가 항상 유지되는 것은 아님. 즉, hot zone의 갯수는 1개로 유지된 채, hot zone의 위치만 이동하는 형태는 아닐 수 있음. hot zone의 갯수가 1개였다가 4개로, 7개로, 줄어서 5개로, 다시 줄어서 2개로, 이런 식으로\n\n== ## bNote-2013-03-05 ==\n\n=== IOWA to ML Formulation ===\n\n* IO trace (궤적)의 변화 예측\n: IO 궤적의 변화를 신속 정확하게 예측하는 것의 중요성\n:: 언제 hot data가 fast-tier에 올라가야 할지,\n:: 현재의 hot data 말고 새로운 hot data는 언제 올라가야할지?\n:: 현재 fast-tier에 올라가 있는 hot data가 언제 unload되어야 할지?\n\ncached/tiered hot data가 계속 hot storage area에 \n: IO hot zone 궤적 변화의 전조로 사용될 수 있을만한 시스템 정보/상태 변화\n: IO hot zone 궤적이 정말로 변화하는 것일지 아니면 기존 궤적에서의 fluctuation인지?\n\n* Challenging Problems\n\n: 단위 시간 당 hot zone의 갯수가 항상 유지되는 것은 아님. 즉, hot zone의 갯수는 1개로 유지된 채, hot zone의 위치만 이동하는 형태는 아닐 수 있음. hot zone의 갯수가 1개였다가 4개로, 7개로, 줄어서 5개로, 다시 줄어서 2개로, 이런 식으로','utf-8'),(67,'== 20130530_134618 ==\n=== Introduction to Association Rule Mining ===\n\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n\n\n\n=== Frequent patterns/items mining examples with R::arules ===\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n* References\n:# [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))]\n:# [http://www.analysis-of-patterns.net/files/bgoethals.pdf Frequent Pattern Mining Slides - Universiteit Antwerpen]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning - Wikipedia]\n:# [http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf Frequent Pattern Mining in Web Log Data]\n:# [http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010 Temporal Pattern Mining Tutorial KDD 2010]\n:# [http://cran.r-project.org/web/packages/arules/index.html arules: Mining Association Rules and Frequent Itemsets]\n:# [http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf Introduction to arules - A computational environment for mining association rules and frequent item sets -- http://cran.r-project.org/]\n:# [http://lyle.smu.edu/IDA/arules/ Intelligent Data Analysis Lab @ SMU]\n:# [http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))]\n:# [http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/ Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning]\n:# [http://en.wikipedia.org/wiki/Sequence_mining Sequence mining]\n:# [http://en.wikipedia.org/wiki/GSP_Algorithm GSP Algorithm]\n:# [http://en.wikipedia.org/wiki/Apriori_algorithm Apriori algorithm]\n:# [http://en.wikipedia.org/wiki/Market_basket_analysis Market basket analysis]\n:# [http://en.wikipedia.org/wiki/Time_series Time series]\n\n\n----','utf-8'),(68,'== Introduction to Association Rule Mining ==\n\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n\n\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* [http://www.analysis-of-patterns.net/files/bgoethals.pdf Frequent Pattern Mining Slides - Universiteit Antwerpen]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning - Wikipedia]\n:# [http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf Frequent Pattern Mining in Web Log Data]\n:# [http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010 Temporal Pattern Mining Tutorial KDD 2010]\n:# [http://cran.r-project.org/web/packages/arules/index.html arules: Mining Association Rules and Frequent Itemsets]\n:# [http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf Introduction to arules - A computational environment for mining association rules and frequent item sets -- http://cran.r-project.org/]\n:# [http://lyle.smu.edu/IDA/arules/ Intelligent Data Analysis Lab @ SMU]\n:# [http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))]\n:# [http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/ Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning]\n:# [http://en.wikipedia.org/wiki/Sequence_mining Sequence mining]\n:# [http://en.wikipedia.org/wiki/GSP_Algorithm GSP Algorithm]\n:# [http://en.wikipedia.org/wiki/Apriori_algorithm Apriori algorithm]\n:# [http://en.wikipedia.org/wiki/Market_basket_analysis Market basket analysis]\n:# [http://en.wikipedia.org/wiki/Time_series Time series]\n\n\n----','utf-8'),(69,'== Introduction to Association Rule Mining ==\n\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n\n\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(70,'== Introduction to Association Rule Mining ==\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(71,'\n== 20130530_144254 ==\n\n=== Introductory Articles to R Statistical Computing Software ===\n\n* http://www.cran.r-project.org/doc/manuals/R-intro.pdf\n: An Introduction to R -- Notes on R: A Programming Environment for Data Analysis and Graphics (Version 3.0.1 (2013-05-16))\n\n\n\n\n== 20130528_213910 ==\n\n=== suppress the command output in R ===\n\n* use sink() function, please!\n <pre>\nsink(file=\"arm_inspect.650k-support_0.012-225x488.log\") # enabling sink operation \n# (to redirect all the stdout to the file specified)\n\ninspect(f_650k_0012)\n\nsink() # disabling sink operation\n# (after this command, you can see the output message to stdout)\n</pre>\n\n\n* References\n\n:* https://stat.ethz.ch/pipermail/r-help/2007-August/138070.html\n:: R sink behavior (stat.ethz.ch)\n\n:* http://stat.ethz.ch/R-manual/R-patched/library/base/html/sink.html\n:: Send R Output to a File\n\n\n\n\n=== defining function in R (user-defined function in R) ===\n\n* use allocation operator \'<-\' to define my custom function\n <pre>\niowa_arm_f <- function (opt_dsrc, opt_support) {\n	print(\">> IOWA ARM: started\");\n	sink(file=\"/dev/null\");\n	t <- as(opt_dsrc, \"transactions\");\n	f <- eclat(opt_dsrc, parameter=list(support=opt_support, tidLists=T));\n	sink();\n	print(\">> IOWA ARM: finished\");\n	dim(tidLists(f));\n	return(f);\n}\n</pre>\n\n* use the function as usual\n <pre>\nf <- iowa_arm_f(ciop_d010, 0.012);\ndim(tidLists(f));\n</pre>\n\n* References\n\n:* http://www.statmethods.net/management/userfunctions.html\n:: User-written Functions -- Quick-R ((B.GOOD))\n\n\n\n== 20130524_103604 ==\n\n\n=== source() ===\n\n <pre>\nsource(\'data_set.R\');\n</pre>\n\n=== arules eclat() algorithm for association rules mining ===\n\n <pre>\nlibrary(arules);\ndata <- list(\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_1187\", \"_addr_23813\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\n			 c(\"_addr_23813\", \"_addr_51276758\"),\n			 c(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\n			 c(\"_addr_51276758\", \"_addr_319275\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\n			 c(\"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\n			 c(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\n			 c(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\n			 c(\"_addr_319275\", \"_addr_51276758\")\n			 );\n\nt <- as(data, \"transactions\");\nf <- eclat(data, parameter=list(tidLists=T, support=0.25))\ndim(tidLists(f))\nas(tidLists(f), \"list\")\nimage(tidLists(f))\ninspect(f)\n</pre>\n\n\n\n\n=== read.table(), write.table(), unlist() ===\n\n <pre>\nblusjune@jimi-hendrix:[basics] $ cat filein.L20 \n\n248425 , 100276969472\n248772 , 78847959040\n249197 , 139913195520\n251828 , 75536334848\n253182 , 76534030336\n254083 , 143595069440\n254961 , 143598755840\n255185 , 150857949184\n255393 , 118433374208\n255755 , 100324941824\n256025 , 85407301632\n258666 , 95264866304\n259078 , 142196629504\n261133 , 88597774336\n263287 , 97312505856\n264135 , 112585678848\n267323 , 96259047424\n267351 , 140665122816\n267634 , 139540049920\n268982 , 117314224128\n</pre>\n\n <pre>\n> datain;\n\n   timestamp      address\n   1     248425 100276969472\n   2     248772  78847959040\n   3     249197 139913195520\n   4     251828  75536334848\n   5     253182  76534030336\n   6     254083 143595069440\n   7     254961 143598755840\n   8     255185 150857949184\n   9     255393 118433374208\n   10    255755 100324941824\n   11    256025  85407301632\n   12    258666  95264866304\n   13    259078 142196629504\n   14    261133  88597774336\n   15    263287  97312505856\n   16    264135 112585678848\n   17    267323  96259047424\n   18    267351 140665122816\n   19    267634 139540049920\n   20    268982 117314224128\n</pre>\n\n <pre>\ndatain <- read.table(\'filein.L20\', col.names=c(\"timestamp\", \"address\"), sep=\",\", header=F);\ndataout <- datain %% 10;\ndataout_ul <- unlist(dataout, use.names=F)\nwrite.table(dataout, file=\"fileout.L20\", append=F, quote=T, sep=\" , \", row.names=F, col.names=T);\n</pre>\n\n\n\n\n=== scan() ===\n\n <pre>\ndatain_double <- scan(file=\"d010\", what=double(), sep=\"\", strip.white=T);\ndatain_char <- scan(file=\"d010\", what=\"character\", sep=\"\\n\", strip.white=T);\ndatain_char <- scan(file=\"d010\", what=\"char\", sep=\";\", strip.white=T);\n</pre>\n\n\n\n\n* Case 1: \'d010\' - data file of wrong format\n\n <pre>\nblusjune@jimi-hendrix:[basics] $ cat d010\n8 4 0 5 7;\n11 5 8;\n8 6 3 5 1;\n2 11 1;\n4 6 3 12 4;\n6 7 0 10 4 7;\n7 6 3;\n3 10 7 6 7 6;\n7 10;\n11 10\n</pre>\n\n </pre>\n> scan(file=\'d010\', what=\"numeric\", sep=\";\\n\", strip.white=T)\nError in scan(file = \"d010\", what = \"numeric\", sep = \";\\n\", strip.white = T) : \n  invalid \'sep\' value: must be one byte\n\n> scan(file=\'d010\', what=\"numeric\", sep=\"\\n\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7;\"    \"11 5 8;\"       \"8 6 3 5 1;\"    \"2 11 1;\"      \n [5] \"4 6 3 12 4;\"   \"6 7 0 10 4 7;\" \"7 6 3;\"        \"3 10 7 6 7 6;\"\n [9] \"7 10;\"         \"11 10\"        \n\n> scan(file=\'d010\', what=\"numeric\", sep=\";\", strip.white=T)\nRead 19 items\n [1] \"8 4 0 5 7\"    \"\"             \"11 5 8\"       \"\"             \"8 6 3 5 1\"   \n [6] \"\"             \"2 11 1\"       \"\"             \"4 6 3 12 4\"   \"\"            \n[11] \"6 7 0 10 4 7\" \"\"             \"7 6 3\"        \"\"             \"3 10 7 6 7 6\"\n[16] \"\"             \"7 10\"         \"\"             \"11 10\"       \n\n> scan(file=\'d010\', what=\"numeric\", sep=\"\", strip.white=T)\nRead 40 items\n [1] \"8\"   \"4\"   \"0\"   \"5\"   \"7;\"  \"11\"  \"5\"   \"8;\"  \"8\"   \"6\"   \"3\"   \"5\"  \n[13] \"1;\"  \"2\"   \"11\"  \"1;\"  \"4\"   \"6\"   \"3\"   \"12\"  \"4;\"  \"6\"   \"7\"   \"0\"  \n[25] \"10\"  \"4\"   \"7;\"  \"7\"   \"6\"   \"3;\"  \"3\"   \"10\"  \"7\"   \"6\"   \"7\"   \"6;\" \n[37] \"7\"   \"10;\" \"11\"  \"10\" \n\n</pre>\n\n\n\n\n* Case 2: \'d020\' - data file of good format\n\n <pre>\nblusjune@jimi-hendrix:[basics] $ cat d020\n8 4 0 5 7; 11 5 8; 8 6 3 5 1; 2 11 1; 4 6 3 12 4; 6 7 0 10 4 7; 7 6 3; 3 10 7 6 7 6; 7 10; 11 10\n</pre>\n\n <pre>\n> scan(file=\'d020\', what=\"numeric\", sep=\"\", strip.white=T)\nRead 40 items\n [1] \"8\"   \"4\"   \"0\"   \"5\"   \"7;\"  \"11\"  \"5\"   \"8;\"  \"8\"   \"6\"   \"3\"   \"5\"  \n[13] \"1;\"  \"2\"   \"11\"  \"1;\"  \"4\"   \"6\"   \"3\"   \"12\"  \"4;\"  \"6\"   \"7\"   \"0\"  \n[25] \"10\"  \"4\"   \"7;\"  \"7\"   \"6\"   \"3;\"  \"3\"   \"10\"  \"7\"   \"6\"   \"7\"   \"6;\" \n[37] \"7\"   \"10;\" \"11\"  \"10\" \n\n> scan(file=\'d020\', what=\"numeric\", sep=\";\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7\"    \"11 5 8\"       \"8 6 3 5 1\"    \"2 11 1\"       \"4 6 3 12 4\"  \n [6] \"6 7 0 10 4 7\" \"7 6 3\"        \"3 10 7 6 7 6\" \"7 10\"         \"11 10\"     \n</pre>\n\n\n\n\n* Case 3: \'d030\' - data file of good format\n\n <pre>\n8 4 0 5 7\n11 5 8\n8 6 3 5 1\n2 11 1\n4 6 3 12 4\n6 7 0 10 4 7\n7 6 3\n3 10 7 6 7 6\n7 10\n11 10\n</pre>\n\n <pre>\n> scan(file=\'d030\', what=\'numeric\', sep=\"\", strip.white=T)\nRead 40 items\n [1] \"8\"  \"4\"  \"0\"  \"5\"  \"7\"  \"11\" \"5\"  \"8\"  \"8\"  \"6\"  \"3\"  \"5\"  \"1\"  \"2\"  \"11\"\n[16] \"1\"  \"4\"  \"6\"  \"3\"  \"12\" \"4\"  \"6\"  \"7\"  \"0\"  \"10\" \"4\"  \"7\"  \"7\"  \"6\"  \"3\" \n[31] \"3\"  \"10\" \"7\"  \"6\"  \"7\"  \"6\"  \"7\"  \"10\" \"11\" \"10\"\n\n> scan(file=\'d030\', what=\'numeric\', sep=\";\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7\"    \"11 5 8\"       \"8 6 3 5 1\"    \"2 11 1\"       \"4 6 3 12 4\"  \n [6] \"6 7 0 10 4 7\" \"7 6 3\"        \"3 10 7 6 7 6\" \"7 10\"         \"11 10\"       \n\n> scan(file=\'d030\', what=\'numeric\', sep=\"\\n\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7\"    \"11 5 8\"       \"8 6 3 5 1\"    \"2 11 1\"       \"4 6 3 12 4\"  \n [6] \"6 7 0 10 4 7\" \"7 6 3\"        \"3 10 7 6 7 6\" \"7 10\"         \"11 10\"  \n</pre>\n\n\n\n\n== 20130502_032415 ==\n\n=== R script ===\n\n <pre>\na1mjjung@secm:[r_stat] $ cat > iowa_anal_1010.R << EOF\n#iowa_anal_1010.R\n#20130430_140506\n\n\nlibrary(e1071);\nlibrary(arules);\nlibrary(scatterplot3d);\n\n\nrm(list=ls());\n\n\nmyd <- read.table(\'r_stat_infile\', col.names=c(\"_hitcnt_\", \"_addr_\", \"_mwid_\"));\nmyd_mwid <- unlist(myd[3]);\nmyd_addr <- unlist(myd[2]);\nmyd_hitcnt <- unlist(myd[1]);\n\n\nmymat <- cbind(myd_mwid, myd_addr, myd_hitcnt);\ncolnames(mymat) <- c(\"_mwid_\", \"_addr_\", \"_hcnt_\");\nnocl <- 22; itmax <- 100; (cl1 <- kmeans(mymat, nocl, iter.max=itmax));\n\n\npng(\'output_col_by_hitcnt.png\', width=1024, height=2048);\nplot(myd_mwid, myd_addr, col=myd_hitcnt);\ndev.off();\npng(\'output_col_by_cluster.png\', width=1024, height=2048);\nplot(myd_mwid, myd_addr, col=cl1$cluster);\ndev.off();\nEOF\n\n</pre>\n\n\n\n== 20130428_233708 ==\n\n\n\n=== R (r_stat) references ===\n\n\n* manual/introduction to R\n:- [http://cran.r-project.org/doc/manuals/R-intro.pdf R introduction]\n:- [http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf arules (Association Rules Mining)]\n:- [http://www.cl.cam.ac.uk/~av308/vlachos_msc_thesis.pdf using SVM]\n\n\n* \'%in%\' and example() function\n\n <pre>\nOn 8 May 2011 21:18, Berwin A Turlach <[hidden email]> wrote:\n> G\'day Dan,\n>\n> On Sun, 8 May 2011 05:06:27 -0400\n> Dan Abner <[hidden email]> wrote:\n>\n>> Hello everyone,\n>>\n>> I am attempting to use the %in% operator with the ! to produce a NOT\n>> IN type of operation. Why does this not work? Suggestions?\n\nAlternatively,\n\nexample(`%in%`)\n\nor\n\n`%ni%` = Negate(`%in%`)\n\nHTH,\n\nbaptiste\n</pre>\n\n\n\n\n=== kmeans-svm #2 :: applying the kmeans result to svm (as a guideline for supervising) ((B.GOOD)) ===\n\n <pre>\n\n# combining the columns xy (time_t1, file_id) and cl4 (cluster_id)\nxy_cl4 <- cbind(xy, cl4$cluster);\n\n# assign the column names\ncolnames(xy_cl4) <- c(\"time_t1\", \"file_id\", \"cluster_id\");\n\n# prepare the x\'s and y for svm\nsmv_x <- subset(xy_cl4, select= -cluster_id);\nsmv_y <- subset(xy_cl4, select= cluster_id);\n\n# load the library \'e1071\'\nlibrary(e1071);\n\n# execute svm analysis to get model \'m\'\nm <- svm(svm_x, svm_y);\n\n# predict a new value with the model \'m\' and x\'s data\npred <- predict(m, svm_x, decision.values = T);\n\n# check the prediction result by comparing the predicted value \'pred\' with original value \'svm_y\'\ntable(pred, svm_y);\n\n# plot the result\nplot(cmdscale(dist(xy_cl4[, -3])), col=pred, pch=c(\"o\", \"+\")[1:120 %in% m$index + 1]);\n\n# write the resultant xy_cl4 table to the file(.csv)\nwrite.csv(xy_cl4, file=\"xy_cl4.csv\", row.names=T, col.names=T);\n\n</pre>\n\n\n\n\n=== kmeans-svm #1 :: kmeans example ((B.GOOD)) ===\n\n <pre>\n\n# clear all the data in memory\nrm(list=ls());\n\n# read the data from file (.csv format)\nmyd <- read.csv(\'traces/iowa_v3.csv\', header=T);\n\n# unlist() to convert \'list\' type data to \'vector\' type data for further numerical calculation\nmyd_c1 <- unlist(myd[1]); # field: time_t0\nmyd_c2 <- unlist(myd[2]); # field: time_t1\nmyd_c3 <- unlist(myd[3]); # field: prog\nmyd_c4 <- unlist(myd[4]); # field: file\n\n# create 2-D matrix (x-y) for 2-D kmeans analysis\nxy <- cbind(myd_c2, myd_c4); # combining columns: \'time_t1\' and \'file\'\n\n# assign the column names for x-label and y-label for better look of plot()\ncolnames(xy) <- c(\"time_t1\", \"file_id\");\n\n# load the library \'e1071\'\nlibrary(e1071);\n\n# execute kmeans analysis until we get satisfactory clustering performance value (value = between_SS / total_SS)\n# number of clusters is set to \'6\' huristically after plotting data xy to see the outlook\n# number of maximum iterations is set to \'100\' huristically after executing kmeans analysis several times\ncl1 <- kmeans(xy, 6, iter.max=100); # initial trial, the clustering performance value is ... (cannot evaluate)\ncl2 <- kmeans(xy, 6, iter.max=100); # second trial, the clustering performance value is higher than the first trial\ncl3 <- kmeans(xy, 6, iter.max=100); # third trial, the clustering performance value is higher than the second trial\n(cl4 <- kmeans(xy, 6, iter.max=100)); # fourth trial, the clustering performance value is not going higher than the last trial\n\n# 2-D plotting\nplot(xy, col=cl4$cluster);\n\n</pre>\n\n\n\n\n* most outer parentheses are used to see the result of execution\n\n <pre>\n\n> (cl4 <- kmeans(xy, 6, iter.max=100))\nK-means clustering with 6 clusters of sizes 19, 22, 19, 15, 20, 25\n\nCluster means:\n   time_t1    file_id\n1 10.52632 32.2105263\n2 14.40909 43.3181818\n3 18.57895 33.5263158\n4 22.00000  0.2666667\n5  6.50000 11.7000000\n6  2.00000  1.0000000\n\nClustering vector:\n  time_t11   time_t12   time_t13   time_t14   time_t15   time_t16   time_t17 \n         6          6          6          6          5          5          5 \n  time_t18   time_t19  time_t110  time_t111  time_t112  time_t113  time_t114 \n         5          1          2          1          1          2          2 \n time_t115  time_t116  time_t117  time_t118  time_t119  time_t120  time_t121 \n         2          2          3          3          3          3          4 \n time_t122  time_t123  time_t124  time_t125  time_t126  time_t127  time_t128 \n         4          4          6          6          6          6          6 \n time_t129  time_t130  time_t131  time_t132  time_t133  time_t134  time_t135 \n         5          5          5          5          1          1          1 \n time_t136  time_t137  time_t138  time_t139  time_t140  time_t141  time_t142 \n         1          2          2          2          2          2          3 \n time_t143  time_t144  time_t145  time_t146  time_t147  time_t148  time_t149 \n         3          3          4          4          4          6          6 \n time_t150  time_t151  time_t152  time_t153  time_t154  time_t155  time_t156 \n         6          6          6          5          5          5          5 \n time_t157  time_t158  time_t159  time_t160  time_t161  time_t162  time_t163 \n         1          1          1          1          2          2          2 \n time_t164  time_t165  time_t166  time_t167  time_t168  time_t169  time_t170 \n         2          3          3          3          3          4          4 \n time_t171  time_t172  time_t173  time_t174  time_t175  time_t176  time_t177 \n         4          6          6          6          6          6          5 \n time_t178  time_t179  time_t180  time_t181  time_t182  time_t183  time_t184 \n         5          5          5          1          1          1          1 \n time_t185  time_t186  time_t187  time_t188  time_t189  time_t190  time_t191 \n         2          2          2          2          3          3          3 \n time_t192  time_t193  time_t194  time_t195  time_t196  time_t197  time_t198 \n         3          4          4          4          6          6          6 \n time_t199 time_t1100 time_t1101 time_t1102 time_t1103 time_t1104 time_t1105 \n         6          6          5          5          5          5          1 \ntime_t1106 time_t1107 time_t1108 time_t1109 time_t1110 time_t1111 time_t1112 \n         1          1          1          2          2          2          2 \ntime_t1113 time_t1114 time_t1115 time_t1116 time_t1117 time_t1118 time_t1119 \n         3          3          3          3          4          4          4 \ntime_t1120 \n         6 \n\nWithin cluster sum of squares by cluster:\n[1]  51.89474 212.09091 117.36842  14.93333  53.20000 122.00000\n (between_SS / total_SS =  98.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"        \n> \n\n</pre>\n\n\n\n\n=== example() function in R ((B.GOOD)) ===\n\nexample() function is very very very useful to grasp the quick overview of some \'concept\' or \'function\' in R\n\n <pre>\n\nexample(svm)\nexample(kmeans)\nexample(\'%in%\')\nexample(rm)\nexample(plot)\n\n</pre>\n\n\n\n\n=== convert list to vector ===\n\nNote: the first line of \'iowa_v3.csv\' file should be the header information line (not the empty line containing just \',\')\n\n <pre>\nx_as_list_type <- read.csv(\'traces/iowa_v3.csv\', header=T)\nx_as_int_vector <- unlist(x_as_list_type, use.name=F)\n</pre>\n\n\n\n\n== 20130329_111903 ==\n\n=== R script file execution from command line ===\n\n$ cat > ex1.R << EOF\n <pre>\nlibrary(\"arules\")\ndata(\"Epub\")\nEpub\nsummary(Epub)\nquit(save=\"no\")\n\n## NOTE:\n# the last line \'quit(save=\"no\")\' is very important\n# to avoid the hang-like situation of R\nEOF\n</pre>\n\n$ R < ex1.R\n <pre>\nJob <13833365> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura009>>\n\nR version 2.15.2 (2012-10-26) -- \"Trick or Treat\"\nCopyright (C) 2012 The R Foundation for Statistical Computing\nISBN 3-900051-07-0\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType \'license()\' or \'licence()\' for distribution details.\n\nR is a collaborative project with many contributors.\nType \'contributors()\' for more information and\n\'citation()\' on how to cite R or R packages in publications.\n\nType \'demo()\' for some demos, \'help()\' for on-line help, or\n\'help.start()\' for an HTML browser interface to help.\nType \'q()\' to quit R.\n\n[Previously saved workspace restored]\n\n> Loading required package: Matrix\nLoading required package: lattice\n\nAttaching package: \'arules\'\n\nThe following object(s) are masked from \'package:base\':\n\n    %in%, write\n\n> > transactions in sparse format with\n 15729 transactions (rows) and\n 936 items (columns)\n> transactions as itemMatrix in sparse format with\n 15729 rows (elements/itemsets/transactions) and\n 936 columns (items) and a density of 0.001758755 \n\nmost frequent items:\ndoc_11d doc_813 doc_4c6 doc_955 doc_698 (Other) \n    356     329     288     282     245   24393 \n\nelement (itemset/transaction) length distribution:\nsizes\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n11615  2189   854   409   198   121    93    50    42    34    26    12    10 \n   14    15    16    17    18    19    20    21    22    23    24    25    26 \n   10     6     8     6     5     8     2     2     3     2     3     4     5 \n   27    28    30    34    36    38    41    43    52    58 \n    1     1     1     2     1     2     1     1     1     1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.646   2.000  58.000 \n\nincludes extended item information - examples:\n   labels\n1 doc_11d\n2 doc_13d\n3 doc_14c\n\nincludes extended transaction information - examples:\n      transactionID           TimeStamp\n10792  session_4795 2003-01-02 10:59:00\n10793  session_4797 2003-01-02 21:46:01\n10794  session_479a 2003-01-03 00:50:38\n> \n</pre>\n\n\n=== paste() example ===\n\n* example 1\n <pre>\n> 1:6\n[1] 1 2 3 4 5 6\n> paste(1:6)\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n> paste(\"A\", 1:6)\n[1] \"A 1\" \"A 2\" \"A 3\" \"A 4\" \"A 5\" \"A 6\"\n> paste(\"A\", 1:6, sep=\"\")\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\" \"A6\"\n> 2:7\n[1] 2 3 4 5 6 7\n> seq(8,3,by=-1)\n[1] 8 7 6 5 4 3\n> paste(\"A\", 1:6, 2:7, seq(8,3,by=-1), sep=\"\")\n[1] \"A128\" \"A237\" \"A346\" \"A455\" \"A564\" \"A673\"\n> paste(\"A\", 1:6, 2:7, seq(8,3,by=-1), sep=\"-\")\n[1] \"A-1-2-8\" \"A-2-3-7\" \"A-3-4-6\" \"A-4-5-5\" \"A-5-6-4\" \"A-6-7-3\"\n> paste(\"A\", 1:6, 2:7, seq(8,3,by=-1), sep=\"/\")\n[1] \"A/1/2/8\" \"A/2/3/7\" \"A/3/4/6\" \"A/4/5/5\" \"A/5/6/4\" \"A/6/7/3\"\n</pre>\n\n* example 2\n <pre>\n> stopifnot(identical(str1 <- paste(\"A\", 1:6, sep=\"\"), str2 <- paste0(\"A\", 1:6)))\n> str1\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\" \"A6\"\n> str2\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\" \"A6\"\n\n> paste(\"Today is\", date())\n[1] \"Today is Fri Mar 29 11:22:15 2013\"\n</pre>\n\n\n\n\n== 20130327_183541 ==\n\n=== list() examples ===\n\n <pre>\n> myl = list(apple=1, banana=2, cherry=3, durian=4, elderberry=5)\n> myl\n$apple\n[1] 1\n\n$banana\n[1] 2\n\n$cherry\n[1] 3\n\n$durian\n[1] 4\n\n$elderberry\n[1] 5\n\n> myl$apple\n[1] 1\n> myl$banana\n[1] 2\n> myl$cherry\n[1] 3\n> myl$durian\n[1] 4\n> myl$elderberry\n[1] 5\n> \n</pre>\n\n\n=== read-from/save-to a file ===\n\n <pre>\n# create a formatted transaction data\n> myd <- paste(\"apple,banana\", \"apple\", \"banana,cherry\", \"banana,cherry,durian\", sep=\"\\n\")> cat(myd)\napple,banana\napple\nbanana,cherry\n\n# write the transaction data to the file\n> write(myd, file = \"myd_basket_2\") \n\n# read the transaction data from the file, and save it to a variable\n> myt <- read.transactions(\"myd_basket_2\", format = \"basket\", sep=\",\")\n\n# inspect the transaction variable\n> inspect(myt)\n  items   \n1 {apple, \n   banana}\n2 {apple} \n3 {banana,\n   cherry}\n4 {banana,\n   cherry,\n   durian}\n</pre>\n\n\n\n\n=== clear workspace (delete data) ===\n\n* References\n* [https://stat.ethz.ch/pipermail/r-help/2007-August/137694.html Clear Workspace in R]\n* [http://stackoverflow.com/questions/11761992/remove-data-from-workspace Advanced method to clear data in R]\n\n* simply, remove three data \'data_1\', \'data_2\', \'data_3\'\n <pre>\nrm(data_1, data_2, data_3)\n</pre>\n\n* remove all the data searchable by ls()\n <pre>\nrm(list = ls())\n# \'list\' is a name of parameter to be passed into \'rm()\' function,\n# so it cannot be changed, it should be \"list\" literally.\n</pre>\n\n* remove all objects whose name begins with the string \"tmp\"\n <pre>\nrm(list = ls()[grep(\"^tmp\", ls())])\nrm(list = ls(pattern=\"^tmp\"))	# making use of the \'pattern\' argument\n</pre>\n\n== 20130313_172639 ==\n\n\n=== SVM example ===\n\n <pre>\n     data(iris)\n     attach(iris)\n     \n     ## classification mode\n     # default with factor response:\n     model <- svm(Species ~ ., data = iris)\n     \n     # alternatively the traditional interface:\n     x <- subset(iris, select = -Species)\n     y <- Species\n     model <- svm(x, y) \n     \n     print(model)\n     summary(model)\n     \n     # test with train data\n     pred <- predict(model, x)\n     # (same as:)\n     pred <- fitted(model)\n     \n     # Check accuracy:\n     table(pred, y)\n     \n     # compute decision values and probabilities:\n     pred <- predict(model, x, decision.values = TRUE)\n     attr(pred, \"decision.values\")[1:4,]\n     \n     # visualize (classes by color, SV by crosses):\n     plot(cmdscale(dist(iris[,-5])),\n          col = as.integer(iris[,5]),\n          pch = c(\"o\",\"+\")[1:150 %in% model$index + 1])\n</pre>\n\n\n=== cmdscale (Classical MultiDimensional Scaling) ===\n\n* Description\n: Classical multidimensional scaling of a data matrix. (a.k.a. principal coordinates analysis (Gower, 1966)\n\n* Usage\n: cmdscale(d, k=2, eig=FALSE, add=FALSE, x.ret=FALSE)\n\n* Arguments\n: \'\'\'d [mandatory]\'\'\': a distance structure such as that returned by \'dist\' or a full symmetric matrix containing the dissimilarities\n: k [optional]: the maximum dimension of the space which the data are to be represented in; must be in {1, 2, ..., n-1}\n: eig [optional]: indicates whether eigenvalues should be returned\n: add [optional]: logical indicating if an additive constant c* should be computed, and added to the non-diagonal dissimilarities such that the modified dissimilarities are Euclidean\n: x.ret [optional]: indicates whether the doubly centered symmetric distance matrix should be returned\n\n* Details\n: Multidimensional scaling takes a set of dissimilarities and returns a set of points such that the distances between the points are approximately equal to the dissimilarities\n\n\n=== dist (Distance matrix computation) ===\n\nThis computes and returns the distance matrix computed by using the specified distance measure to compute the distances between the rows of a data matrix\n\n* Usage\n: dist(x, method = \"euclidean\", diag = FALSE, upper = FALSE, p = 2)\n\n* Arguments\n: x\n:: a numeric matrix, data frame or \'dist\' object\n: method\n:: the distance measure to be used. this must be one of \"euclidean\", \"maximum\", \"manhattan\", \"canberra\", \"binary\", or \"minkowski\" (any unambiguous substring can be given)\n: diag\n\n* Examples (by blusjune)\n\n <pre>\n> mat_a\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    1    1\n[2,]    2    2    2    2\n[3,]    3    3    3    3\n[4,]    0    0    0    0\n> dist(mat_a)\n  1 2 3\n2 2    \n3 4 2  \n4 2 4 6\n> cmdscale(dist(mat_a))\n     [,1]          [,2]\n[1,]    1  5.809542e-08\n[2,]   -1  3.057654e-09\n[3,]   -3  9.172961e-09\n[4,]    3 -9.172961e-09\n> dist(cmdscale(dist(mat_a)))\n  1 2 3\n2 2    \n3 4 2  \n4 2 4 6\n</pre>\n\n <pre>\n> mat_b\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    2    2    2\n[3,]    0    0    0\n> dist(mat_b)\n         1        2\n2 1.732051         \n3 1.732051 3.464102\n> cmdscale(dist(mat_b))\n              [,1]         [,2]\n[1,]  7.412908e-33 1.564993e-08\n[2,] -1.732051e+00 2.477578e-09\n[3,]  1.732051e+00 2.477578e-09\n> dist(cmdscale(dist(mat_b)))\n         1        2\n2 1.732051         \n3 1.732051 3.464102\n\n> ((1-2)**2 + (1-2)**2 + (1-2)**2) ** 0.5\n[1] 1.732051\n> ((2-0)**2 + (2-0)**2 + (2-0)**2) ** 0.5\n[1] 3.464102\n</pre>\n\n\n== 20130306_185022 ==\n\n=== R package (\'e1071\') installation from command line ===\n\n: Assumes that already downloaded and unpacked properly the \'e1071_1.6-1.tar.gz\' file\n\n <pre>\n\n1mjjung@secm:[e1071] $ R CMD INSTALL e1071\nJob <13412785> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura007>>\nWarning: invalid package \'e1071\'\nError: ERROR: no packages specified\n\n\n\n\na1mjjung@secm:[e1071] $ cd ..\nla1mjjung@secm:[e1071_build] $ l\ntotal 280\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:45 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\n\n\n\n\na1mjjung@secm:[e1071_build] $ R CMD INSTALL e1071\nJob <13412786> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n* installing to library \'/home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15\'\n* installing *source* package \'e1071\' ...\n** package \'e1071\' successfully unpacked and MD5 sums checked\nchecking for C++ compiler default output file name... a.out\nchecking whether the C++ compiler works... yes\nchecking whether we are cross compiling... no\nchecking for suffix of executables... \nchecking for suffix of object files... o\nchecking whether we are using the GNU C++ compiler... yes\nchecking whether g++ accepts -g... yes\n** libs\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c Rsvm.c -o Rsvm.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cmeans.c -o cmeans.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cshell.c -o cshell.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c floyd.c -o floyd.o\ng++ -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c svm.cpp -o svm.o\ng++ -shared -L/usr/local/lib64 -o e1071.so Rsvm.o cmeans.o cshell.o floyd.o svm.o\ninstalling to /home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15/e1071/libs\n** R\n** inst\n** preparing package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n   \'svmdoc.Rnw\' \n** testing if installed package can be loaded\n\n* DONE (e1071)\n\n\n\n\na1mjjung@secm:[e1071_build] $ R\nJob <13412787> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n\nR version 2.15.2 (2012-10-26) -- \"Trick or Treat\"\nCopyright (C) 2012 The R Foundation for Statistical Computing\nISBN 3-900051-07-0\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType \'license()\' or \'licence()\' for distribution details.\n\nR is a collaborative project with many contributors.\nType \'contributors()\' for more information and\n\'citation()\' on how to cite R or R packages in publications.\n\nType \'demo()\' for some demos, \'help()\' for on-line help, or\n\'help.start()\' for an HTML browser interface to help.\nType \'q()\' to quit R.\n\n> ?svm\nNo documentation for \'svm\' in specified packages and libraries:\nyou could try \'??svm\'\n> library(e1071) \nLoading required package: class\n> ?svm\n> \nSave workspace image? [y/n/c]: y\n\n\n\n\na1mjjung@secm:[e1071_build] $ pwd\n/home/X0101/a1mjjung/x/R/e1071_build\n\na1mjjung@secm:[e1071_build] $ ls -alF\ntotal 284\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:47 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\n-rw------- 1 a1mjjung X0101     25 Mar  6 18:47 .Rhistory\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:46 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\n\n</pre>\n\n\n\n== 20130304_190007 ==\n\n\n\n=== test type of some object ===\n\n <pre>\n> x <- scan(\"/tmp/scan.txt\", what=list(NULL, name=character()))\n> x <- x[sapply(x, length) > 0]\n> is.vector(x)\n\n\n> x = mat.or.vec(100,1)\n> if (is.integer(x) == TRUE) { print (\"YES\") } else { print (\"NO\") }\n[1] \"NO\"\n> if (is.vector(x) == TRUE) { print (\"YES, vector\") } else { print (\"NO, NOT vector\") }\n[1] \"YES, vector\"\n</pre>\n\n\n\n\n=== Data import (load data from a file) ===\n\n* scan()\n <pre>\n > x1 <- scan(\"/etc/hosts\", what=character())\n\n > x1     \n [1] \"127.0.0.1\"       \"localhost\"       \"#127.0.1.1\"      \"stones\"         \n [5] \"#\"               \"The\"             \"following\"       \"lines\"          \n [9] \"are\"             \"desirable\"       \"for\"             \"IPv6\"           \n[13] \"capable\"         \"hosts\"           \"::1\"             \"ip6-localhost\"  \n[17] \"ip6-loopback\"    \"fe00::0\"         \"ip6-localnet\"    \"ff00::0\"        \n[21] \"ip6-mcastprefix\" \"ff02::1\"         \"ip6-allnodes\"    \"ff02::2\"        \n[25] \"ip6-allrouters\"  \"10.0.2.15\"       \"stones-eth0\"     \"#192.168.1.109\" \n[29] \"stones\"          \"hd-master-01\"    \"#192.168.1.110\"  \"pavement\"       \n[33] \"hd-slave-0001\"   \"192.168.1.112\"   \"stones\"          \"hd-master-01\"   \n[37] \"hd-slave-0001\"   \"kandinsky\"       \"192.168.1.110\"   \"pavement\"       \n[41] \"hd-slave-0002\"  \n</pre>\n\n* read.table()\n <pre>\n> iot_r <- read.table(\'tracelog.msn_filesrvr.R\')  \n</pre>\n\n\n\n=== function defintion, for loop in R ===\n\n <pre>\n> avg_smoothing <- function(src, srcl, sf) {\n    tgtl = srcl + 1 - sf\n    tgt <- mat.or.vec(tgtl, 1)\n    for (i in 1:tgtl) {\n        tgt[i] = mean(src[i:(i+sf-1)])\n    }\n    return (tgt)\n}\n\n> vec1 <- rnorm(100, mean=10, sd=1)\n> vec1_sf2 <- avg_smoothing(vec1, 100, 2)\n> vec1_sf4 <- avg_smoothing(vec1, 100, 4)\n> vec1_sf8 <- avg_smoothing(vec1, 100, 8)\n\n> plot(vec1, col=\"gray\", type=\"l\")\n> points(vec1_sf2, col=\"red\", type=\"l\")\n> points(vec1_sf4, col=\"blue\", type=\"l\")\n> points(vec1_sf8, col=\"green\", type=\"l\")\n</pre>\n\n\n\n\n== 20130127_225539 ==\n\n=== R Installation ===\n\n* to install R statistical computing software\n** me@matrix$ sudo apt-get install r-base\n\n* to start R from command line, just type \'R\' in your terminal\n** me@matrix$ R\n\n* RStudio download [http://www.rstudio.com/ide/download/]\n** RStudio Desktop [http://www.rstudio.com/ide/download/desktop]\n**: If you run R on your desktop\n** RStudio Server [http://www.rstudio.com/ide/download/server]\n**: If you run R on a Linux server and want to enable users to remotely access RStudio using a web browser\n*** RStudio Server Getting Started [http://www.rstudio.com/ide/docs/server/getting_started]\n\n <pre>\nsudo apt-get install r-base\nwget http://download2.rstudio.org/rstudio-server-0.97.312-amd64.deb\nsudo gdebi rstudio-server-0.97.312-amd64.deb \nbsc.adm.create_me\ngoogle-chrome http://kandinsky:8787 # type ID/PW (me?!)\n</pre>\n\n\n* Debian Packages of R Software [http://cran.r-project.org/bin/linux/debian/README.html]\n\n=== R Guide/Tutorial/Example ===\n\n* R Tutorial [http://www.r-tutor.com/]\n** R Tutorial:: Data Import [http://www.r-tutor.com/r-introduction/data-frame/data-import]\n\n\n* Example R scripts [http://people.eng.unimelb.edu.au/aturpin/R/index.html]]\n\n\n* R by example [http://www.mayin.org/ajayshah/KB/R/index.html]\n\n\n* R example:: Kmeans [http://stat.ethz.ch/R-manual/R-patched/library/stats/html/kmeans.html]\n\n\n* R package install howto\n; e1071\n: Misc Functions of the Department of Statistics (e1071), TU Wien\n:* package-installation and loading\n install.packages(\"e1071\") # installing the package \'e1071\'\n library(\"e1071\") # loading the installed package \'e1071\'\n\n\n* Importing SAS, SPSS and Stata files into R [http://staff.washington.edu/glynn/r_import.pdf]\n\n\n=== R Troubleshooting ===\n\n* Problems importing csv file/converting from integer to double in R [http://stackoverflow.com/questions/8381839/problems-importing-csv-file-converting-from-integer-to-double-in-r]\n\n\n=== Misc. ===\n\n* WEKA Tutorial [http://www.cs.utexas.edu/users/ml/tutorials/Weka-tut/]\n* weka is a metric prefix for 10^30','utf-8'),(72,'== Introduction to Association Rule Mining ==\n\n\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(73,'== Introduction to Association Rule Mining ==\n\n\n\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(74,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(75,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n\n\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(76,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n\n\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ((B.GOOD)) ===\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(77,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n\n\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series','utf-8'),(78,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(79,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n: FIM/ARM (Frequent Itemset Mining / Association Rule Mining)\n\n\n== FIM/ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM/ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(80,': FIM/ARM (Frequent Itemset Mining / Association Rule Mining)\n\n== FIM/ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM/ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(81,'FIM/ARM (Frequent Itemset Mining / Association Rule Mining)\n\n== FIM/ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM/ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(82,'FIM/ARM (Frequent Itemset Mining / Association Rule Mining)\n\n\n\n== FIM/ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM/ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(83,'Frequent Itemset Mining (FIM) / Association Rule Mining (ARM)\n\n\n\n\n== FIM/ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM/ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(84,'#REDIRECT [[Frequent Itemset Mining (FIM) / Association Rule Mining (ARM)]]','utf-8'),(85,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(86,'#REDIRECT [[PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]','utf-8'),(87,'#REDIRECT [[Bnote Frequent Itemset Mining (FIM) / Association Rule Mining (ARM)]]','utf-8'),(88,'== Lessons learned ==\n\n* \"An Analysis of Traces from a Production MapReduce Cluster,\" CCGRID 2010\n\n* CCGRID 2010에 소개되었던 CMU의 연구인 [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5493490 An Analysis of Traces from a Production MapReduce Cluster // Kavulya, S. CMU // CCGRID 2010]에는 Yahoo!의 M45 Supercomputing Cluster에서의 10달 간의 MapReduce tracelog에 대한 분석 결과가 있다. 이를 보면, task type과 failure 와의 연관성이 있음을 알 수 있다.\n\n* Failure와 straggler 간의 관계는 다음과 같다. straggler는 아직 명확한 exception이 일어나서 task가 명시적인 failure로 mark가 된 상태는 아니지만, 통상적인 경우보다 꽤 느리게 task가 수행되고 있는 상태를 의미한다.\n\n\n== Excerpts from paper ==\n\n=== Data Summary ===\n\n{| class=\"wikitable\" border=\"1\" \n|+ Summary M45 dataset\n! Key\n! Value\n|-\n| Log Period\n|\n: Apr 25 - Nov 12, 2008\n: Jan 19 - Apr 24, 2009\n|-\n| Hadoop versions\n|\n: 0.16: Apr 2008 -\n: 0.17: Jun 2008 -\n: 0.18: Jan 2009 -\n|-\n| Number of active users\n| 31\n|-\n| Number of jobs\n| 171079\n|-\n| Successful jobs\n| 165948 (97%)\n|-\n| Failed jobs\n| 4100 (2.4%)\n|-\n| Cancelled jobs\n| 1031 (0.6%)\n|-\n| Average maps per job\n| 154 +/- 558 sigma\n|-\n| Average reduces per job\n| 19 +/- 145 sigma\n|-\n| Average nodes per job\n| 27 +/- 22 sigma\n|-\n| Maximum nodes per job\n| 299\n|-\n| Average job duration\n| 1214 +/- 13875 seconds\n|-\n| Maximum job duration\n| 6.84 days\n|-\n| Node days used\n| 132624\n|}\n\n\n=== Memo ===\n\n* There were 4100 failed jobs and 1031 incomplete jobs in our dataset. These failures accounted for 3% of the total jobs run.\n\n* We observed that:\n# <span style=\"color:red\">\'\'\'Most jobs fail within 150 seconds after the first aborted task\'\'\'</span>. Figure 3(c) shows that 90% of jobs exhibited an error latency of less than 150 seconds from the first aborted task to the last retry of that task (the default number of retries for aborted tasks was 4). We observed a maximum error latency of 4.3 days due to a copy failure in a single reduce task. Better diagnosis and recovery approaches are needed to reduce error latency in long-running tasks.\n# Most failures occurred during the map phase. Failures due to task exceptions in the <span style=\"color:red\">\'\'\'map phase\'\'\'</span> were the most prevalent - 36% of these failures were due to <span style=\"color:red\">\'\'\'array indexing errors\'\'\'</span> (see Figure 4). <span style=\"color:red\">\'\'\'IO exceptions\'\'\'</span> were common in the <span style=\"color:red\">\'\'\'reduce phase\'\'\'</span> accounting for 23% of failures. Configuration problems, such as missing files, led to failures during job initialization.\n# Application hangs and node failures were more prevalent in cancelled jobs. Task timeouts, which occur when a task hangs and fails to report progress for more than 10 minutes, and lost TaskTracker daemons due to node or process failures were more prevalent in cancelled jobs than in failed jobs.\n# Spurious exceptions exist in the logs. The logs contained spurious exceptions due to debugging statements that were turned on by default in certain versions of Hadoop. For example, a debug exception to troubleshoot a problem in the DFS client attributed to data buffering, and an exception due to a disabled feature that ignored the nonzero exit codes in Hadoop streaming, accounted for 90% of exceptions from January 21 to February 18, 2009. This motivates the need for error-log analysis tools that highlight important exceptions to users','utf-8'),(89,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(90,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n=== Pretty Good As An Introduction to Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(91,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(92,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(93,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(94,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(95,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(96,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Examples and Resources on Association Rule Mining with R (R-bloggers.com)\n===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n* http://www.rdatamining.com/resources/onlinedocs\n: A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD))\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(97,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Examples and Resources on Association Rule Mining with R (R-bloggers.com)\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== Association Rule Mining with R ===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n* http://www.rdatamining.com/resources/onlinedocs\n: A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD))\n\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(98,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Examples and Resources on Association Rule Mining with R (R-bloggers.com)\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== Examples of Association Rule Mining with R ===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(99,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n: A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Examples and Resources on Association Rule Mining with R (R-bloggers.com)\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== Examples of Association Rule Mining with R ===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(100,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n: A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== Examples of Association Rule Mining with R ===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(101,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n: A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== Examples of Association Rule Mining with R ===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique. (R-bloggers.com)\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(102,'\n== ## bNote-2013-05-31 ==\n\n=== Samsung PC Desktop Specification ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information\n\n\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(103,'\n== ## bNote-2013-05-31 ==\n\n=== Samsung PC Desktop Specification ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(104,'\n== ## bNote-2013-05-31 ==\n\n=== Samsung PC Desktop Specification ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and 16 GB maximum supported memory size for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and 8 GB maximum supported memory size for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(105,'\n== ## bNote-2013-05-31 ==\n\n=== Samsung PC Desktop Specification ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(106,'\n== ## bNote-2013-05-31 ==\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(107,'\n== ## bNote-2013-05-31 ==\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* [http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid= DB-P100 Information (Intel Q43 Chipset)]\n: DB-P100 Information (Intel Q43 Chipset)\n\n* [http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm Intel Q43 Chipset]\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(108,'\n== ## bNote-2013-05-31 ==\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(109,'#REDIRECT [[((news.article)) 네트워크 업계 \"기다렸다, 런던올림픽\"]]','utf-8'),(110,'#REDIRECT [[(news.article) 네트워크 업계 \"기다렸다, 런던올림픽\"]]','utf-8'),(111,'#REDIRECT [[News 네트워크 업계 \"기다렸다, 런던올림픽\"]]','utf-8'),(112,'#REDIRECT [[(News) 네트워크 업계 \"기다렸다, 런던올림픽\"]]','utf-8'),(113,'#REDIRECT [[(News) 아카마이, \"CDN 넘어 하이퍼커넥티드로\"]]','utf-8'),(114,'#REDIRECT [[(News) 아카마이, \"스트리밍/보안 서비스를 빠르고 안전하게\"]]','utf-8'),(115,'#REDIRECT [[(News) 아카마이, \"쌩쌩 웹사이트 만들려면\"]]','utf-8'),(116,'#REDIRECT [[(News) 아카마이, \'블레이즈\' 인수 - 인텔리전트 플랫폼 강화]]','utf-8'),(117,'#REDIRECT [[(News) 아카마이, CDN 장악 가속화 ... 코텐도 인수설]]','utf-8'),(118,'\n== ## bNote-2013-05-31 ==\n\n=== blktrace 에서 GBL phase flag 의미 ===\n\n;Q\n: block layer의 request queue routine에 의해 I/O request가 handling 되기 시작\n\n;G\n: block layer 내에서 I/O request 핸들링 위한 resource 생성\n\n;I\n: I/O request가 queue에 삽입됨\n\n;D\n: I/O request가 device로 issue됨\n\n;C\n: I/O completed\n\n;R\n: Re-queue (device 단에서 처리 불가한 상황이거나 해서 다시 I/O queuing)\n\n;S\n: block layer 내에 I/O request 생성에 필요한 resource가 부족하여 잠시 기다려야 하는 상황\n\n;M, F\n: merging the I/O (그냥 M은 backward merge, F는 지금 도착한 I/O의 address가 이전 I/O의 address가 시작하는 곳 앞에 위치하는 경우 의미 - 둘 간의 차이에 그다지 신경쓸 필요 없음)\n\n: blktrace 2.0.0 기준\n\n\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(119,'\n== ## bNote-2013-05-31 ==\n\n=== blktrace 에서 GBL phase flag 의미 ===\n\n;Q\n: block layer의 request queue routine에 의해 I/O request가 handling 되기 시작\n\n;G\n: block layer 내에서 I/O request 핸들링 위한 resource 생성\n\n;I\n: I/O request가 queue에 삽입됨\n\n;D\n: I/O request가 device로 issue됨\n\n;C\n: I/O completed\n\n;R\n: Re-queue (device 단에서 처리 불가한 상황이거나 해서 다시 I/O queuing)\n\n;S\n: block layer 내에 I/O request 생성에 필요한 resource가 부족하여 잠시 기다려야 하는 상황\n\n;M, F\n: merging the I/O (그냥 M은 backward merge, F는 지금 도착한 I/O의 address가 이전 I/O의 address가 시작하는 곳 앞에 위치하는 경우 의미 - 둘 간의 차이에 그다지 신경쓸 필요 없음)\n\n(blktrace 2.0.0 기준)\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(120,'\n== ## bNote-2013-05-31 ==\n\n=== 수퍼컴 업무 협조 공문 ===\n\n <pre>\n\nI/O Workload 분석 & 시뮬레이션 환경 병렬화\n\n(과제명: Intelligent Data Management - PL: 이주평 전문, NPR 과제)\n\n---------------------------------------------------------\n\n \n\nIntelligent Data Management 과제 (구 Data-intensive Storage)에서는\n\nI/O 예측에 기반한 Proactive Data Placement 기술 연구를 위해\n\nI/O Workload 분석을 수행하고 있습니다.\n\n \n\n수십 GB에서 수 TB에 이르는 데이터에 대해\n\nData Mining / Machine Learning 기반 분석을 진행해야 하기 때문에\n\n고성능의 수퍼컴 활용 및 효과적인 병렬화가 필수적입니다.\n\n \n\n이에, 수퍼컴센터와의 협력을 통해\n\n분석/시뮬레이션 작업의 TAT 단축 및\n\n분석 체계 혁신을 이루고자 합니다.\n\n \n\n협업 대상 업무 및 진행 계획은 다음과 같습니다.\n\n - 슈퍼컴 시스템 로그 수집 건 (6월: 로그 수집 모듈 작성, 7월: 수집 작업)\n\n - 병렬화/RSP 적용 건 (6월 ~ 7월: 전처리 및 분석 모듈 병렬화 구축)\n\n\n\n상기 내용으로 수퍼컴 센터와 협업을 하고자 하오니\n\n검토 후 결재 부탁 드립니다.\n\n\n\n\n\n※ 추진 일정 등 자세한 내용은 첨부 파일을 참고하시기 바랍니다.\n\n첨부 1: 수퍼컴 센터 업무 지원 요청\n\n첨부 2: 사전 회의록 (5/20, 김혁호 책임, 정명준 전문)\n\n \n\n----- 이상 -----\n\n \n\n</pre>\n\n\n=== blktrace 에서 GBL phase flag 의미 ===\n\n;Q\n: block layer의 request queue routine에 의해 I/O request가 handling 되기 시작\n\n;G\n: block layer 내에서 I/O request 핸들링 위한 resource 생성\n\n;I\n: I/O request가 queue에 삽입됨\n\n;D\n: I/O request가 device로 issue됨\n\n;C\n: I/O completed\n\n;R\n: Re-queue (device 단에서 처리 불가한 상황이거나 해서 다시 I/O queuing)\n\n;S\n: block layer 내에 I/O request 생성에 필요한 resource가 부족하여 잠시 기다려야 하는 상황\n\n;M, F\n: merging the I/O (그냥 M은 backward merge, F는 지금 도착한 I/O의 address가 이전 I/O의 address가 시작하는 곳 앞에 위치하는 경우 의미 - 둘 간의 차이에 그다지 신경쓸 필요 없음)\n\n(blktrace 2.0.0 기준)\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(121,'\n== ## bNote-2013-05-31 ==\n\n=== 수퍼컴 업무 협조 공문 ===\n\n <pre>\n\nI/O Workload 분석 & 시뮬레이션 환경 병렬화\n\n(과제명: Intelligent Data Management / PL: 이주평 전문 / NPR 과제)\n\n---------------------------------------------------------\n\n \n\nIntelligent Data Management 과제 (구 Data-intensive Storage)에서는\n\nI/O 예측에 기반한 Proactive Data Placement 기술 연구를 위해\n\nI/O Workload 분석을 수행하고 있습니다.\n\n \n\n수십 GB에서 수 TB에 이르는 데이터에 대해\n\nData Mining / Machine Learning 기반 분석을 진행해야 하기 때문에\n\n고성능의 수퍼컴 활용 및 효과적인 병렬화가 필수적입니다.\n\n \n\n이에, 수퍼컴센터와의 협력을 통해\n\n분석/시뮬레이션 작업의 TAT 단축 및\n\n분석 체계 혁신을 이루고자 합니다.\n\n \n\n협업 대상 업무 및 진행 계획은 다음과 같습니다.\n\n - 슈퍼컴 시스템 로그 수집 건 (6월: 로그 수집 모듈 작성, 7월: 수집 작업)\n\n - 병렬화/RSP 적용 건 (6월 ~ 7월: 전처리 및 분석 모듈 병렬화 구축)\n\n\n\n상기 내용으로 수퍼컴 센터와 협업을 하고자 하오니\n\n검토 후 결재 부탁 드립니다.\n\n\n\n\n\n※ 추진 일정 등 자세한 내용은 첨부 파일을 참고하시기 바랍니다.\n\n첨부 1: 수퍼컴 센터 업무 지원 요청\n\n첨부 2: 사전 회의록 (5/20, 김혁호 책임, 정명준 전문)\n\n \n\n----- 이상 -----\n\n \n\n</pre>\n\n=== blktrace 에서 GBL phase flag 의미 ===\n\n;Q\n: block layer의 request queue routine에 의해 I/O request가 handling 되기 시작\n\n;G\n: block layer 내에서 I/O request 핸들링 위한 resource 생성\n\n;I\n: I/O request가 queue에 삽입됨\n\n;D\n: I/O request가 device로 issue됨\n\n;C\n: I/O completed\n\n;R\n: Re-queue (device 단에서 처리 불가한 상황이거나 해서 다시 I/O queuing)\n\n;S\n: block layer 내에 I/O request 생성에 필요한 resource가 부족하여 잠시 기다려야 하는 상황\n\n;M, F\n: merging the I/O (그냥 M은 backward merge, F는 지금 도착한 I/O의 address가 이전 I/O의 address가 시작하는 곳 앞에 위치하는 경우 의미 - 둘 간의 차이에 그다지 신경쓸 필요 없음)\n\n(blktrace 2.0.0 기준)\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(122,'\n== ## bNote-2013-05-31 ==\n\n=== 수퍼컴 업무 협조 공문 ===\n\n <pre>\n\nI/O Workload 분석 & 시뮬레이션 환경 병렬화\n\n(과제명: Intelligent Data Management / PL: 이주평 전문 / NPR 과제)\n\n---------------------------------------------------------\n\n \n\nIntelligent Data Management 과제 (구 Data-intensive Storage)에서는\n\nI/O 예측에 기반한 Proactive Data Placement 기술 연구를 위해\n\nI/O Workload 분석을 수행하고 있습니다.\n\n \n\n수십 GB에서 수 TB에 이르는 데이터에 대해\n\nData Mining / Machine Learning 기반 분석을 진행해야 하기 때문에\n\n고성능의 수퍼컴 활용 및 효과적인 병렬화가 필수적입니다.\n\n \n\n이에, 수퍼컴센터와의 협력을 통해\n\n분석/시뮬레이션 작업의 TAT 단축 및\n\n분석 체계 혁신을 이루고자 합니다.\n\n \n\n협업 대상 업무 및 진행 계획은 다음과 같습니다.\n\n - 슈퍼컴 시스템 로그 수집 건 (6월: 로그 수집 모듈 작성, 7월: 수집 작업)\n\n - 병렬화/RSP 적용 건 (6월 ~ 7월: 전처리 및 분석 모듈 병렬화 구축)\n\n\n\n상기 내용으로 수퍼컴 센터와 협업을 하고자 하오니\n\n검토 후 결재 부탁 드립니다.\n\n\n\n\n\n※ 추진 일정 등 자세한 내용은 첨부 파일을 참고하시기 바랍니다.\n\n첨부 1: 수퍼컴 센터 업무 지원 요청\n\n첨부 2: 사전 회의록 (5/20, 김혁호 책임, 정명준 전문)\n\n \n\n----- 이상 -----\n\n \n\n</pre>\n\n=== blktrace 에서 GBL phase flag 의미 ===\n\n;Q\n: block layer의 request queue routine에 의해 I/O request가 handling 되기 시작\n\n;G\n: block layer 내에서 I/O request 핸들링 위한 resource 생성\n\n;I\n: I/O request가 queue에 삽입됨\n\n;D\n: I/O request가 device로 issue됨\n\n;C\n: I/O completed\n\n;R\n: Re-queue (device 단에서 처리 불가한 상황이거나 해서 다시 I/O queuing)\n\n;S\n: block layer 내에 I/O request 생성에 필요한 resource가 부족하여 잠시 기다려야 하는 상황\n\n;M, F\n: merging the I/O (그냥 M은 backward merge, F는 지금 도착한 I/O의 address가 이전 I/O의 address가 시작하는 곳 앞에 위치하는 경우 의미 - 둘 간의 차이에 그다지 신경쓸 필요 없음)\n\n;P\n: Device로 내려가는 I/O를 마개로 막아놓고 (Plug) Merging을 하겠다는 의미\n\n;U\n: Unplug를 함으로써 Merge된 I/O들이 밑으로 (Device쪽으로) 내려갈 수 있게 됨\n\n\n(blktrace 2.0.0 기준)\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8');
/*!40000 ALTER TABLE `radiohead_text` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `radiohead_transcache`
--

DROP TABLE IF EXISTS `radiohead_transcache`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
