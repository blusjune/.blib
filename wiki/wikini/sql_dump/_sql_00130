INSERT INTO `radiohead_text` VALUES (1,'\'\'\'MediaWiki has been successfully installed.\'\'\'\n\nConsult the [//meta.wikimedia.org/wiki/Help:Contents User\'s Guide] for information on using the wiki software.\n\n== Getting started ==\n* [//www.mediawiki.org/wiki/Manual:Configuration_settings Configuration settings list]\n* [//www.mediawiki.org/wiki/Manual:FAQ MediaWiki FAQ]\n* [https://lists.wikimedia.org/mailman/listinfo/mediawiki-announce MediaWiki release mailing list]\n* [//www.mediawiki.org/wiki/Localisation#Translation_resources Localise MediaWiki for your language]','utf-8'),(2,'정명준','utf-8'),(3,'== Radiohead Hot Pages ==\n\n* [http://kandinsky/radiohead/index.php?title=Special%3ASearch&search=bnote&go=Go Bnote Search]\n* [[Bnote_2013]] | [[Bnote_patidea_2013]] | [[Bnote - References ( Papers / Patents / Articles )]]\n* [http://kandinsky/radiohead/index.php/Special:AllPages Special:AllPages] | [http://kandinsky/radiohead/index.php/Special:ListFiles Special:ListFiles]\n\n== Links ==\n* [https://mail.google.com/mail Gmail] | [http://www.naver.com/ Naver] | [http://github.com/ GitHub] | [http://mobile.olleh.com/index.asp?code=A000000 Olleh_SMS]\n* [http://klimt/ Klimt (Rolling-Stones)]\n\n== Getting started ==\n* [http://www.mediawiki.org/wiki/Manual:Configuration_settings Configuration settings list] | [http://www.mediawiki.org/wiki/Manual:FAQ MediaWiki FAQ] | [https://lists.wikimedia.org/mailman/listinfo/mediawiki-announce MediaWiki release mailing list] | [http://meta.wikimedia.org/wiki/Help:Contents User\'s Guide]','utf-8'),(4,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n[[Bnote patidea 2013-003]]\n\n\n== PATENT-BRIAN-2013-004 ==\n\n=== Event-based I/O Prediction ===\n\n=== 요약 ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n=== 기술 상세 ===\n\n* I/O 발생 메커니즘의 모델링\n: I/O prediction을 위해, type-I event (indirect I/O trigger)와 type-II event (direct I/O trigger)를 관찰 및 분석함. Type-I event의 예로서, user의 log-in, 기 log-in되어 있던 user session의 상태 변경 (idle-to-busy), system service에 대한 invocation from local or remote node 등이 포함될 수 있으며, type-II event의 예로서, user application 의 실행, system service의 실행 등이 포함될 수 있음. Type-I event는 type-II event의 발생에 영향을 미치고, type-II event는 I/O request event (data access)의 발생에 영향을 미치므로, \'type-I event\' -> \'type-II event\' -> \'I/O request event\' 까지 이어지는 인과사슬 (cause-effect chain) 기반으로 I/O 발생 메커니즘을 모델링하였음.\n\n\n* Event 정보 수집: type-I event 정보 및 type-II event 정보 정의 및 수집 방법.\n\n\n* I/O 패턴 학습 (Offline Learning): 수집된 Event 정보를 기반으로 I/O 패턴을 learning하는 방법\n\n\n* Run-time I/O 예측 (Online Matching): 시스템 모니터링에서 얻어지는 최근 및 현재의 event들과 learning으로 얻어진 I/O 패턴 DB에 기반하여 Run-time으로 미래 I/O 예측\n\n\n* 시스템 구조: 정보 수집 및 패턴 학습 모듈의 시스템 내 위치 (I/O 패턴 DB 등의 위치 및 실시간 예측 모듈과의 관계)\n\n\n* 순서도: 수집부터 학습까지 전체적인 task의 흐름\n\n\n* 예상 효과: 성능적 측면에서 사용자가 체감하는 lower latency, higher throughput 효과, 이미 발생해버린 congestion time을 피해 미리 적절한 곳에 data를 배치시킴으로써 얻어지는 bottleneck prevention 효과.\n\n\n=== Data Collection ===\n\n* 역할: I/O 패턴 학습/발굴에 필요한 정보 수집\n\n* 수집 정보 종류\n: 전조 기반 예측에 필요한 간접 I/O 유발자, 직접 I/O 유발자, I/O 발생 이벤트 등 I/O 예측에 도움이 될 수 있는 다양한 정보를 수집. 다음 정보들이 포함될 수 있음.\n:- indirect I/O trigger trace: {(user-id | user-type), status-type, before-status, after-status, event-time}\n:- direct I/O trigger trace: {(program-id | program-type), status-type, before-status, after-status, process-owner, event-time [, process-current-working-directory, files-opened]}\n:- I/O trace: {access-time, address-accessed, read/write-flag, io-size, file-path, process, co-accessed-addresses}\n\n\n* 수집 모듈의 위치\n: storage-server가 분리되어 있는 구조인 SAN/NAS 타입의 architecture인 경우에는 application이 구동되는 server-side에 정보 수집 모듈이 위치한다. storage-server가 결합되어 있는 converged-architecture 기반의 system인 경우에는 해당 system 내에 정보 수집 모듈이 위치한다.\n\n\n* 정보 수집 방식\n: 운영체체가 제공하는 시스템 상태 정보 제공 interface를 이용하거나, blktrace, lsof, debug file system 등 이용 (e.g., /sys/kernel/debug/ 등) 혹은 /proc file system 등을 이용 가능.\n\n== PATENT-BRIAN-2013-005 ==\n\n=== Coaccess-based I/O Prediction ===\n\n\n=== 요약 ===\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n\n=== 기술 상세 ===\n\n* Data의 coaccess 특성에 기반한 proactive data placement 기법\n: 소정의 time window 내에 같이 access되는 확률이 높은 data 그룹이 있을 때, 그 그룹 내의 data가 access되는 것을 알게 되면, 그 그룹 내의 나머지 data들도 미리 fast media에 가져다 놓음으로써 성능 향상을 얻을 수 있게 하는 방법임.\n\n\n\n\n* 시뮬레이션 기반의 성능 향상 효과 비교\n:- Fast media의 4KB random read latency는 96.47 usec (SSD 830 128GB 기준), slow media의 4KB random read latency는 1,884.03 usec (HDD 7,200rpm 2TB 기준)으로 설정 (iozone에 의한 실측값 사용)\n:- 실험 시작 전에는 4GB 크기의 target data가 slow media (HDD)에 저장되어 있음.\n:- 여기에 1,000,000 개의 4KB random read request로 구성되는 workload를 인가. (각 read request들은 서로 중복되는 data를 가리키지 않으며, 4GB 크기의 target data를 한 번씩 다 request하게 된다고 가정)\n:- Workload 패턴 학습 결과, 4GB의 전체 data 중, \'가\' type의 coaccess 특성을 가지는 10MB 크기의 data 그룹 (data set 당 2,500개의 4KB data 포함)이 200개가 있으며, \'나\' type의 coaccess 특성을 가지는 1MB 크기의 data 그룹 (data set 당 250개의 4KB data 포함)이 1,000개 있는 것을 발견한 것으로 가정.\n:- \'가\' type의 coaccess 패턴은 해당 data 그룹 내 100개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 80%인 것으로 정의.\n:- \'나\' type의 coaccess 패턴을 해당 data 그룹 내 50개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 100%인 것으로 정의.\n\n\n\n\n: 에 대해 본 기법을 적용한 경우와 그렇지 않은 경우의 성능 차이를 IOPS로 비교해보면 다음과 같다. 기법을 적용하기 전인 baseline의 경우에는 \n\n\n* coaccessness 분석에 필요한 parameter로서, \n\n\n\n\n\n\n\n: 예를 들어 설명하면 다음과 같다.\n: data 1, data 2, ..., data 10000으로 구성된 data set이 있으며, 각 data들의 크기는 4KB이다. 학습을 통해서, (1) 이 중 어느 한 data가 access되었을 때, 1분 이내에 나머지 data들도 같이 access되게 될 확률은 55%, (2) 이 중 2개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 80%, (3) 이 중 3개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 95%라는 것을 알고 있다고 가정한다.\n: fast media의 크기는 4GB 이며, 기존에 \n\n\n   이라는 것을 알고 있다면, data 1, 2, 3이 access되었을 때 data 4~128까지도 미리 proactive하게 fast tier에 배치시킴으로써, 그로 인한 성능 이득을 얻을 수 있게 된다. (!!!실제 실험 사례?!!!)\n\n== Memo for {PATENT-BRIAN-2013-004, PATENT-BRIAN-2013-005} ==\n\n\n=== Think outside the box // Questions ===\n\n* 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n\n* Tiering을 좀 더 다양한 각도에서 바라보자\n:- 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 것이 기존의 tiering 개념임.\n:- SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n:- SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n:- access pattern-aware optimal placement (APOP) - 이런 것은 어떨까?\n:- Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n\n* write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반의 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지?\n\n\n* 만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? SSD-internal memory buffer를 이용해서 SSD를 NVRAM처럼 쓰도록 하는 것이 실제로 말이 되는 이야기일까?\n\n\n\n\n\n=== 기존 기술의 한계점 (Intelligent I/O Prediction 기술의 필요성) ===\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache만으로는 성능 향상을 기대하기 어렵게 됨.\n\n <pre>\n실제 tracelog 사례를 보여주고 hot area가 변화하고 있음을 보일 것.\n</pre>\n\n* Data Placement (Tiering) 관점에서 본 EMC의 FAST, FAST-VP 기술의 한계점\n:- Coarse-grained Data Migration (768KB 까지 작게 나눌 수 있다고는 하지만, 실제로 그렇게 해서 성능이 얼마나 잘 나오는지는 확인 필요. 귀중한 자원인 DRAM을 관리를 위한 metadata로 가득 채우게 될 위험도 있음)\n:- Flexible하지 않은 Tiering 주기\n:- Busy-hour 지난 후 Idle-time에 Tiering 실시\n:- 낮은 지능의 미래 I/O 예측 능력\n:: data access count를 기반으로 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두도록 하는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n\n=== 본 발명의 차별화 포인트 ===\n\n* 본 기술의 특징 1: I/O를 triggering하는 전조 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Indicator는 Run-time에 저비용으로 감지될 수 있는 형태의 정보들로 구성\n:- 미래에 access될 data를 예측케 하는 Indicator간의 mapping은 Bayesian Inference 이용\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n=== 특허 검색/분석 결과 {storage I/O workload prediction} ===\n\n* 검색식: storage I/O workload prediction\n\n* http://portal.uspto.gov/pair/PublicPair 에서 특허 상태 확인 가능.\n\n* I/O prediction을 기반으로 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n:- 기존엔 proactive tiering이 없었나? -> IBM 특허가 나름 유사함. (US7930505, US7512766)\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n==== 1. 비교가 될만한 특허들 ====\n\n예측에 대한 언급이 되어 있는 스토리지 시스템 관련 특허들. 크게 두 가지로서, 그 중의 하나는 가상 volume에 대해 peak I/O가 발생하는 주기를 분석하여, peak I/O 발생시점이 닥치기 전에 미리 가상 volume의 data를 migration 시키는 방법에 대한 것이나, 어느 위치의 data가 언제 access될지에 대한 예측은 아님 (IBM 특허). 다른 하나는 I/O 예측을 기반으로 미래에 자주 access될만한 data를 미리 여러 노드에 분산시키는 방법에 대한 것이나, 예측 방법 자체에 대한 내용은 다루고 있지 않음 (HP 특허)\n\n\n; US7930505, US7512766 (IBM)\n\n* US7930505 (Application Number: US 12/053582), \"Controlling preemptive work balancing in data storage,\" IBM, 특허일 2011-04-19\n: storage network의 I/O 관찰을 통해 peak I/O activity의 repeating instance에 대한 signature를 VLU (virtual logical unit) 단위로 생성하고, 이에 기반하여 언제 heavy I/O가 도래할 것인지를 예측하여, heavy I/O가 오기 전에 미래 data를 faster media로 migration하도록 하는 tiering 기술에 대한 특허. (이미 I/O peak time이 온 후에 hot data를 옮기는 것은 오히려 시스템의 IO성능을 악화시킬 수도 있기 때문에 미리 migration을 하고자 하는 것임)\n: VLU 단위로, peak I/O activity가 어느 주기로 발생할지에 대한 예측임. 지능 수준과 Fine-granularity 측면에서 초보적인 수준임.\n\n* US7512766 (Application Number: US 11/213494), \"Controlling preemptive work balancing in data storage\", IBM, 특허일 2009-03-31\n: 위의 US7930505 특허와 동일 건으로 보임\n\n\n; US6912635 (HP)\n\n* US6912635 (Application Number: US 10/140968), \"Distributing workload evenly across storage media in a storage array\", HP, 특허일 2005-06-28\n: 미래에 자주 access될만한 data를 여러 arrayed storage device에 분산시키겠다는 기술. 그러나 I/O에 대한 예측 방법은 다루어지고 있지 않음.\n\n==== 2. 검색은 되었으나 관련은 없는 특허들 ====\n\n무언가를 예측한다는 측면, Storage System에 대한 기술이라는 측면에서는 관련이 있으나, 미래에 Access될 Data를 예측한다는 측면에서 제안 기술과 관련 없는 특허들\n\n\n; US7640231 (IBM)\n* US7640231 (Application Number: 11/280012), \"Approach based on self-evolving models for performance guarantees in a shared storage system\", IBM, 특허일 2009-12-29\n: network 기반 storage system에서, average latency 혹은 I/O request rate 등에 대한 주기적인 performance sampling을 기반으로, 주어진 objective function의 값을 최대화할 수 있도록 system resource를 제어할 수 있게 하는 기술.\n\n\n; US7774491, US7519725 (IBM)\n\n* US7774491 (Application Number: 12/359377), \"Utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2010-08-10\n: 복수의 클라이언트들을 대상으로 통계적인 성능 QoS를 보장하기 위한 informed throttling 방법\n: A system for utilizing informed throttling to guarantee quality of service to a plurality of clients\n\n* US7519725 (Application Number: US 10/444712), \"System and method for utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2009-04-14\n: resolve the problem of providing statistical performance guarantees for applications generating streams of read/write accesses (I/Os) on a shared, potentially distributed storage system of finite resources, by initiating throttling whenever an I/O stream is receiving insufficient resources\n\n\n; US7827283 (IBM)\n* US7827283 (Application Number: 10/367444), \"System for managing and controlling storage access requirements\", IBM, 2010-11-02\n: SAN 등의 환경에서, storage bandwidth를 관리하고 reserve하는 기술\n: A Resource Management and Reservation System (RMRS) for managing and reserving storage bandwidth, is a platform independent middleware layer that provides an interface to applications, their database management systems, or some other higher level data management systems like ADRS which does data management on behalf of the applications\n\n\n; US8433554, US8140319 (IBM)\n\n* US8433554, \"Predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2013-04-30\n: An approach for predicting performance and capacity of an information technology (IT) system before the IT system is built is described, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n* US8140319, \"Method and system for predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2012-03-20\n: A method and system for predicting performance and capacity of an information technology (IT) system before the IT system is built, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n\n\n; US8255915 (HP)\n* US8255915, \"Workload management for computer system with container hierarchy and workload-group policies\" HP, 특허일 2012-08-28\n\n\n; US7050956 (HP)\n* US7050956 (Application Number: US 09/843930), \"Method and apparatus for morphological modeling of complex systems to predict performance\", HP, 특허일 2006-05-23\n: I/O 성능 예측을 위한 complex storage system에서의 data flow 모델링 방법\n\n\n; US6718434 (HP)\n* US6718434 (Application Number: US 09/871043), \"Method and apparatus for assigning raid levels\", HP, 특허일 2004-04-06\n: Data를 disk array에 load하기 전에, workload 명세 및 device 명세에 기반하여, 적합한 RAID level을 판단하여 disk array에 할당 (RAID build)하는 방법\n\n\n; US8112586 (EMC)\n* US8112586 (Application Number: US 12/190071), \"Predicting and optimizing I/O performance characteristics in a multi-level caching system\", EMC, 특허일 2012-02-07\n: multi-level caching 시스템에서, cost대비 performance를 최적화할 수 있도록, workload에 따른 resource allocation design에 대한 recommendation 기술 (multi-level cache들 간의 workload propagation을 예측하는 것이 핵심)\n\n\n; US6629266 (IBM)\n* US6629266 (Application Number: US 09/442001), \"Method and system for transparent symptom-based selective software rejuvenation\", IBM, 특허일 2003-09-30\n: software가 언제 동작이 멈추는지를 예측함으로써, software의 dependability를 향상할 수 있게 하는 방법\n\n\n; US5088058 (Unisys)\n* US5088058 (Application Number: 07/237120), \"Apparatus and method for evaluating and predicting computer I/O performance using I/O workload snapshots for model input\", Unisys Corporation, 특허일 1992-02-11\n: I/O delay 등에 대한 확률적 모형을 기반으로 disk 시스템의 성능을 검증하고 예측하는 시뮬레이션 기반의 방법\n: A method of evaluating and predicting the performance of an I/O disk system comprised of one or more disk controllers and cooperating disk units using a simulation model containing a stact constructed such that delays are formulated as sets of stochastic processes\n\n=== Comparison of I/O Prediction Approaches ===\n\n미래에 access될 data의 위치와 시기를 예측하기 위한 방법으로 event-based I/O prediction, coaccess-based I/O prediction, periodicity-based I/O prediction 방식이 제안되며, 각각의 방식은 예측에 필요한 연산량 및 input data의 richness, 그리고 workload 변화에 대한 적응성 측면에서 장단점을 가지고 있음. 따라서, 상황에 따라서 적절한 방식을 이용하면 됨.\n\n{| style=\"border-collapse: collapse; border: 1px solid #000\" border=1\n| style=\"width: 25%\" | 비교 항목\n| style=\"width: 25%\" | periodicity-based I/O prediction\n| style=\"width: 25%\" | coaccess-based I/O prediction\n| style=\"width: 25%\" | event-based I/O prediction \n|-\n| context richness required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| computation overhead required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| adaptability to workload changes (lower is better)\n| high\n| high\n| high\n|-\n|}\n\n\n* 전조 기반 예측 vs. 주기성 기반 예측\n{| border=1\n| 비교 항목\n| 전조 기반 예측\n| 주기성 기반 예측\n|-\n| 기술 요약\n| 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식\n| address 별 access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식\n|-\n| 분석 오버헤드\n| High (학습/마이닝에 드는 비용임)\n| Low (주기성 파악 비용임)\n|-\n| Run-time 오버헤드\n| Mid (실시간 전조 모니터링 및 매칭 비용)\n| Low (주기성 계산)\n|-\n| Workload 변화 적응성\n| High\n| High (Workload 변경 사실 알림 hint 필요)\n|-\n| ...\n| ...\n| ...\n|-\n| Incrementality of Mining/Learning\n| ...\n| ...\n|}\n\n* 전조 기반 예측은, 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식임.  주기성 기반 예측은, data access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식임.\n\n\n=== Intelligent Data Placement ===\n\n* Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case\n: 여기에 HML이 사용될 수 있는 것이라면, 어떤 방식으로 어떤 이유로 그런 것일까?\n\n\n* I/O 예측 방식으로서 다음 방식이 제안되며, 각 방식에 따라서 prediction model, 수집되어야 하는 context data, I/O 패턴 DB에 저장되는 정보의 종류 및 형태가 달라질 수 있음. 예측에 이용할 수 있는 context 정보의 richness에 따라 적절한 방식을 사용할 수 있다. (서로 다른 발명이므로, 별도의 특허로 낸다)\n:- event-based I/O prediction\n:- coaccess-based I/O prediction\n:: 여기서, periodicity-based I/O prediction은 기존 특허에서 언급되었으므로, 본 발명에서는 크게 언급하지 않기로 한다.\n\n\n* I/O prediction 결과에 따라 적절한 위치에 data를 배치하게 되는데, 여기서 적절한 위치라 함은 SSD와 같은 fast tier가 될 수도 있겠으나, sequential read/write 같은 경우 striping된 HDD array가 될 수도 있고, 혹은 SAS HDD가 될 수도 있겠음. 또한 SSD 중에서도 SLC, MLC, TLC 로 구분되는 fine-grained fast tier로 구분될 수도 있음\n\n\n* 미래에 오게 될 I/O의 type도 알 수 있을까? 즉, {random/sequential}x{read/write} type 중 어느 것에 해당되는지를 안다면, 그리고 I/O의 intensiveness도 예상할 수 있다면 그에 맞춰서 placement 시 적절한 tier 혹은 적절한 node로 찾아가게 할 수 있을 것임.\n\n\n* context 정보는, 수집되는 위치에 따라, server-side에서 수집되는 context data와 storage-side에서 수집되는 context data로 분류될 수 있음. Event context data는 server-side에서 수집 가능하며, coaccess context data 및 periodicity context data는 storage-side에서 수집될 수 있음.\n\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n: 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n\n=== Design/Implementation Issues ===\n\n* workload type/class tagging 필요\n\n* cache와 tiering을 같이 쓸 때, DRAM cache가 아니라 Flash Cache의 경우 쓸데없이 모든 data가 Flash Cache에 다 들어가게 두기 보다는, tiering에서 cover하지 못하는 것들 위주로 caching하도록 하면 Flash Cache의 공간 낭비를 줄임으로써, 유효 cache 공간을 늘일 수 있게 되고, 그만큼의 성능 향상으로 연결될 수 있음.\n\n* DRAM 기반의 write cache를 적극 이용하여 마치 log-structured file system에서 제공하는 I/O merging and sequentialization 효과를 얻을 수 있음.\n\n* Tiering되는 data는 caching되는 data보다 상대적으로 coarse-grained하게 grouping하여 다룸으로써, 한꺼번에 많은 data를 move할 수 있도록 하고, meta data 관리에 필요한 overhead를 줄일 수 있게 한다. 당장 access되지 않더라도 일정 시간 내에 같이 access될 수 있는 data들은 함께 faster tier로 옮겨진다. 이때 필요한 정보가 coaccess 정보임.\n\n* Cache - Tiering 기법의 연동 방법: 예측이 실패했거나, target tier media의 가용 공간이 부족하여 tiering에서 놓치는 missing data가 있을 수 있음. 이러한 missing data들이 temporal locality가 있는 경우에는 당장은 faster tier에서 serve하지 못하더라도 cache에서 serve될 수 있는데, temporal locality가 적은 경우에는 cache만으로 cover되지 않는다. \n\n* Management table size optimization issue - 만약 모든 LBA 별로 event-based I/O prediction을 해야 한다면 현실적으로 사용 가능한 아키텍쳐가 아닐 수 있음. 만약 spatial locality가 있어서 LBA 1,000개 단위로 묶어서 대응할 수 있다면, 최소 1/1000 의 management table size 감소 효과를 얻을 수 있음. pattern mining 혹은 machine learning 실행 시에는 record 수가 감소함으로써 얻는 이득은 1,000배 이상이 될 수 있음. 관건은 얼마나 LBA 묶음을 크게 만들어서 예측에 사용할 수 있는지임.\n\n\n=== I/O Insight ===\n\n* ML 기반의 I/O Insight 발굴 (Data Placement를 위한 Macroscopic Guideline)\n: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:-> 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n\n* I/O Insight의 요건:\n:# indication, as simple as possible\n:# indication, as specific as possible\n:# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n:# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n\n* I/O Insight의 형태\n: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n=== X-Y Modeling ===\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n=== 예측 실패 대비 (별도 특허화 가능) ===\n\n* I/O 예측이 실패할 수 있으며, 이로 인한 성능 penalty를 최소화하기 위한 방법 필요.\n:- fast tier에 여유공간이 있다면 1타겟과 2타겟을 모두 올려놓아도 되지 않을까? 1타겟과 2타겟을 placement함으로써 예측 정확도를 95%까지 올릴 수 있다면, 꽤 쓸만할 수 있겠음. 그리고 1타겟과 2타겟의 모든 데이터를 다 fast tier에 올려두지 말고 일부만 fast tier에 올려두었다가, 둘 중에 하나의 타겟이 hit되었을 때, hit된 타겟의 나머지 data를 재빨리 fast tier에 올리도록 하는 incremental 방법을 사용할 수 있겠음.\n:- 그러나 이것 또한 예측 실패에 대한 완벽한 prevention이 될 수는 없다. 따라서 예측 실패가 발생하더라도 재빨리 그 성능 penalty를 mitigation할 수 있는 방법이 필요. \n:- cache를 이용하여 mitigation할 수 있을까?\n\n=== Data Read/Write 관련 Issues ===\n\n* read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n\n* 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n\n* write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간에, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면?\n\n* 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있음.\n\n* 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간이 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 scatter했던 I/O들을 나중에 다시 읽어들이려 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 원하는 고성능을 낼 수 있게 된다.\n\n* replica-based dynamic clustering\n: 위와 같은 write-and-then-read-again 상황에서, 해당 data를 저장했었던 HDD의 I/O busy/idle 여부에 상관 없이, 최고의 효과를 낼 수 있게 하기 위해, replica management를 적극적으로 활용하는 것은 어떨까? HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까?\n\n* data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 논의가 아님. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 논의임.\n\n* 그러나, 조심해야 할 것은 위의 논의는 상당히 case-dependency를 상당히 많이 탈 수도 있다는 것임. 책상 위의 아이디어로 끝날 수도 있기 때문에 조심스럽게 접근하는 것이 필요함.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-008 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n\n\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n----\n=== (SmartSSD API) SSD-internal I/O pattern logging interface and mechanism ===\n\n\n==== Questions ====\n\n* SmartSSD에서 제공할 수 있는 logging 서비스로서 어떤 것들이 있을 수 있나?\n:- 어느? 정도의 free space를 필요로하는 critical (heavy) I/O가 어떤? 주기로 도래할 지 알 수 있을까?\n:- I/O prediction에 도움될 수 있는 정보를 SmartSSD가 기록했다가 필요한 때 (안전한 방법으로?) 줄 수 있을까?\n:- I/O prediction까지는 아니더라도 해당 SSD의 I/O wellness(어떻게 정의?)를 측정했다가 bottleneck을 회피하는 데 유용한 정보를 제공할 수 있을까?\n:- 어떤 SSD가 현재 주어지고 있는 I/O workload에 대해 list performance spec을 만족하는데 어려움을 겪고 있다는 것을 알려줄 수 있는 정보가 어떤 것이 있을까? (SSD 내 write buffer의 fullness 혹은 I/O queue의 유동성? amount of free space in over-provisioning area?) (-> I/O wellness 라는 metric을 이것과 연관시켜 정의할 수도 있으며, 결국 SLA 혹은 Performance QoS와 연관시킬 수 있음)\n:- 지금까지의 S.M.A.R.T.와 비교하였을 때, 어떤 차별점이 있는가? 기존의 S.M.A.R.T.가 제공하지 못하던 타입의 정보를 제공하고, 이로 인해 기존에는 불가능했던 새로운 알고리즘 구현 혹은 새로운 서비스 제공이 가능해짐을 보이면 좋겠음.\n\n\n\n:- workload의 history?\n:- 각 erase block이 erase되어야만 했던 주기?\n:- free space의 추이 (얼마나 많이 남아 돌던지)?\n:- I/O의 heaviness 패턴?\n:- SSD내 write buffer (혹은 write cache)의 utilization정보? (즉, write buffer가 넘칠 정도가 되어서 I/O wait을 해야만 했던 상황이 얼마나 자주 발생했는지, write buffer의 가득찬 정도를 백분율로 표현한다고 했을 때, 평균/표준편차 등으로 대표할 수 있는 normal distribution을 따르는지?\n:- 전체 sequential read/write \n\n\n==== 배경 ====\n\n* 각 SSD 모델별로, 또 동일한 모델의 SSD라 하더라도 어떤 workload에 노출되어왔는지에 따라, 현재 낼 수 있는 I/O performance가 동일하지 않을 수 있다 (!check! 관련 실험 결과 - SSD 830을 이용하여 같은 시간 동안 하나의 host에서 생성되는 workload들을 나누어 받았다 하더라도 내부적인 free block의 갯수 및 random write에 의한 block 내 파편화등의 상태가 상이하여 성능에 차이가 나는 것을 보여줄 것. uFLIP 혹은 filebench 활용 가능). 한 예로, SSD가 어느 호스트의 어느 file system에 매핑되었는지에 따라 그 SSD가 받는 workload의 특성이 다를 수 있다. file system mount point가 달라지거나, 기존과 다른 새로운 응용이 설치/서비스되는 상황이 되면 \n\n\n----\n=== I/O Prediction-based Proactive Data Placement ===\n\n* 미래에 access될 data를 예측하여 적소에 미리 가져다 놓는 기술로서, (1) I/O prediction을 위한 정보 수집 모듈, (2) 주어진 정보들에 기반하여 다가올 I/O를 예측하는 모듈, (3) data placement를 수행하는 모듈로 구성됨.\n\n\n* 본 기술의 특징 1: I/O를 triggering하는 indicator (전조) 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Run-time에 저비용으로 감지될 수 있는 Indicator 정보와, 이에 의해 access될 data 간의 mapping은 Bayesian Inference 이용\n\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n\n----\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n----\n\n=== Tiering Method Avoiding I/O Bottleneck ===\n\n* I/O Prediction에 의해 예측된 미래의 Hot Data를 Proactive하게 Placement한다고 할 때, 예상되는 access pattern으로 인해 발생 가능한 I/O Bottleneck을 최소화할 수 있도록 배치하는 방법\n\n* 예를 들어 미래의 hot data block 1 (200MB), hot data block 2 (100MB), hot data block 3 (400MB)이 1분 후에 access 될 것으로 예측되었다고 가정. data block 1에 대해서는 4KB 단위로 random read가 dominant한 access pattern이 예상되고, data block 2에 대해서는 1024KB 단위의 sequential read가 dominant한 access pattern이 예상되고, data block 3에 대해서는 4KB 단위의 random read와 512KB 단위의 sequential write이 각각 80:20의 비율로 mix된 형태의 access pattern이 예상된다고 했을 때, 각 data block을 어디에 어떤 형태로 placement시킬 것인가?\n\n: machine learning 혹은 pattern mining에 기반한 base workload pattern data를 기반으로 incremental하게 update될 수 있는 data access의 spatial locality와 periodicity\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n\n* 기존 기술과의 대비 (naive I/O prediction vs. full-fledged I/O prediction)\n: 기존 기술은 누적 hit count 정보에 의지하는 naive I/O prediction으로 볼 수 있음. 이러한 방식은 workload의 변화를 제대로 반영하지 못하기 때문에, static한 workload에 대해서는 잘 동작하지만, moving target처럼 dynamic한 workload에 대해서는 오히려 과거의 누적 hit count metric이 현재의 workload 패턴을 왜곡하는 문제점을 가지고 있음. (temporal locality와 spatial locality가 깨지는 순간이 여러번 존재할 수록 기존의 naive I/O prediction 방식으로 성능 향상은 어려워짐)\n\n\n\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(5,'== Papers ==\n\n* [http://research.cs.wisc.edu/htcondor/doc/grid2007.pdf Data Placement for Scientific Applications in Distributed Environments // GRID 2007 // University of Wisconsin Madison]\n* [http://research.microsoft.com/pubs/63596/usenix-08-ssd.pdf Design Tradeoffs for SSD Performance // USENIX 2008 // Microsoft Research, Silicon Valley; University of Wisconsin-Madison]\n* [http://research.microsoft.com/en-us/um/people/srikanth/data/imc09_dcTraffic.pdf The Nature of Datacenter Traffic: Measurements & Analysis // IMC 2009 // Microsoft Research]\n* [http://www.cs.ucsb.edu/~arijitkhan/Papers/multiple_timeseries_prediction.pdf Workload Characterization and Prediction in the Cloud: A Multiple Time Series Approach // UCSB, IBM Watson Research]\n* [http://infolab.stanford.edu/~ragho/hive-icde2010.pdf Hive - A Petabyte Scale Data Warehouse Using Hadoop // Facebook]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf Dremel: Interactive Analysis of Web-scale Datasets // VLDB 2010 // Google]\n* [http://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf PACMan: Coordinated Memory Caching for Parallel Jobs]\n* [http://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf Spark and Shark // AMPLab UC Berkeley]\n* [http://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf MR-Scope: A Real-time Tracing Tool for MapReduce]\n* [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGRID 2010]\n* [http://bnrg.eecs.berkeley.edu/~randy/Courses/CS268.F08/papers/42_osdi_08.pdf Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008 // UC Berkeley]\n* [http://webhdd.ru/library/files/RebalanceDesign6.pdf Rebalance an HDFS Cluster]\n* [http://www.ibm.com/developerworks/web/library/wa-introhdfs/ An introduction to the Hadoop Distributed File System -- HDFS Data Block Rebalancing]\n* [http://www.stanford.edu/~cdel/epic.talk.workloads.pdf Data Center Workload Characterization]\n* [http://seelab.ucsd.edu/virtualefficiency/related_papers/42_caecw05.pdf Data Center Workload Monitoring, Analysis, and Emulation]\n* [https://amplab.cs.berkeley.edu/projects/real-life-workloads/ AMPLab UC Berkeley - Real-life Workloads]\n* [http://davidmeisner.org/wp-content/uploads/2011/04/meisner-exert10.pdf Stochastic Queuing Simulation for Data Center Workloads]\n* [http://research.microsoft.com/en-us/um/people/srikanth/data/imc09_dcTraffic.pdf The Nature of Datacenter Traffic: Measurements & Analysis // MSR]\n* [http://nowlab.cse.ohio-state.edu/publications/tech-reports/2005/vaidyana-caecw05-tr.pdf Workload-driven Analysis of File Systems in Shared Multi-tier Data Centers over InfiniBand // Ohio State University]\n* [http://research.microsoft.com/pubs/81782/2009-06-19%20Data%20Center%20Tutorial%20-%20SIGMETRICS.pdf What Goes Into a Data Center? // MSR]\n* [http://delivery.acm.org/10.1145/1780000/1773400/p34-mishra.pdf?ip=202.20.193.254&acc=ACTIVE%20SERVICE&key=986B26D8D17D60C855C5A4E2351BBB67&CFID=209231705&CFTOKEN=89669612&__acm__=1366870790_f6ca461a6119bb2a2d6f017ca1bc9ce7 Towards Characterizing Cloud Backend Workloads: Insights from Google Compute Clusters // ACM SIGMETRICS Performance Evaluation Review Vol. 37, Issue 4, 2010-03 // Google]\n\n== Patents ==\n\n* [http://www.google.com/patents/US7792882 Method and system for block allocation for hybrid drives // Oracle // US 7792882 B2]\n* [http://www.google.com/patents/US8285758 Tiering storage between multiple classes of storage on the same container file system // EMC // US 8285758 B1]\n\n== Articles ==\n\n\n=== Cloud Computing ===\n\n* [http://www.kcaresearch.kr/upload/webzine/pdf/1_2013012510132826.pdf 국내외 주요 기관이 제시하는 2013년 ICT 트렌드 전망 종합 분석 // 정책연구본부 방송통신연구부 // 클라우드, 빅데이터, 모바일]\n* [http://www.datanet.co.kr/news/articleView.html?idxno=59880 가트너, 5대 클라우드 컴퓨팅 동향 발표 // 2012-04-04]\n* [http://ettrends.etri.re.kr/PDFData/28-1_009-020.pdf 고성능 컴퓨팅 클라우드의 산업 동향 및 이슈 // ETRI // 빅데이터 처리 및 분석 기술 특집, 2013-02-xx]\n* [http://ettrends.etri.re.kr/PDFData/27-2_137-148.pdf 클라우드 컴퓨팅 생태계 및 정책 방향 // ETRI // 전자통신동향분석 2012-04-xx]\n* [http://navigatingthroughthecloud.com/wp-content/uploads/2012/02/CCRA.IBMSubmission.pdf Introduction and Architecture Overview - IBM Cloud Computing Reference Architecture 2.0 // IBM // 2011-02-xx]\n* [https://www.ibm.com/developerworks/community/blogs/c2028fdc-41fe-4493-8257-33a59069fa04/entry/chapter_13_cloud_computing_reference_architecture1?lang=en Cloud Computing Central // IBM // 2011-03-27]\n\n\n=== White Papers from EMC (Benchmark) ===\n\n* [http://www.emc.com/collateral/hardware/white-papers/h8131-storage-tiering-oracle-vmax-wp.pdf Implementing FAST VP and Storage Tiering for Oracle Database 11g and EMC Symmetrix VMAX // 2011-04 // ((B.GOOD))]\n* [http://www.emc.com/collateral/hardware/white-papers/h11210-p690-emc-vnx7500-scaling-performance-oracle-11g-vsphere-wp.pdf EMC VNX7500 Scaling Performance for Oracle 11gR2 RAC on VMware vSphere 5.1 // 2012-12 ((B.GOOD))]\n* [http://www.compucom.com/sites/default/files/Whitepaper-Deploying-Oracle-Database-Apps-on-EMC-VNX.pdf Deploying Oracle Database Applications on EMC VNX Unified Storage // 2011-04]\n* [http://www.emc.com/collateral/hardware/white-papers/h8850-oracle-performance-vnx-fastcache-wp.pdf EMC Performance for Oracle (EMC VNX, Enterprise Flash Drives, FAST Cache, VMware vSphere // 2011-12]\n* [http://www.emc.com/collateral/white-papers/h11209-p773-osc-integration-vmax-mgt-virtualized-oracle-ebs-wp.pdf EMC Oracle VM Storage Connect Integration Module - Efficient Infrastructure Management for a Virtualized Oracle E-Business Suite // 2013-03]\n* [http://www.emc.com/collateral/analyst-reports/esg-20091208-fast.pdf ESG Whitepaper - Automate and Optimize a Tiered Storage Environment - FAST // 2009-12]\n* [http://www.emc.com/collateral/hardware/white-papers/h8091-leveraging-fast-sql-wp.pdf White Paper - Leveraging EMC Fully Automated Storage Tiering (FAST) and FAST Cache for SQL Server Enterprise Deployments // 2010-11]\n* [http://www.emc.com/collateral/software/white-papers/h10938-vnx-best-practices-wp.pdf EMC VNX Unified Best Practices for Performance // 2012-08]\n* [http://www.emc.com/collateral/white-papers/h11122-vmax10k-fastvp-tiering-oracledb-wp.pdf Implementing FAST VP and Storage Tiering for Oracle Database 11g and EMC Symmetrix VMAX 10K // 2012-09]\n* [http://www.theregister.co.uk/2013/03/05/emc_xtremsf/ EMC declares war on all-flash array, server flash card rivals - Rolls out XtremIO array, renamed VFCache - XtremSF Server Flash, XtremSW Cache Software, XtremIO all-flash array // 2013-03-05]','utf-8'),(6,'== Table of Contents ==\n\n# [[Bnote 2013-01]]\n# [[Bnote 2013-02]]\n# [[Bnote 2013-03]]\n# [[Bnote 2013-04]]\n# [[Bnote 2013-05]]','utf-8'),(7,'== ## bNote-2013-01-31 ==\n\n\n\n=== Proactive Data Placement ===\n\n* Proactive Data Placement는 Caching과 무엇이 달라야 할까?\n* 얼마나 멀리 예측할 수 있을까?\n*얼마나 자주 placement를 할 수 있을까?\n* Caching과 Tiering이 공존한다면 어떤 모습일까?\n* 사용자에게 가장 이상적인 IO의 모습은 어떤 형태일까?\n: 내가 원할 때 이미 미리 DRAM에 올라가 있는 data (proactive data placement)\n: \n: 모든 것이 Sequential IO처럼 \n\n\n* Hit count distribution과 Rehit interval을 같이 봐야 하는 이유.\nSSD cache media에게 있어 최적의 chunk 단위는 (최적 chunk 단위의 최소는 erase block이고, SSD 내부 병렬성을 높이기 위해서는 channel의 갯수만큼 정수배로 늘어날 수 있다)\n\n=== {EMC, Fusion IO} Technical Benchmarking ===\n==== EMC Technologies ====\n\n\n; Caching/Tiering related terms\n: Fully Automated Storage Tiering (FAST), Fully Automated Storage Tiering (FAST) Cache\n\n; Deduplication related terms\n: Data Deduplication, Data Deduplication Rate, Fixed-length Deduplication, Inline Deduplication, Variable-length Deduplication\n\n; Others\n: Backup Throughput, Block Data Compression, Backup and Recovery, Cloud Computing, Data Archiving, Data Center Consolidation, Data Integrity, Disaster Recovery, Infrastructure-as-a-Service (IaaS), Network Attached Storage (NAS), Network-Efficient Replication, Platform-as-a-Service (PaaS), Replication, Single-Instance Storage, Software-as-a-Service (SaaS), Unified Storage, Unisphere, Virtual Tape Library\n\n\n\n==== Fusion IO Technologies ====\n\n; directCache\n:\n\n; ioTurbine\n:\n\n; ioCache\n:\n\n; VSL\n:\n\n== ## bNote-2013-01-30 ==\n\n\n=== IOWA:: Hit Ratio, Rehit Interval, Spatial Locality ===\n\n==== Work Directory ====\n <pre>\nb@ub04:[anal] $ pwd\n/x/var/iowa/now/tracelog_analysis.v02.Q_phase.2_mysqld/anal\n</pre>\n\n==== Test case: [[TPC-C]] benchmark (realistic workload) ====\n----\n\n: Sampling time (benchmark running time): 48 hours (172,800 sec)\n: DB table size: 250GB\n: DB memory buffer pool size: 5GB\n: Transactions during sampling time: 6,335,396\n: TPS = 36.66\n\n==== Trace log ====\n----\n\n: Tracing tool: blktrace\n\n: File size: 55,741,135,177 bytes (55.7GB)\n\n: Total IO (mixed: mysqld, [[kworker]], kjournald)\n:: Number of Accesses\n::: <span style=\"color:blue\">195,735,765 RWs</span> (avg., \'\'\'1132.73 RWs/sec\'\'\' = 195,735,765 RWs / (48 * 3600) secs)\n::: <span style=\"color:blue\">4,113,312 Reads</span> (avg., \'\'\'23.8 Reads/sec\'\'\' = 4,113,312 Reads / (48 * 3600) secs)\n::: <span style=\"color:blue\">191,622,453 Writes</span> (avg., \'\'\'1108.92 Writes/sec\'\'\' = 191,622,453 Writes / (48 * 3600) secs)\n\n: MySQL DB IO\n:: Number of Accesses\n::: <span style=\"color:blue\">91,530,594 RWs</span> (avg., \'\'\'529.69 RWs/sec\'\'\' = 91,530,594 RWs / (48 * 3600) secs)\n::: <span style=\"color:blue\">4,113,230 Reads</span> (avg., \'\'\'23.8 Reads/sec\'\'\' = 4,113,230 Reads / (48 * 3600) secs)\n::: <span style=\"color:blue\">87,417,364 Writes</span> (avg., \'\'\'505.88 Writes/sec\'\'\' = 87,417,364 Writes / (48 * 3600) secs)\n\n==== Measured items ====\n----\n\n; Hit counts distribution\n: identifying which address (range) is hot or cold\n: >> can be used to \n\n; Rehit interval\n: measuring the gap between each access to corresponding address (or address chunk)\n: >> can be used to\n\n; Runs\n: measuring the subsequent access to the corresponding address (or address chunk)\n: >> can be used to\n\n==== Raw data ====\n----\n: Total I/O (mixed)\n\n <pre>\nb@ub04:[preproc] $ pwd\n/x/iowa/now/tracelog_analysis.v02.Q_phase.1_all/preproc\n\nb@ub04:[preproc] $ l\ntotal 22466000\ndrwxrwxr-x 2 b b        4096 Jan 30 16:52 ./\ndrwxrwxr-x 5 b b        4096 Jan 29 20:36 ../\n-rwxrwxr-x 1 b b          63 Jan 30 14:30 .bdx.0100.y.preproc__extract_base_data.sh*\n-rwxr-xr-x 1 b b         393 Jan 30 16:23 .tmp.print_seekdist.py*\nlrwxrwxrwx 1 b b          50 Jan 30 15:55 .tracelog -> ../tracelog/tracelog_full.iowa.20121126_102808.log\n-rw-rw-r-- 1 b b 17814000149 Jan 30 16:41 .tracelog.1.preproc.out\n-rw-rw-r-- 1 b b  2038997542 Jan 30 16:44 .tracelog.A.addr\n-rw-rw-r-- 1 b b   557857532 Jan 30 16:49 .tracelog.A.seek\n-rw-rw-r-- 1 b b    43906222 Jan 30 16:50 .tracelog.R.addr\n-rw-rw-r-- 1 b b    36224259 Jan 30 16:50 .tracelog.R.seek\n-rw-rw-r-- 1 b b  1995091320 Jan 30 16:52 .tracelog.W.addr\n-rw-rw-r-- 1 b b   519060512 Jan 30 16:58 .tracelog.W.seek\n\nb@ub04:[preproc] $ wc -l .tracelog*\n  718979637 .tracelog\n  195735765 .tracelog.1.preproc.out\n  195735765 .tracelog.A.addr\n  195735765 .tracelog.A.seek\n    4113312 .tracelog.R.addr\n    4113312 .tracelog.R.seek\n  191622453 .tracelog.W.addr\n  191622453 .tracelog.W.seek\n 1697658462 total\n\n</pre>\n\n----\n: mysqld IO\n <pre>\nb@ub04:[preproc] $ pwd\n/x/iowa/now/tracelog_analysis.v02.Q_phase.2_mysqld/preproc\n\nb@ub04:[preproc] $ l\ntotal 10379140\ndrwxrwxr-x 2 b b       4096 Jan 30 17:21 ./\ndrwxrwxr-x 5 b b       4096 Jan 29 20:36 ../\n-rwxrwxr-x 1 b b         63 Jan 30 14:30 .bdx.0100.y.preproc__extract_base_data.sh*\n-rwxr-xr-x 1 b b        393 Jan 30 17:09 .tmp.print_seekdist.py*\nlrwxrwxrwx 1 b b         62 Jan 30 15:55 .tracelog -> ../tracelog/tracelog_full.iowa.20121126_102808.log.grep_mysqld\n-rw-rw-r-- 1 b b 8190640030 Jan 30 17:15 .tracelog.1.preproc.out\n-rw-rw-r-- 1 b b  962083939 Jan 30 17:16 .tracelog.A.addr\n-rw-rw-r-- 1 b b  257803938 Jan 30 17:19 .tracelog.A.seek\n-rw-rw-r-- 1 b b   43905459 Jan 30 17:19 .tracelog.R.addr\n-rw-rw-r-- 1 b b   36223476 Jan 30 17:19 .tracelog.R.seek\n-rw-rw-r-- 1 b b  918178480 Jan 30 17:21 .tracelog.W.addr\n-rw-rw-r-- 1 b b  219360508 Jan 30 17:23 .tracelog.W.seek\n\nb@ub04:[preproc] $ wc -l .tracelog.*\n   91530594 .tracelog.1.preproc.out\n   91530594 .tracelog.A.addr\n   91530594 .tracelog.A.seek\n    4113230 .tracelog.R.addr\n    4113230 .tracelog.R.seek\n   87417364 .tracelog.W.addr\n   87417364 .tracelog.W.seek\n  457652970 total\n\n</pre>\n\n\n----\n: kjournald IO\n <pre>\nb@ub04:[preproc] $ pwd\n/x/iowa/now/tracelog_analysis.v02.Q_phase.3_kjournald/preproc\n\nb@ub04:[preproc] $ l\ntotal 11618220\ndrwxrwxr-x 2 b b       4096 Jan 30 17:41 ./\ndrwxrwxr-x 5 b b       4096 Jan 29 20:36 ../\n-rwxrwxr-x 1 b b         63 Jan 30 14:30 .bdx.0100.y.preproc__extract_base_data.sh*\n-rwxr-xr-x 1 b b        393 Jan 30 17:28 .tmp.print_seekdist.py*\nlrwxrwxrwx 1 b b         65 Jan 30 15:55 .tracelog -> ../tracelog/tracelog_full.iowa.20121126_102808.log.grep_kjournald\n-rw-rw-r-- 1 b b 9251596027 Jan 30 17:35 .tracelog.1.preproc.out\n-rw-rw-r-- 1 b b 1034962365 Jan 30 17:36 .tracelog.A.addr\n-rw-rw-r-- 1 b b  287740440 Jan 30 17:39 .tracelog.A.seek\n-rw-rw-r-- 1 b b        528 Jan 30 17:39 .tracelog.R.addr\n-rw-rw-r-- 1 b b        257 Jan 30 17:39 .tracelog.R.seek\n-rw-rw-r-- 1 b b 1034961837 Jan 30 17:41 .tracelog.W.addr\n-rw-rw-r-- 1 b b  287740324 Jan 30 17:43 .tracelog.W.seek\n\nb@ub04:[preproc] $ wc -l .tracelog.*\n  100177473 .tracelog.1.preproc.out\n  100177473 .tracelog.A.addr\n  100177473 .tracelog.A.seek\n         48 .tracelog.R.addr\n         48 .tracelog.R.seek\n  100177425 .tracelog.W.addr\n  100177425 .tracelog.W.seek\n  500887365 total\n\n</pre>\n\n----\n: kworker IO\n <pre>\nb@ub04:[preproc] $ pwd\n/x/iowa/now/tracelog_analysis.v02.Q_phase.4_kworker/preproc\n\nb@ub04:[preproc] $ l\ntotal 20\ndrwxrwxr-x 2 b b 4096 Jan 30 17:49 ./\ndrwxrwxr-x 5 b b 4096 Jan 29 20:36 ../\n-rwxrwxr-x 1 b b   63 Jan 30 14:30 .bdx.0100.y.preproc__extract_base_data.sh*\n-rwxr-xr-x 1 b b  393 Jan 30 17:48 .tmp.print_seekdist.py*\nlrwxrwxrwx 1 b b   63 Jan 30 15:55 .tracelog -> ../tracelog/tracelog_full.iowa.20121126_102808.log.grep_kworker\n-rw-rw-r-- 1 b b    0 Jan 30 17:48 .tracelog.1.preproc.out\n-rw-rw-r-- 1 b b    0 Jan 30 17:49 .tracelog.A.addr\n-rw-rw-r-- 1 b b    0 Jan 30 17:49 .tracelog.A.seek\n-rw-rw-r-- 1 b b    0 Jan 30 17:49 .tracelog.R.addr\n-rw-rw-r-- 1 b b    0 Jan 30 17:49 .tracelog.R.seek\n-rw-rw-r-- 1 b b    0 Jan 30 17:49 .tracelog.W.addr\n-rw-rw-r-- 1 b b    0 Jan 30 17:49 .tracelog.W.seek\n\nb@ub04:[preproc] $ wc -l .tracelog.*\n0 .tracelog.1.preproc.out\n0 .tracelog.A.addr\n0 .tracelog.A.seek\n0 .tracelog.R.addr\n0 .tracelog.R.seek\n0 .tracelog.W.addr\n0 .tracelog.W.seek\n0 total\n\n</pre>\n\n=== Supermicro 16-node Cluster ===\n==== iu05 information ====\n\n* uname -a\n <pre>\nroot@ub05:[~] # uname -a\nLinux ub05 2.6.35-22-server #33-Ubuntu SMP Sun Sep 19 20:48:58 UTC 2010 x86_64 GNU/Linux\n</pre>\n\n* boot info (GRUB)\n <pre>\n### BEGIN /etc/grub.d/10_linux ###\nmenuentry \'Ubuntu, with Linux 2.6.35-32-server\' --class ubuntu --class gnu-linux --class gnu --class os {\n        recordfail\n        insmod part_msdos\n        insmod ext2\n        set root=\'(hd0,msdos4)\'\n        search --no-floppy --fs-uuid --set b62b37f9-8615-4a98-971b-8b0057388424\n        linux   /boot/vmlinuz-2.6.35-32-server root=UUID=b62b37f9-8615-4a98-971b-8b0057388424 ro   quiet\n        initrd  /boot/initrd.img-2.6.35-32-server\n}\n\n</pre>\n\n* network\n <pre>\nroot@ub05:[~] # ifconfig\neth1      Link encap:Ethernet  HWaddr 00:25:90:62:a5:37  \n          inet addr:10.0.3.5  Bcast:10.0.3.255  Mask:255.255.255.0\n          UP BROADCAST MULTICAST  MTU:1500  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000 \n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\n          Interrupt:17 Memory:fb6e0000-fb700000 \n\neth2      Link encap:Ethernet  HWaddr 00:02:c9:52:c5:b1  \n          inet addr:10.0.1.5  Bcast:10.0.1.255  Mask:255.255.255.0\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:2491695 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:2502347 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000 \n          RX bytes:709180802 (709.1 MB)  TX bytes:560063042 (560.0 MB)\n\nib0       Link encap:UNSPEC  HWaddr 80-00-00-59-FE-80-00-00-00-00-00-00-00-00-00-00  \n          inet addr:10.0.2.5  Bcast:10.0.2.255  Mask:255.255.255.0\n          UP BROADCAST RUNNING MULTICAST  MTU:2044  Metric:1\n          RX packets:5060 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:256 \n          RX bytes:1234029 (1.2 MB)  TX bytes:0 (0.0 B)\n\nlo        Link encap:Local Loopback  \n          inet addr:127.0.0.1  Mask:255.0.0.0\n          UP LOOPBACK RUNNING  MTU:16436  Metric:1\n          RX packets:509145 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:509145 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:105598972 (105.5 MB)  TX bytes:105598972 (105.5 MB)\n\nvirbr0    Link encap:Ethernet  HWaddr d2:23:30:e6:b6:64  \n          inet addr:192.168.122.1  Bcast:192.168.122.255  Mask:255.255.255.0\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\n\nroot@ub05:[~] # route -n\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n10.0.1.0        0.0.0.0         255.255.255.0   U     0      0        0 eth2\n10.0.2.0        0.0.0.0         255.255.255.0   U     0      0        0 ib0\n10.0.3.0        0.0.0.0         255.255.255.0   U     0      0        0 eth1\n192.168.122.0   0.0.0.0         255.255.255.0   U     0      0        0 virbr0\n0.0.0.0         10.0.1.1        0.0.0.0         UG    100    0        0 eth2\n\nroot@ub05:[~] # cat /etc/resolv.conf \ndomain sait.samsung.co.kr\nsearch sait.samsung.co.kr\nnameserver 10.41.128.98\nnameserver 10.100.181.206\n\n</pre>\n\n* mount\n <pre>\nb@ub05:[~] $ mount\n/dev/sda4 on / type ext4 (rw,errors=remount-ro)\nproc on /proc type proc (rw,noexec,nosuid,nodev)\nnone on /sys type sysfs (rw,noexec,nosuid,nodev)\nfusectl on /sys/fs/fuse/connections type fusectl (rw)\nnone on /sys/kernel/debug type debugfs (rw)\nnone on /sys/kernel/security type securityfs (rw)\nnone on /dev type devtmpfs (rw,mode=0755)\nnone on /dev/pts type devpts (rw,noexec,nosuid,gid=5,mode=0620)\nnone on /dev/shm type tmpfs (rw,nosuid,nodev)\nnone on /var/run type tmpfs (rw,nosuid,mode=0755)\nnone on /var/lock type tmpfs (rw,noexec,nosuid,nodev)\nnfsd on /proc/fs/nfsd type nfsd (rw)\nbinfmt_misc on /proc/sys/fs/binfmt_misc type binfmt_misc (rw,noexec,nosuid,nodev)\niu04:/disk/fio1 on /disk/_nfs1 type nfs (rw,vers=4,addr=10.0.1.4,clientaddr=10.0.1.5)\niu05:/disk/fio1 on /disk/_nfs2 type nfs (rw,vers=4,addr=10.0.1.5,clientaddr=10.0.1.5)\n/dev/sdb1 on /disk/hdd1 type ext4 (rw)\n/dev/sdd1 on /disk/hdd2 type ext4 (rw)\n</pre>\n\n=== Ubuntu Upgrade from Maverick (10.10 to newer version) ===\n\n; problem\n\n!!! Need to install missing software packages into some nodes in 16-node cluster. But the problem is too old (end-of-life) distribution version to get support or to install. So I searched the way to solve this problem, the following is the result. (it works!)\n\n; solution (B.GOOD)\n\nHow to install software or upgrade from old unsupported release? [http://askubuntu.com/questions/91815/how-to-install-software-or-upgrade-from-old-unsupported-release]\n: answered Dec 31 \'11 at 18:54\n\nThe repositories for older releases that are not supported (like 9.04, 9.10, and 10.10) get moved to an archive server. There are repositories available at http://old-releases.ubuntu.com\n\nThe reason for this is that it is now out of support and no longer receiving updates and security patches.\n\nI would urge you to consider a supported distribution. If your computer is too old in terms of memory or processor then you should consider a distribution such as Lubuntu or Xubuntu.\n\nIf you want to continue using an outdated release then edit <span style=\"color:red\">\'\'\'/etc/apt/sources.list\'\'\'</span> and change archive.ubuntu.com to <span style=\"color:red\">\'\'\'old-releases.ubuntu.com\'\'\'</span>\n\nYou can do this with sed\n\n sudo sed -i -e \'s/archive.ubuntu.com/old-releases.ubuntu.com/g\' /etc/apt/sources.list\n\nthen update with\n\n sudo apt-get update && sudo apt-get dist-upgrade\n\nSee also:\n\n[https://help.ubuntu.com/community/EOLUpgrades/ EOL (End-of-Life) Upgrades]\n\n=== GNUPLOT Tips ===\n\n\n* Gnuplot 4.2 Tutorial [http://people.duke.edu/~hpgavin/gnuplot.html]\n\n4. CUSTOMIZING YOUR PLOT\n\nMany items may be customized on the plot, such as the ranges of the axes, the labels of the x and y axes, the style of data point, the style of the lines connecting the data points, and the title of the entire plot.\n\n4.1 plot command customization\n\nCustomization of the data columns, line titles, and line/point style are specified when the plot command is issued. Customization of the data columns and line titles were discussed in section 3.\n\nPlots may be displayed in one of eight styles: lines, points, linespoints, impulses, dots, steps, fsteps, histeps, errorbars, xerrorbars, yerrorbars, xyerrorbars, boxes, boxerrorbars, boxxyerrorbars, financebars, candlesticks or vector To specify the line/point style use the plot command as follows:\n\n      gnuplot> plot \"force.dat\" using 1:2 title \'Column\' with lines, \\\n                    \"force.dat\" u 1:3 t \'Beam\' w linespoints\nNote that the words: using , title , and with can be abbreviated as: u , t , and w . Also, each line and point style has an associated number.\n\n4.2 set command customization\n\nCustomization of the axis ranges, axis labels, and plot title, as well as many other features, are specified using the set command. Specific examples of the set command follow. (The numerical values used in these examples are arbitrary.) To view your changes type: replot at the gnuplot> prompt at any time.\n\n      Create a title:                  > set title \"Force-Deflection Data\" \n      Put a label on the x-axis:       > set xlabel \"Deflection (meters)\"\n      Put a label on the y-axis:       > set ylabel \"Force (kN)\"\n      Change the x-axis range:         > set xrange [0.001:0.005]\n      Change the y-axis range:         > set yrange [20:500]\n      Have Gnuplot determine ranges:   > set autoscale\n      Move the key:                    > set key 0.01,100\n      Delete the key:                  > unset key\n      Put a label on the plot:         > set label \"yield point\" at 0.003, 260 \n      Remove all labels:               > unset label\n      Plot using log-axes:             > set logscale\n      Plot using log-axes on y-axis:   > unset logscale; set logscale y \n      Change the tic-marks:            > set xtics (0.002,0.004,0.006,0.008)\n      Return to the default tics:      > unset xtics; set xtics auto\n\nOther features which may be customized using the set command are: arrow, border, clip, contour, grid, mapping, polar, surface, time, view, and many more. The best way to learn is by reading the on-line help information, trying the command, and reading the Gnuplot manual. You may also post questions to the newsgroup comp.graphics.apps.gnuplot\n\n \n\n\n=== I/O Intelligence Needs ===\n\n==== Workload Analytics based (Proactive) Data Placement ====\n\n{| border=1 style=\"text-align: left; color: darkblue; border-collapse: collapse; border: 1px solid #000; border-spacing: 10; padding: 10\" align=top\n| 비교 항목\n| style=\"width:40%\" | 경쟁기술 (Auto Tiering)\n| style=\"width:40%\" | SAIT 기술 (Workload Analytics 기반 Data Placement)\n|-\n| Approach\n|\n; Access 빈도 체크 방식: 과거에 access 빈도가 높으면 fast-tier로, 낮으면 slow-tier로 이동하는 방식\n|\n; Workload analytics: 다각적 workload 분석 및 IO 패턴 모델링\n; Proactive data placement: IO 패턴 모델과 지속적 IO 양상 변화 감지에 기반한 data의 pre-placement \n|-\n| IO Intelligence 수준\n|\n; 낮음\n: IO의 다양한 측면 중 액세스 빈도 특성만 관측\n: IO 패턴 모델에 기반한 미래 예측 기능 없음\n|\n; 높음\n: frequency, recency, periodicity, spatial-locality 등 IO에 대한 다각적 분석\n: IO 패턴 모델에 기반한 미래 IO 패턴 예측\n|-\n| 대응 속도\n|\n; As-fast-as-reactive\n; 주기적 tiering: 주기적으로 bulk data 이동 실시 (e.g., 하루에 한 번 -> 즉각적 IO 병목 해결 어려움)\n| \n; As-fast-as-proactive, on-the-fly action\n: IO 패턴 예측에 기반한 선제적 data 배치로 미리 대응 가능\n: 지속적인 IO 양상 감지에 기반한 placement triggering\n|}\n\n\n=== Task Assignment ===\n\n\n\n구본철 전문 : \n  [관리] KAIST산학, \n  [short-term research] [needs to be defined]\n  [long-term research] 분산-Scale Data Dedup, (Bio-inspired Storage?)\n\n서정민 전문 :\n  [관리] 신사업추진단, 센싱\n  [short-term research] new RAID5 (parity offload) ---> 분산-scale I/O Coordination\n  [long-term research] 분산-scale Metadata 관리\n\n신현정 전문 :\n  [관리] SAIT India\n  [short-term research] Dedup-SSD Vertical Optimization\n  [long-term research] 분산-Scale Data Dedup\n\n정명준 전문 :\n  [관리] 서울대산학\n  [short-term research] hybrid cache ---> 분산-scale data placement\n  [long-term research] 분산-scale data placement, I/O Workload Analysis\n\n이주평 전문 (PL) :\n  [관리] SAIT America, \n  [short-term research] new RAID1 ---> 분산-scale replication\n  [long-term research] 분산-scale I/O Coordination, Metadata 관리\n\n유개원 박사 (예정) :\n  [short-term research] new RAID1\n  [long-term research] 분산-scale data placement\n\nSAIT India :\n  [~4월] new RAID1 프로토 구현 (fault detection&recovery, managed early commit& early trim)\n  SSD emulator, OpenSSD기반 프로토\n  분산-scale infra 프로토 구현 [needs to be defined]\n\nSAIT America :\n  DRAM기반 분산스토리지 기술 (Programmable DRAM Controller; App-aware DRAM)\n  Programmable SSD (App-aware SSD) 측면 확대 검토\n\n== ## bNote-2013-01-29 ==\n\n=== pgtrace (Page Trace) ===\n\n* SHOULD understand the mechanism of debugfs(?) used by blktrace program\n\n\n=== Fusion IO Atomic Writes ===\n\n*; About Fusion IO Atomic Writes (2011-10-04) [http://www.fusionio.com/overviews/accelerate-mysql-with-fusions-atomic-writes-extension/]\n: \'\'Accelerate MySQL with Fusion\'s Atomic Writes Extension\'\'\n: Atomic writes \'\'\'guarantee\'\'\' consistency and integrity of data for operations in which a process simultaneously writes multiple non-contiguous storage blocks as a single transaction. <span style=\"color:blue\">Atomicity ensures that operations are completed in their entirety or not at all</span>. <span style=\"color:red\">MySQL databases normally provide atomicity via a double write buffer that requires all writes happen twice.</span> Fusion’s Atomic Writes extension <span style=\"color:blue\">eliminates</span> the need for this double-write buffer cache, moving the atomicity burden from the application layer into the storage stack. This greatly improves performance, reduces code complexity, and can even double the flash hardware endurance.\n\n[[File:Fusion IO Atomic Writes.jpg | 700px]]\n\n=== Yanpei Chen Research Work === \n\n* Yanpei Chen home page [http://www.eecs.berkeley.edu/~ychen2/]\n\n:* Workload-Driven Design and Evaluation of LargeScale Data-Centric Systems (Ph.D Thesis) [http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-73.pdf]\n\n:* \'\'\'<span style=\"color:darkblue\">Design Implications for Enterprise Storage Systems via Multi-Dimensional Trace Analysis (SOSP 2011)</span>\'\'\' [http://www.eecs.berkeley.edu/~ychen2/professional/CifsTraceAnalysisSOSP2011CameraReady1.pdf]\n::* Key questions\n::*# Do applications exhibit clear access patterns?\n::*# What are the user-level access patterns?\n::*# Any correlation between users and applications?\n::*# Do all applications interact with files in the same way?\n::*: >> 이러한 insight들이 있다면, per-session QoS (at server) 혹은 per-application SLO (Service level objectieve) guarantee 시에 활용 가능.\n::* We need to understand\n::*# how files are grouped within a directory\n::*# cross-file dependencies\n::*# directory organization\n::*# ...\n::* We perform\n::*# Multi-layer and cross-layer dependency analysis\n::* Client side observations and design implications\n:::\'\'\'1. Client sessions with IO sizes > 128KB are read only or write only\'\'\'\n:::: -> Clients can consolidate sessions based on only the read-write ratio.\n::::: >> <span style=\"color:darkred\">이것은 \'\'\'shared caching\'\'\'과 \'\'\'consolidation policies across sessions\'\'\' 시 반영 가능한 Insight임. 한 예로, client OS는 read-only 특징을 가지는 cache-sensitive session과 write-only 특징을 가지는 cache insensitive session을 감지하여 co-locate 시킴으로써, cache utilization 및 consolidation을 향상시킬 수 있다. (즉, server 당 session 밀도 (density of sessions per server)를 높일 수 있음)</span>\n:::\'\'\'2. Client Sessions with duration > 8 hours do ~= 10MB of IO\'\'\'\n:::: -> Client caches can already fit an entire day\'s IO.\n:::\'\'\'3. Number of client sessions drops off linearly by 20% from Monday to Friday\'\'\'\n:::: -> Servers can get an extra \"day\" for background tasks by running at appropriate times during week days.\n:::\'\'\'4. Applications with < 4KB of IO per file open and many opens of a few files do only random IO\'\'\'\n:::: -> Clients should always cache the first few KB of IO per file per application.\n:::\'\'\'5. Applications with > 50% sequential read or write access entire files at a time\'\'\'\n:::: -> Clients can request file prefetch (read) or delegation (write) based on only the IO sequentiality.\n:::\'\'\'6. Engineering applications with > 50% sequential read and sequential write are doing code compile tasks, based on file extensions\'\'\'\n:::: -> Servers can identify compile tasks; server has to cache the output of these tasks.\n::* Server side observations and design implications\n:::\'\'\'7. Files with > 70% sequential read or write have no repeated reads or overwrites\'\'\'\n:::: -> Servers should delegate sequentially accessed files to clients to improve IO performance.\n::::: <span style=\"color:darkred\">이 access pattern은 다음 네 개의 특성과 관련되어 있음. \'\'\'read sequentiality, write sequentiality, repeated read behavior, overwrite behavior\'\'\'. 즉 sequentially accessed file은 server에서 cache될 필요가 없으며 (no repeated reads), 이는 buffer cache의 효율적인 운용으로 연결될 수 있다.</span>\n:::\'\'\'8. Engineering files with repeated reads have random accesses\'\'\'\n:::: -> Servers should delegate repeatedly read files to clients; clients need to store them in flash or memory.\n:::\'\'\'9. All files are active (have opens, IO, and metadata access) for only 1-2 hours in a few months\'\'\'\n:::: -> Servers can use file idle time to compress or deduplicate to increase storage capacity.\n:::\'\'\'10. All files have either all random access or > 70% sequential access. (Seen in past studies too)\'\'\'\n:::: -> Servers can select the best storage medium for each file based on only access sequentiality.\n:::\'\'\'11. Directories with sequentially accessed files almost always contain randomly accessed files as well\'\'\'\n:::: -> Servers can change from per-directory placement policy (default) to per-file policy upon seeing any sequential IO to any files in a directory.\n:::\'\'\'12. Some directories aggregate only files with repeated reads and overwrites\'\'\'\n:::: -> Servers can delegate these directories entirely to clients, tradeoffs permitting.\n\n\n:* Challenges and Opportunities for Managing Data Systems Using Statistical Models [http://www.eecs.berkeley.edu/~ychen2/professional/ieeeDataEngStatsModels-publish.pdf]\n\n:* Interactive Analytical Processing in Big Data Systems: A Cross-Industry Study of MapReduce Workloads [https://amplab.cs.berkeley.edu/wp-content/uploads/2012/05/mainVLDB2012.pdf]\n\n== ## bNote-2013-01-28 ==\n\n\n=== Defense Logic:: Big Data Platform Research in SAIT ===\n\n* [[Defense Logic -- Big Data Platform Research in SAIT]]\n\n\n=== (RAM/SSD) Caching and Distributed/Networked File Systems ===\n\n\n* [http://tarasistem.com/pdf/Parallel%20Network%20File%20System%20Configuration%20and%20Best%20Practices%20for%20Data%20ONTAP%208.1%20Cluster-Mode.pdf \"Parallel Network File System Configuration and Best Practices for Data ONTAP 8.1 Cluster-Mode,\" Amrutha Naik, Bikash Roy Choudhury, NetApp, May 2012 -- NetApp Technical Report (pdf)]\n\n\n* [http://www.cyberciti.biz/faq/centos-redhat-install-configure-cachefilesd-for-nfs/ CentOS / RHEL CacheFS: Speed Up Network File System (NFS) File Access]\n apt-get install cachefilesd\n\n\n* [http://dlc.sun.com/osol/nfsv41/downloads/20090221/FAST_BoF_Final.pdf pNFS Architecture (pdf)]\n\n\n* [http://www.alacritech.com/nfs/nfs-tutorial-part-1-metadata-challenge/ \"An NFS Tutorial ? Part 1: The Metadata Challenge,\" By Andrew Zillman, On March 21, 2011]\n\n\n* [http://www.alacritech.com/category/metadata-caching/ \"An NFS Tutorial - Part 3: Caching the ANX Way,\" alacritech]\n\n\n* [http://www.scidb.org/forum/viewtopic.php?f=11&t=576 Metadata and NFS]\n <pre>\nby liberatti ≫ Fri Oct 05, 2012 10:18 am\n\nThe user guide says that it is necessary a NFS folder, in this case /share, do run scidb on cluster (metadata var), but my master node is not using it. Am I doing something wrong ?\n\nscidb-version=11.12\n====================================================================\n[cluster_test]\nnode-0=master,0\nnode-1=worker,4\ndb_user=scidb\ndb_passwd=scidb\ninstall_root=/opt/scidb/11.12\nmetadata=/share/meta.sql\npluginsdir=/opt/scidb/11.12/lib/scidb/plugins\nlogconf=/opt/scidb/11.12/share/scidb/log4cxx.properties\nbase-path=/home/scidb/database\nbase-port=1239\ninterface=eth0\nno-watchdog=false\n====================================================================\n</pre>\n\n\n* [http://www.storage-switzerland.com/Blog/Entries/2013/1/18_NAS_Acceleration_Is_More_Than_Just_SSD.html Storage Sitzerland -- \"NAS Acceleration Is More Than Just SSD (and a flash in the pan)\", George Crump, January 18, 2013]\n\n\n* [http://searchstorage.techtarget.com/pNFS-spec-for-faster-file-service-arrives-but-NAS-systems-lack-capable-clients \"pNFS spec for faster file service arrives, but NAS systems lack capable clients,\" 2010, searchstorage.techtarget]\n\n\n* [http://tldp.org/HOWTO/NFS-HOWTO/performance.html \"Linux NFS Howto -- 5. Optimizing NFS Performance,\" Tavis Barr, Nicolai Langfeldt, Seth Vidal, Tom McNeal, 2002-08-25]\n*# Setting Block Size to Optimize Transfer Speeds\n*# Packet Size and Network Drivers\n*# Overflow of Fragmented Packets\n*# NFS over TCP\n*# Timeout and Retransmission Values\n*# Number of Instances of the NFSD Server Daemon\n*# Memory Limits on the Input Queue\n*# Turning Off Autonegotiation of NICs and Hubs\n*# Synchronous vs. Asynchronous Behavior in NFS\n*# Non-NFS-Related Means of Enhancing Server Performance\n*## use RAID 1/0 for both write speed and redundancy (RAID 5 gives you good read speeds but lousy write speeds)\n*## exploit journalling feature (to reduce reboot time in the event of a system crash)\n*## exploit journalling feature (to maximize the performance - updates go first to the journal, go later to main file system)\n*## some manufactures (e.g., NetApp, HP, and others) provide NFS accelerators in the form of NVRAM\n*##: NVRAM will boost access speed to stable storage up to the equivalent of async access\n\n\n* [http://www.snia.org/sites/default/files2/SDC2012/presentations/NFS/BikashRoyChoudhury_Scaling_Oracle_with_pNFS.pdf \"Scaling Oracle with pNFS: Improving Database Efficiency for Scale-out Architecture,\" Bikash Roy Choudhury, Solutions Architect, NetApp, SNIA, SDC 2012]\n*# Comparison between NFSv3, NFSv4 and NFSv4.1\n*# How is pNFS different\n*# pNFS Implementation\n*# Oracle on NetApp Cluster-Mode (Scale-out)\n*# Oracle, pNFS and NetApp Cluster-Mode\n\n<br/>\n\n=== B\'s Opinion on NFS & (Meta) data Caching ===\n\n* 예전의 NFS version (v3, v4 등)에서는 여러 고질적인 문제점들이 있었습니다. 그 중 하나가 meta data와 data가 같은 I/O path를 share하는 구조이구요, 이때문에 meta data와 data를 별도로 다루기가 까다로운 점이 있었습니다 (이것 말고도 NFS에 참여하는 node들의 scalability가 제한적이라는 구조적 문제도 많이 언급됩니다). 이런 문제들을 풀어내고자 새롭게 정의되고 있는 pNFS (Parallel NFS)에서는 meta data와 data가 별도의 I/O path로 다루어질 수 있도록 설계/구현하고 있는 것으로 알고 있습니다. 따라서, pNFS로 가게 되면 meta data path를 별도로 잡을 수 있게 될 것이고, SSD를 cache로 이용하고자 할 때 이 점을 활용할 수 있을 것 같습니다.\n\n=== R Statistical Computing Software ===\n\n* [[Statistical computing software]]\n\n=== USENIX FAST 2013 ===\n\n* [[USENIX FAST 2013]]\n\n<br/>\n----\n\n== ## bNote-2013-01-25 ==\n\n=== In-Memory Computing and IMDB ===\n\n* [[Why In-Memory Computing Needs Flash]] // November 16, 2012 // flashdba [http://flashdba.com/category/database/in-memory/]\n** In-Memory Computing (IMC) definition\n** How does In-Memory Computing Deliver Faster Performance?\n** What are the Barriers to Success with In-Memory Computing?\n** NAND Flash Allows for New Possibilities\n\n\n* [[Thoughts on In Memory Databases (Part 1)]] // October 9, 2012 // flashdba [http://flashdba.com/2012/10/09/in-memory-databases-part1/]\n** What is memory?\n** Primary Storage versus Secondary Storage\n** Volatile versus Persistent\n**: Everyone is talking about \'\'\'In Memory\'\'\' at the moment. On blogs, in tweets, in the press, in the Oracle marketing department, in books by SAP employees, even my Violin colleagues ...\n\n\n* [[In Memory Databases: HANA, Exadata X3 and Flash Memory (Part 2)]] // October 10, 2012 // flashdba [http://flashdba.com/2012/10/10/in-memory-databases-part2/]\n** What Is An In Memory Database?\n** Why is an In-Memory Database Fast?\n** Is My Database An In Memory Database?\n**# Database Running in DRAM - e.g., SAP HANA\n**# Database Running on Flash Memory - e.g., on Violin Memory\n**# Database Accessing Remote DRAM and Flash Memory - e.g., Oracle Exadata X3\n** Conclusion\n\n<br/>\n\n=== Amazon AWS Glacier - Backup Storage ===\n\n* [http://www.bloter.net/archives/123771 Amazon Glacier -- Bloter.net]\n*: AWS는 자사 웹사이트를 통해 백업용 대용량 스토리지 서비스인 ‘아마존 글레이셔(glacier)’를 출시한다고 발표했다. 글레이셔는 대용량 저가 스토리지 서비스로 몇십 페타바이트(PB) 단위의 데이터를 안전하게 보관할 수 있다\n\n<br/>\n\n=== 기술원 유선 전화 사용법 ===\n\n<!--\n{| style=\"text-align: left; color: darkblue; border-collapse: collapse; border: 10px solid #fff; border-spacing: 10; padding: 10\"\n|-\n| style=\"border-style: solid; border-width: 1px 1px 1px 1px; border-spacing: 10; padding: 10\" | * 41 4 01028141130 *\n| style=\"border-style: solid; border-width: 1px 1px 1px 1px; border-spacing: 10; padding: 10\" | Ringing 없이 바로 착신 전환\n|-\n| style=\"border-style: solid; ; border-spacing: 10; padding: 10\" | * 41 9243\n| style=\"border-style: solid; ; border-spacing: 10; padding: 10\" | Ringing 없이 바로 사서함\n|-\n| style=\"border-style: solid; border: 10px solid #f00\" | #41\n| style=\"border-style: solid; border: 10px solid #00f\" | \'*41\' 시리즈 설정 해제\n|-\n| * 43 4 01028141130 *\n| Ringing 2번 후 착신 전환\n|-\n| * 43 9243\n| Ringing 2번 후 사서함\n|-\n| #43\n| \'*43\' 시리즈 설정 해제\n|-\n|}\n-->\n\n\n\n{| class=\"wikitable\" border=\"1\" style=\"border-collapse:collapse; width:500px; height:200px\"\n|-\n| * 41 4 01028141130 *\n| Ringing 없이 바로 착신 전환\n|-\n| * 41 9243\n| Ringing 없이 바로 사서함\n|-\n| #41\n| \'*41\' 시리즈 설정 해제\n|-\n| * 43 4 01028141130 *\n| Ringing 2번 후 착신 전환\n|-\n| * 43 9243\n| Ringing 2번 후 사서함\n|-\n| #43\n| \'*43\' 시리즈 설정 해제\n|-\n|}\n\n\n<br/>\n----\n\n== ## bNote-2013-01-24 ==\n\n=== How much DRAM can be mounted on intel x86 motherboard? ===\n\n* what\'s the maximum capacity of DRAM in the extreme case and in common case?\n<!--\n\n-->\n*# Extreme case: (still not sure this is the case of the maximum capacity)\n*#; [http://www-03.ibm.com/systems/x/options/memory/max5/index.html IBM MAX5 for System x]\n*#: Expand memory capacity of x3850 X5 and x3690 X5 systems by up to 1TB\n*#: MAX5 for System x V2: 1.35V 1.5V [[DIMM]], 4GB, 8GB, 16GB, <span style=\"color:red\">32GB [[DIMM]]</span> support\n*#: 32 [[DIMM]] slots, 8 memory buffers\n*#: -> So, max case is: 32GB [[DIMM]] * 32 [[DIMM]] slots = 1024GB memory = 1TB memory\n<!--\n\n-->\n*# Normal case: <span style=\"color:red\">(Mainboard + 768GB DRAM 가격: 13,860,000원)</span>\n*#; 768GB DRAM + main board 가격\n*#: 위 부품 기준으로, 768GB DRAM을 장착한 보드의 값은 13,860,000원 (= 900,000원 + 540,000원 x 24)\n*#: Note: CPU, HDD 등 제외한 가격으로 순수 mainboard + DRAM 가격임, 여기에 이정도 전력을 커버할 수 있는 power supply 가격도 많이 올라갈 수 있음.\n<!--\n\n-->\n*#; Main board\n*#: TyanKorea의 mainboard, 768GB DRAM까지 지원, memory slot 24개 (2013-01-25 현재 900,000 원)\n*#: [http://prod.danawa.com/info/?pcode=1745198&cate1=861&cate2=875&cate3=968&cate4=0 Mainboard -- TYAN S7052GM3NR TyanKorea Intel CPU (scale up to 768GB DRAM)]\n*#; DRAM\n*#: 32GB DDR3 PC3-19200 (2013-01-25 기준으로 540,000원)\n*#: [http://prod.danawa.com/info/?pcode=1701537&cate1=861&cate2=874&cate3=11043&cate4=0 G.SKILL DDR3 32G PC3-19200 CL10 TRIDENT TX (8Gx4) 티뮤정품]\n{| align=\"center\" width=\"80%\"\n| [[File:Dram 32gb gskill trident.jpg|200px|DRAM DDR3 32G PC3-19200]]\n| [[File:Motherboard max 768gb memory tyan s7052gm3nr tyankorea.jpg|200px|Mainboard up to 768GB DRAM]]\n|-\n| DRAM DDR3 32G PC3-19200\n| Mainboard up to 768GB DRAM\n|}\n<!--\n\n-->\n* DRAM+SSD case <span style=\"color:red\">(Mainboard + 32GB DRAM + 1TB SSD 가격: 1,738,240 원)</span>\n*#; Main board + 32GB DRAM + 512 SSD x2 가격: \n*#: 1,738,240 원 = (240,000 + 136,120 x 2 + 613,000 x 2)\n*#; Main board\n*#: ASUS P8Z77-V STCOM (2013-01-25 현재 240,000원)\n*#: [http://prod.danawa.com/info/?pcode=1637081&cate1=861&cate2=875&cate3=968&cate4=0 ASUS P8Z77-V STCOM]\n*#; DRAM\n*#: 삼성전자 DDR3 16G PC3-12800 ECC/REG LP (2013-01-25 현재 136,120 원)\n*#: [http://prod.danawa.com/info/?pcode=1685015&cate1=861&cate2=874&cate3=11043&cate4=0 삼성전자 DDR3 16G PC3-12800 ECC/REG LP]\n*#; SSD\n*#: 삼성전자 840 Pro 512GB (2013-01-25 현재 613,000원)\n*#: [http://prod.danawa.com/info/?pcode=1799035&cate1=861&cate2=32617&cate3=32623&cate4=0 삼성전자 840 PRO Series (512GB, MZ-7PD512B/KR, 정품)]\n\n== eX5: The fifth generation of IBM X-Architecture ==\n\n* [http://public.dhe.ibm.com/common/ssi/ecm/en/xsd03054usen/XSD03054USEN.PDF IBM System x3850 X5 and x3950 X5]\n\n\n<br/>\n----\n\n== ## bNote-2013-01-23 ==\n\n=== Python:: filter(), map(), reduce() ===\n\n* [http://docs.python.org/2/tutorial/datastructures.html filter(), map(), reduce() -- functional programming tools]\n\n <pre>\n\n5.1.3. Functional Programming Tools\nThere are three built-in functions that are very useful when used with lists: filter(), map(), and reduce().\n\nfilter(function, sequence) returns a sequence consisting of those items from the sequence for which function(item) is true. If sequence is a string or tuple, the result will be of the same type; otherwise, it is always a list. For example, to compute a sequence of numbers not divisible by 2 and 3:\n\n>>>\n>>> def f(x): return x % 2 != 0 and x % 3 != 0\n...\n>>> filter(f, range(2, 25))\n[5, 7, 11, 13, 17, 19, 23]\nmap(function, sequence) calls function(item) for each of the sequence’s items and returns a list of the return values. For example, to compute some cubes:\n\n>>>\n>>> def cube(x): return x*x*x\n...\n>>> map(cube, range(1, 11))\n[1, 8, 27, 64, 125, 216, 343, 512, 729, 1000]\nMore than one sequence may be passed; the function must then have as many arguments as there are sequences and is called with the corresponding item from each sequence (or None if some sequence is shorter than another). For example:\n\n>>>\n>>> seq = range(8)\n>>> def add(x, y): return x+y\n...\n>>> map(add, seq, seq)\n[0, 2, 4, 6, 8, 10, 12, 14]\nreduce(function, sequence) returns a single value constructed by calling the binary function function on the first two items of the sequence, then on the result and the next item, and so on. For example, to compute the sum of the numbers 1 through 10:\n\n>>>\n>>> def add(x,y): return x+y\n...\n>>> reduce(add, range(1, 11))\n55\nIf there’s only one item in the sequence, its value is returned; if the sequence is empty, an exception is raised.\n\nA third argument can be passed to indicate the starting value. In this case the starting value is returned for an empty sequence, and the function is first applied to the starting value and the first sequence item, then to the result and the next item, and so on. For example,\n\n>>>\n>>> def sum(seq):\n...     def add(x,y): return x+y\n...     return reduce(add, seq, 0)\n...\n>>> sum(range(1, 11))\n55\n>>> sum([])\n0\nDon’t use this example’s definition of sum(): since summing numbers is such a common need, a built-in function sum(sequence) is already provided, and works exactly like this.\n\n</pre>\n\n=== Python:: read lines from stdin ===\n\n\n* [http://stackoverflow.com/questions/1450393/how-do-you-read-from-stdin-in-python read lines from stdin in python]\n\n <nowiki>\nimport fileinput\n\nfor line in fileinput.input():\n    pass\n</nowiki>\n\n* My version (2013-01-23 20:30)\n\n <nowiki>\nimport fileinput\n\n_count=0\nfor line in fileinput.input():\n    _count += 1\n    print str(_count) + \" : \" + \"\\\"\" + line.strip() + \"\\\"\"\n</nowiki>\n\n=== IOWA:: TPC-C 250GB 48h block I/O trace ===\n\n==== IOWA rehit pattern analysis ====\n\n:* hit count for each 5MB-addr-range (1250 pages)\n\n::* data tranformation\n <pre>\nme@matrix$\ncat out.range_ptrns-20130129_163501.addr_range_countdist.log | awk \'{ print $2, $4 }\' | sort -n  > d01/addr_range_sorted__countdist\n</pre>\n\n::* gnuplot\n <pre>\ngnuplot>\nset title \"hit count for each 5MB-addr-range (1250 pages)\";\nset ylabel \"hit count for each 5MB-addr-range\";\nset xlabel \"logical block address (512 bytes)\";\nplot \"./d01/addr_range_sorted__countdist\" using 1:2 with points;\n</pre>\n\n\n:* rehit interval for 5MB-addr-range (1250 pages)\n\n::* data tranformation\n <pre>\nme@matrix$\ncat out.range_ptrns-20130129_163501.rehit_interval.log | awk \'{ print $2, $4 }\' | sort -n > d01/rehit_interval_sorted__countdist\n</pre>\n\n::* gnuplot\n <pre>\ngnuplot>\nset title \"rehit interval for 5MB-addr-range (1250 pages)\";\nset ylabel \"number of 5MB-addr-range (1250 pages)\";\nset xlabel \"rehit interval (hit-by-hit)\";\nplot \"./d01/rehit_interval_sorted__countdist\" using 1:2 with points;\n</pre>\n\n\n:* runs countdist for 5MB-addr-range (1250 pages)\n\n::* data transformation\n <pre>\nme@matrix$\ncat out.range_ptrns-20130129_163501.runs_range_countdist.log | awk \'{ print $2, $4 }\' | sort -n > d01/runs_range_sorted__countdist\n</pre>\n\n::* gnuplot\n <pre>\ngnuplot>\n\n</pre>\n\n\n\n==== TPC-C 250GB 40h block I/O trace information (Q phase) ====\n\n\n* iowa on jimi-hendrix\n\n <pre>\nblusjune@jimi-hendrix:[now] $ pwd\n/x/var/iowa/now\n\nblusjune@jimi-hendrix:[now] $ df -h .\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sdb2       1.9T   92G  1.7T   6% /var\n\nblusjune@jimi-hendrix:[now] $ l\ntotal 28\ndrwxrwxr-x 6 blusjune blusjune 4096 Jan 30 14:37 ./\ndrwxrwxr-x 5 blusjune blusjune 4096 Jan 30 14:40 ../\n-rwxr-xr-x 1 blusjune blusjune  550 Jan 30 14:15 .bdx.0100.y.tracelog_analysis.sh*\ndrwxrwxr-x 5 blusjune blusjune 4096 Jan 29 20:36 tracelog_analysis.v02.Q_phase.1_all/\ndrwxrwxr-x 5 blusjune blusjune 4096 Jan 29 20:36 tracelog_analysis.v02.Q_phase.2_mysqld/\ndrwxrwxr-x 5 blusjune blusjune 4096 Jan 29 20:36 tracelog_analysis.v02.Q_phase.3_kjournald/\ndrwxrwxr-x 5 blusjune blusjune 4096 Jan 29 20:36 tracelog_analysis.v02.Q_phase.4_kworker/\n\nblusjune@jimi-hendrix:[now] $ _BDX \nBDX[ /x/var/iowa/now ]# 0100 : tracelog_analysis\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.1_all/preproc ]# 0100 : preproc__extract_base_data\n[_/ 20130130_144525 ]# bsc.iowa.extract_rawdata_from_blkparselog\n#>> extracting raw data from \'.tracelog\' for further processing ...\n#>> generating \'.tracelog.1.preproc.out\' ...\n#>> generating \'.tracelog.A.addr\' ...\n#>> generating \'.tracelog.A.seek\' ...\n#>> generating \'.tracelog.R.addr\' ...\n#>> generating \'.tracelog.R.seek\' ...\n#>> generating \'.tracelog.W.addr\' ...\n#>> generating \'.tracelog.W.seek\' ...\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.1_all/anal/t01 ]# 0100 : analexec__range_ptrns\n#>> START: 20130130_153340\n#>> END: 20130130_154551\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.2_mysqld/preproc ]# 0100 : preproc__extract_base_data\n[_/ 20130130_154616 ]# bsc.iowa.extract_rawdata_from_blkparselog\n#>> extracting raw data from \'.tracelog\' for further processing ...\n#>> generating \'.tracelog.1.preproc.out\' ...\n#>> generating \'.tracelog.A.addr\' ...\n#>> generating \'.tracelog.A.seek\' ...\n#>> generating \'.tracelog.R.addr\' ...\n#>> generating \'.tracelog.R.seek\' ...\n#>> generating \'.tracelog.W.addr\' ...\n#>> generating \'.tracelog.W.seek\' ...\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.2_mysqld/anal/t01 ]# 0100 : analexec__range_ptrns\n#>> START: 20130130_160451\n#>> END: 20130130_161030\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.3_kjournald/preproc ]# 0100 : preproc__extract_base_data\n[_/ 20130130_161038 ]# bsc.iowa.extract_rawdata_from_blkparselog\n#>> extracting raw data from \'.tracelog\' for further processing ...\n#>> generating \'.tracelog.1.preproc.out\' ...\n#>> generating \'.tracelog.A.addr\' ...\n#>> generating \'.tracelog.A.seek\' ...\n#>> generating \'.tracelog.R.addr\' ...\n#>> generating \'.tracelog.R.seek\' ...\n#>> generating \'.tracelog.W.addr\' ...\n#>> generating \'.tracelog.W.seek\' ...\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.3_kjournald/anal/t01 ]# 0100 : analexec__range_ptrns\n#>> START: 20130130_163027\n#>> END: 20130130_163608\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.4_kworker/preproc ]# 0100 : preproc__extract_base_data\n[_/ 20130130_163616 ]# bsc.iowa.extract_rawdata_from_blkparselog\n#>> extracting raw data from \'.tracelog\' for further processing ...\n#>> generating \'.tracelog.1.preproc.out\' ...\n#>> generating \'.tracelog.A.addr\' ...\n#>> generating \'.tracelog.A.seek\' ...\n#>> generating \'.tracelog.R.addr\' ...\n#>> generating \'.tracelog.R.seek\' ...\n#>> generating \'.tracelog.W.addr\' ...\n#>> generating \'.tracelog.W.seek\' ...\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.4_kworker/anal/t01 ]# 0100 : analexec__range_ptrns\n#>> START: 20130130_163742\n#>> END: 20130130_163742\n\nblusjune@jimi-hendrix:[now] $ df -h .\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sdb2       1.9T  142G  1.6T   9% /var\n\nblusjune@jimi-hendrix:[now] $ \n\n</pre>\n\n\n* iowa on radiohead\n\n <pre>\nblusjune@radiohead:[now] $ l\ntotal 28\ndrwxrwxr-x 6 blusjune blusjune 4096 Jan 30 14:37 ./\ndrwxrwxr-x 5 blusjune blusjune 4096 Jan 30 14:41 ../\n-rwxr-xr-x 1 blusjune blusjune  550 Jan 30 14:15 .bdx.0100.y.tracelog_analysis.sh*\ndrwxrwxr-x 5 blusjune blusjune 4096 Jan 29 20:36 tracelog_analysis.v02.Q_phase.1_all/\ndrwxrwxr-x 5 blusjune blusjune 4096 Jan 29 20:36 tracelog_analysis.v02.Q_phase.2_mysqld/\ndrwxrwxr-x 5 blusjune blusjune 4096 Jan 29 20:36 tracelog_analysis.v02.Q_phase.3_kjournald/\ndrwxrwxr-x 5 blusjune blusjune 4096 Jan 29 20:36 tracelog_analysis.v02.Q_phase.4_kworker/\n\nblusjune@radiohead:[now] $ _BDX \nBDX[ /x/var/iowa/now ]# 0100 : tracelog_analysis\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.1_all/preproc ]# 0100 : preproc__extract_base_data\n[_/ 20130130_144441 ]# bsc.iowa.extract_rawdata_from_blkparselog\n#>> extracting raw data from \'.tracelog\' for further processing ...\n#>> generating \'.tracelog.1.preproc.out\' ...\n#>> generating \'.tracelog.A.addr\' ...\n#>> generating \'.tracelog.A.seek\' ...\n#>> generating \'.tracelog.R.addr\' ...\n#>> generating \'.tracelog.R.seek\' ...\n#>> generating \'.tracelog.W.addr\' ...\n#>> generating \'.tracelog.W.seek\' ...\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.1_all/anal/t01 ]# 0100 : analexec__range_ptrns\n#>> START: 20130130_153259\n#>> END: 20130130_155022\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.2_mysqld/preproc ]# 0100 : preproc__extract_base_data\n[_/ 20130130_155101 ]# bsc.iowa.extract_rawdata_from_blkparselog\n#>> extracting raw data from \'.tracelog\' for further processing ...\n#>> generating \'.tracelog.1.preproc.out\' ...\n#>> generating \'.tracelog.A.addr\' ...\n#>> generating \'.tracelog.A.seek\' ...\n#>> generating \'.tracelog.R.addr\' ...\n#>> generating \'.tracelog.R.seek\' ...\n#>> generating \'.tracelog.W.addr\' ...\n#>> generating \'.tracelog.W.seek\' ...\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.2_mysqld/anal/t01 ]# 0100 : analexec__range_ptrns\n#>> START: 20130130_160941\n#>> END: 20130130_161804\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.3_kjournald/preproc ]# 0100 : preproc__extract_base_data\n[_/ 20130130_161821 ]# bsc.iowa.extract_rawdata_from_blkparselog\n#>> extracting raw data from \'.tracelog\' for further processing ...\n#>> generating \'.tracelog.1.preproc.out\' ...\n#>> generating \'.tracelog.A.addr\' ...\n#>> generating \'.tracelog.A.seek\' ...\n#>> generating \'.tracelog.R.addr\' ...\n#>> generating \'.tracelog.R.seek\' ...\n#>> generating \'.tracelog.W.addr\' ...\n#>> generating \'.tracelog.W.seek\' ...\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.3_kjournald/anal/t01 ]# 0100 : analexec__range_ptrns\n#>> START: 20130130_163844\n#>> END: 20130130_164626\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.4_kworker/preproc ]# 0100 : preproc__extract_base_data\n[_/ 20130130_164646 ]# bsc.iowa.extract_rawdata_from_blkparselog\n#>> extracting raw data from \'.tracelog\' for further processing ...\n#>> generating \'.tracelog.1.preproc.out\' ...\n#>> generating \'.tracelog.A.addr\' ...\n#>> generating \'.tracelog.A.seek\' ...\n#>> generating \'.tracelog.R.addr\' ...\n#>> generating \'.tracelog.R.seek\' ...\n#>> generating \'.tracelog.W.addr\' ...\n#>> generating \'.tracelog.W.seek\' ...\nBDX[ /x/var/iowa/now/tracelog_analysis.v02.Q_phase.4_kworker/anal/t01 ]# 0100 : analexec__range_ptrns\n#>> START: 20130130_164809\n#>> END: 20130130_164809\n\nblusjune@radiohead:[now] $ pwd\n/x/var/iowa/now\n\nblusjune@radiohead:[now] $ df -h .\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sdc1       1.8T  187G  1.6T  11% /xd\n\nblusjune@radiohead:[now] $ \n</pre>\n\n\n\n <pre>\nblusjune@radiohead:[phase_Q] $ wc -l *\n  221675054 tracelog_full.iowa.20121126_102808.log.grep_kjournald\n   39231188 tracelog_full.iowa.20121126_102808.log.grep_kworker\n  212434820 tracelog_full.iowa.20121126_102808.log.grep_mysqld\n\nblusjune@radiohead:[phase_Q] $ pwd\n/x/var/tracelog/phase_Q\n\nblusjune@radiohead:[tracelog] $ wc -l tracelog_full.iowa.20121126_102808.log\n718979637 tracelog_full.iowa.20121126_102808.log\n\nblusjune@radiohead:[tracelog] $ pwd\n/x/var/tracelog\n</pre>\n\n\n <pre>\nblusjune@jimi-hendrix:[preproc] $ l\ntotal 22466004\ndrwxrwxr-x 2 blusjune blusjune        4096 Jan 29 21:35 ./\ndrwxrwxr-x 5 blusjune blusjune        4096 Jan 29 20:36 ../\n-rwxrwxr-x 1 blusjune blusjune          53 Jan 29 20:34 .bdx.0100.y.preproc__extract_base_data.sh*\n-rwxr-xr-x 1 blusjune blusjune         393 Jan 29 20:53 .tmp.print_seekdist.py*\nlrwxrwxrwx 1 blusjune blusjune          50 Jan 29 20:34 .tracelog -> ../tracelog/tracelog_full.iowa.20121126_102808.log\n-rw-rw-r-- 1 blusjune blusjune 17814000149 Jan 29 21:20 .tracelog.1.preproc.out\n-rw-rw-r-- 1 blusjune blusjune  2038997542 Jan 29 21:24 .tracelog.A.addr\n-rw-rw-r-- 1 blusjune blusjune   557857532 Jan 29 21:31 .tracelog.A.seek\n-rw-rw-r-- 1 blusjune blusjune    43906222 Jan 29 21:32 .tracelog.R.addr\n-rw-rw-r-- 1 blusjune blusjune    36224259 Jan 29 21:32 .tracelog.R.seek\n-rw-rw-r-- 1 blusjune blusjune  1995091320 Jan 29 21:35 .tracelog.W.addr\n-rw-rw-r-- 1 blusjune blusjune   519060512 Jan 29 21:42 .tracelog.W.seek\n\nblusjune@jimi-hendrix:[preproc] $ wc -l .tracelog.*\n  195735765 .tracelog.1.preproc.out\n  195735765 .tracelog.A.addr\n  195735765 .tracelog.A.seek\n    4113312 .tracelog.R.addr\n    4113312 .tracelog.R.seek\n  191622453 .tracelog.W.addr\n  191622453 .tracelog.W.seek\n  978678825 total\n\nblusjune@jimi-hendrix:[preproc] $ wc -l .tracelog  \n718979637 .tracelog\n</pre>\n\n==== TPC-C 250GB 40h block I/O trace information (D phase) ====\n\n\n <pre>\nblusjune@radiohead:[d00] $ l tracelog_full.iowa.20121126_102808.log.*\n-rw-rw-r-- 1 blusjune blusjune 16927814600 Jan 29 19:09 tracelog_full.iowa.20121126_102808.log.kjournald\n-rw-rw-r-- 1 blusjune blusjune  3122994721 Jan 29 19:14 tracelog_full.iowa.20121126_102808.log.kworker\n-rw-rw-r-- 1 blusjune blusjune 15536776113 Jan 29 19:21 tracelog_full.iowa.20121126_102808.log.mysqld\n\nblusjune@radiohead:[d00] $ wc -l tracelog_full.iowa.20121126_102808.log.*\n  221675054 tracelog_full.iowa.20121126_102808.log.kjournald\n   39231188 tracelog_full.iowa.20121126_102808.log.kworker\n  212434820 tracelog_full.iowa.20121126_102808.log.mysqld\n  473341062 total\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[preproc] $ l .tracelog.[1ARW].*\n-rw-rw-r-- 1 blusjune blusjune 3859645316 Jan 28 16:33 .tracelog.1.preproc.out\n-rw-rw-r-- 1 blusjune blusjune  420029146 Jan 28 16:34 .tracelog.A.addr\n-rw-rw-r-- 1 blusjune blusjune  215120084 Jan 28 16:35 .tracelog.A.seek\n-rw-rw-r-- 1 blusjune blusjune   46307640 Jan 28 16:36 .tracelog.R.addr\n-rw-rw-r-- 1 blusjune blusjune   34675312 Jan 28 16:36 .tracelog.R.seek\n-rw-rw-r-- 1 blusjune blusjune  373721506 Jan 28 16:36 .tracelog.W.addr\n-rw-rw-r-- 1 blusjune blusjune  178129442 Jan 28 16:38 .tracelog.W.seek\n\nblusjune@jimi-hendrix:[preproc] $ wc -l .tracelog.[1ARW].*\n  40534797 .tracelog.1.preproc.out\n  40534797 .tracelog.A.addr\n  40534797 .tracelog.A.seek\n   4338598 .tracelog.R.addr\n   4338598 .tracelog.R.seek\n  36196199 .tracelog.W.addr\n  36196199 .tracelog.W.seek\n 202673985 total\n\nblusjune@jimi-hendrix:[preproc] $ \n</pre>\n\n <pre>\n\nblusjune@jimi-hendrix:[t01] $ wc -l out.range_ptrns-20130129_163501*\n  1340580 out.range_ptrns-20130129_163501\n    96772 out.range_ptrns-20130129_163501.addr_range_countdist.log\n    96772 out.range_ptrns-20130129_163501.addr_range_hit_interval.log\n  1146936 out.range_ptrns-20130129_163501.rehit_interval.log\n      100 out.range_ptrns-20130129_163501.runs_range_countdist.log\n  2681160 total\n\nblusjune@jimi-hendrix:[t01] $ l\ntotal 853828\ndrwxrwxr-x 2 blusjune blusjune      4096 Jan 29 16:38 ./\ndrwxrwxr-x 4 blusjune blusjune      4096 Jan 18 16:55 ../\nlrwxrwxrwx 1 blusjune blusjune        39 Jan 18 13:59 .bdx.0100.y.analexec__range_ptrns.sh -> ../.bdx.0100.y.analexec__range_ptrns.sh*\nlrwxrwxrwx 1 blusjune blusjune        11 Jan 18 16:55 .gnuplot -> ../.gnuplot/\nlrwxrwxrwx 1 blusjune blusjune        19 Jan 18 09:53 infile -> ../.tracelog.A.addr\n-rw-rw-r-- 1 blusjune blusjune 437153404 Jan 29 16:38 out.range_ptrns-20130129_163501\n-rw-rw-r-- 1 blusjune blusjune   3919577 Jan 29 16:38 out.range_ptrns-20130129_163501.addr_range_countdist.log\n-rw-rw-r-- 1 blusjune blusjune 398263280 Jan 29 16:38 out.range_ptrns-20130129_163501.addr_range_hit_interval.log\n-rw-rw-r-- 1 blusjune blusjune  34967179 Jan 29 16:38 out.range_ptrns-20130129_163501.rehit_interval.log\n-rw-rw-r-- 1 blusjune blusjune      3368 Jan 29 16:38 out.range_ptrns-20130129_163501.runs_range_countdist.log\n\nblusjune@jimi-hendrix:[t01] $ pwd\n/x/.fastdisk/iowa/tracelog_analysis/anal/t01\n\n</pre>\n\n\n* number of disk accesses during 48-hour running time\n : 234.6 accesses / second\n : = 40534797 / (48 * 3600)\n\n* wc -l infile (<- .tracelog.A.addr)\n : 40534797 infile\n\n----\n\n== ## bNote-2013-01-22 ==\n\n=== previous study:: spatial locality detection ===\n\n* [http://academic.research.microsoft.com/Publication/272779/run-time-spatial-locality-detection-and-optimization \"Run-time spatial locality detection and optimization\", Teresa L. Johnson, Matthew C. Merten, Wen-Mei W. Hwu, Conference: International Symposium on Microarchitecture - MICRO , pp. 57-64, 1997]\n** [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=645797 PDF]\n\n* [http://academic.research.microsoft.com/Publication/251063/exploiting-spatial-locality-in-data-caches-using-spatial-footprints \"Exploiting Spatial Locality in Data Caches using Spatial Footprints\", Sanjeev Kumar @ princeton, Christopher B. Wilkerson @ intel, Journal: ACM Sigarch Computer Architecture News , vol. 26, no. 3, pp. 357-368, 1998]\n** [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=694794 PDF]\n\n* [http://academic.research.microsoft.com/Publication/1791298/dulo-an-effective-buffer-cache-management-scheme-to-exploit-both-temporal-and-spatial-localities \"DULO: An Effective Buffer Cache Management Scheme to Exploit Both Temporal and Spatial Localities\", Song Jiang, Xiaoning Ding, Feng Chen, Enhua Tan, Xiaodong Zhang, Conference: USENIX Conference on File and Storage Technologies , pp. 8-8, 2005]\n** [http://static.usenix.org/event/fast05/tech/full_papers/jiang/jiang.pdf PDF]\n\n* [https://www.usenix.org/conference/fast-03/arc-self-tuning-low-overhead-replacement-cache \"ARC: A Self-Tuning, Low Overhead Replacement Cache\", Nimrod Megiddo, IBM Almaden Research Center; Dharmendra S. Modha, IBM Almaden Research Center, FAST 2003]\n** [http://static.usenix.org/events/fast03/tech/full_papers/megiddo/megiddo.pdf PDF]\n\n=== IBM GPFS I/O tracing ===\n\n* [http://publibfp.dhe.ibm.com/epubs/pdf/a2322215.pdf IBM GPFS Administration and Programming Reference ((B.GOOD))]\n\n* [http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=%2Fcom.ibm.cluster.gpfs.v3r5.0.7.gpfs500.doc%2Fbl1pdg_mmtrace.htm The GPFS trace facility ((B.GOOD))]\n\n==== Generating GPFS trace reports ====\n\nGPFS Problem Determination Guide [http://publibz.boulder.ibm.com/epubs/pdf/a7604157.pdf]\n\nGA76-0415-07 \n\n----\n\nUse the mmtracectl command to configure trace-related configuration variables and to start and stop the trace facility on any range of nodes in the GPFS™ cluster.\n\nTo configure and use the trace properly:\n\n:* 1. Issue the mmlsconfig dataStructureDump command to verify that a directory for dumps was created when the cluster was configured. The default location for trace and problem determination data is /tmp/mmfs. Use mmtracectl as instructed by service personnel to set trace configuration parameters as required if the default parameters are insufficient. For example, if the problem results in GPFS shutting down, set the traceRecyle variable with --trace-recycle as described in the mmtracectl command in order to ensure that GPFS traces are performed at the time the error occurs.\n\n:* If desired, specify another location for trace and problem determination data by issuing this command:\n mmchconfig dataStructureDump=path for storage of dumps\n\n:* 2. To start the tracing facility on all nodes, issue this command:\n mmtracectl --start\n:* 3. Re-create the problem.\n:* 4. When the event to be captured occurs, stop the trace as soon as possible by issuing this command:\n mmtracectl --stop\n:* 5. The output of the GPFS trace facility is stored in /tmp/mmfs, unless the location was changed using the mmchconfig command in Step 1. Save this output.\n:* 6. If the problem results in a shutdown and restart of the GPFS daemon, set the traceRecycle variable as necessary to start tracing automatically on daemon startup and stop the trace automatically on daemon shutdown.\n\nIf the problem requires more detailed tracing, the IBM® Support Center personnel might ask you to modify the GPFS trace levels. Use the mmtracectl command to establish the required trace classes and levels of tracing. For example:\n mmtracectl --set --trace=def\n\nOnce the trace levels are established, start the tracing by issuing:\n mmtracectl --start\n\nAfter the trace data has been gathered, stop the tracing by issuing:\n mmtracectl --stop\n\nTo clear the trace settings and make sure tracing is turned off, issue:\n mmtracectl --off\n\nOn AIX®, the ?aix-trace-buffer-size option can be used to control the size of the trace buffer in memory.\n\nOn Linux nodes only, use the mmtracectl command to change the following:\n* The trace buffer size in blocking mode\n* The raw data compression level\n* The trace buffer size in overwrite mode\n* When to overwrite the old data\n\nFor example:\n\n:* To set the trace buffer size in blocking mode to 8K, issue:\n mmtracectl --set --tracedev-buffer-size=8K\n\n:* To set the trace raw data compression level to the best ratio, issue:\n mmtracectl --set --tracedev-compression-level=9\n\n:* To set the trace buffer size in overwrite mode to 32K, issue:\n mmtracectl --set --tracedev-overwrite-buffer-size=32K\n\n:* To wait to overwrite the data until the trace data is written to the local disk and the buffer is available again, issue:\n mmtracectl --set --tracedev-write-mode=blocking\n\n----\n\n* [https://www.e-techservices.com/redbooks/GPFSSizing+Tuning.pdf Sizing and Tuning GPFS]\n\n* [http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=%2Fcom.ibm.cluster.gpfs.doc%2Fgpfsbooks.html General Parallel File System (GPFS) documents list]\n\n* [http://www.brown.edu/Departments/CCV/sites/brown.edu.Departments.CCV/files/CCV-Parallel-IO-2010-04-04.pdf Parallel I/O Libraries and Techniques]\n\n* [https://www.sit.auckland.ac.nz/GPFS_Test_Trace_Logs GPFS Test Trace Logs]\n\n=== FAWN related articles ===\n\n* [http://vijay.vasu.org/static/papers/vijay_dissertation.pdf \"Energy-efficient Data-intensive Computing with a Fast Array of Wimpy Nodes\" Vijay R. Vasudevan CMU-CS-11-131 October 2011]\n\n\n<br/>\n\n----\n\n== ## bNote-2013-01-18 ==\n\n=== Terms ===\n\n\n; EMC Symmetrix DMX\n: Direct Matrix Interconnect\n: [http://www.emc.com/collateral/hardware/solution-overview/c1011-symm-dmx-architecture-prod-desc-gd.pdf EMC Symmetrix DMX Architecture Product Description Guide]\n\n\n<br/>\n\n=== how to connect to remote X server in Ubuntu? ===\n\n\n* [http://askubuntu.com/questions/72812/how-to-disable-nolisten askubuntu -- How to disable -nolisten?]\n\n; Question\n <nowiki>\nI just installed ubuntu 11.10. I want to run an x-app from another system.\n\nI updated ./xinit/xserverrc and removed the -nolisten option. I rebooted.\n\nI can see that X is started with the nolisten option.\n\nI don\'t have /etc/gdm subdir.\n</nowiki>\n\n; Answer (GOOD)\n <nowiki>\nYou need to edit /etc/lightdm/lightdm.conf and add xserver-allow-tcp=true to it. Here\'s what mine looks like:\n\n[SeatDefaults]\ngreeter-session=unity-greeter\nuser-session=ubuntu\nxserver-allow-tcp=true\n\n[XDMCPServer]\nenabled=true\nAfter that, run this:\n\nsudo restart lightdm\nIf you have problems restarting, just \'ps ax | grep lightdm\' and kill all the associated processes, then:\n\nsudo start lightdm\n</nowiki>\n\n\n<br/>\n----\n\n== ## bNote-2013-01-11 ==\n\n=== Misc. info (각종 업무 담당자 연락처) ===\n\n;기술원 에어컨 안나올때\n: #9120 기술원 통합 방재 센터 (과장) (지원팀 > 환경안전그룹)\n\n;VDI (SBC) 문제 있을 떄\n: #8272 (VDI HelpDesk)\n\n; 네트워크 안될 때 (방화벽 등)\n: 어디로?\n\n; CLMS 시스템 문의\n: 한지연 선임 / 서초 인사 CI 그룹.\n\n=== HDM (Hadoop Monitoring) ===\n\n <nowiki>\nroot@ub01:[hdmon] # pwd\n/usr/local/home/www/hdmon\nroot@ub01:[hdmon] # cat index.html \n<head>\n	<title>Hadoop Cluster Monitor</title>\n</head>\n<body bgcolor=\"#ffffff\">\n	<table width=\"100%\" height=\"100%\">\n		<tr>\n			<td>\n				<iframe width=\"1200\" height=\"700\" src=\"http://hd-master-01:50070/\">Namenode Monitor @hd-master-01</iframe>\n				<iframe width=\"1200\" height=\"700\" src=\"http://hd-master-01:50030/\">JobTracker Monitor @hd-master-01</iframe>\n			</td>\n		</tr>\n		<tr>\n			<td>\n				<iframe width=\"600\" height=\"600\" src=\"http://hd-slave-0001:50060/\">TaskTracker Monitor @hd-slave-0001</iframe>\n				<iframe width=\"600\" height=\"600\" src=\"http://hd-slave-0002:50060/\">TaskTracker Monitor @hd-slave-0002</iframe>\n				<iframe width=\"600\" height=\"600\" src=\"http://hd-slave-0003:50060/\">TaskTracker Monitor @hd-slave-0003</iframe>\n				<iframe width=\"600\" height=\"600\" src=\"http://hd-slave-0004:50060/\">TaskTracker Monitor @hd-slave-0004</iframe>\n			</td>\n		</tr>\n	</table>\n</body>\n</nowiki>\n\n=== Hadoop Cluster Problem Solved ===\n\n* [http://wiki.apache.org/hadoop/CouldOnlyBeReplicatedTo Could Only Be Replicated To ...]\n <nowiki>\nCould Only Be Replicated To ...\n\nA common message people see is \"could only be replicated to 0 nodes, instead of ...\".\n\nWhat does this mean? It means that the Block Replication mechanism of HDFS could not make any copies of a file it wanted to create. This can be caused by\n\nNo DataNode instances being up and running. Action: look at the servers, see if the processes are running.\nThe DataNode instances cannot talk to the server, through networking or Hadoop configuration problems. Action: look at the logs of one of the DataNodes.\nYour DataNode instances have no hard disk space in their configured data directories. Action: look at the dfs.data.dir list in the node configurations, verify that at least one of the directories exists, and is writeable by the user running the Hadoop processes. Then look at the logs.\nYour DataNode instances have run out of space. Look at the disk capacity via the Namenode web pages. Delete old files. Compress under-used files. Buy more disks for existing servers (if there is room), upgrade the existing servers to bigger drives, or add some more servers.\nThe reserved space for a DN (as set in dfs.datanode.du.reserved is greater than the remaining free space, so the DN thinks it has no free space\nYou may also get this message due to permissions, eg if JT can not create jobtracker.info on startup.\nThis is not a problem in Hadoop, it is a problem in your cluster that you are going to have to fix on your own. Sorry.\n</nowiki>\n\n<br/>\n----\n\n== ## bNote-2013-01-10 ==\n\n\n=== Hadoop Monitoring ===\n\n# Web UI (available by default)\n#* http://localhost:50070/ ? web UI of the NameNode daemon\n#* http://localhost:50030/ ? web UI of the JobTracker daemon\n#* http://localhost:50060/ ? web UI of the TaskTracker daemon\n# Chukwa: Hadoop Subproject for Large-scale Log Collection and Analysis\n#* [http://wiki.apache.org/hadoop/Chukwa Chukwa]\n# Ganglia: a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\n#* [http://ganglia.info/ Ganglia]\n# Nagios: Industry Standard In IT Infrastructure Monitoring\n#* [http://www.nagios.org/ Nagios]\n\n\n\n<br/>\n\n=== What If? ===\n* what\'s the difference between the followings?\n*# hadoop over virtualized environment (Hadoop over VMware, SAIT Supercom Center)\n*#: >> can \'SSD Cache\' be a good solution for I/O bottleneck of this case?\n*# hadoop over baremetal enviornment\n\n<br/>\n\n=== Hadoop Error ===\n* critical problem regarding hadoop daemon start-up [http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201205.mbox/%3C846784449.4714.1337188742328.JavaMail.tomcat@hel.zones.apache.org%3E \"Great Solution!\"]\n*: \'\'\'SOLUTION\'\'\': you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not permitted to contain the underscore (\"_\") character.\n\n <nowiki> hduser@prince:[logs] $ cat hadoop-hduser-datanode-prince.log \n2013-01-10 21:36:50,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting DataNode\nSTARTUP_MSG:   host = prince/10.123.1.7\nSTARTUP_MSG:   args = []\nSTARTUP_MSG:   version = 1.0.4\nSTARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1393290; compiled by \'hortonfo\' on Wed Oct  3 05:13:58 UTC 2012\n************************************************************/\n2013-01-10 21:36:50,675 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2013-01-10 21:36:50,684 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.\n2013-01-10 21:36:50,685 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2013-01-10 21:36:50,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2013-01-10 21:36:50,807 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.\n2013-01-10 21:36:50,810 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!\n2013-01-10 21:36:50,846 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2013-01-10 21:36:50,917 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://hd_master_01/\n	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:222)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:337)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:299)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1582)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1539)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1665)\n	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1682)\n\n2013-01-10 21:36:50,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at prince/10.123.1.7\n************************************************************/\n</nowiki>\n\n <nowiki>\nAaron T. Myers resolved HDFS-3430.\n----------------------------------\n\n       Resolution: Invalid\n    Fix Version/s:     (was: 1.0.1)\n\nHi Hiten, you\'re attempting to use an invalid hostname for you NameNode. Hostnames are not\npermitted to contain the underscore (\"_\") character.\n                \n> Start-all.sh Error \n> -------------------\n>\n>                 Key: HDFS-3430\n>                 URL: https://issues.apache.org/jira/browse/HDFS-3430\n>             Project: Hadoop HDFS\n>          Issue Type: Test\n>          Components: data-node, hdfs client, name-node\n>    Affects Versions: 1.0.1\n>         Environment: Linux\n>            Reporter: Hiten Tathe\n>              Labels: hadoop\n>         Attachments: Screenshot.png\n>\n>   Original Estimate: 5h\n>  Remaining Estimate: 5h\n>\n> Hi,\n> m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error\nplease help me the Error is as follow :- \n> 2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG:\n\n> /************************************************************\n> STARTUP_MSG: Starting NameNode\n> STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62\n> STARTUP_MSG:   args = []\n> STARTUP_MSG:   version = 1.0.2-SNAPSHOT\n> STARTUP_MSG:   build =  -r ; compiled by \'root\' on Wed May 16 12:30:17 IST 2012\n> ************************************************************/\n> 2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties\nfrom hadoop-metrics2.properties\n> 2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean\nfor source MetricsSystem,sub=Stats registered.\n> 2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled\nsnapshot period at 10 second(s).\n> 2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode\nmetrics system started\n> 2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException:\nDoes not contain a valid host:port authority: hdfs://sra_hadoop:9000\n>         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)\n>         at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)\n> 2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:\n\n> /************************************************************\n> SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62\n\n--\nThis message is automatically generated by JIRA.\nIf you think it was sent incorrectly, please contact your JIRA administrators: https://issues.apache.org/jira/secure/ContactAdministrators!default.jspa\nFor more information on JIRA, see: http://www.atlassian.com/software/jira\n\n</nowiki>\n\n<br/>\n\n=== Python Implementation for Bayesian Inference ===\n\n* [http://code.google.com/p/bayesian-inference/ Python Package for Object-oriented Bayesian Inference]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/ PyMC: Bayesian Stochastic Modelling in Python]\n\n<br/>\n\n=== 미팅 일정 ===\n* 1월 11일 (금) 오후 1시, 이호경 책임 (연구개발혁신센터)과 미팅 예정\n** 수퍼컴 기반 Hadoop 클러스터 관련 collaboration issue (이건 아니었던 듯)\n** Online Data Analysis 툴을 어떻게 사용할 수 있을런지에 대한 논의였음\n** VDI 되는 미팅룸 하나 예약해둘것\n<br/>\n\n=== Hadoop Distributed Grep Case ===\n\n\n4.4GB 크기의 데이터에 대한 키워드 검색을 분산으로 처리하기 위해 distributed grep 작업 수행. (실은 한 노드에서 수행하였으며, Hadoop 내부적으로는 chaining map-reduce 태스크로 구성됨. 즉, 70개의 Map -> 1개의 Reduce -> 1개의 Map -> 1개의 Reduce)\n\n <nowiki>\n\nHDFS_BYTES_READ=4,406,074,200 (about 4.4GB)\nFILE_BYTES_READ=1,406\nHDFS_BYTES_WRITTEN=112\nFILE_BYTES_WRITTEN=1,540,263 (about 1.5MB)\n# of Map Tasks: 70\n# of Reduce Tasks: 1\n\nFILE_BYTES_READ=26\nHDFS_BYTES_READ=229\nFILE_BYTES_WRITTEN=42549\nHDFS_BYTES_WRITTEN=17\n# of Map Tasks: 1\n# of Reduce Tasks: 1\n\n</nowiki>\n\n\n처리 과정에서, 분산 FS Layer에서 발생하는 I/O와 Local FS Layer에서 발생하는 I/O 패턴 간의 Gap이 존재함을 확인. 단순히 크기의 차이 뿐만 아니라 I/O Access 패턴에서도 차이가 존재할 것으로 보여짐. (Random/Sequential 혹은 Read-intensive, Write-intensive, Mixed-I/O 패턴 등) 분산 처리에서 발생하는 I/O 병목의 현상 규명의 실마리가 될 수 있을 것으로 보여짐.\n\n<br/>\n----\n\n== ## bNote-2013-01-09 ==\n\n\n=== Misc. ===\n\n* [https://help.ubuntu.com/community/Installation/FromUSBStick \"Install Ubuntu with USB memory stick\"] #BEEE: Yes, It works well.\n<br/>\n\n=== Hadoop Workload Analysis ===\n\n* [http://nuage.cs.washington.edu/pubs/UW-CSE-12-06-01.pdf \"Hadoop’s Adolescence: A Comparative Workload Analysis from Three Research Clusters\", CMU, Washington Univ., 2012]\n* [http://docs.hortonworks.com/CURRENT/index.htm#About_Hortonworks_Data_Platform/Hardware_Recommendations_For_Apache_Hadoop.htm \"Hardware Recommendations for Apache Hadoop\", Hortonworks]\n* [http://research.yahoo.com/files/ycsb.pdf \"Benchmarking Cloud Serving Systems with YCSB\"]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/papers/mapreduce-sigmetrics09-tutorial.pdf \"Google MapReduce - The Programming Model and Practice\", Jerry Zhao, Jelena Pjesivac-Grbovic]\n* [http://developer.yahoo.com/blogs/hadoop/posts/2009/05/hadoop_sorts_a_petabyte_in_162/ \"Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds\", Yahoo! Developer Network]\n\n* [http://www.cs.wayne.edu/~weisong/papers/ren12-taobao.pdf \"Workload Characterization on a Production Hadoop Cluster: A Case Study on Taobao\"]\n\n* [http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=1167145 \"Monte Carlo simulation of photon migration in a cloud computing environment with MapReduce\", J. Biomed. Opt. 16(12), 125003 (November 22, 2011). doi:10.1117/1.3656964]\n<br/>\n\n=== Hadoop Tutorial ===\n* [http://cs.smith.edu/dftwiki/index.php/Hadoop_Tutorial_1.1_--_Generating_Task_Timelines \"Hadoop Tutorial 1.1 - Generating Task Timelines\"]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ \"Running Hadoop On Ubuntu Linux (Single-Node Cluster)\", Michael G. Noll]\n* [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/ \"Running Hadoop On Ubuntu Linux (Multi-Node Cluster)\", Michael G. Noll]\n<br/>\n\n=== Server Workload Patterns ===\n* [http://gerardnico.com/wiki/data_storage/input_output_access_pattern \"Data Storage - I/O Workload (Access Pattern)\"]\n<br/>\n\n=== Supermicro 16-node Server ===\n\n010-4287-2271 / 권오순 과장 / 드림시스넷 / 수원지원\n1/11일 오전에 기술원 방문 예정\n\n<br/>\n\n<br/>\n----\n\n== ## bNote-2013-01-08 ==\n\ncache media로 동작하는 SSD에 떨어지는 I/O의 특성은 어떨까? 다음 추측에 대해서 실험적으로 검증할 수 있을까?\n\n1. write는 chunk 단위로 log-structured (sequential, cyclic) 형태로 발생\n\n2. read는 page단위로 random하게 발생\n\n3. write보다는 read가 intensive하게 발생\n\n[[dm-cache]]에 대해서 이러한 사항을 검증해볼것.\n\n<br/>\n-----\n\n== ## bNote-2013-01-03 ==\n\n=== Hadoop and (distributed) Cache ===\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\nhttp://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf\nMR-scope realtime mapreduce tracing\n\nhttp://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf\n\nhttp://infolab.stanford.edu/~ragho/hive-icde2010.pdf\n\nhttp://hadoop.apache.org/docs/r0.20.2/api/org/apache/hadoop/filecache/DistributedCache.html\n\n\nhttp://www.bicdata.com/bbs/board.php?bo_table=develop_hadoop&wr_id=96\npig 와 hive 차이\n\nhttp://www.mapr.com/\n\nhttp://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/\n\nhttp://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf\n\nhttp://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/\n\nhttp://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html\n\nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04016500\n\nhttp://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf','utf-8'),(8,'== ## bNote-2013-02-28 ==\n\n=== 수원임직원주차장 방문예약 ===\n[https://www.sdigitalcity.com/security/visit/login.do 수원임직원주차장 방문예약]\n\n== ## bNote-2013-02-26 ==\n\n=== iowa.sidewinder ===\n\n* construction: filebench workload generator\n\n== ## bNote-2013-02-21 ==\n\n\n=== Daily.Kickoff ===\n\n* acsptrn outlook program\n:- kag (K-address-group)\n::- can \'K\' be a signature of a IO pattern?\n:::- what kind of implication can be extracted from \'KAG\'?\n::: : what size of chunk can have a meaning?\n::: : unit of tiering IO? unit of proactive caching? unit of IO dump size to the SSD?\n::- hamming distance (proper choice! why? difference value means the # of items to be added in cache) (older elements identified as a different one can be evicted or not, according to the cache space condition)\n:- outlook seek distance stat-dist\n:: - can \'seek distance\' be a metric for identifying hot zone?\n\n* cache simulation program\n:- hit ratio calculation, performance gain calculation\n\n* ML approach planning\n:- what to do for proactive data placement?\n::- what to do for static(?) macro insight?\n::- what to do for dynamic(?) micro prediction? (periodicity?)\n\n== ## bNote-2013-02-20 ==\n\n=== IOWA planning with Yanpei Chen Paper (SOSP 2011) ===\n\n\n\n\n\n\n\n=== Intel releases SSD cache acceleration software for Linux servers ===\n\n인텔이 Linux Server를 위한 CAS (Cache Acceleration Software) 기술을 Release했다는 소식입니다.\n\n이 CAS라는 기술은 작년에 인수한 Nevex사의 CacheWorks 기술에 기반하고 있습니다.\n\n특징으로 볼 수 있는 것은 기존 block cache의 문제점으로 지적되었던 DRAM에도 올라간 data가 SSD cache에도 올라가는 현상을 피하기 위해 초보적인 수준의 intelligence를 도입했다는 점입니다.\n\n(very hot data는 DRAM에 보내되, 덜 hot하지만 still I/O active data는 SSD로 나누어 보낸다는 것이 핵심입니다) \n\n\'\'\'Intel releases SSD cache acceleration software for Linux servers\'\'\'\n\nSSD-based CAS for Linux servers can offer up to 18 times the performance for read-intensive applications, Intel says. \n\n[http://www.infoworld.com/d/open-source-software/intel-releases-ssd-cache-acceleration-software-linux-servers-212673?page=0,0 InfoWorld News FEBRUARY 12, 2013]\n\n\n발췌 내용:\n...\nTo ensure it does not duplicate work already being performed in DRAM, the CAS software takes control of all data access. The hottest data is placed on DRAM, while less hot, but still I/O active data, is placed on SSD.\n\nIntel\'s CAS software provides cooperative integration between the standard server DRAM cache and the Intel SSD cache, creating a multi-level cache that optimizes the use of system memory and automatically determines the best cache level for active data.\n...\n\n== ## bNote-2013-02-19 ==\n\n\n=== Workload generation and tracing (with filebench, blktrace) ===\n\n* target workloads\n:- fileserver.f\n:- mongo.f\n:- netsfs.f\n:- networkfs.f\n:- oltp.f\n:- tpcso.f\n:- varmail.f\n:- videoserver.f\n:- webserver.f\n\n\n* video server workload (running time: 3600 seconds) (20130219_143400)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\n#>> cleaning up \'30.tmp/\' directory\n#>> ready for generating workload specified in \'20.conf/.tconf.sh\'\n#>> please run iotrace (blktrace) in \'34.iotrace/\'\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\n#>> [20130219_143400] waiting for iotrace getting started \n.................................\n----------------------------------------------------\n#>> Ok, now start workload generation\nstarted: 20130219_143433\n----------------------------------------------------\nFilebench Version 1.4.9.1\n 3599: 0.000: Allocated 170MB of shared memory\n 3599: 0.001: Eventgen rate taken from variable\n 3599: 0.001: Video Server Version 3.0 personality successfully loaded\n 3599: 0.001: Creating/pre-allocating files and filesets\n 3599: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n 3599: 0.002: Re-using fileset passivevids.\n 3599: 0.002: Creating fileset passivevids...\n 3599: 2227.782: Preallocated 104 of 194 of fileset passivevids in 2228 seconds\n 3599: 2227.800: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n 3599: 2227.800: Re-using fileset activevids.\n 3599: 2227.800: Creating fileset activevids...\n 3599: 2227.805: Preallocated 32 of 32 of fileset activevids in 1 seconds\n 3599: 2227.805: waiting for fileset pre-allocation to finish\n 3974: 4324.570: Starting 1 vidreaders instances\n 3974: 4324.599: Starting 1 vidwriter instances\n 3976: 4324.780: Starting 1 vidwriter threads\n 3975: 4324.790: Starting 48 vidreaders threads\n 3599: 4325.877: Running...\n 3599: 7926.189: Run took 3600 seconds...\n 3599: 7926.234: Per-Operation Breakdown\nserverlimit          423882ops      118ops/s   0.0mb/s    305.7ms/op    60431us/op-cpu [0ms - 2292ms]\nvidreader            423983ops      118ops/s  29.4mb/s    407.1ms/op    39674us/op-cpu [0ms - 2292ms]\nreplaceinterval      17ops        0ops/s   0.0mb/s  10000.1ms/op        0us/op-cpu [10000ms - 10000ms]\nwrtclose             17ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               17ops        0ops/s  47.2mb/s 134156.2ms/op 17297647us/op-cpu [3919ms - 421911ms]\nwrtopen              18ops        0ops/s   0.0mb/s     26.3ms/op     1111us/op-cpu [0ms - 247ms]\nvidremover           18ops        0ops/s   0.0mb/s  60936.6ms/op   281667us/op-cpu [69ms - 181905ms]\n 3599: 7926.234: IO Summary: 424053 ops, 117.782 ops/s, (118/0 r/w),  76.6mb/s,      0us cpu/op, 415.0ms latency\n 3599: 7926.234: Shutting down processes\n----------------------------------------------------\nfinished: 20130219_164641\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n\n* video server workload (running time: 60 seconds) (20130219_111030)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\n#>> cleaning up \'30.tmp/\' directory\n#>> ready for generating workload specified in \'20.conf/.tconf.sh\'\n#>> please run iotrace (blktrace) in \'34.iotrace/\'\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\n#>> [20130219_111030] waiting for iotrace getting started \n...........\n----------------------------------------------------\n#>> Ok, now start workload generation\nstarted: 20130219_111041\n----------------------------------------------------\nFilebench Version 1.4.9.1\n 2322: 0.000: Allocated 170MB of shared memory\n 2322: 0.001: Eventgen rate taken from variable\n 2322: 0.001: Video Server Version 3.0 personality successfully loaded\n 2322: 0.001: Creating/pre-allocating files and filesets\n 2322: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n 2322: 0.720: Removed any existing fileset passivevids in 1 seconds\n 2322: 0.720: making tree for filset /mnt/hdd_4/filebench/passivevids\n 2322: 0.721: Creating fileset passivevids...\n 2322: 5398.571: Preallocated 104 of 194 of fileset passivevids in 5398 seconds\n 2322: 5398.580: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n 2322: 5441.697: Removed any existing fileset activevids in 44 seconds\n 2322: 5441.698: making tree for filset /mnt/hdd_4/filebench/activevids\n 2322: 5441.698: Creating fileset activevids...\n 2322: 7548.330: Preallocated 32 of 32 of fileset activevids in 2107 seconds\n 2322: 7548.330: waiting for fileset pre-allocation to finish\n 2939: 9496.045: Starting 1 vidreaders instances\n 2939: 9496.077: Starting 1 vidwriter instances\n 2941: 9496.275: Starting 1 vidwriter threads\n 2940: 9496.294: Starting 48 vidreaders threads\n 2322: 9497.515: Running...\n 2322: 9557.521: Run took 60 seconds...\n 2322: 9557.540: Per-Operation Breakdown\nserverlimit          22911ops      382ops/s   0.0mb/s    116.4ms/op   129288us/op-cpu [0ms - 5662ms]\nvidreader            23044ops      384ops/s  95.8mb/s     11.3ms/op     3815us/op-cpu [0ms - 2135ms]\nreplaceinterval      0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtclose             0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtopen              1ops        0ops/s   0.0mb/s      0.1ms/op        0us/op-cpu [0ms - 0ms]\nvidremover           1ops        0ops/s   0.0mb/s   5112.0ms/op   110000us/op-cpu [5111ms - 5111ms]\n 2322: 9557.540: IO Summary: 23046 ops, 384.062 ops/s, (384/0 r/w),  95.8mb/s,      0us cpu/op,  11.6ms latency\n 2322: 9557.540: Shutting down processes\n----------------------------------------------------\nfinished: 20130219_135000\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n\n* video server workload (running time: 3600 seconds) (20130218_161548)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\nrm: cannot remove `wloadsrc.f\': No such file or directory\n#>> [20130218_161548] waiting for iotrace getting started \n........\n----------------------------------------------------\n#>> Ok, now start workload generation\n----------------------------------------------------\n20130218_161556\nFilebench Version 1.4.9.1\n30523: 0.000: Allocated 170MB of shared memory\n30523: 0.001: Eventgen rate taken from variable\n30523: 0.001: Video Server Version 3.0 personality successfully loaded\n30523: 0.001: Creating/pre-allocating files and filesets\n30523: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n30523: 5.754: Removed any existing fileset passivevids in 6 seconds\n30523: 5.754: making tree for filset /mnt/hdd_4/filebench/passivevids\n30523: 5.755: Creating fileset passivevids...\n30523: 5296.248: Preallocated 104 of 194 of fileset passivevids in 5291 seconds\n30523: 5296.261: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n30523: 5296.331: Removed any existing fileset activevids in 1 seconds\n30523: 5296.331: making tree for filset /mnt/hdd_4/filebench/activevids\n30523: 5296.331: Creating fileset activevids...\n30523: 8052.228: Preallocated 32 of 32 of fileset activevids in 2756 seconds\n30523: 8052.228: waiting for fileset pre-allocation to finish\n31345: 10248.763: Starting 1 vidreaders instances\n31345: 10248.804: Starting 1 vidwriter instances\n31346: 10248.868: Starting 48 vidreaders threads\n31347: 10249.084: Starting 1 vidwriter threads\n30523: 10250.088: Running...\n30523: 13850.427: Run took 3600 seconds...\n30523: 13850.506: Per-Operation Breakdown\nserverlimit          1228002ops      341ops/s   0.0mb/s    105.3ms/op    71013us/op-cpu [0ms - 2410ms]\nvidreader            1228102ops      341ops/s  85.3mb/s    140.0ms/op    46691us/op-cpu [0ms - 1919ms]\nreplaceinterval      0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtclose             0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtopen              1ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nvidremover           1ops        0ops/s   0.0mb/s     68.8ms/op    70000us/op-cpu [68ms - 68ms]\n30523: 13850.506: IO Summary: 1228104 ops, 341.104 ops/s, (341/0 r/w),  85.3mb/s,      0us cpu/op, 140.0ms latency\n30523: 13850.506: Shutting down processes\n20130218_200648\n----------------------------------------------------\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n== ## bNote-2013-02-18 ==\n\n=== Misc. ===\n\n* [https://h30613.www3.hp.com/media/files/downloads/Non-FilmedSessions/TB2216_Becoming.pdf Become a Flash expert in minutes! SAMSUNG]\n\n* [http://static.usenix.org/event/lsf08/tech/FS_shepler.pdf Filebench Spencer Shepler]\n\n\n=== blktrace knowledge ===\n\n\n <pre>\n  8,48   4     1657     0.426331441  3443  A   W 2593896448 + 1024 <- (8,49) 2593894400\n  8,48   4     1658     0.426332295  3443  Q   W 2593896448 + 1024 [flush-8:48]\n  8,48   4     1659     0.426334674  3443  G   W 2593896448 + 1024 [flush-8:48]\n  8,48   4     1660     0.426503502  3443  A   W 2593897472 + 1024 <- (8,49) 2593895424\n  8,48   4     1661     0.426503961  3443  Q   W 2593897472 + 1024 [flush-8:48]\n  8,48   4     1662     0.426505840  3443  G   W 2593897472 + 1024 [flush-8:48]\n  8,48   4     1663     0.426666260  3443  A   W 2593898496 + 1024 <- (8,49) 2593896448\n  8,48   4     1664     0.426666707  3443  Q   W 2593898496 + 1024 [flush-8:48]\n  8,48   4     1665     0.426668036  3443  G   W 2593898496 + 1024 [flush-8:48]\n  8,48   4     1666     0.426829266  3443  A   W 2593899520 + 1024 <- (8,49) 2593897472\n  8,48   4     1667     0.426829714  3443  Q   W 2593899520 + 1024 [flush-8:48]\n  8,48   4     1668     0.426831154  3443  G   W 2593899520 + 1024 [flush-8:48]\n</pre>\n\n <pre>\n2593896448 + 1024 <- (8,49) 2593894400\n2593896448 - 2593894400 = 2048\n\n2593897472 + 1024 <- (8,49) 2593895424\n2593897472 - 2593895424 = 2048\n\n2593898496 + 1024 <- (8,49) 2593896448\n2593898496 - 2593896448 = 2048\n\n2593899520 + 1024 <- (8,49) 2593897472\n2593899520 - 2593897472 = 2048\n</pre>\n\n\'\'\'2048\'\'\' is the offset of the partition (/dev/sdd1) from the start point of the block device (/dev/sdd)\n\n <pre>\nblusjune@radiohead:[20130219_143400--videoserver] $ sudo fdisk -l /dev/sdd\n[sudo] password for blusjune: \n\nDisk /dev/sdd: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x000c526d\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdd1            2048  3907026943  1953512448   83  Linux\n</pre>\n\n== ## bNote-2013-02-15 ==\n\n\n\n\n=== Hierarchical Temporal Memory ===\n\n* Numenta Hierarchical Temporal Memory - including HTM Cortical Learning Algorithms Version 0.2.1, September 12, 2011 [https://www.numenta.com/htm-overview/education/HTM_CorticalLearningAlgorithms.pdf Numenta Hierarchical Temporal Memory]\n\n# HTM Overview\n# HTM Cortical Learning Algorithms\n# Spatial Pooling Implementation and Pseudocode\n# Temporal Pooling Implementation and Pseudocode\n# Appendix A: A Comparison between Biological Neurons and HTM Cells\n# Appendix B: A Comparison of Layers in the Neocortex and an HTM Region\n\n==== About Numenta ====\n\n: Numenta, Inc. (www.numenta.com) was formed in 2005 // to develop HTM technology for both commercial and scientific use\n: Use of Numenta\'s software and intellectual property is free for research purposes.\n: Numenta will generate revenue by selling support, licensing software, and licensing intellectual property for commercial deployments.\n: Numenta is based in Redwood City, California. (Privately funded)\n\n\n==== Chapter 1. HTM Overview ====\n\n: Hierarchical Temporal Memory (HTM) is a machine learning technology that aims to capture the structural and algorithmic properties of the neocortex.\n\n: The neocortex is the seat of intelligent thought in the mammalian brain.\n\n: HTM\n\n=== SSD dirty script (iometer) ===\n\n[http://www.iometer.org/doc/downloads.html IOmeter dirty SSD script]\n\n\n=== BDP 근거 ===\n\n* 48시간 TPC-C 벤치마크한 blktrace log: 54.4GB\n* 1차 pre-processing: 52.2 GB\n\n== ## bNote-2013-02-14 ==\n\n\n=== 특허화 ===\n\n==== Patent #3 ====\n\n* (data + io insight) packaging 관련 특허\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n==== Patent #4 ====\n\n* IOWA based Proactive Data Placement 자체에 대한 특허\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 clue를 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보이는 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# clue, as simple as possible\n::::# clue, as specific as possible\n::::# clue, efficiently traversable - corresponding clue case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# clue, easily extensible - clue 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n==== Patent #5 ====\n\n* SSD Retention Time Controlling for Caching/Tiering 관련 특허\n\n\n==== Patent #6 ====\n\n* HML (Hierarchical ML)에서 FPGA, ASIC 등을 이용해서 Acceleration할 수 있는 방법은 없을까?\n\n== ## bNote-2013-02-12 ==\n\n\n\n=== IBdM 팀 회의 (15:40 ~ 17:00, 실험실) ===\n\n* 과제목표 (현재안): Intelligent Big Data Management 문제를 “H-ML을 통해” 남들보다 (EMC보다) 훨씬 잘 풀겠다!\n:- 요구사항\n::- 성공적으로 연구를 진행했을 경우의 최종 “기대치” 가 order-of-magnitude 높아야 함, 해결하는 문제가 수년간의 심도깊은 연구를 요구하여야 함\n::- I/O 성능 혁신 <- Data Placement 문제 : H-ML을 통한 Proactive data placement [70x improvement]\n::- 저장 효율 혁신 <- Data Reduction 문제 : H-ML을 통한 Clustered Deduplication [XXXX improvement] “Big Data를 Small Data로 만드는 기법”\n\n* Big Data Era에는 기존 Data Management 문제가 어떻게 달라지나?\n:- data양 폭증, IT budget증가 제한적 -> data reduction을 통한 IT 비용 절감 -> Data Deduplication 기술\n:- data capacity planning이 더 어려워짐 -> 필요에 따라 on-demand로 용량/성능 증설 요구 -> Scale-out 기술\n:- 비정형 data의 증가 -> NoSQL (?), Stream Processing (?)\n:- 비정형 data를 표현하는 데이터의 요구 증가 -> tagged data(metadata) 관리기술\n:: (data에 tag되어 data의 과거 access이력, access pattern, 복제/분열 history 등 intelligence정보를 저장)\n:- decentralized data(?) -> 자동화된 데이터 관리 중요 -> self-managed architecture 기술\n\n* 우리의 차별화\n:- Tool : hierarchical machine learning\n:- Input Data : 풍부한 tagged data (I/O workload, File I/O log, Application-level log, …)\n:- Motto\n::- Big Data의 근본적 이해를 통한 Big Data 관리기법 지능화\n::- Big Data의 생성, 확장, 분열, 복제, 소멸 등 full-cycle에 대한 근본적 이해\n\n* Action Item\n:- H-ML 기법 적용을 위한 data placement 문제 formulate, 기대치 estimation [융]\n:- H-ML 기법 적용을 위한 deduplication 문제 formulate, 기대치 estimation [신,구]\n:- “big data era에는 기존 data 관리 문제가 어떻게 달라지나?” [all]\n\n=== IOWA based Proactive Data Placement ===\n\n심상무님 지시.\n\n---- \n <pre>\n\nTM-20130212-IOWAPDP\n\n[ IO Workload Analysis 기반의\nProactive Data Placement ]\n\n\n\n2013-02-12\n\n\n\nData-intensive Storage\nIntelligence Group\nIntelligent Computing Lab\n</pre>\n\n----\n==== expected benefit ====\n\n <pre>\n이전에 얘기한 바가 있는데, proactive data placement와\nreactive data placement의 논리적인 비교를 통해\n원리상 기대되는 benefit의 크기 규모를 얘기할 수 있을까요?\n\n> 네, IO 성능 Penalty 최소화 측면과 SSD cache media 효율성 향상 측면에서\n  가늠해볼 수 있습니다. \n  \n  가) IO penalty 최소화\n    : 기존 방식은 미리 cache (혹은 fast-tier)에 proactive하게 올린다는 개념이 없었기 때문에\n      일단 최초 한 번은 HDD로부터 access되는 것이 필요하며,\n      그만큼 성능 penalty를 겪을 수 밖에 없습니다.\n      문제는 이렇게 성능 penalty를 겪는 경우가 얼마나 많이 존재하느냐입니다.\n\n      특히, \n      many-times-of-rehit 되는 small-number-of-data와 ................... (1)\n      small-times-of-rehit 되는 large-number-of-data 의 ................... (2)\n      분포 양상이 중요한데,\n      (1), (2) 각각이 전체 IO 에 얼만큼의 contribution을 해내는지가\n      proactive 방식과 reactive 방식과의 성능 차이에 영향을 미칩니다.\n\n      7200rpm의 HDD의 Read latency는 8500us,\n      SLC 기반의 NAND의 Read latency는 25us 라고 하고,\n      총 1,000 번의 IO access가 있었다고 했을 때,\n      A, B, C 각 workload 경우에 따른 IO 처리 소요 시간을 계산함으로써\n      reactive data placement 대비 proactive data placement의\n      성능 향상 정도를 가늠해보겠습니다.\n\n      (참고로, 아래 성능 비교 계산은 reactive Vs. proactive의 \n       핵심 logic에만 집중한 highly-abstracted 계산으로서,\n       블럭 레이어에서의 IO scheduling, HDD의 read-ahead, SATA/PCIe 버스 구조,\n       스토리지 디바이스 내부 구조 등 세부 시스템 적인 고려는 전혀 되어 있지 않음.\n       또한 cache media로 사용되는 SSD의 용량은 충분히 크다고 가정하였음)\n\n      A) 1개의 addr가 1,000번의 access를 다 받아낸 경우 (active range: 1) // 극단적 (1)의 경우\n         reactive 방식: 1 x 8500us + 999 x 25us = 33,475us\n         proactive 방식 (예측 적중률 100%): 1000 x 25us = 25,000us // IO 시간 1.3배 단축\n         proactive 방식 (예측 적중률 100% 미만): 1 x 8500us + 999 x 25us = 33,475us // IO 시간 단축 없음\n\n      B) 50개 addr가 400번의 access를, 150개 addr가 600번의 access를 받은 경우 (active range: 200)\n         reactive 방식: (50 x 8500us + 350 x 25us) + (150 x 8500us + 450 x 25us) = 1,720,000us\n         proactive 방식 (예측 적중률 100%): 1000 x 25us = 25,000us // IO 시간 68배 단축\n         proactive 방식 (예측 적중률 90%): 900 x 25us + 100 x 8500us = 872,500us // IO 시간 1.97배 단축\n\n      C) 1,000개 addr가 1번씩 다 access된 경우 (active range: 1,000) // 극단적 (2)의 경우\n         reactive 방식: 1000 x 8500us = 8,500,000us\n         proactive 방식 (100% 예측 적중시): 1000 x 25us = 25,000us // IO 시간 340배 단축\n   \n  나) SSD cache 미디어 효율성 향상\n    : 기존에는 미래에 다가올 IO에 대한 예측을 가지고 있지 않기 때문에\n      무조건 hit된 data들을 cache에 올리게 되며, 이후 자주 access될 data들과\n      그렇지 않은 data들이 모두 cache되므로, cache 공간의 낭비가 커집니다.\n      특히, data 규모가 커져서 SSD cache를 쓸 필요가 있는 경우,\n      SSD에 대한 write 및 erase를 많이 유발하게 되는 기존 방식은\n      SSD cache의 성능을 저하 시키는 중요한 요인이 될 수 있습니다.\n      이 측면에 대한 수치적 검증은, SSDsim 등을 이용하거나,\n      실제 SSD에 주어진 workload를 인가하여 IO 성능을 측정함으로써\n      실험적으로 증명해볼 수 있을 것 같습니다.\n</pre>\n\n----\n\n==== what is the heart of proactive data placement? ====\n <pre>\n대개 caching 알고리즘들은 데이터 I/O 패턴만 이용해서 cache에 남겨둘 데이터를 고르잖아요.\nproactive data placement 기술의 핵심은 미래의 데이터 I/O 패턴을 예측하는 것이라고 할 수 있나요?\n\n> 네, 맞습니다.\n  proactive data placement 기술의 핵심은 다가올 data IO 패턴 예측입니다.\n\n  상무님 말씀하신 것처럼,\n  기존의 cache 알고리즘들은 이미 cache된 data들에 대한\n  replacement (즉 eviction) 알고리즘이라고 보시면 됩니다.\n  즉, 일단은 access된 data들을 모두 cache에 올려두었다가,\n  그 cahced data들 중에서 \'미래에 덜 필요할 것\'같은 데이터를 골라내는 것입니다.\n  그러나 cache되어야 할 data를 예측하여 \'proactive 하게 미리\'\n  cache에 가져다놓는 것은 고려되어 있지 않습니다.\n  (이렇게 하지 못한 이유는, CPU L2/L3 cache와 DRAM 간의 cache이다보니\n   복잡한 알고리즘을 돌릴 시간 및 공간이 절대적으로 부족하기 때문이었습니다)\n	\n  기존 cache 알고리즘의 장점은 cache라는 공간에 data를 placement하는 알고리즘은\n  극도로 심플하지만, (별도의 알고리즘 없이 access된 data는 무조건 cache에 두므로)\n  일단 한 번이라도 data가 access되어야만 caching이 될 수 있다는 한계가 있습니다.\n\n  앞에서 이미 잠시 언급하였습니다만,\n  small-number-of-data가 many-times-of-rehit 이 된다면,\n  reactive data placement 방식과 proactive data placement 방식 간에\n  성능 차이가 크지 않을 수 있습니다.\n  그러나, active data range가 매우 넓은 경우에는\n  small-number-of-data가 many-times-of-rehit 되는 경우 뿐만 아니라,\n  large-number-of-data들이 small-times-of-rehit되는 경향 (long-tail)이\n  보여질 수 있기 때문에 proactive data placement에 의한 이득이 커질 수 있습니다.\n  (Active range가 매우 커지는 Big data 상황에서는\n   이렇게 될 가능성이 더 커질 것으로 보입니다)\n\n</pre>\n\n----\n\n==== Patent: IOWA based PDP ====\n <pre>\n미래의 데이터 I/O 패턴을 예측하는데 과거의 데이터 I/O 패턴만 볼 것이 아니라,\n데이터 자체의 특성, 파일 단위의 특성 (예를 들면, 이미지인지, 서버 로그인지, conf 파일인지),\nblock 자체의 특성 (뭐가 있을까요?), creator (user, process)의 종류,\nlast change/update time, 현재 active한 process들의 종류 등,\n다양한 데이터를 이용하는 것이 어떨까요?\n이런 방식이 새로운 아이디어라면 특허 출원을 검토해 주세요.\n\n> 네, 상무님, 다양한 데이터를 이용하는 것이 필요합니다.\n  이에, trace 기반의 access 패턴 뿐만 아니라 호스트 시스템에서 얻을 수 있는\n  정보들을 최대한 활용하고자 하고 있습니다.\n  그리고 이 부분에서 ML 기술의 적극적인 도입이 필요합니다.\n\n  (이 정보들이 IO 패턴 발굴 및 예측에 도움을 줄 수 있는지 유효성을 검증하고자\n   일부 정보들을 활용하여 이미 실험을 시작하고 있습니다.\n   현재는 IO 유발자 정보를 고려하여 실험 중입니다.)\n\n  고려중인 정보/접근 방식들은 다음과 같습니다.\n\n  - IO 액세스 패턴\n    - trace 기반의 address 별 access pattern\n      : periodicity 뿐만 아니라 recency, 누적 access count,\n        read/write intensiveness, access randomness, data usage-cycle 패턴\n        (read-modify-write(update)-read 등) 등\n      : 수집 시 blktrace 등 IO trace 툴 활용\n    - 시스템 meta 정보 기반 access pattern\n      : inode 등에서 얻어낼 수 있는 last create/update/access time 등\n      : 수집 시 inode 등 file, filesystem 정보 활용\n\n  - IO 유발자 정보\n    - user ID, group ID, process ID 등 creator 정보 및 현재 해당 file을 이용하는 \n      process / class 등\n      : 수집 시 debugfs, proc, sysfs 등의 메커니즘 활용\n      : IO 유발자가 어느 category에 속하는지 category 별로 분석\n        (web, DB, auth, monitoring, kernel thread (journald, kworker 등), ...)\n\n<!--\n  - State machine 기반의 macro IO pattern analsysis\n    : 유의미한 parameter 몇 가지로 정의되는 state machine을 정의하고,\n      이 state 간의 transition 확률을 이용하여 다가올 macro IO 패턴을 예측\n-->\n\n  특허 출원에 대해 말씀드리자면, 위의 기법 각각에 대한 특허에 대해서는\n  일부 시스템 등록, 혹은 현재 직무발명서 작성 중이었습니다만,\n  말씀 주신 바처럼 현재 저희가 진행하고 있는\n  IOWA 기반의 proactive data placement 자체에 대한\n  특허 가능성도 검토하도록 하겠습니다.\n\n  참고로, 2/12일 (오늘) 현재, 삼성 특허 검색 사이트에서, 검색식\n  ((proactive data placement) OR (data preplacement))\n  (cach* OR tiering)\n  ((IO OR I/O OR workload) analy*)\n  으로 찾아본 결과, 검색된 관련 특허는 없었습니다만,\n  검색 범위를 더 넓혀서 추가로 더 찾아보겠습니다.\n\n</pre>\n\n----\n\n==== Realistic sample trace log collection ====\n<pre>\n이렇게 다양한 데이터를 이용해서 데이터 I/O 패턴을 예측하려면 많은 sample log들을 모아서\n예측 함수를 만들어야 합니다. 분산 스토리지가 특정 용도에 이용되고 있다면\n(예로, 스트리밍 서버, 웹 서버, VDI 서버 등), 그것별로 sample log들을 많이 모아야 할 것 같습니다.\n우리가 어떻게 이런 다양한 로그들을 모을 수 있을까요?\n\n> 네, 전적으로 동감합니다. 수집 현황/계획은 아래와 같습니다.\n\n  1. Web Server IO Log\n    : 메모리사에서도 SAVL 타겟으로 우선적으로 관심을 가졌던 workload로서\n      SPECweb benchmark을 통해 realistic workload를 얻을 수 있습니다.\n      (메모리 사 응용기술팀으로부터 시스템 구축에 필요한 파일 받아옴)\n\n  2. DB IO Log\n    : TPC-C benchmark을 통해 realistic workload를 얻을 수 있습니다.\n      (이미 환경 구축 완료 및 사용 중)\n\n  3. Supercomputing IO Log\n    : 현재 기술원 3층의 수퍼컴퓨터 관리팀과 접촉 중\n      (Matlab, R, MapReduce-like RSP framework에 대한 workload 존재)\n\n  4. Streaming IO Log\n    : 아직 확보 방안 마련되지 않음\n		\n  5. VDI IO Log\n    : 기술원 정보전략팀과 의논 가능\n      (\'12년에 1차로 trace log 확보를 시도해본 적 있으나\n       정보전략팀과 SDS와의 관계가 어려워져 실패)\n\n</pre>\n\n----\n\n==== problem formulation considering on-line learning ====\n<pre>\n\n[ 지시 1 ]\n\n앞 경우는 sample log들을 모아서 예측 함수를 만드는 것이고,\n실제 deploy되었을 때는 dynamic 하게 예측 함수가 adaption할 수 있어야 하는데,\n그런 adaptation을 on-line learning으로 볼 수 있습니다.\n최희열 전문과 협력해서 위 문제들을 formulation 하기 바랍니다.\n\n> 네, 위의 내용들을 정리하여 최희열 전문과 함께 위의 문제들을 명확히 정리해보겠습니다.\n\n</pre>\n\n----\n\n== ## bNote-2013-02-08 ==\n\n=== Git Server on My Local Machine ===\n\n==== 4.4 Git on the Server - Setting Up the Server ====\n\n* [http://git-scm.com/book/en/Git-on-the-Server-Setting-Up-the-Server Setting up the server]\n\nLet\'s walk through setting up SSH access on the server side. In this example, you\'ll use the authorized_keys method for authenticating your users. We also assume you\'re running a standard Linux distribution like Ubuntu. First, you create a \'git\' user and a .ssh directory for that user.\n\n <pre>\n$ sudo adduser git\n$ su git\n$ cd\n$ mkdir .ssh\n</pre>\nNext, you need to add some developer SSH public keys to the authorized_keys file for that user. Let\'s assume you\'ve received a few keys by e-mail and saved them to temporary files. Again, the public keys look something like this:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCB007n/ww+ouN4gSLKssMxXnBOvf9LGt4L\nojG6rs6hPB09j9R/T17/x4lhJA0F3FR1rP6kYBRsWj2aThGw6HXLm9/5zytK6Ztg3RPKK+4k\nYjh6541NYsnEAZuXz0jTTyAUfrtU3Z5E003C4oxOj6H0rfIF1kKI9MAQLMdpGW1GYEIgS9Ez\nSdfd8AcCIicTDWbqLAcU4UpkaX8KyGlLwsNuuGztobF8m72ALC/nLF6JLtPofwFBlgc+myiv\nO7TCUSBdLQlgMVOFq1I2uPWQOkOWQAHukEOmfjy2jctxSDBQ220ymjaNsHT4kgtZg2AYYgPq\ndAv8JggJICUvax2T9va5 gsg-keypair\n</pre>\n\nYou just append them to your authorized_keys file:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.josie.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.jessica.pub >> ~/.ssh/authorized_keys\n</pre>\n\nNow, you can set up an empty repository for them by running git init with the --bare option, which initializes the repository without a working directory:\n\n <pre>\n$ cd /opt/git\n$ mkdir project.git\n$ cd project.git\n$ git --bare init\n</pre>\n\nThen, John, Josie, or Jessica can push the first version of their project into that repository by adding it as a remote and pushing up a branch. Note that someone must shell onto the machine and create a bare repository every time you want to add a project. Let\'s use gitserver as the hostname of the server on which you\'ve set up your \'git\' user and repository. If you\'re running it internally, and you set up DNS for gitserver to point to that server, then you can use the commands pretty much as is:\n\n <pre>\n# on Johns computer\n$ cd myproject\n$ git init\n$ git add .\n$ git commit -m \'initial commit\'\n$ git remote add origin git@gitserver:/opt/git/project.git\n$ git push origin master\n</pre>\nAt this point, the others can clone it down and push changes back up just as easily:\n\n <pre>\n$ git clone git@gitserver:/opt/git/project.git\n$ cd project\n$ vim README\n$ git commit -am \'fix for the README file\'\n$ git push origin master\n</pre>\nWith this method, you can quickly get a read/write Git server up and running for a handful of developers.\n\n\n==== Install GitWeb ====\n\n* [http://blog.countableset.ch/2012/04/29/ubuntu-12-dot-04-installing-gitolite-and-gitweb/ Installing gitolite and gitweb on Ubuntu 12.04]\n\nNote this is only tested on Ubuntu 12.04 Server with apache2, I\'m sure it would work on the desktop version also.\n\n===== Installing Git =====\n <pre>\nsudo apt-get install git-core\n</pre>\nOptional, setup git global settings:\n\n <pre>\ngit config --global user.name \"Your Name\"\ngit config --global user.email your@email.com\n</pre>\n\n===== SSH Key =====\nGenerate ssh public/private key on the machine you would like to access git repo from and copy it to the server into the /tmp/ directory, for reference here is the command:\n\n <pre>\nssh-keygen -t rsa -C \"name@computer\"\n</pre>\n\n===== Installing Gitolite =====\n\n <pre>\nsudo apt-get install gitolite\n</pre>\n\nCreate a user to access gitolite with, in this case I chose git since I don\'t like to type:\n\n <pre>\nsudo adduser \\\n    --system \\\n    --shell /bin/bash \\\n    --gecos \'git version control\' \\\n    --group \\\n    --disabled-password \\\n    --home /home/git \\\n    git\n</pre>\n\nNow login to the newly created user, and set the path, and move to its home directory:\n\n <pre>\nsudo su git\necho \"PATH=$HOME/bin:$PATH\" > ~/.bashrc\ncd\n</pre>\n\nRun the gitolite setup command with the public key you copied to the tmp directory to initialized the location for gitolite use. Then change the $REPO_UMASK to 0027 when it opens the .gitolite.rc for editing during the installation process. If it didn\'t open the file for any reason just open it up in vim (Note this is only if you\'d like to use gitweb):\n\n <pre>\ngl-setup /tmp/rachel.pub\n# change $REPO_UMASK = 0077; to $REPO_UMASK = 0027; # gets you \'rwxr-x---\'\n</pre>\n\nAfterward, it has made the gitolite-admin.git, testing.git repo and all other necessary files. Check to see that everything works by cloning the repo on the machine with the public/private key.\n\n <pre>\ngit clone git@<server>:gitolite-admin.git\n</pre>\n\nHere is a resource about the syntax for the config file and adding users.\n\n===== Install Gitweb =====\nThis is the tricky bit... Install gitweb and the highlight app. Gitweb is located at \'/usr/share/gitweb\'\n\n <pre>\nsudo apt-get install highlight gitweb\n</pre>\n\nEdit the gitweb config to the locations of the project list and repos, and add the highlighting bit at the end of the file:\n\n <pre>\nsudo vim /etc/gitweb.conf\n# change $projectroot to /home/git/repositories\n# change $projects_list to /home/git/projects.list\n# Add Highlighting at the end\n$feature{\'highlight\'}{\'default\'} = [1];\n</pre>\n\nChange gitolite instance to allow access for gitweb. First append www-data to git group so gitweb can access the repos, then change the permissions for git repos and the projects list, finally restart apache:\n\n <pre>\nsudo usermod -a -G git www-data\nsudo chmod g+r /home/git/projects.list\nsudo chmod -R g+rx /home/git/repositories\nsudo service apache2 restart\n</pre>\n\nFinally you need to tell gitolite which repo you want to show up in gitweb. To do this edit the gitolite.conf file from the gitolite-admin.git repo:\n\n <pre>\nrepo testing\n  RW+ = @all\n  R = gitweb\n</pre>\n\n\n\n\n== ## bNote-2013-02-07 ==\n\n=== World Largest Data Centers ===\n\n* [http://wikibon.org/blog/inside-ten-of-the-worlds-largest-data-centers/ Inside Ten of the World’s Largest Data Centers, on March 25, 2010]\n\n\n=== Enterprise Storage Related Articles ===\n\n----\n==== The future of enterprise storage? ====\n: Henry Baltazar / Senior Analyst / Storage & Systems / The 451 Group\n: [http://www.snia.org/sites/default/files2/sdc_archives/2010_presentations/general_session/HenryBaltazar_Cloud_Convergence_Consolidation.pdf Clouds, Convergence, and Consolidation - SNIA SDC 2010]\n\n----\n==== Tiering vs. Caching ====\n; Automated Storage Tiering\n: Has become a popular technology in the past two years with multiple vendors embedding this feature within their storage systems.\n: Allows storage systems to migrate \'hot\' data to high speed flash-based storage tiers, while simultaneously moving stale data to inexpensive SATA hard drives.\n; Storage Caching\n: Primarily being driven by NetApp (i.e. Flash Cache) and a few startups, storage systems with caching use solid state storage as a memory extension technology.\n: Caching proponents claim that the migration required to do tiering is inefficient and reacts too slowly to eliminate hotspots.\n----\n\n=== Comparison of NAS vs. SAN // Pain Points of Storage System ===\n\n어제 회의의 Action Items 중 하나로, 제가 맡았던 부분인\n\"현재 Arch.가 야기하는 기술적 Poin Points 추가 정리\" 관련 메일입니다.\n\n내용은 다음과 같이 크게 두 가지로 구분됩니다.\n\n1. NAS vs. SAN 비교\n\n2. 기존 Storage System의 Pain Points (HP StorageWorks 제품 Feature를 통해 바라본)\n  -> 여기에는 이전문님께서 말씀하시곤 했던 ILM(Info. Lifecycle Mgmt.)에 \n     대한 언급도 나오네요. (더 자세한 내용은 첨부파일에 있습니다)\n\n// 아래 내용들은  HP에서 만든 \"Storage Overview and Architecture (HP, Arvind Shrivastava)\" 자료를 참고하였습니다. (첨부 1)  다소 NAS에 불리한 점들이 많이 부각되어 보입니다만, 다르게 보면, \"기존 NAS의 구조적 한계점이 이러했으니 Performance Drop 없이도 Scale-out 되는 NAS로 가기 위해서는 이러한 단점들이 극복되어야하겠다\" 라는 내용으로 받아들일 수도 있겠습니다.\n\n\n==== Comparisons of SAN, NAS (and DAS) ====\n\n{| border=1\n|+ \'\'\'DAS vs. NAS vs. SAN\'\'\'\n| Direct Attached Storage (DAS)\n| Network Attached Storage (NAS)\n| Storage Area Network (SAN)\n|-\n|\n*+ Low cost solution\n*+ Simple to configure\n*- De-centralized storage management\n*- No storage consolidation\n*- No high availability\n*- Low performance\n|\n*+ Heterogeneous environment\n*+ Centralized storage management\n*- Low performance\n*- Limited scalability\n*- Network congestion during backups & restore\n*- Ethernet limitations\n|\n*+ Heterogeneous environment\n*+ Centralized storage management\n*+ High degree of fault tolerance\n*+ Best and superior performance\n*+ Storage consolidation\n*+ Fast and efficient backups and restores\n*+ Dynamic scalability\n*- Expensive solution for small setups\n|-\n|}\n\n\n\n{| border=1\n|+ \'\'\'NAS vs. SAN\'\'\'\n| Items\n| style=\"width:40%\" | Network Attached Storage (NAS)\n| style=\"width:40%\" | Storage Area Network (SAN)\n|-\n| Network\n| IP\n| Fibre Channel\n|-\n| Reliability\n| Unreliable\n: >> FCoE, iSCSI 도 있는데 정말 그렇게 Unreliable인걸까요? FC에 비해 상대적으로 그렇다는 의미로 받아들이면 될까요?\n| Reliable\n|-\n| CPU overhead\n| Extremely High\n: >> \"Extremely High\" 까지일 줄은 몰랐네요. Block IO에 비해 File System Operation까지 해야 하는 NAS의 경우 CPU overhead가 더 있을 수 있겠다는 생각은 했었지만, 생각보다 overhead가 꽤 있는지도 모르겠습니다. (NetApp NAS, EMC Isilon 확인 필요).\n| Extremely Low\n: >> 반대로, SAN이 상대적으로 low-overhead라고 해서 \"Extremely\" 까지일 줄은...\n|-\n| Blocks\n| Large number of small blocks\n| Large blocks of data\n|-\n| Backup\n| LAN backup\n| LAN-free backup\n|-\n| Access and control by application\n| Applications driven by universal access to files\n| Applications managing own data specs\n|-\n| Typical usage\n|\n* Office applications file serving\n* File sharing between office users and CAD users\n* Content management for web servers\n* Consolidation of \'home directories\' for controlled and centralized backup\n* Boot device for diskless server farms\n* Rich media such as audio or video streaming\n|\n* Physical storage consolidation of any application mix\n* Physical storage sharing for cluster configurations\n* Sharing of backup devices (libraries)\n* Long distance data mirroring and/or data replication on storage device level\n* Redundant, high speed disk access for large database servers\n|}\n\n<br/>\n----\n\n==== HP Storage Product - StorageWorks ====\n\n\'\'\'Pain Points\'\'\'\n* HP의 storage 제품인 StorageWorks에서 언급된 pain points를 요약해보면 다음과 같습니다.\n\n# 네트워크 트래픽 증가로 인한 스토리지 시스템 성능 저하 (여기서 WAN을 언급하는 것으로 보아 물리적으로 멀리 떨어진 스토리지 간의 트래픽을 언급하는 것으로 보입니다)\n# 저장된 data의 대부분을 차지하는 (operational data에 비해 3~4배 이상 크기) reference data의 지속적 증가 (여기서 reference data라는 것은, 당장 operation되고 있지는 않지만, 이전에 생성되어 사용된 적은 있으나 폐기되지 않고 저장되어 있는, 그렇게 계속 쌓여가는 data들을 의미하는 것으로 보여집니다. 한마디로, operational data가 hot-data라면, reference data는 cold-data라고 볼 수 있을 것 같습니다)\n# 관리되고 저장된 data의 증가에 따라 속도 저하 및 TCO 증가\n# hetero한 사양 및 상태로 여러 곳에 흩어져있는 storage system의 관리\n\n\n\n\'\'\'HP StorageWorks - Reference Information Storage System\'\'\'\n:* 일종의 ILM (Information Lifecycle Management)\n:* Cost 절감 위해 application server/storage로부터 잘 사용되지 않는 static data 및 중복 data 감소.\n:* data의 integrity 보장.\n:* Power indexing 및 Grid computing을 통해 훨씬 빠른 search/retrieval을 달성하겠다.\n\n[[File:20130207 HP Storage Product Slides Scale is the problem .png|500px]]\n\n\n* Changing digital storage demands \n:* Majority is reference information (operational data 보다는 reference 정보가 저장된 data의 대부분 차지)\n\n[[File:20130207 HP Storage Product Slides - Changing digital storage demands .png|500px]] \n\n\n* WAN acceleration\n:* WAN을 통해 연결되는 스토리지 트래픽 감소 기술\n(100배 이상 빠른 throughput 달성 목표)\n{| border=1\n| Problems\n| style=\"width:40%\" | HP\'s solution\n| style=\"width:40%\" | What it does\n|-\n| Not enough bandwidth\n|\n* Scalable data referencing (SDR)\n|\n* Remove repeated bytes from the WAN for all TCP traffic\n|-\n| TCP chattiness and inefficiency\n|\n* Virtual Window Expansion\n* High speed TCP optimizations\n|\n* Optimizes performance of TCP\n* Enables much higher throughput on \"LFNs\"\n|-\n| Application chattiness and inefficiency\n| Transaction prediction (CIFS, MAPI)\n| Optimizes the performance of specific applications\n|}\n\n== ## bNote-2013-02-06 ==\n\n\n=== Defining: Target Segment, Product, Architecture ===\n금일 회의때 논의한 내용을 정리합니다.\n\n\n\'\'\'Consensus\'\'\'\n\n* Big data 시장만 보면 현재로서는 시장이 크지 않다. \n\n* HW Box를 같이 팔아야 한다. 그렇지 않으면 돈이 안된다.\n\n* Google, NHN등 big data center player에게 팔것은 아니다.\n\n* 삼성이 Enterprise Storage 사업 진입한다고 가정 (사업 진입유무를 우리가 고민하지 말자)\n\n* Large-scale system에서 data/node scale이 증가할수록 manageability issue가 커진다.\n\n* storage 신사업 초기에는 기존 infra에 대한 당사 제품의 compatibility가 매우 중요할 것이다. 이 부분은 신사업추진단이 고민할 것이고 우리는 2nd/3rd generation에 해당하는 innovative architecture에 집중한다.\n\n* new architecture는 고객의 현재/미래의 pain point에 기반해야 한다.\n\n \n\'\'\'Pain Point (고객입장)\'\'\'\n\n* \"Google같은 flexible/efficient data center 를 우리 회사 scale에 맞게 내부에서 소유하고 관리하고 싶습니다.\" (Google-in-a-box)\n\n* \"여러 인프라에 흩어진 기업데이터를 통합 관리하고 싶습니다.\" (for bigdata analytics?)\n\n* \"어떠한 경우에도 data loss가 없고 available한 불멸의 스토리지를 소유하고 싶습니다.\"\n\n* \"우리가 소유한 모든 infra를 통합관리하는 극강의 manageability가 필요합니다. 충분히 자동화되고 self-healing되어 관리자 1명당 관리노드의 갯수를 혁신적으로 올릴 수 있으면 좋겠습니다.\" (ML, Brain-inspired computing 적용가능성 있음)\n\n* replication 기술 (InformationWeek 자료에서 니즈 언급)\n\n\n\'\'\'Big Question\'\'\'\n\n* SAN ? NAS ? SAN+NAS ?\n\n* Server+Storage ?\n\n* up to which scale does the distribute architecture need to be expanded?\n\n* 고객이 별도의 big data platform (HW+SW) 을 구매할까? (in addition to already owned infra)\n\n* SW-defined storage 시대가 올까?\n\n* 왜 Google/NHN은 직접 data center를 buildup 했을까? 왜 EMC를 안쓸까?\n\n* InformationWeek 자료의 기술선호도가 2018년에는 어떻게 바뀔까?\n\n* Replication 기술이 중요한만큼 매우 어려울까?\n\n* 2018년에는 Cloud Storage 의존성이 어떻게 될까?\n\n* Data center model이 어떻게 다를까? : Google vs. Amazon (Fusion-IO구매) vs. Apple (Isilon구매)\n\n* service fee 로 먹고사는 biz model이 나올 수 있나?\n\n* Hadoop + Dedup? (data가 있는곳에 processing을! 이라는 hadoop philosophy가 dedup에 의해 어떤 영향을 받는가?)\n\n* block I/F, file I/F 이후 new interface가 어떻게 확산될까? 왜 object I/F는 확산되지 못했을까? \n\n* brain-like한 I/F는 어떤 형태가 될까?\n\n\n\'\'\'Action Items\'\'\'\n\n* 이: Google의 data center evolution history\n\n* 서: SAN/NAS, Converged/Storage, 분산 scale(100s? 1000? 10000?)\n\n* 신+구: 분산-scale storage에 분산-scale dedup이 적용되면 architecture가 어떻게 바뀌나? dedup manager의 위치가 어디가 되어야 하나?\n\n* 융: 현 architecture가 야기하는 기술적인 Pain Point 추가정리\n\n\n\'\'\'Check this out\'\'\'\n* 451 report provided by Seo-C: Automatic Tiering is the very hot technology\n\n== ## bNote-2013-02-05 ==\n\n\n=== IOWA:: machine learning ===\n\nk-addr-group\n\ndistance calculation and clustering\n\nhamming distance\n\n\n[[ bsc.iowa.lsp.addr.access_ptrn ]]\n\n\n[http://en.wikipedia.org/wiki/Levenshtein_distance Levenshtein distance]\n\n\n <pre>\n__k_addr_group__ 0 : [720616224, 3282792808, 3282793000, 3282793096]\n__k_addr_group__ 1 : [728218248, 730235992, 817521776, 3765476016]\n__k_addr_group__ 2 : [721421456, 724475672, 724475864, 3712136296]\n__k_addr_group__ 3 : [724475960, 828267560, 895981360, 3277437312]\n__k_addr_group__ 4 : [725469848, 726381104, 726381496, 726793728]\n__k_addr_group__ 5 : [723112360, 730730224, 3677713128, 3773683448]\n__k_addr_group__ 6 : [725235304, 828274704, 896778072, 3277440104]\n__k_addr_group__ 7 : [720987560, 724481496, 724727104, 725008864]\n__k_addr_group__ 8 : [545503608, 896780128, 3069345832, 3076725824]\n__k_addr_group__ 9 : [721386480, 721386960, 3752324048, 3767545408]\n__k_addr_group__ 10 : [725420616, 726671672, 727735192, 3066446120]\n__k_addr_group__ 11 : [721030800, 721030896, 721411176, 827916488]\n...\n__k_addr_group__ 245 : [725218552, 822170744, 870855200, 3642881480]\n__k_addr_group__ 246 : [457025608, 722055120, 3685634288, 3779707504]\n__k_addr_group__ 247 : [722055216, 722055312, 722055376, 725157784]\n__k_addr_group__ 248 : [721285200, 721285296, 728117992, 728118184]\n__k_addr_group__ 249 : [725218680, 725218808, 725218936, 725219032]\n</pre>\n\n\n\n=== Python:: numerical sort for string list ===\n\n <pre>\n\n>>> typeof(mlist)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name \'typeof\' is not defined\n>>> help(mlist)\n\n>>> mlist = {}\n>>> mlist[0] = []\n>>> mlist[1] = []\n>>> mlist[0] = [\"11243\", \"92\", \"2132\", \"42\"]\n>>> mlist[1] = [\"243\", \"892\", \"19132\", \"3242\"]\n>>> mlist[0] = [ int(x) for x in mlist[0] ]\n>>> mlist[0]\n[11243, 92, 2132, 42]\n>>> mlist[1]\n[\'243\', \'892\', \'19132\', \'3242\']\n>>> mlist[1] = [ int(x) for x in mlist[1] ]\n>>> mlist[1]\n[243, 892, 19132, 3242]\n>>> mlist   \n{0: [11243, 92, 2132, 42], 1: [243, 892, 19132, 3242]}\n>>> mlist[0].sort()\n>>> mlist\n{0: [42, 92, 2132, 11243], 1: [243, 892, 19132, 3242]}\n>>> mlist[1].sort()\n>>> mlist\n{0: [42, 92, 2132, 11243], 1: [243, 892, 3242, 19132]}\n>>> \n\n</pre>\n\n== ## bNote-2013-02-04 ==\n\n\n\n=== CWD ===\n\n <pre>\nb@ub04:[anal] $ pwd\n/x/iowa/now/tracelog_analysis.v02.Q_phase.2_mysqld/anal\nb@ub04:[anal] $ l\ntotal 36\ndrwxrwxr-x 8 b b 4096 Feb  4 19:23 ./\ndrwxrwxr-x 5 b b 4096 Feb  4 19:23 ../\n-rw-rw-r-- 1 b b  127 Jan 30 19:42 .bdx.0100.y.exec_iowa_for_all_cases.sh\ndrwxrwxr-x 2 b b 4096 Jan 30 19:38 .tools/\nlrwxrwxrwx 1 b b   27 Jan 30 15:55 .tracelog.A.addr -> ../preproc/.tracelog.A.addr\nlrwxrwxrwx 1 b b   27 Jan 30 18:48 .tracelog.R.addr -> ../preproc/.tracelog.R.addr\nlrwxrwxrwx 1 b b   27 Jan 30 18:48 .tracelog.W.addr -> ../preproc/.tracelog.W.addr\ndrwxrwxr-x 2 b b 4096 Jan 30 19:39 t00.R.xLBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:43 t01.R.1LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t02.R.100LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t03.R.10000LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t04.R.20000LBA/\n</pre>\n\n=== IOWA.data.tpcc_250gb_48h.mysql.R.addr:: Offset 100K, Len 1K ===\n\n* [[IOWA.data.tpcc_250gb_48h.mysql.R.addr:: Offset 100K, Len 1K]]\n\n=== Frequent Item Detection/Discovery ===\n\n==== Stanford Open Book - Mining of Massive Datasets ((B.GOOD)) ====\n\n* MMD Book [http://i.stanford.edu/~ullman/mmds.html]\n\n: The book has now been published by Cambridge University Press. The publisher is offering a 20% discount to anyone who buys the hardcopy Here. By agreement with the publisher, you can still download it free from this page. Cambridge Press does, however, retain copyright on the work, and we expect that you will obtain their permission and acknowledge our authorship if you republish parts or all of it. We are sorry to have to mention this point, but we have evidence that other items we have published on the Web have been appropriated and republished under other names. It is easy to detect such misuse, by the way, as you will learn in Chapter 3.\n: Anand Rajaraman (@anand_raj) and Jeff Ullman\n\n: Ch 3. Finding Similar Items [http://i.stanford.edu/~ullman/mmds/ch3.pdf]\n:: 3.1 Applications of Near-Neighbor Search\n::: 3.1.1 Jaccard Similarity of Sets\n:: 3.2 Shingling of Documents\n:: 3.3 Similarity-preserving Summaries of Sets\n:: 3.4 Locality-sensitive Hashing for Documents\n:: 3.5 Distance Measures\n:: 3.6 The Theory of Locality-sensitive Functions\n:: 3.7 LSH Families for Other Distance Measures\n:: 3.8 Applications of Locality-sensitive Hashing\n:: 3.9 Methods for High Degrees of Similarity\n\n: Ch 6. Frequent Itemsets [http://i.stanford.edu/~ullman/mmds/ch6.pdf]\n:: 6.1 The Market-Basket Model\n:: 6.2 Market Baskets and the A-Priori Algorithm\n:: 6.3 Handling Larger Datasets in Main Memory\n:: 6.4 Limited-pass Algorithms\n:: 6.5 Counting Frequent Items in a Stream\n\n==== Orange - open source data visualization and analysis tool ====\n\nOpen source data visualization and analysis for novice and experts. Data mining through visual programming or Python scripting. Components for machine learning. Add-ons for bioinformatics and text mining. Packed with features for data analytics.\n\n\n==== SAX - Symbolic Aggregate Approximation ====\n\nSAX (Symbolic Aggregate approXimation) Homepage [http://www.cs.ucr.edu/~eamonn/SAX.htm]\n\nSAX is the first symbolic representation for time series that allows for dimensionality reduction and indexing with a lower-bounding distance measure. In classic data mining tasks such as clustering, classification, index, etc., SAX is as good as well-known representations such as Discrete Wavelet Transform (DWT) and Discrete Fourier Transform (DFT), while requiring less storage space. In addition, the representation allows researchers to avail of the wealth of data structures and algorithms in bioinformatics or text mining, and also provides solutions to many challenges associated with current data mining tasks. One example is motif discovery, a problem which we defined for time series data. There is great potential for extending and applying the discrete representation on a wide class of data mining tasks.\n\n\n==== The Amadeus motif discovery platform ====\n\nAmadeus Home [http://acgt.cs.tau.ac.il/amadeus/]\n\nAmadeus is a user-friendly software platform for genome-scale detection of known and novel motifs (recurring patterns) in DNA sequences, applicable to a wide range of motif discovery tasks. Amadeus outperforms extant tools and sets a new standard for motif discovery software in terms of accuracy, running time, range of application, wealth of output information and ease of use.\n\n==== Motif Detection ====\n\n* Python for Bioinformatics [http://telliott99.blogspot.kr/2009/06/motif-discovery.html]\n* MOODS: Motif Occurrence Detection Suite - ALGOrithmic Data ANalysis [http://www.cs.helsinki.fi/group/pssmfind/]\n* CMU CS Lecture -- Motif Detection [http://www.cs.cmu.edu/~epxing/Class/10810-05/Lecture6.pdf]\n\n==== Biopython ====\n\nBiopython is a set of freely available tools for biological computation written in Python by an international team of developers. [http://biopython.org/wiki/Main_Page Biopython]\n\n* Required Python Modules [http://www.scipy.org/more_about_SciPy NumPy / SciPy (SciPy.org)]\n** NumPy: N-dimensional Array Manipulations\n** SciPy: Scientific tools for Python\n\n* References\n# [http://biopython.org/DIST/docs/tutorial/Tutorial.pdf Biopython Tutorial]\n\n<br/>\n----\n\n=== RAM-SSD 하이브리드 Page Cache 아키텍쳐 :: WIPS 의견 대응 ===\n <pre>\n접수번호 : RN-201301-004-1\n\n명칭 : RAM-SSD 하이브리드 Page Cache 아키텍쳐\n\n기술 핵심 : RAM과 SSD를 이용하여 디스크 data를 caching하는 구조가\npage cache layer에서 동작하도록 함으로써 hot access를 check하고\naccess 정도에 따라 RAM 또는 SSD에 저장(tier-1~2)하거나 disk에서 access되도록 하는 것\n\n구성 : A) page cache에서 동작하는 RAN-SSD hybrid cache architecture\n      B) tier-1 page cache(RAM), tier-2 page cache(SSD)\n      C) page에 대한 hit pattern으로 page들을 tier-1,2로 class 지정\n\n\n--\n안녕하세요? 정명준입니다.\n다루는 내용이 많았음에도 깔끔하게 잘 정리해주시느라 고생이 많으셨습니다. ^_^\n꼭 추가되었으면 하는 내용과, 서술의 톤이 살짝 바뀌면 좋을만한 부분이 있어\n아래 보내주신 메일 본문에 추가하였습니다.\n(제 의견은 파란색으로 표시되어있습니다)\n\n1) \'기술 핵심\' 부분\n\n	\"RAM과 SSD를 이용하여 디스크 data를 caching하는 구조가 page cache layer에서 동작하도록 함으로써\" \n		-> 본 발명의 핵심 부분인 부분을 잘 적어주셨습니다.\n\n	\"hot access를 check하고 access 정도에 따라\"\n		-> 이 부분은 \"access pattern을 점검하여 rehit-potential에 따라\" 라는 표현으로 바뀌면 더욱 좋겠습니다.\n		기존에는 recently hot (recency) 위주로 cache될지 evict될지를 판단하였는데,\n		본 발명에서는 rehit potential을 보고 판단하겠다는 점이 다릅니다.\n		(once-hot data 라든지, periodicity 등을 파악)\n\n	\"RAM 또는 SSD에 저장(tier-1~2)하거나 disk에서 access되도록 하는 것\"\n		-> 여기에 \"page chunk 단위로 page tiering을 한다\"는 언급이 되었으면 좋겠습니다. (청구의 범위를 너무 제한하는 것이 아니라면요)\n\n\n2) \'구성\' 부분\n\n	아래 \'가~바\' 부분이 본 발명에서 청구하고자 하는 주요 내용들입니다.\n	이 중에서 \'가\' 부분이 보내주신 메일에서 A, B, C로 적어주신 \'구성\' 부분인데요,\n	대표 청구항을 적자면 아래 \'가~바\' 부분이 모두 필요할 것 같습니다.\n\n	가) RAM-SSD hybrid page cache architecture\n		page cache layer에서 동작하고;\n		tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는;\n		page tiering 기반의 hybrid page cache 아키텍쳐.\n\n	나) rehit-potential aware page classification, and page class based cache data placement\n\n	다) page tiering operation in host side\n\n	라) page tiering operation in SSD side\n		chunk I/O in SSD in {log-structured and/or parallelism-maximized} manner\n\n	마) TRIM ahead 메커니즘 (page chunk writes latency를 줄이기 위해 미리 공간 확보하는)\n\n	바) victim page chunk selection in SSD\n		oldest-first\n		most-mark-first\n		with upper-tier-backup\n		\n\n오후에 전화로 말씀 나누면서 설명 드리겠습니다.		\n감사합니다.\n정명준 드림.\n</pre>\n\n\n=== 서울대 2013-02-01 Meeting Minutes ===\n\n1. 실험 관련\n: HDD 개수와 map slot의 개수를 align하여 실험 진행 후 기존 결과와 함께 분석\n: <span style=\"color:blue\">>>네, 본 실험에서 나타난 \'재미있는\' 현상이 Hadoop 권장 환경 구성과 맞지 않아서 발생한 것이 아니라 Hadoop 시스템 근원적으로 가지고 있는 \'한계\' 때문이라는 것으로 주장이 되려면, HDD 수와 execution slot 간 alignment는 필수적일 것 같습니다. 잘 부탁드리겠습니다.[1]</span>\n\n: In-memory buffer 크기 변화를 통한 spill file 개수 및 phase elapsed time의 관계를 exponential expression 으로 분석\n: <span style=\"color:blue\">>> 아, 제 발음이 좋지 않았던 것 같습니다. ^^;; exponential expression이라기 보다는 extrapolation이 가능한 equation으로 나오면 좋겠다는 것으로 이해해주시면 좋겠습니다.  아마 이게 되려면 충분한 경우에 대해서 실험이 되어야 할 것인데, equation이 나오기만 한다면 무척 중요한 의미를 갖게 될 것 같습니다. 전체적인 phase elapsed time 뿐만 아니라 p.15에 색깔로 구분된 각 영역들(map - map/shuffle - shuffle - reduce)의 변화 양상도 잘 설명이 될 수 있다면 좋겠습니다.[2]</span>\n\n2. SSD 관련\n: <span style=\"color:blue\">>> 특히 보내주신 발표 슬라이드의 p.18에 언급된 SSD 사용 시 9.5% 감소 부분은 충격이었습니다. 이 현상을 유발한 원인이 명확하게 규명되고 (1), 그 규명된 원인이 기존에 알려지지 않았던 것이며 (2), 그것을 해소할 수 있는 방법도 같이 제시될 수 있다면 (3), 본 산학 과제가 더욱 큰 의미를 갖게 될 것 같습니다.[3]</span>\n\n: random write는 피하는 방향으로 SSD을 통한 하둡 수행 성능 향상이 나올 수 있도록 I/O contention의 오버헤드가 SSD 성능과 상쇄될 수 있도록 detail하게 실험.\n: network i/o와 disk i/o의 trade-off\n: <span style=\"color:blue\">>> 네, combine 등을 통해서 network overhead를 local overhead (local computation 및 local I/O)로 trade-off 시키게 되지만, 그 local overhead를 SSD로 상쇄시킬 수 있는 best 방법 및 예상 효과가 나온다면 매우 의미가 있을 것 같습니다.[4]</span>','utf-8'),(9,'== ## bNote-2013-03-29 ==\n\n=== DailyTask ===\n\n* IOWA\n\n\n=== MBO 2013 (목표 최종 확정) ===\n\n* 정명준 MBO\n <pre>\n\n30%, ~10/31\n- I/O Workload Analysis 기술 연구\n  : Dominant I/O Pattern Mining 및 Machine Learning 기반의\n    I/O 패턴 모델링 및 예측\n  : I/O Pattern 분석/예측 모델 수립\n  : I/O Pattern Mining/Learning 엔진 구현 (Python, R, Shell-script)\n\n30%, ~10/31\n- Data Placement 기술 연구\n  : Workload Analysis 결과로 얻어진 I/O Insight/Prediction을\n    활용하여 Data를 적소에 미리 배치\n  : Linux Kernel Module 형태로 Tiering 기술 형태로 구현\n  : Proactive Data Placement를 통해 분산 스토리지의 I/O 성능\n    80% 이상 개선 검증 (시뮬레이션, 혹은 Real 시스템 기반)\n  \n20%, ~10/31\n- A급 특허 3건 작성 및 심의 통과\n\n20%, ~10/31\n- 논문 1편 (To be accepted)\n\n</pre>\n\n\n* 과제 MBO (이전문님)\n <pre>\n* 분산 플랫폼 관련 특허 15편 이상 특허심의 통과 (전략출원 2편 이상 심의 통과) (30%)\n* 분산 플랫폼 관련 논문 2편 이상 accept (20%)\n* I/O coordination과 Proactive Placement를 통해 분산 스토리지의 I/O 성능 80% 이상 개선 검증\n  (HW RAID 대비) (15%)\n* 분산 Deduplication 기술을 통해 분산 스토리지에서 데이터 제거효율 3배, Coverage 4 node 달성 (3x@4node) (15%)\n* 분산 I/O Coordination 관련 기술이전 1건 (20%)\n</pre>\n\n=== 과제 변경 ===\n\n <pre>\n\n과제명: Intelligent Large-scale Data Management\n (구과제명) Real-Time Big Data Platform\n\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n\n</pre>\n\n== ## bNote-2013-03-28 ==\n\n=== SW설계기술리더양성 교육 지원 ===\n* SW Architect 사전 교육\n\n==== 현업 프로젝트 기획서 ====\n\n* 지원과정\n: SW 설계 리더 양성과정\n\n* 과제명\n: Data-intensive Storage\n\n* 프로젝트 참여자\n <pre>\n이주평	전문 연구원	Project Leader\n정명준	전문 연구원	시스템 설계, 요소기술 연구, 기능모듈 구현\n유개원	전문 연구원	요소기술 연구, 기능모듈 구현\n이형주	SDS 차장	기능모듈 구현, 기능/성능 검증\n</pre>\n\n* 과제 담당 임원\n: 심은수 상무\n\n* 과제 개요\n <pre>\n[배경 및 현안]\n□ 데이터 폭증으로 데이터센터/기업의 클라우드 스토리지 니즈 증대\n□ 클라우드 스토리지의 핵심 경쟁력은 성능 및 용량 향상 기술에 있음\n□ H/W 수준을 높이거나 S/W 최적화 기반으로 시스템의 성능을 개선\n   하는 기존 접근 방식으로는 H/W 한계를 넘어서는 성능 향상은 어려움\n□ 데이터 I/O 속도와 데이터 저장 효율을 획기적으로 개선할 수 있게 하는\n   지능적 Data Management 기술은 클라우드 스토리지 시스템의 경쟁력을\n   혁신하는 핵심 S/W 기술임\n\n[목적]\n□ 본 Sub Task에서는 지능적 Data Management 기술 중,\n   데이터 I/O 속도 향상 기술을 연구/개발한다\n   * I/O Workload Analysis에 기반한 Proactive Data Placement 기술 확보\n     - Real trace data에 대한 I/O Workload Analysis를 통해\n       dominant workload 패턴 발굴 및 I/O 예측 모델 학습\n     - I/O 예측 모델에 기반한 multi-tier (horizontal - vertical) 간\n       proactive data 배치 수행\n</pre>\n\n* 목표\n <pre>\n[기능/성능/품질]\n□ I/O Workload Analysis에 기반한 Proactive Data Placement\n  - I/O Workload Analysis 모듈\n    - Real trace data 수집 기능\n    - Trace data parsing 및 transform 기능 (analysis를 위한 전처리)\n    - Dominant workload pattern 추출 및 I/O model 학습\n  - Proactive Data Placement 모듈\n    - Tier management 및 data move 기능\n	- I/O monitoring 및 hot/cold 판단 기능\n\n[중간 산출물]\n□ Hot/Cold Data Placement 모듈\n  - 핵심적인 automated tiering 기능 구현\n    : Data access 패턴 관찰을 통해, hot data는 고속의 storage tier에,\n      cold data는 상대적으로 느린 속도의 storage tier에, 주기적 배치\n  - 동일 수준의 시스템에서, 동일 workload에 대해,\n    경쟁사 (E사) 수준의 성능 달성 (metric: average IOPS)\n\n[최종 결과물]\n□ Proactive Data placement 모듈\n  - I/O 예측 모델에 기반한 proactive data placement\n    : Dominant workload 패턴 분석 및 I/O 예측 모델에 기반한\n	  multi-tier 간 선제적 data 배치를 통해 I/O 성능 향상\n  - 동일 수준의 시스템에서, 동일 workload에 대해,\n    경쟁사 (E사) 대비 100% 이상의 성능 향상 달성 (metric: average IOPS)\n</pre>\n\n* 기대 효과\n <pre>\n□ 지능적 Data Management 기술은 Big Data를 다루는\n   클라우드 스토리지 서비스의 핵심 기술로 활용 가능\n   - Big Data 시장에서는 특히 스토리지 분야가 연간 61.4%의 성장율로\n     전체 시장 성장을 주도\n</pre>\n\n* 과제 구성\n <pre>\n[전체 Architecture]\n□ Proactive Data Placement 시스템은\n   Workload Analysis 모듈과 Data Placement 모듈로 구성됨\n  * Workload Analysis 모듈은 Trace Log 데이터에 대한\n    Off-line I/O Analysis를 수행하여 I/O 패턴에 대한 Insight을 확보함\n    (e.g., 어느 위치의 Data가 언제쯤 Access될 것인지를 예측)\n  * Data Placement 모듈은 I/O 패턴에 대한 Insight 정보와, 실시간으로\n    모니터링되는 시스템 상태 정보를 이용하여, Data를 미리 적소에 배치함\n\n[과제 적용부 기술항목]\n□ Workload Analysis 모듈: I/O Prediction Model Optimization 이슈\n  - 응용 및 시스템 특성에 따라 Workload 특성이 다를 수 있음\n    Workload 별로 Prediction Model을 구성하는 주요 X\'s 의 최적화 필요\n□ Data Placement 모듈: Overhead 최소화 및 Tiering 구현 최적화 이슈 \n  - Real-time Monitoring으로 인해 시스템에 가해지는 Overhead 최소화 필요\n  - Tiering 기능 구현 시 I/O 특성 및 시스템 구조를 반영한 최적화 필요\n</pre>\n\n\n==== 입과 추천서 ====\n\n[본인 업무 이력]\n\n* 2004.08 ~ 2007.09 : Security & Trusted Computing 기술 연구/개발\n:- 휴대폰 Content/Right Protection 기술인 OMA DRM S/W 개발, 무선사에 기술 이전\n:- Secure MMC를 위한 Crypto Engine 개발 참여 및 MMC IOP T/F 활동, 메모리사에 기여\n:- System의 무결성 보장 기술인 Trusted Computing 기술 연구 주도, Mandatory Access Control 기술을 무선사에 이전, LiMo (Linux Mobile) Security 표준에 반영 (SubPL)\n:- A급 특허 6건 출원, 논문 2건 (ACM SACMAT \'08 등)\n\n* 2007.10 ~ 2008.05 : 전사 6시그마 MBB (Master Black Belt) 양성 과정\n:- 제 16기 6시그마 MBB 과정에 입과하여 6시그마 이론 연구 및 실습 과제를 진행하고 BB 교육 과정 강의를 진행하였음. MBB 인증 시험 통과\n\n* 2008.06 ~ 2010.10 : Virtualization 및 Operating System 기술 연구/개발\n:- H/W가상화 기술인 Xen Hypervisor의 Security 연구 참여\n:- OS가상화 기술 기반의 State Migration S/W 개발, 스토리지사업부로 기술이전(SubPL)\n:- Russia연구소와 협력, Android 부팅속도를 향상시키는 FastBoot 기술 연구 (SubPL)\n:- 본사 사업지원팀 Vision 2020 T/F에 핵심 멤버로 참여, 15개 미래 기술 테마 발굴\n:- A급 특허 6건 출원 (전략 출원 2건), 논문 1건 (MobiCom \'09)\n\n* 2010.11 ~ 2013.현재 : Data-intensive Storage 기술 연구/개발\n:- I/O Workload Analysis에 기반한 Proactive Data Placement 기술 연구 주도\n::- Workload Analysis에서 획득한 I/O에 대한 근본적인 이해를 바탕으로 Data Management 알고리즘을 혁신, 스토리지 시스템 성능을 향상시키는 기술임\n:- 본 과제는 메모리사의 사업영역 확장 및 \'클라우드 스토리지 서비스\'를 위한 스토리지 시스템 기술 확보에 기여하고 있음\n:- A급 특허 6건 출원, 논문 3건 (ICCE 등)\n\n[소속부서장 추천 사유]\n\n* (양성 후 활용계획)\n:- 스토리지 시스템 설계/구현 시 S/W Architect로 활용\n* (인물평 및 추천사유)\n:- 정명준 전문은 시스템 분야에 대한 깊은 기술적 이해와 원만한 커뮤니케이션 능력을 바탕으로한 성공적인 프로젝트 발굴/주도 경험을 가지고 있습니다.\n:- 향후 Architect로서, 해당 과제의 S/W 설계 리딩을 통해 스토리지 시스템의 차별화된 기술 경쟁력을 만들어 내는 데에 기여할 수 있을 것으로 판단되어, 금번 S/W 설계 리더 과정에 추천합니다.\n\n== ## bNote-2013-03-26 ==\n\n=== DailyTask ===\n\n* IOWA Proactive Data Placement Formulation\n* Data Representation (as a pre-processing for association rules mining)\n\n* Patentization\n:- Distributed Multi-level Caching\n:- IO Pattern-optimal Data Placement for Tiering\n:- Virtualization-aware Caching/Tiering/Placement\n\n* Study\n:- Btier\n:- Bcache\n:- Fusion IO Caching Technology (directCache, ioTurbine)\n:- EMC FAST (Fully Automated Storage Tiering)\n:- OpenStack\n:- Xen\n:- VASA, VAAI (VMware의 storage virtualization 기술들)\n:- PCIe fabric switching\n:- Software Defined Storage\n:- Virstore? (VMware가 인수?)\n\n* 심상무님께 주간보고 내용\n:- Tiering Test SW Platform 구축 건 (Open source 활용, SDS 이형주 차장님과 함께)\n:- Real Trace Log Data 확보 진행 건 (수퍼컴센터의 Analytics Workload Trace, VDI Trace)\n\n=== Patentization ===\n\n* Access Pattern Aware Tiering\n\n\n=== Memo ===\n\n* Turbine: <기계> 높은 압력의 유체를 날개바퀴의 날개에 부딪치게 함으로써 회전하는 힘을 얻는 원동기. 사용하는 유체의 종류에 따라 수력 터빈, 증기 터빈, 가스 터빈 따위가 있다.\n\n== ## bNote-2013-03-25 ==\n\n=== DailyTask ===\n\n* 업무 File 정리\n* IOWA Proactive Data Placement Formulation\n\n=== Patidea ===\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n:- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [http://www-03.ibm.com/systems/software/gpfs/][http://public.dhe.ibm.com/common/ssi/ecm/en/pos03096usen/POS03096USEN.PDF]\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n* advanced tiering: access pattern-aware optimal placement (APOP)\n:- Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n:: 예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n:- 이에 필요한 data access pattern 모니터링/분석 방법\n:: 데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\n::: NIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n:- 이를 위해 필요한 system architecture 구조\n:: 기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n=== Formulation: IOWA based Proactive Data Placement ===\n\n* Formulating [[IOWA]] or [[I/O Workload Analysis]]\n:- to clarify the total amount of work\n:- to clarify the sub-tasks (can be modularized)\n:- to clarify the area to get focused\n\n\n=== Information: S사, K사 ===\n\n* Kaminario, Solidfire\n: From 서정민 전문\n: SSIC 미팅노트로부터 FACT를 각색한 정보를 공유합니다.  제 의견은 반영하지 않았습니다.\n\n* SolidFire (SSIC 초기미팅 결과)\n:* 1. Company Overview\n::- 3 years, 82 people\n::- $37M in funding (현재 Series B 단계로, 2013년 Series C 가능성 있음)\n::- 12 customers, 4 announced: 2 private cloud enterprise customers. \n::- Multi-tenancy가 기반인 Cloud 시장을 타겟으로 제품 제작 (OpenStack, CloudStack 연동)\n:* 2. Technology: QoS, Scalability, Inline deduplication/Compression\n:: (a) QoS\n::: OS 내에서 QoS를 Volume 단위로 관리 \n::: IOPS/latency QoS support (No R/W separate QoS) \n:: (b) Scalability\n::: Full data distribution across all the nodes \n::: All the nodes contributes to rebuilds\n:* 3. Current Arch./Tech. (GA)\n::- System configuration: 5~100 nodes (they have 40 nodes in test)\n::- H/W Configuration\n:: (a) CPU: Dual 2.5GHz Sandy Bridge with 6 cores each. \n::: 10 Cores는 mostly compute intensive work including dedup and compression. \n::: 2 Core는 handles IO to SSDs\n:: (b) SSD: Viking for boot/metadata, Intel SATA SSDs (relies on supercap in SSD)\n:: (c) Network: iSCSI (FC/NFS in the future, NFS just for small filer)\n::- Performance\n::: Latency Avg is .5ms to 2ms. Worst is 20 to 30 ms. \n:* 4. 금년도 추가 개발계획 (일부)\n::- Remote replication, sync and async, coming in Q3\n::- Encryption is also on the roadmap. \n\n* Kaminario (SSIC 초기 미팅 결과)\n:* 1. Company Overview\n::- Found in 2008.3, Sequoia(VC) funded\n::- 30 patents (the engineers have 76 from the previous jobs)\n::- Target: general-purpose storage system (OLTP, OLAP, VDI)\n::: focusing Latency, Throughput, IOPS all\n::- Shipping scale-out systems for the last 2 and 1/2 years\n::- Competitors: XtremIO, SolidFire\n::- 엔터프라이즈 기능 포커스: resiliency, self-healing, automation 중심\n:* 2. Technology\n::- Core 기술에 대한 파악 결과 없음\n:* 3. Previous Arch.\n::- Dell Blade 서버 방식으로 Fusion-IO 탑재\n:* 4. Current Arch. (개발 중)\n::- 1U rack server 기반 SMART or STEC SAS SSDs 사용\n::: low cost SSDs, low end Xeon, 32GB memory 등 Cost를 줄이는 방식 채용\n::: \"They use LSI SAS controller but don’t use dual port functionality.\"\n::: No SATA SSD (SATA SSD는 신뢰성 문제 야기하는 것으로 판단)\n::: -> SSIC 전문가는 SAS Dual port 기술 개발을 실패하지 않았는가 하는 의문 제기\n::- Performance is about 100,000 IOPS/node.\n::- No Dedup/compression \n::- \"Their SPC-1 result has 20x better price performance than previous SPC results. \"\n::- Currently focus on reducing long tail numbers.  (already has good IOPS)\n::: Performance degradation: < 25% at loss of data node\n\n=== References ===\n\n* [http://www.kaseya.com/download/en-us/white_papers/KaseyaBuyersGuidePaper.pdf IT Systems Management Buyers’ Guide // Kaseya]\n* [http://www.sata-io.org/technology/6Gbdetails.asp SATA-IO Revision 3.1 Specification // Queued Trim Command]\n\n----\n\n== ## bNote-2013-03-22 ==\n\n <pre>\n(EMC (Forum OR World) VNX) ((performance OR \"iops\") AND (\"per dollar\" OR \"dollar per\" OR \"per $\" OR \"/$\" OR \"$/\")) \"vs\" (filetype:pdf OR filetype:ppt OR filetype:pptx)\n</pre>\n\n=== DailyTask ===\n\n----\n==== Books of Machine Learning / Data Mining ====\n* \"Machine Learning\" // Tom Mitchell, McGraw Hill, 1997 ((B.GOOD))\n:- [http://www.cs.cmu.edu/~tom/mlbook.html Book]\n:- [http://www.cs.cmu.edu/~tom/mlbook-chapter-slides.html Slides]\n\n* \"Mining of Massive Datasets\" // Anand Rajaraman, Jeffrey David Ullman ((B.GOOD))\n:- [http://i.stanford.edu/~ullman/mmds.html Book - Online Version]\n:- [http://i.stanford.edu/~ullman/mmds/book.pdf Download the latest book (PDF, 415 pages, approximately 2.5MB)]\n\n----\n\n==== Machine Learning / Data Mining ====\n\n* [http://en.wikipedia.org/wiki/Gradient_descent Gradient Descent]\n* [http://ko.wikipedia.org/wiki/%EC%9D%8C%ED%95%A8%EC%88%98]\n* [http://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95%EC%82%AC%EC%83%81]\n* [http://ko.wikipedia.org/wiki/%EB%AF%B8%EC%A0%81%EB%B6%84%ED%95%99]\n* [http://ko.wikipedia.org/wiki/%ED%8E%B8%EB%AF%B8%EB%B6%84]\n* [http://ko.wikipedia.org/wiki/%ED%8F%89%EA%B7%A0%EA%B0%92_%EC%A0%95%EB%A6%AC]\n* [http://www.iiswc.org/iiswc2008/Papers/012.pdf] Characterization of Storage Workload Traces from Production Windows Servers // Swaroop Kavalanekar, Bruce Worthington, Qi Zhang, Vishal Sharda\n\n----\n\n== ## bNote-2013-03-21 ==\n\n=== Official Death of ... ===\n* What to do? why?\n\n=== DailyTask ===\n\n* list of candidate tasks\n\n:- (~) IOWA DM/ML Planning\n::- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n::- Hadoop case 고려 (in-local-node vertical placement between DRAM-SSD-HDD 뿐만 아니라 horizontal placement (among distributed nodes in Hadoop cluster)도 고려)\n\n:- IOWA ML Study\n::- IOWA:: Bayesian Network study\n::- IOWA:: Neural Network study\n::- IOWA:: HMM study\n::- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n::- (~) IOWA:: Association Rule Mining, Frequent Item Sets Mining (Market Basket Problem)\n\n:- page cache to be revisited\n\n:- Real IO trace 확보 작업\n::- 김혁호 책임이 보내준 업무요청 사례 기반, 업무요청 문서 작성할 것\n::- (V) NetApp (김주영 과장)에게 연락 -> 스토리지 모델 정보 요청 -> 추가 문의 (Management Console에서 보다 자세한 정보를 볼 수는 없는지? NetApp OnCommand System Manager 같은 Tool을 사용해서라도..) -> 김주영 과장과 통화 (오전 10:20 ~ 46), 이번 주 중에 찾아뵙고 말씀 나누는 걸로 ...\n::- EMC 연락\n::- (~) Dell (지근영 대리)에게 연락\n:::- Dell EqualLogic PS6100XV iSCSI arrays [http://www.dell.com/us/enterprise/p/equallogic-ps6100xv/pd] [http://www.equallogic.com/products/default.aspx?id=10661]\n\n:- (~) Distributed Computing Problem Study\n::- CAP theorem (CAP: Consistency, Availability, Partition tolerance) [http://en.wikipedia.org/wiki/CAP_theorem]\n::- Paxos [http://en.wikipedia.org/wiki/Paxos_(computer_science)]\n::- State Machine Replication [http://en.wikipedia.org/wiki/State_machine_replication]\n\n----\n=== Formulation: IOWA Proactive Data Placement (moved to ## bNote-2013-03-25) ===\n----\n\n== ## bNote-2013-03-20 ==\n\n=== DailyTask ===\n\n* list of candidate tasks\n\n:- (~) IOWA DM/ML Planning\n::- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n::- Hadoop case 고려 (in-local-node vertical placement between DRAM-SSD-HDD 뿐만 아니라 horizontal placement (among distributed nodes in Hadoop cluster)도 고려)\n\n:- IOWA ML Study\n::- IOWA:: Bayesian Network study\n::- IOWA:: Neural Network study\n::- IOWA:: HMM study\n::- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n::- (~) IOWA:: Association Rule Mining, Frequent Item Sets Mining (Market Basket Problem)\n\n:- page cache to be revisited\n\n:- Real IO trace 확보 작업\n::- 김혁호 책임이 보내준 업무요청 사례 기반, 업무요청 문서 작성할 것\n::- (V) NetApp (김주영 과장)에게 연락 -> 스토리지 모델 정보 요청 -> 추가 문의 (Management Console에서 보다 자세한 정보를 볼 수는 없는지? NetApp OnCommand System Manager 같은 Tool을 사용해서라도..) -> 김주영 과장과 통화 (오전 10:20 ~ 46), 이번 주 중에 찾아뵙고 말씀 나누는 걸로 ...\n::- EMC 연락\n::- (~) Dell (지근영 대리)에게 연락\n:::- Dell EqualLogic PS6100XV iSCSI arrays [http://www.dell.com/us/enterprise/p/equallogic-ps6100xv/pd] [http://www.equallogic.com/products/default.aspx?id=10661]\n\n:- (V) 2013년 MBO 작성\n::- IOWA PDP (I/O Workload Analysis based Proactive Data Placement) 와 IOBA (I/O Bottleneck Analysis) 두 아이템으로 작성\n\n:- (~) Distributed Computing Problem Study\n::- CAP theorem (CAP: Consistency, Availability, Partition tolerance) [http://en.wikipedia.org/wiki/CAP_theorem]\n::- Paxos [http://en.wikipedia.org/wiki/Paxos_(computer_science)]\n::- State Machine Replication [http://en.wikipedia.org/wiki/State_machine_replication]\n\n----\n\n=== Formulation: IOWA Proactive Data Placement (moved to #bNote-2013-03-21) ===\n\n* [[http://kandinsky/wikini/index.php/Bnote_2013#Formulation:_IOWA_Proactive_Data_Placement]]\n\n=== Supercom Usage Statistics ===\n\n <pre>\nblusjune@jimi-hendrix:[~] $ ssh a1mjjung@supercom\na1mjjung@supercom\'s password:\nLast login: Tue Mar 19 19:28:27 2013 from 75.2.93.158\n----------------------------------------------------------\n| During : 20130311 ~ 20130317                            |\n| Username : a1mjjung , Application(Total jobs) : unix(3)\n----------------------------------------------------------\nTotal RUN time : 2 min 36 secs\nAverage RUN time : 52 secs\nMaximum RUN time : 1 min 14 secs\nAverage Wait time 1 secs\nMaximum Wait time 1 secs\n ---------------------------------------------------------\n</pre>\n\n----\n== ## bNote-2013-03-19 ==\n\n\n\n=== The Market-Basket Model ===\n\n\n=== IOWA::Outlook (MSN FileServer IO Trace // msnfs) ===\n\n* # of IOs (Read/Write/All)\n<pre>\na1mjjung@secm:[microsoft_msn_filesrvr_6h] $ wc -l tracelog.msn_filesrvr.[ARW]\n\n  29345085 tracelog.msn_filesrvr.A\n  19729611 tracelog.msn_filesrvr.R\n   9615474 tracelog.msn_filesrvr.W\n</pre>\n\n* Reads Outlook\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130313_154605.sigio_25.iowsz_2000.t1_2000] $ cat __simout.sigio_25.iowsz_2000.t1_2000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  17\n__valu__sig__ _n_o_sigaddrs :  173401\n__valu__sig__ _sigioc_acc :  5005517\n__valu__sig__ _sigaddrs_efficiency :  28.8667135714\n__valu__sig__ _n_o_addr_total :  5053016\n__valu__sig__ _ioc_total :  19729611\n</pre>\n\n* Writes Outlook\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130313_155325.sigio_25.iowsz_2000.t1_2000] $ cat __simout.sigio_25.iowsz_2000.t1_2000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  8\n__valu__sig__ _n_o_sigaddrs :  211100\n__valu__sig__ _sigioc_acc :  2989803\n__valu__sig__ _sigaddrs_efficiency :  14.1629701563\n__valu__sig__ _n_o_addr_total :  4506823\n__valu__sig__ _ioc_total :  9615474\n</pre>\n\n* Microsoft Production Workload Trace - Related Articles\n\n:- \"Characterization of Storage Workload Traces from Production Windows Servers\", IISWC 2008, Swaroop Kavalanekar, Bruce Worthington, Qi Zhang, Vishal Sharda, Microsoft Corporation [http://www.iiswc.org/iiswc2008/sildes/4_3.pdf Slides], [http://www.iiswc.org/iiswc2008/Papers/012.pdf Papers]\n\n:- \"Write Off-Loading: Practical Power Management for Enterprise Storage\" [http://static.usenix.org/event/fast08/tech/full_papers/narayanan/narayanan.pdf FAST 2008]\n\n=== R Tutorial (Data Frame, Preview) ===\n\n----\n==== Data Frame ====\nA data frame is used for storing data tables. It is a list of vectors of equal length. For example, the following variable df is a data frame containing three vectors n, s, b.\n\n <pre>\n> n = c(2, 3, 5) \n> s = c(\"aa\", \"bb\", \"cc\") \n> b = c(TRUE, FALSE, TRUE) \n> df = data.frame(n, s, b)       # df is a data frame\n</pre>\n\n----\n==== Built-in Data Frame ====\nWe use built-in data frames in R for our tutorials. For example, here is a built-in data frame in R, called mtcars.\n\n <pre>\n> mtcars \n               mpg cyl disp  hp drat   wt ... \nMazda RX4     21.0   6  160 110 3.90 2.62 ... \nMazda RX4 Wag 21.0   6  160 110 3.90 2.88 ... \nDatsun 710    22.8   4  108  93 3.85 2.32 ... \n               ............\n</pre>\n\nThe top line of the table, called the header, contains the column names. Each horizontal line afterward denotes a data row, which begins with the name of the row, and then followed by the actual data. Each data member of a row is called a cell.\n\nTo retrieve data in a cell, we would enter its row and column coordinates in the single square bracket \"[]\" operator. The two coordinates are separated by a comma. In other words, the coordinates begins with row position, then followed by a comma, and ends with the column position. The order is important.\n\nHere is the cell value from the first row, second column of mtcars.\n\n <pre>\n> mtcars[1, 2] \n[1] 6\n</pre>\n\nMoreover, we can use the row and column names instead of the numeric coordinates.\n\n <pre>\n> mtcars[\"Mazda RX4\", \"cyl\"] \n[1] 6\n</pre>\n\nLastly, the number of data rows in the data frame is given by the nrow function.\n\n <pre>\n> nrow(mtcars)    # number of data rows \n[1] 32\n</pre>\n\nAnd the number of columns of a data frame is given by the ncol function.\n\n <pre>\n> ncol(mtcars)    # number of columns \n[1] 11\n</pre>\n\nFurther details of the mtcars data set is available in the R documentation.\n\n <pre>\n> help(mtcars)\n</pre>\n\n----\n\n==== Preview ====\n\nPreview\nInstead of printing out the entire data frame, it is often desirable to preview it with the head function beforehand.\n\n <pre>\n> head(mtcars) \n               mpg cyl disp  hp drat   wt ... \nMazda RX4     21.0   6  160 110 3.90 2.62 ... \n               ............\n</pre>\n\n----\n\n==== Data Import ====\n\n\n\n\n\n\n\n\nIt is necessary to import the sample textbook data into R before you start working on your homework.\n\n* Excel File\n: Quite often, the sample data is in Excel format, and needs to be imported into R prior to use. For this, we use the read.xls function from the gdata package. It reads from an Excel spreadsheet and returns a data frame. The following shows how to load an Excel spreadsheet named \"mydata.xls\". As the package is not in the core R library, it has to be installed and loaded into the R workspace.\n\n <pre>\n> library(gdata)                   # load the gdata package \n> help(read.xls)                   # documentation \n> mydata = read.xls(\"mydata.xls\")  # read from first sheet\n</pre>\n\n* Minitab File\n: If the data file is in Minitab Portable Worksheet format, it can be opened with the read.mtp function from the foreign package. It returns a list of components in the Minitab worksheet.\n\n <pre>\n> library(foreign)                 # load the foreign package \n> help(read.mtp)                   # documentation \n> mydata = read.mtp(\"mydata.mtp\")  # read from .mtp file\n</pre>\n\n* SPSS File\n: For the data files in SPSS format, it can be opened with the read.spss function from the foreign package. There is a \"to.data.frame\" option for choosing whether a data frame is to be returned.\n\n <pre>\n> library(foreign)                 # load the foreign package \n> help(read.spss)                  # documentation \n> mydata = read.spss(\"myfile\", to.data.frame=TRUE)\n</pre>\n\n* Table File\n: A data table can resides in a text file. The cells inside the table are separated by blank characters. Here is an example of a table with 4 rows and 3 columns.\n\n <pre>\n100   a1   b1 \n200   a2   b2 \n300   a3   b3 \n400   a4   b4\n</pre>\n\nNow copy and paste the table above in a file named \"mydata.txt\" with a text editor. Then load the data into the workspace with the read.table function.\n\n <pre>\n> mydata = read.table(\"mydata.txt\")  # read text file \n> mydata                             # print data frame \n   V1 V2 V3 \n1 100 a1 b1 \n2 200 a2 b2 \n3 300 a3 b3 \n4 400 a4 b4\n</pre>\n\nFor further detail of the read.table function, please consult the R documentation.\n\n <pre>\n> help(read.table)\n</pre>\n\n* CSV File\nThe sample data can also be in comma separated values (CSV) format. Each cell inside such data file is separated by a special character, which usually is a comma, although other characters can be used as well.\n\nThe first row of the data file should contain the column names instead of the actual data. Here is a sample of the expected format.\n\n <pre>\nCol1,Col2,Col3 \n100,a1,b1 \n200,a2,b2 \n300,a3,b3\n</pre>\n\nAfter we copy and paste the data above in a file named \"mydata.csv\" with a text editor, we can read the data with the read.csv function.\n\n <pre>\n> mydata = read.csv(\"mydata.csv\")  # read csv file \n> mydata                           # print data frame \n  Col1 Col2 Col3 \n1  100   a1   b1 \n2  200   a2   b2 \n3  300   a3   b3\n</pre>\n\nIn various European locales, as the comma character serves as decimal point, the read.csv2 function should be used instead. For further detail of the read.csv and read.csv2 functions, please consult the R documentation.\n\n <pre>\n> help(read.csv)\n</pre>\n\n----\n\n== ## bNote-2013-03-18 ==\n\n=== DailyPlan ===\n\n* list of candidate tasks\n\n:- Real IO trace 확보 작업\n::- 김혁호 책임이 보내준 업무요청 사례 기반, 업무요청 문서 작성할 것\n::- [V://j] NetApp (김주영 과장)에게 연락 -> 스토리지 모델 정보 요청 -> 추가 문의 (Management Console에서 보다 자세한 정보를 볼 수는 없는지? NetApp OnCommand System Manager 같은 Tool을 사용해서라도..) -> 김주영 과장과 통화 (오전 10:20 ~ 46), 이번 주 중에 찾아뵙고 말씀 나누는 걸로 ...\n::- EMC 연락\n::- [~] Dell (지근영 대리)에게 연락\n:::- Dell EqualLogic PS6100XV iSCSI arrays [http://www.dell.com/us/enterprise/p/equallogic-ps6100xv/pd] [http://www.equallogic.com/products/default.aspx?id=10661]\n\n:- IOWA ML Planning\n::- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n::- Hadoop case 고려 (in-local-node vertical placement between DRAM-SSD-HDD 뿐만 아니라 horizontal placement (among distributed nodes in Hadoop cluster)도 고려)\n\n:- IOWA ML Study\n::- IOWA:: Bayesian Network study\n::- IOWA:: Neural Network study\n::- IOWA:: HMM study\n::- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n\n:- Netflix의 Cloud Computing Challenge 내용 파악\n(심상무님 지시: 거기가 우리보다 앞서 있으니, 어떤 기술들이 필요한지, 이슈가 무엇인지에 대한 힌트를 얻을 수 있을 것임)\n\n:- page cache to be revisited\n\n\n----\n\n=== Linux File Systems: Ext2 vs. Ext3 vs. Ext4 ===\n\n* [http://www.thegeekstuff.com/2011/05/ext2-ext3-ext4/]\n\next2, ext3 and ext4 are all filesystems created for Linux. This article explains the following:\n\n:- High level difference between these filesystems.\n:- How to create these filesystems.\n:- How to convert from one filesystem type to another.\n\n==== Ext2 ====\n\n* Ext2 stands for second extended file system.\n* It was introduced in 1993. Developed by Remy Card.\n* This was developed to overcome the limitation of the original ext file system.\n* Ext2 does not have journaling feature.\n* On flash drives, usb drives, ext2 is recommended, as it doesn’t need to do the over head of journaling.\n* Maximum individual file size can be from 16 GB to 2 TB\n* Overall ext2 file system size can be from 2 TB to 32 TB\n\n\n==== Ext3 ====\n\n* Ext3 stands for third extended file system.\n* It was introduced in 2001. Developed by Stephen Tweedie.\n* Starting from Linux Kernel 2.4.15 ext3 was available.\n* The main benefit of ext3 is that it allows journaling.\n* Journaling has a dedicated area in the file system, where all the changes are tracked. When the system crashes, the possibility of file system corruption is less because of journaling.\n* Maximum individual file size can be from 16 GB to 2 TB\n* Overall ext3 file system size can be from 2 TB to 32 TB\n* There are three types of journaling available in ext3 file system.\n:- Journal ? Metadata and content are saved in the journal.\n:- Ordered ? Only metadata is saved in the journal. Metadata are journaled only after writing the content to disk. This is the default.\n:- Writeback ? Only metadata is saved in the journal. Metadata might be journaled either before or after the content is written to the disk.\n* You can convert a ext2 file system to ext3 file system directly (without backup/restore).\n\n\n==== Ext4 ====\n\n* Ext4 stands for fourth extended file system.\n* It was introduced in 2008.\n* Starting from Linux Kernel 2.6.19 ext4 was available.\n* Supports huge individual file size and overall file system size.\n* Maximum individual file size can be from 16 GB to 16 TB\n* Overall maximum ext4 file system size is 1 EB (exabyte). 1 EB = 1024 PB (petabyte). 1 PB = 1024 TB (terabyte).\n* Directory can contain a maximum of 64,000 subdirectories (as opposed to 32,000 in ext3)\n* You can also mount an existing ext3 fs as ext4 fs (without having to upgrade it).\n* Several other new features are introduced in ext4: multiblock allocation, delayed allocation, journal checksum. fast fsck, etc. All you need to know is that these new features have improved the performance and reliability of the filesystem when compared to ext3.\n* In ext4, you also have the option of turning the journaling feature “off”.\n\n\n----\n\n=== Frequent patterns/items mining examples with R::arules ===\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n* References\n:# [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))]\n:# [http://www.analysis-of-patterns.net/files/bgoethals.pdf Frequent Pattern Mining Slides - Universiteit Antwerpen]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning - Wikipedia]\n:# [http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf Frequent Pattern Mining in Web Log Data]\n:# [http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010 Temporal Pattern Mining Tutorial KDD 2010]\n:# [http://cran.r-project.org/web/packages/arules/index.html arules: Mining Association Rules and Frequent Itemsets]\n:# [http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf Introduction to arules - A computational environment for mining association rules and frequent item sets -- http://cran.r-project.org/]\n:# [http://lyle.smu.edu/IDA/arules/ Intelligent Data Analysis Lab @ SMU]\n:# [http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))]\n:# [http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/ Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning]\n:# [http://en.wikipedia.org/wiki/Sequence_mining Sequence mining]\n:# [http://en.wikipedia.org/wiki/GSP_Algorithm GSP Algorithm]\n:# [http://en.wikipedia.org/wiki/Apriori_algorithm Apriori algorithm]\n:# [http://en.wikipedia.org/wiki/Market_basket_analysis Market basket analysis]\n:# [http://en.wikipedia.org/wiki/Time_series Time series]\n\n\n----\n\n=== R ===\n\n* png file output work-around against \'plot() error\' in R\n:- [http://www.mail-archive.com/r-help@r-project.org/msg40658.html]\n <pre>\nThe png() device does not need an X server to connect to. I think it\nused to in versions gone by, but not any more. Here I\'ve disabled X so\nthat X11() doesn\'t work, but png() still does:\n\n > x11()\n Error in X11(d$display, d$width, d$height, d$pointsize, d$gamma,\nd$colortype,  :\n   unable to start device X11cairo\n In addition: Warning message:\n In x11() : unable to open connection to X11 display \'\'\n > png(file=\"foo2.png\")\n > plot(1:10)\n > dev.off()\n null device\n          1\n\n I suspect your R was compiled without png support. What does the\n\'capabilities()\' function in R tell you?\n\n > capabilities()\n    jpeg      png     tiff    tcltk      X11     aqua http/ftp  sockets\n    TRUE     TRUE     TRUE     TRUE    FALSE    FALSE     TRUE     TRUE\n  libxml     fifo   cledit    iconv      NLS  profmem    cairo\n    TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE\n</pre>\n\n\n\n==== An Introduction to R ====\n\n* [http://cran.r-project.org/doc/manuals/R-intro.html#The-read_002etable_0028_0029-function An Introduction to R - Table of Contents]\n\n\n==== R Tutorial - (http://www.r-tutor.com/) ====\n\n* [http://www.r-tutor.com/gpu-computing/gaussian-process/rvbm Bayesian Classification with Gaussian Process]\n* [http://www.r-tutor.com/content/r-tutorial-ebook R Tutorial with Bayesian Statistics Using OpenBUGS]\n* [http://www.r-tutor.com/bayesian-statistics/openbugs Bayesian Inference Using OpenBUGS]\n* [http://www.r-tutor.com/gpu-computing/svm/rpusvm-2 Support Vector Machine with GPU, Part II]\n\n==== R: Input and output: scripts, saving and loading data ((B.GOOD)) ====\n\n* [http://egret.psychol.cam.ac.uk/statistics/R/savingloading.html Cambridge University]\n\n\n* General file-handling commands\n <pre>\nsetwd(\"c:/myfiles\") # use / or \\\\ to separate directories under Windows (\\\\ becomes \\ once processed through the escape character mechanism)\ndir() # list the contents of the current directory\n</pre>\n\n\n* Running scripts\n <pre>\nsource(\"myfile.R\") # load and execute a script of R commands\n</pre>\n\n* For a startup script\n: edit \".Rprofile\" in your home directory (for details see ?Startup). Here\'s an example\n <pre>\n# RNC ~/.Rprofile\n\n# auto width adjustment\n.adjustWidth <- function(...){\n       options(width=Sys.getenv(\"COLUMNS\"))\n       TRUE\n}\n.adjustWidthCallBack <- addTaskCallback(.adjustWidth)\n\n.First <- function() cat(\"\\n   Script ~/.Rprofile executed.\\n\\n\")\n.Last <- function()  cat(\"\\n   Goodbye!\\n\\n\")\n</pre>\n\n\n* Redirecting output\n <pre>\nsink(\"myfile.txt\") # redirect console output to a file\nsink() # restore output to the screen\n\npdf(\"mygraph.pdf\") # subsequent graphical output will go to a PDF\npng(\"mygraph.png\") # subsequent graphical output will go to a PNG\njpeg(\"mygraph.jpeg\") # subsequent graphical output will go to a JPEG\nbmp(\"mygraph.bmp\") # subsequent graphical output will go to a BMP\npostscript(\"mygraph.ps\") # subsequent graphical output will go to a PostScript file\ndev.off() # back to the screen\n</pre>\n\n\n* Text files\n <pre>\nmy.data = read.csv(filename)\nmy.data = read.csv(file.choose())\n# Note: (1) = and <- are synonymous, and are the assignment operator (while == tests for equality)\n#       (2) file.choose() pops up a live filename picker\n#       (3) The default is to assume a header row with variable names (header=TRUE),\n#           and no row names, but you can change all these defaults (e.g. row.names=1 reads\n#           row names from the first column).\n\nattach(my.data) # you might then want to attach the new data to the path, though this is optional\n\nwrite.csv(my.data, filename2) # Write the data to a new file. There are several options available; see the help (use ?write.csv)\nwrite.csv(my.data, file=\"d:/temp/newfile.csv\", row.names=FALSE) # Here\'s one: turn off row names to avoid creating a spurious additional column.\n\nread.table(...)  # } A more generic way to read/write tabular data from/to disk\nwrite.table(...) # } (read.csv and write.csv are specialized versions of read.table and write.table)\n</pre>\n\n\n* Microsoft Excel spreadsheets\n <pre>\nlibrary(RODBC)\nchannel <- odbcConnectExcel(\"Osteomalacia_data.xls\") # specify the filename\npatientdata <- sqlFetch(channel, \"Vitamin_D_levels\") # specify a sheet within the spreadsheet\nindexcasedata <- sqlFetch(channel, \"Sheet2\") # by default Excel names individual sheets Sheet1, Sheet2, ..., though you may have renamed them something more informative\nodbcClose(channel)\n</pre>\n\n\n* SPSS data\n <pre>\nlibrary(foreign)\nmydata <- data.frame(read.spss(\"filename.sav\"))\n# Remember you can also use file.choose() in place of the filename, as above.\n</pre>\n\n\n* ODBC data sources (databases)\n <pre>\n# 1. Connect\nlibrary(RODBC)\nchannel <- odbcConnect(\"my_DSN\") # specify your DSN here\n# if you need to specify a username/password, use:\n#  channel <-odbcConnect(\"mydsn\", uid=\"username\", pwd=\"password\")\n\n# 2. List all tables\nsqlTables(channel)\n\n# 3. Fetch a whole table into a data frame\nmydataframe <- sqlFetch(channel, \"my_table_name\") # fetch a table from the database in its entirety\nclose(channel)\n\n# 4. Fetch the results of a query into a data frame. Example:\nmydf2 <- sqlQuery(channel, \"SELECT * FROM MonkeyCantab_LOOKUP_TaskTypes WHERE TaskType < 6\")\n</pre>\n\nIf you\'re using MySQL, you can talk to the database directly:\n <pre>\nlibrary(RMySQL) # use install.packages(\"RMySQL\") if this produces an error\n# if the install.packages() command produces an error, under Ubuntu:\n# use \"sudo apt-get install libmysql++-dev\" (in addition to MySQL itself, i.e. the\n# \"mysql-server mysql-client mysql-navigator mysql-admin\" packages)\ncon <- dbConnect(MySQL(), host=\"localhost\", port=3306, dbname=\"mydatabase\", user=\"myuser\", password=\"mypassword\")\ndbListTables(con)\ndbListFields(con, \"table_name\")\nd <- dbReadTable(con, \"table_name\")\ne <- dbGetQuery(con, \"SELECT COUNT(*) FROM table_name\")\n# and much more possible\n</pre>\n\n\n* R native format\n <pre>\nsave(myobject1, myobject2, ..., file=\"D:/temp/mydata.rda\")\nload(file=\"D:/temp/mydata.rda\")\n# note that the load command recreates the \"mydata\" object without prompting\n# you can also use save.image() to save a whole workspace\n</pre>\n\n\n* Other data-moving techniques\nTo export the definition of an R object (which you can then re-import using \"object = THISTHING\"):\n <pre>\ndput(object, \"\")\n</pre>\n\nTo read a tabular object with a header row from the clipboard\n <pre>\nobject = read.table(\"clipboard\", header=T)\n</pre>\n\n----\n\n=== Samsung SSD 840 Series Information ===\n\n* [http://thessdreview.com/our-reviews/samsung-840-series-240gb-ssd-review-the-worlds-first-tlc-ssd-takes-the-stage/4/ Samsung 840 Series 250GB SSD Review ? The Worlds First TLC SSD Takes Center Stage]\n\n* [http://www.techspot.com/review/578-samsung-840-pro-ssd/ Samsung 840 Pro SSD Review]\n\n----\n\n=== Gnuplot Tips ===\n\n\n* How to unset key [http://people.duke.edu/~hpgavin/gnuplot.html]\n <pre>\n      Create a title:                  > set title \"Force-Deflection Data\" \n      Put a label on the x-axis:       > set xlabel \"Deflection (meters)\"\n      Put a label on the y-axis:       > set ylabel \"Force (kN)\"\n      Change the x-axis range:         > set xrange [0.001:0.005]\n      Change the y-axis range:         > set yrange [20:500]\n      Have Gnuplot determine ranges:   > set autoscale\n      Move the key:                    > set key 0.01,100\n      Delete the key:                  > unset key\n      Put a label on the plot:         > set label \"yield point\" at 0.003, 260 \n      Remove all labels:               > unset label\n      Plot using log-axes:             > set logscale\n      Plot using log-axes on y-axis:   > unset logscale; set logscale y \n      Change the tic-marks:            > set xtics (0.002,0.004,0.006,0.008)\n      Return to the default tics:      > unset xtics; set xtics auto\n</pre>\n\n* Mouse and hotkey support in interactive terminals\n\n: Interaction with the current plot via mouse and hotkeys is supported for the X11, OS/2 Presentation Manager, ggi and Windows terminals. See `mouse input` for more information on mousing. See help for bind for information on hotkeys. Also see the documentation for individual mousing terminals `ggi`, `pm`, `windows` and `x11`.\n\n: Here are briefly some useful hotkeys. Hit \'h\' in the interactive interval for help. Hit \'m\' to switch mousing on/off. Hit \'g\' for grid, \'l\' for log and \'e\' for replot. Hit \'r\' for ruler to measure peak distances (linear scale) or peak ratios (log scale), and \'5\' for polar coordinates inside a map. Zoom by mouse (MB3), and move in the zoom history by \'p\', \'u\', \'n\'; hit \'a\' for autoscale. Use other mouse buttons to put current mouse coordinates to clipboard (double click of MB1), add temporarily or permanently labels to the plot (middle mouse button MB2). Rotate a 3D surface by mouse. Hit spacebar to switch to the gnuplot command window.\n\n: Sample script: mousevariables.dem\n\n* [http://www.gnuplot.info/docs_4.0/gnuplot.html#Mouse_and_hotkey_support_in_interactive_terminals Mouse and hotkey support in interactive terminals -- Gnuplot info]\n\n=== NetApp Storage System Management Software ===\n\n* NetApp OnCommand System Manager [http://www.netapp.com/us/products/management-software/system-manager.aspx]\n:\n\n== ## bNote-2013-03-15 ==\n\n=== DailyPlanning 2013-03-15 ===\n\n* list of candidate tasks\n\n:- [V] HML basic concept study, 오늘 AP 주제에 대해 lightreading\n\n:- [V] 엄교수님께 특허 일정 전달\n\n:- Real IO trace 확보 작업\n::- [V] 김혁호 책임과 미팅 > 13:30 미팅 수행 (업무요청하기로 함)\n::- NetApp, Dell, EMC 측과 연락\n::- 지근영 대리에게 연락\n\n:- IOWA:: Bayesian Network study\n:- IOWA:: Neural Network study\n:- IOWA:: HMM study\n:- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n\n:- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n\n:- page cache to be revisited\n\n:- Data Placement 이슈: Media Difference (RAM,SSD,HDD) 외에 어떤 이슈가 있는가? Hadoop 같은 경우 노드 간 수평적 이동 이슈 있음. 매우 중요.\n\n:- 계획 외 업무들\n::- 이주평전문님의 본사팀과의 Conference Call 위해, 상무님 회의 대신 참석 (차주 화요일: 소장님께보고, 목요일: 부원장님께보고), 액션아이템 이전문님과 팀원께 전달.\n::- AP 세미나 참석 (최희열 전문)\n::- 팀 미팅: 소장님보고 자료 대응 방안 논의 -> IO Prediction 기반의 time 차원 제어로 공간적인 IO 속도 제약 극복 (마치 SS랩의 cooperative caching case처럼)\n\n\n=== HML Study:: \"Reducing the Dimensionality of Data with Neural Networks\" ===\n\n* Gradient descent [http://en.wikipedia.org/wiki/Gradient_descent]\n\n:- Gradient descent is a first-order optimization algorithm [http://en.wikipedia.org/wiki/First-order_approximation]\n\n:- Gradient descent to find the local minimum, gradient ascent to find the local maximum\n:: Gradient descent를 이용하여 function의 local \'\'\'minimum\'\'\'을 찾아내기 위해서는, 현재 지점에서의 function의 \'\'\'negative\'\'\' of the gradient (or of the approximate gradient)에 비례하는 taking steps를 한다. 만약 \'\'\'positive\'\'\' of the gradient에 비례하여 taking step한다면 그 function의 local \'\'\'maximum\'\'\'에 다가가게 된다. 이러한 절차는 gradient ascent라고 한다.\n\n\n* Gradient [http://en.wikipedia.org/wiki/Gradient]\n: Vector calculus에서, scalar field의 gradient는 다음 조건을 만족하는 vector field이다.\n:: direction은 scalar field의 증가분 (rate of increase)이 가장 최대가 되는 방향이다\n:: magnitude는 그 증가분이 된다 {{ In vector calculus, the gradient of a scalar field is a vector field that points in the direction of the greatest rate of increase of the scalar field, and whose magnitude is that rate of increase. }}\n\n\n* Orders of approximation [http://en.wikipedia.org/wiki/First-order_approximation]\n: terms for how precise an approximation is.\n: to indicate progressively more refined approximations: in increasing order of precision, a zeroth order approximation, a first order approximation, a second order approximation, and so forth\n: (Formally) an nth order of approximation\n:: one where the order of magnitude of the error is at most x^n, 혹은 big O notation으로 나타낸다면, error는 O(x^n) 이다.\n: detailed explanation with examples\n::- Zeroth-order (constant; a flat line with no slope; a polynomial of degree 0)\n::: x = [0, 1, 2]\n::: y = [3, 3, 5]\n::: y ~ f(x) = 3.67\n::- First-order (a linear approximation; straight line with a slope; a polynomial of degree 1)\n::: x = [0, 1, 2]\n::: y = [3, 3, 5]\n::: y ~ f(x) = x + 2.67\n::- Second-order (a quadratic polynomial; geometrically, a parabola; a polynomial of degree 2)\n::: x = [0, 1, 2]\n::: y = [3, 3, 5]\n::: y ~ f(x) = x^2 - x + 3\n\n=== Memo ===\n\n* 엄교수님과 연락 내용\n\n:- 엄교수님께 특허 일정 전달 (2013-03-15, 10\n:: 교수님, 안녕하세요? 기술원의 정명준전문입니다. 특허일정을 알아본 결과 3월 29일까지 직무발명서 시스템 등록을 하면 된다고 합니다. 앞으로 2주 정도 여유가 있네요 ^^ 그동안 실험결과를 정리하고 핵심아이디어 및 청구항을 잘 정리하면 될 것 같습니다. 차주 목요일 쯤에 한 번 조박사와 통화하여 기술상세/청구항/기존특허비교/침해적발등을 같이 논의해보면 어떨까합니다만 교수님 보시기에는 어떠신지요? 오늘도 멋진 하루 보내시구요, 항상 감사합니다. 정명준 드림.\n\n=== 연락처 (자주 사용하는) ===\n\n* 기술원 이주평 전문 : 01025984182, 010-2598-4182, #9956 : jupyung.lee@samsung.com\n* 기술원 신현정 전문 : 0173249294, 017-324-9294, #9747 : pharoah@samsung.com\n* 기술원 서정민 전문 : 01025441231, 010-2544-1231, #9817 : tony.seo@samsung.com\n* 기술원 구본철 전문 : 01091905907, 010-9190-5907, #9704 : bc.gu@samsung.com \n* 기술원 유개원 전문 : : gaewon.you@samsung.com\n* 기술원 최희열 전문 : 01096236578, 010-9623-6578, #9692 : heeyoul.choi@samsung.com\n* 기술원 문민영 전문 : , , #9716 :\n* 기술원 최영상 전문 : , , #9951 :\n* 기술원 박상도 전문 : , , #9586 :\n* 기술원 전바롬 전문 : , , #9547 :\n* 기술원 송인철 전문 : , , #9962 :\n* 기술원 박정현 연구원 : , , #9238 :\n\n* 기술원 심은수 상무 : 01020518077, 010-2051-8077, #9950 : eunsoo.shim@samsung.com\n* 기술원 서영완 전문 : 01030020208, 010-3002-0208, #9843 : sywpro@samsung.com\n* 기술원 유연아 사원 : 01090338452, 010-9033-8452, #9858 : yeonah78.yu@samsung.com\n\n* 삼성 SDS ESDM 인프라그룹 이형주 차장님: _ : hj001.lee@partner.samsung.com\n\n* 기술원 에어컨 안나올 때 (기술원 통합 방재 센터, 과장, 지원팀 > 환경안전그룹): #9120 :\n* VDI (SBC) 문제 있을 때 (VDI HelpDesk): #8272 : \n* 네트워크 안될 때 (방화벽 등) 어디로?: :\n* CLMS 시스템 문의 - 한지연 선임 / 서초 인사 CI 그룹: :\n\n* 서울대 컴퓨터공학부 엄현상 교수님 : 0162324667, 016-232-4667, 02-880-6755 : hseom@cse.snu.ac.kr\n* 서울대 컴퓨터공학부 조인순 박사 : 01051317886, 010-5131-7886, 02-880-9330 : insoonjo@gmail.com\n* 서울대 컴퓨터공학부 성민영 석사과정 : 01047245304, 010-4724-5304 : mysung@dcslab.snu.ac.kr\n\n=== HML (Hierarchical Machine Learning) AP (Advanced Program) ===\n\n* 세미나 일정\n\n{| border=\"1\"\n| 이름\n| 논문제목\n| 날짜\n|-\n| 최희열\n| Reducing the dimensionality of data with neural networks [http://www.cs.toronto.edu/~hinton/science.pdf]\n| 03월 15일 \n|-\n| 민윤홍	\n| A fast learning algorithm for deep belief nets [http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf]\n| 03월 22일 \n|-\n| 성재모	\n| Graphical Models \n| 03월 29일 \n|-\n| 정명준	\n| Hierarchical Temporal Memory including HTM Cortical Learning Algorithms \n| 04월 05일 \n|-\n| 박상도 	\n| How to Grow a Mind: Statistics, Structure, and Abstraction	\n| 04월 12일\n|-\n| 전바롬	\n| Learning Hierarchical Models of Scenes, Objects, and Parts\n| 04월 19일\n|-\n| 이호섭\n| Building high-level features using large scale unsupervised learning\n| 4월 26일\n|-\n| 박정현\n| High-Performance Neural Networks for Visual Object Classification\n| 05월 03일\n|-\n| 이호식\n| Deep Neural Networks for Acoustic Modeling in Speech Recognition\n| 5월 10일\n|-\n| 이예하\n| Unsupervised feature learning for audio classification using convolutioinal deep belief networks\n| 05월 24일\n|-\n| 송인철\n| Multimodal Deep Learning\n| 05월 31일\n|-\n|}\n\n== ## bNote-2013-03-14 ==\n\n\n=== HMM (Hidden Markov Model) ===\n\n* Reference: http://en.wikipedia.org/wiki/Hidden_Markov_model [http://en.wikipedia.org/wiki/Hidden_Markov_model]\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n\nhttp://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n\n----\n* R의 HMM 기능들\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n* References\n:* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n:* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n:* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n:* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n:* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n:* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n:* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n:* HTK [http://htk.eng.cam.ac.uk/register.php]\n:* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n== ## bNote-2013-03-13 ==\n\n\n=== Supercom Usage Statistics ===\n\n <pre>\n----------------------------------------------------------\n| During : 20130304 ~ 20130310                            |\n| Username : a1mjjung , Application(Total jobs) : matlab(1)\n----------------------------------------------------------\nTotal RUN time : 2 min 16 secs\nAverage RUN time : 2 min 16 secs\nMaximum RUN time : 2 min 16 secs\nAverage Wait time 1 secs\nMaximum Wait time 1 secs\n ---------------------------------------------------------\n|                       Application(Total jobs) : unix(14)\n----------------------------------------------------------\nTotal RUN time : 13 min 32 secs\nAverage RUN time : 58 secs\nMaximum RUN time : 3 min 23 secs\nAverage Wait time 1 secs\nMaximum Wait time 2 secs\n ---------------------------------------------------------\n\n</pre>\n\n=== ACM Transactions on Storage ===\n\n삼성 SDS 강석우 상무님 요청으로 우리 팀이 Review하게 됨.\n\n* [http://mc.manuscriptcentral.com/tos Welcome to the ACM Transactions on Storage manuscript submission site]\n\n== ## bNote-2013-03-12 ==\n\n\n=== SNIA Real IO Traces ===\n\n\n----\n==== Microsoft Production MSNStorageFileServer ( msnfs ) ====\n\n* Summary (Reads/Writes - All)\n: 2008-03-10 01:00 + 6 hours\n: Total # of IOs\n:: = 29,345,085 (total)\n:: = 19,729,611 (reads) + 9,615,474 (writes)\n:: = 29345085 = 19729611 + 9615474\n: Average IOPS\n:: = 1358.56 (= 29345085 / (6 * 3600))\n: Average interval time between IOs\n:: = 736 micro-seconds (= (6 * 3600 * 10^6 ) / 29345085)\n\n\n* Summary (Reads)\n <pre>\na1mjjung@secm:[R] $ pwd\n/home/X0101/a1mjjung/x/iowa.sidewinder/sidewinder/microsoft_msn_filesrvr_6h/MSNStorageFileServer/tdir/iowa-mw30m/R\n\na1mjjung@secm:[R] $ grep __valu__sig__ f030.infile_R.iowa.anal_s0010 \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  17\n__valu__sig__ _n_o_sigaddrs :  173401\n__valu__sig__ _sigioc_acc :  5005517\n__valu__sig__ _sigaddrs_efficiency :  28.8667135714\n__valu__sig__ _n_o_addr_total :  5053016\n__valu__sig__ _ioc_total :  19729611\n</pre>\n\n\n* Trace Log Field Information\n <pre>\n\n       1,         2,                 3,        4,      5,          6,      7,           8,       9,       10,          11,      12,       13,         14,       15\nDiskRead, TimeStamp, ProcessName (PID), ThreadID, IrpPtr, ByteOffset, IOSize, ElapsedTime, DiskNum, IrpFlags, DiskSvcTime, I/O Pri,  VolSnap, FileObject, FileName\n\nDiskWrite, TimeStamp, ProcessName (PID), ThreadID, IrpPtr, ByteOffset, IOSize, ElapsedTime, DiskNum, IrpFlags, DiskSvcTime, I/O Pri, VolSnap, FileObject, FileName\n</pre>\n\n\n* [[Trace Data Preprocessing Screenshot - Microsoft Production Trace - MSN FileServer]]\n\n\n----\n\n==== MSR Cambridge IO Traces ====\n\n\n* Summary\n: 22.4GB Trace Data from Data center servers\n:: \'\'\'13 servers, 36 volumes, 179 disks, 1 week\'\'\'\n\n\n* Backgrounds\n: Many enterprise servers are less I/O intensive than TPC benchmarks, which are specifically designed to stress the system under test. Enterprise workloads also show significant variation in usage over time, for example due to diurnal patterns.\n: In order to understand better the I/O patterns generated by standard data center servers, we instrumented the core servers in our building\'s data center to generate per volume block-level traces for one week.\n\n\n* References\n:* \"Write Off-Loading: Practical Power Management for Enterprise Storage\" - FAST 2008 [http://www.usenix.org/event/fast08/tech/narayanan.html]\n::- Messages from this paper\n::: The traditional view is that enterprise workloads make spinning disks down ineffective because idle periods are too short. We analyzed block-level traces from 36 volumes in an enterprise data center for one week and concluded that significant idle periods exist, and that they can be further increased by modifying the read/write patterns using write off-loading. Write off-loading allows write requests on spun-down disks to be temporarily redirected to persistent storage elsewhere in the data center.\n:* \"Self-organizing Storage (SOS) Project - Software\" \n::- Tools: nfsdump/nfsscan [http://www.eecs.harvard.edu/sos/software/index.html]\n::- SOS Project Traces [http://www.eecs.harvard.edu/sos/traces.html]\n\n\n* Trace Log Field Names\n: Timestamp, Hostname, DiskNumber, Type(Read/Write), Offset, Size, ResponseTime\n:- Timestamp: the time the I/O was issued in \"Windows filetime\"\n:- Hostname: the hostname (should be the same as that in the trace file name)\n:- DiskNumber: the disknumber (should be the same as in the trace file name)\n:- Type: \"Read\" or \"Write\"\n:- Offset: starting offset of the I/O in bytes (from the start of the logical disk)\n:- Size: transfer size of the I/O request in bytes\n:- ResponseTime: time taken by the I/O to complete, in \"Windows filetime\"\n\n\n* Trace Log Example\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ head -100 rsrch_0.csv \n\n128166372003061629,rsrch,0,Read,7014609920,24576,41286\n128166372016382155,rsrch,0,Write,1317441536,8192,1963\n128166372026382245,rsrch,0,Write,2436440064,4096,1835\n128166372036348580,rsrch,0,Write,3196526592,57344,35436\n128166372036379390,rsrch,0,Write,3154132992,4096,4626\n128166372036382264,rsrch,0,Write,3154124800,4096,1752\n128166372053100669,rsrch,0,Write,7609925632,10240,2053\n128166372053101032,rsrch,0,Write,15282630656,16384,1691\n128166372053101054,rsrch,0,Write,7612473344,16384,1668\n</pre>\n\n\n* IO Trace Nodes\n{| border=\"1\"\n| Node\n| Description\n| # of volumes\n| # of IOs\n|-\n| usr\n| User home directories\n| 3\n|-\n| proj\n| Project directories\n| 5\n|-\n| prn\n| Print server\n| 2\n|-\n| hm\n| Hardware monitoring\n| 2\n|-\n| rsrch\n| Research projects\n| 3\n|-\n| prxy\n| Firewall/WebProxy\n| 2\n|-\n| src1\n| Source control\n| 3\n|-\n| src2\n| Source control\n| 3\n|-\n| stg\n| Web staging\n| 2\n|-\n| ts\n| Terminal server\n| 1\n|-\n| web\n| Web/SQL server\n| 4\n|-\n| mds\n| Media server\n| 2\n|-\n| wdev\n| Test web server\n| 4\n|-\n|}\n\n\n* # of IOs\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ wc -l *.csv\n    3993316 hm_0.csv\n     609311 hm_1.csv\n    1211034 mds_0.csv\n    1637711 mds_1.csv\n    5585886 prn_0.csv\n   11233411 prn_1.csv\n    4224524 proj_0.csv\n   23639742 proj_1.csv\n   29266482 proj_2.csv\n    2244644 proj_3.csv\n    6465639 proj_4.csv\n   12518968 prxy_0.csv\n  168638964 prxy_1.csv\n    1433655 rsrch_0.csv\n      13780 rsrch_1.csv\n     207587 rsrch_2.csv\n   37415613 src1_0.csv\n   45746222 src1_1.csv\n    1907773 src1_2.csv\n    1557814 src2_0.csv\n     657774 src2_1.csv\n    1156885 src2_2.csv\n    2030915 stg_0.csv\n    2196861 stg_1.csv\n    1801734 ts_0.csv\n    2237889 usr_0.csv\n   45283980 usr_1.csv\n   10570046 usr_2.csv\n    1143261 wdev_0.csv\n       1055 wdev_1.csv\n     181266 wdev_2.csv\n        682 wdev_3.csv\n    2029945 web_0.csv\n     160891 web_1.csv\n    5175368 web_2.csv\n      31380 web_3.csv\n  434212008 total\n</pre>\n\n\n* List of traces\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ l\ntotal 22956880\ndrwxrwxr-x 2 a1mjjung X0101       4096 Mar 12 16:07 ./\ndrwxrwxr-x 3 a1mjjung X0101       4096 Mar 12 16:04 ../\n-r--r--r-- 1 a1mjjung X0101       1262 Oct 31  2008 DISCLAIMER.txt\n-r--r--r-- 1 a1mjjung X0101       1712 Oct 31  2008 MD5.txt\n-r--r--r-- 1 a1mjjung X0101       1815 Oct 31  2008 README.txt\n-r--r--r-- 1 a1mjjung X0101  202270441 Oct 30  2008 hm_0.csv\n-r--r--r-- 1 a1mjjung X0101   29765989 Oct 30  2008 hm_1.csv\n-r--r--r-- 1 a1mjjung X0101   63955502 Oct 30  2008 mds_0.csv\n-r--r--r-- 1 a1mjjung X0101   88337038 Oct 30  2008 mds_1.csv\n-r--r--r-- 1 a1mjjung X0101  291595716 Oct 30  2008 prn_0.csv\n-r--r--r-- 1 a1mjjung X0101  597136927 Oct 30  2008 prn_1.csv\n-r--r--r-- 1 a1mjjung X0101  233038754 Oct 30  2008 proj_0.csv\n-r--r--r-- 1 a1mjjung X0101 1305533029 Oct 30  2008 proj_1.csv\n-r--r--r-- 1 a1mjjung X0101 1614727432 Oct 30  2008 proj_2.csv\n-r--r--r-- 1 a1mjjung X0101  119913539 Oct 30  2008 proj_3.csv\n-r--r--r-- 1 a1mjjung X0101  350117046 Oct 30  2008 proj_4.csv\n-r--r--r-- 1 a1mjjung X0101  658840568 Oct 30  2008 prxy_0.csv\n-r--r--r-- 1 a1mjjung X0101 9043988744 Oct 30  2008 prxy_1.csv\n-r--r--r-- 1 a1mjjung X0101   77717781 Oct 31  2008 rsrch_0.csv\n-r--r--r-- 1 a1mjjung X0101     755814 Oct 31  2008 rsrch_1.csv\n-r--r--r-- 1 a1mjjung X0101   11154823 Oct 31  2008 rsrch_2.csv\n-r--r--r-- 1 a1mjjung X0101 2077380082 Oct 31  2008 src1_0.csv\n-r--r--r-- 1 a1mjjung X0101 2536095762 Oct 31  2008 src1_1.csv\n-r--r--r-- 1 a1mjjung X0101  101236500 Oct 31  2008 src1_2.csv\n-r--r--r-- 1 a1mjjung X0101   82511780 Oct 31  2008 src2_0.csv\n-r--r--r-- 1 a1mjjung X0101   35607343 Oct 31  2008 src2_1.csv\n-r--r--r-- 1 a1mjjung X0101   63026546 Oct 31  2008 src2_2.csv\n-r--r--r-- 1 a1mjjung X0101  105682669 Oct 31  2008 stg_0.csv\n-r--r--r-- 1 a1mjjung X0101  116358242 Oct 31  2008 stg_1.csv\n-r--r--r-- 1 a1mjjung X0101   93309044 Oct 31  2008 ts_0.csv\n-r--r--r-- 1 a1mjjung X0101  118478959 Oct 31  2008 usr_0.csv\n-r--r--r-- 1 a1mjjung X0101 2451360295 Oct 31  2008 usr_1.csv\n-r--r--r-- 1 a1mjjung X0101  574047026 Oct 31  2008 usr_2.csv\n-r--r--r-- 1 a1mjjung X0101   60262085 Oct 31  2008 wdev_0.csv\n-r--r--r-- 1 a1mjjung X0101      56014 Oct 31  2008 wdev_1.csv\n-r--r--r-- 1 a1mjjung X0101    9593588 Oct 31  2008 wdev_2.csv\n-r--r--r-- 1 a1mjjung X0101      35650 Oct 31  2008 wdev_3.csv\n-r--r--r-- 1 a1mjjung X0101  107571350 Oct 31  2008 web_0.csv\n-r--r--r-- 1 a1mjjung X0101    8507311 Oct 31  2008 web_1.csv\n-r--r--r-- 1 a1mjjung X0101  276121116 Oct 31  2008 web_2.csv\n-r--r--r-- 1 a1mjjung X0101    1649607 Oct 31  2008 web_3.csv\n</pre>\n\n\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ wc -l *.csv\n    3993316 hm_0.csv\n     609311 hm_1.csv\n    1211034 mds_0.csv\n    1637711 mds_1.csv\n    5585886 prn_0.csv\n   11233411 prn_1.csv\n    4224524 proj_0.csv\n   23639742 proj_1.csv\n   29266482 proj_2.csv\n    2244644 proj_3.csv\n    6465639 proj_4.csv\n   12518968 prxy_0.csv\n  168638964 prxy_1.csv\n    1433655 rsrch_0.csv\n      13780 rsrch_1.csv\n     207587 rsrch_2.csv\n   37415613 src1_0.csv\n   45746222 src1_1.csv\n    1907773 src1_2.csv\n    1557814 src2_0.csv\n     657774 src2_1.csv\n    1156885 src2_2.csv\n    2030915 stg_0.csv\n    2196861 stg_1.csv\n    1801734 ts_0.csv\n    2237889 usr_0.csv\n   45283980 usr_1.csv\n   10570046 usr_2.csv\n    1143261 wdev_0.csv\n       1055 wdev_1.csv\n     181266 wdev_2.csv\n        682 wdev_3.csv\n    2029945 web_0.csv\n     160891 web_1.csv\n    5175368 web_2.csv\n      31380 web_3.csv\n  434212008 total\n</pre>\n\n=== Real IO Trace 수집 (삼성 SDS 강석우 상무) ===\n\n* 삼성SDS 강석우 상무 (클라우드 플랫폼 팀장)\n\n* 삼성 SDS 박성록 수석보 (클라우드 플랫폼 운영그룹)\n\n* 진행 현황\n\n:* EMC\n::- 박정원 과장에게 연락함. 담당자인 이임호 부장 소개해줌.\n::- 이임호 부장은 아직 연락 못함\n\n:* Dell\n::- 지근영 대리와 통화/메일 (TraceLog 요청사항을 Dell에게 전달하겠다고 함)\n:::- Trace Log 데이터 예제 전달\n\n:* NetApp\n::- 김주영 과장과 통화/메일\n:::- Trace Log 데이터 예제 전달\n\n:* Supercom 센터\n\n\n* 스토리지 상주 지원 인력\n:* EMC\n::- 노영변부장 : 010-5384-6426 (EMC KOREA 삼성 PMO)\n::- 박정원과장 : 010-9052-7805 (EMC KOREA 센터 상주지원)(연락하였음)\n:::- Phone call, IO trace 수집에 대해 설명 -> 담당자 연결 시켜줌 (이임호 부장)\n::- 이임호 부장: 010-3203-7823 (EMC 삼성전담, 프리세일즈 기술컨설턴트)\n:::- Not yet connected (another phone call)\n:* Dell\n::- 이정민차장 : 010-2908-0759 \n::- 지근영대리 : 010-9530-2819 (한빛마이크로 센터상주지원)\n:::- DELL 기술지원 업무(수원ICT센터) 상주 : (6층 전산실 DELL EQL스토리지 기술지원)\n:::- 연락처: <dell.korea@samsung.com> (443-803  경기?수원시?영통구?매탄3동 410-1 삼성SDS 수원ICT S/W연구소 4층)\n:::- 3/12 연락하였음. (전화/메신저/메일로 상황 설명 하였으며, 현재 Dell에 요청 전달된 상태임)\n:* NetApp(상주지원 없음)\n::- 최병석이사 : 010-8998-7138(NetApp Korea)\n::- 김주영과장 : 010-9577-4272 (아리라)\n:::- Email sent, Phone call\n\n <pre>\n\n\n------- Original Message -------\n\nSender : 강석우<sukwkang@samsung.com> 상무/팀장/클라우드플랫폼팀/삼성SDS\n\nDate : 2013-03-08 14:06 (GMT+09:00)\n\nTitle : Re: Re: Fwd: 스토리지 벤더 현황입니다.\n\n \n\n아마 스토리지 벤더 별로 자체 테스트 시스템이 있기 때문에 테스트 시스템에서 로그수집이 가능할 겁니다. 아니면 본사에서 이미 가지고 있을수도 있구요.\n\n \n\n강석우 드림\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-03-08 14:02 (GMT+09:00)\n\nTitle : Re: Fwd: 스토리지 벤더 현황입니다.\n\n \n\n대단히 감사합니다, 강 상무님.\n\n \n\n심은수 드림\n\n \n\n------- Original Message -------\n\nSender : 강석우<sukwkang@samsung.com> 상무/팀장/클라우드플랫폼팀/삼성SDS\n\nDate : 2013-03-08 13:58 (GMT+09:00)\n\nTitle : Fwd: 스토리지 벤더 현황입니다.\n\n \n\n심상무님,\n\n \n\n아래의 스토리지 벤더에 연락해서 요청을 하시면 됩니다. SDS의 클라우드 팀 강석우 상무 소개로 연락했다고 말씀하시구요. 만약 협조를 잘 안하면 저에게 다시 연락주세요. 제가 협조하도록 만들겠습니다. :-)\n\n \n\n강석우 드림\n\n \n\n------- Original Message -------\n\nSender : 박성록<rocky@samsung.com> 수석보/클라우드플랫폼운영그룹/삼성SDS\n\nDate : 2013-03-08 13:48 (GMT+09:00)\n\nTitle : 스토리지 벤더 현황입니다.\n\n \n\n \n\n안녕하십니까?  클라우드플랫폼운영그룹 박성록수석보입니다.\n\n \n\n스토리지 상주 지원 인력입니다.\n\n1. EMC \n\n  - 노영변부장 : 010-5384-6426 (EMC KOREA 삼성 PMO)\n\n  - 박정원과장 : 010-9052-7805  (EMC KOREA 센터 상주지원)\n\n \n\n2. Dell\n\n  - 이정민차장 : 010-2908-0759 \n\n  - 지근영대리 : 010-9530-2819 (한빛마이크로 센터상주지원)\n\n \n\n3. NetApp(상주지원 없음)\n\n - 최병석이사 : 010-8998-7138(NetApp Korea)\n\n - 김주영과장 : 010-9577-4272 (아리라)\n  \n</pre>\n\n=== Multimedia Streaming vs. IOWA-based PDP ===\n\n* eMBMS: Evolved Multimedia Broadcast Multicast Service\n: Expway\'s eMBMS [http://blog.expway.com/ Expway\'s eMBMS Solution Allows Mobile Operators to off-Load Mobile Traffic by 20%]\n:- Key Technical Features\n::- FLUTE (File Delivery over Unidirectional Transport) protocol\n::- Forward Error Correction\n::- File Repair\n::- Service Announcement\n::- DASH Video Protocol\n:- Mobile Traffic Prediction (in 2016)\n::- 70% of mobile traffic will be video\n::- 10% of all TV viewing will be on tablets\n::- This equauls to 25 million DVDs sent every single hour\n:- Expway Company\n::- 7 years of experience focused on mobile broadcast software\n:::- Robust and Mature Products\n:::- Optimized Bandwidth Usage\n:::- Low Footprint Terminal Stack\n\n\n\n* DASH: Dynamic Adaptive Streaming over HTTP (a.k.a MPEG-DASH)\n: Internet 상으로 media content에 대한 고품질 streaming을 가능하게 하는 기술 (기존 HTTP 웹서버들로부터 deliver됨)\n: 컨텐츠를 small HTTP-based file segement들로 쪼개어 다룬다는 점에서 Apple의 HTTP Live Streaming (HLS)와 유사하다고 볼 수 있음. (컨텐츠 예: movie, sports event의 live broadcast 등)\n\n\n* HTTP Live Streaming (HLS): HTTP-based media streaming communications protocol (QuickTime과 iOS software의 일부로 Apple이 구현함)\n:- 동작원리\n:: overall stream을 작은 HTTP-based file downloads로 쪼개어 다룬다 (각각의 download는 overall potentially unbounded transport stream의 하나의 short chunk를 담당). stream 세션 시작 시에는, available한 variouis sub-stream들에 대한 metadata를 포함하고 있는 extended M2U (m3u8) playlist를 download한다.\n:- 장점\n:: 표준화된 HTTP transaction만을 사용하기 때문에, HLS는 일반 HTTP traffic을 허용하는 firewall, proxy server들은 모두 통과 가능 (RTP와 같은 UDP 기반 프로토콜은 그렇지 못함)하며, 널리 사용 가능한 CDN 인프라를 통해서 쉽게 deliver될 수 있음.\n:- 특징\n:: AES와 같은 암호화 메커니즘 및 HTTPS 기반의 secure key distribution 방법, simple DRM 시스템을 제공함\n:: HLS의 이후 버전에서는 [[trick mode]][http://en.wikipedia.org/wiki/Trick_mode] 기반의 fast-forward/rewind 및 subtitle의 통합도 지원할 예정임 (2013-03-12 현재)\n:- 표준화\n:: Apple에서는 HLS (HTTP Live Streaming)를 Internet Draft로 작성하였음. (first stage in the process of submitting it to the IETF, as an Informational Request For Comments)\n\n\n\n=== Technical Articles ===\n\n* 상무님께서 보내주신 \"Future of Cloud Storage\" 메일에 대한 신전문님 정리\n\n아래 글들을 읽은 소감 or 요약입니다.\n글들이 이것저것 다양하네요.\n\n* [http://gigaom.com/2011/10/29/the-future-beyond-the-cloud-is-in-our-hands/ The future beyond the cloud is in our hands]\n\n: \"Cloud is all\"의 관점은 mobile device가 ubiquitous, unlimited, low-cost conntection 인데요.. 이런 생각은 현재의 mobile network의 한계와 mobile device의 빠른 발전까지 고려치 않은 비전이라는 비판입니다.\n\n:# 첫째, LTE 같은 mobile network이 향후 cloud를 모두 책임지지는 못한다는 것이고요. (WAN에서의 Bandwidth란 이미 wireless network을 사용하고 있는 user가 쓰는 용량으로 계산된 것이므로)\n:# 둘째, 향후 mobile은 n-core의 multi-gigaherts procesor와 1TB 이상의 local storage 이므로 이를 cloud에서 활용하자는 얘기입니다.\n\n: 예전의 장수석님의 Mobile Cloud 과제가 생각납니다. ^^; Mobile의 능력과 wireless network를 활용하자는 겁니다.\n::- what if those devices can talk to one another in a peer-to-peer or mesh network? \n::- What’s the aggregate power and capability of billions of these things, especially if there will be ways for them to work with and talk to one another both alone and in conjunction with cloud-based services?\n\n: 예로 든 것이 Amazon Silk과 Google의 Offline Mail입니다.\n::- Silk는 클라우드를 이용해 acceleration하는 브라우저입니다. 클라우드에서 사용자의 웹 패턴을 분석하여 미리 preloading하고 mobile에 최적화하여 속도를 높이는 건데요.역으로 mobile에 맞게 웹페이지를 작게 축소해서 mobile에 가져오기 때문에 클라우드가 동작하지 않더라도 속도를 유지시킬 수 있다고 합니다. 클라우드와 mobile간에 서로 보완하는 거지요.\n::- offline Mail은 말그대로 메일서버가 되지 않아도 전송외에 모든 메일 관리가 가능하도록 mobile에 데이터를 미리 다 가져다 놓는 겁니다.\n \n\n* [http://blog.cloudhq.net/post/36559564253/the-future-of-cloud-storage The Future of Cloud Storage]\n\n: 정확히는 이런 문제점이 나올 거다라는 cloud issue에 관한 prediction이라고 볼 수 있습니다.\n\n:# End of Files and Folders : 클라우드 서비스들이 전통적인 File이나 folder 개념을 사용하지 않고 자기만의 interface, 데이터 분류 방식을 쓰므로 나중에 cloud 간의 호환성 문제가 발생함\n:# End of Free Storage: cloud storage 시장이 mature된다면 결국 free storage는 없어질 거다. 지금은 홍보용인 거다.\n:# Data ownership Troubles : 클라우드에 데이터가 올라간 순간 사용자는 data에 대한 control를 잃어버린다.\n:# Encryption will become necessary : encryption이 필수..\n\n\n* [http://www.cs.berkeley.edu/~kubitron/talks/printable/CISCO-cloudstore.pdf Lessons from the OceanStore Project - UC Berkeley]\n\n: 첫번째 글에서 좀 더 나아간 형태..Cloud 시대를 맞이한 P2P의 재조명 되겠습니다. 과거 P2P가 공짜 미디어를 훔치는 수단이었다면 새로운 P2P는 extreme scale를 제공할 수 잇는 system의 새 디자인으로 사용할 수 있다는 얘기입니다. 여러 Client뿐 아니라 여러 Cloue Storage Provider들도 같이 참여하면 서로 윈윈할 수 있다는 얘기입니다.\n\n \n* [http://www.cloudsigma.com/blog/13-the-future-of-cloud-storage The Future of Cloud Storage (and what is wrong with the present)]\n: SAN도 local Storage도 이제 끝났다.  Distributed Replicated Block Device(DRBD) 라고 얘기하고 있습니다만, converged server+storage 형태로 Server 노드에 Storage까지 합체한 형태로 죽 붙이고 replication을 다른 node에 함으로써 latency, fail over 등의 Converged architecture장점을 얘기하고 있네요. Open Solution으로는 sheepdog이 있고 상용화버전으로는 Amplidata가 있다고 합니다.\n::>> open source로서 Linux Kernel (2.6.33 version 부터) 에 구현되어 있는 DRBD도 있음. (아래 그림 참고) HA (High Availability) 제공에 초점이 맞춰져 있고, replication mode도 fully-synchronous와 asynchronous mode, 그리고, 그 사이에, protection level과 performance 간의 tradeoff를 고려한, semi-synchronous mode (memory synchronous mode 라고도 합니다)가 지원됨.  이 Linux의 DRBD는, 우리 RACS 1 의 기술이 networked storage로 확장될 때 매우 유용한 기술적 base가 될 수 있을 것 같습니다. (마치 Local Disk Array에 대한 RACS 1이 Linux MD를 활용하여 구현되고 있는 것처럼, Networked Replicated Disk Array 기술 구현 시에 Linux DRBD를 잘 활용할 수도 있을 것임) [http://www.ibm.com/developerworks/linux/library/l-drbd/index.html High availability with the Distributed Replicated Block Device]\n\n <pre>\n\n------- Original Message -------\n\nDate : 2013-03-11 10:44 (GMT+09:00)\nTitle : Fwd: future of cloud storage\n\nFYI, \n\n저희 궁극의 시나리오가 \'무한대의 local 저장용량을 제공하는\' pervasive storage 쪽으로 잡히면서,\n상무님이 cloud storage의 핵심기술에 대한 깊은 이해를 요구하고 계십니다.\n아래 메일도 참고하시고 시간을 내어 관련 기술에 대한 study를 진행하도록 하겠습니다.\n\n\n------- Original Message -------\nDate : 2013-03-08 20:55 (GMT+09:00)\nTitle : future of cloud storage\n\n이 전문,\n\n\n아마 이 전문도 구글 검색하면 금방 찾을 글들일텐데, 하여간 내가 본 것들입니다.\n바로 아래 것은 많은 시사점을 주는 것 같습니다.\n\nhttp://gigaom.com/2011/10/29/the-future-beyond-the-cloud-is-in-our-hands/\n\n \n그 외의 글들.\n\nhttp://blog.cloudhq.net/post/36559564253/the-future-of-cloud-storage\n\nhttp://www.cs.berkeley.edu/~kubitron/talks/printable/CISCO-cloudstore.pdf\n\nhttp://www.cloudsigma.com/blog/13-the-future-of-cloud-storage\n\n \n우리가 pervasive storage로 서비스 시나리오를 잡은 만큼, 그 분야의 서비스/기술 발전 전망을 할 수 있어야겠습니다.\n</pre>\n\n== ## bNote-2013-03-11 ==\n\n\n=== Akamai - CDN Acceleration ===\n\n* 관련 기사들\n:# [[아카마이, \"쌩쌩 웹사이트 만들려면\"]] [http://www.bloter.net/archives/141513 bloter.net, 2013-01-24]\n:# [[아카마이, \'블레이즈\' 인수 - 인텔리전트 플랫폼 강화]] [http://www.bloter.net/archives/95561 bloter.net, 2012-02-09]\n:# [[아카마이, \"CDN 넘어 하이퍼커넥티드로\"]] [http://www.bloter.net/archives/92711 bloter.net, 2012-01-19]\n:# [[아카마이, CDN 장악 가속화 ... 코텐도 인수설]] [http://www.bloter.net/archives/85622 bloter.net, 2011-11-28]\n:# [[아카마이, \"스트리밍/보안 서비스를 빠르고 안전하게\"]] [http://www.bloter.net/archives/67854 bloter.net, 2011-07-13]\n:# [[네트워크 업계 \"기다렸다, 런던올림픽\"]] [http://www.bloter.net/archives/92823 bloter.net, 2012-01-22]\n\n\n=== 과제 목표 설정 ===\n\n* IOWA-based PDP 과제 목표 metric\n: 경쟁사, 기술원 현수준, 기술원 목표수준, 접근방식 등\n:- 필수 고려 사항\n:: 어째서 그러한 목표 수준을 잡았는지?\n:- 점검 사항\n:: EMC FAST 등 Automatic Tiering 기술의 현수준 파악 필요\n:- 접근 방식의 독창성/진보성\n::- ProactiveDP가 MWC 2013에 언급된 eMBMS, DASH 등과 어떤 차별점을 갖는가?\n:::- eMBMS (Evolved Multimedia Broadcast Multicast Service): LTE를 이용해 수많은 사용자에게 방송 컨텐츠를 동시에 효과적으로 배포하는 기술임. 스트리밍 전송 및 비 피크타임에 전송해 단말에 저장된 형태로 있다가 사용자가 원할 때 시청. <span style=\"color:blue\">level of intelligence</span>가 중요한 비교점이 될 수 있음.\n:::- DASH (Dynamic Adaptive Streaming over HTTP)\n::- CDN (Content Delivery Network)에서 Akamai와는 어떻게 차별되나?\n\n* 기술 진화 고민\n: evolution of technology as a driving force from old-age to the pervasive storage\n\n=== IOWA: bpo_a.20130305_104633.real_whole_trace.log ===\n\n* Trace information\n:- Machine under IOTracing: radiohead (Linux 3.2.0-34)\n:- Tracing time: 72 hours\n:- Trace log file: /x/var/iowa/sidewinder/iowa/preproc/tdir/myrealtrace/bpo_a.20130305_104633.real_whole_trace.log\n\n==== further Write pattern analysis (LBA-to-name processing, for top 18 addresses) ====\n\n* IO statistical summary\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  1115\n__valu__sig__ _n_o_sigaddrs :  18\n__valu__sig__ _sigioc_acc :  40010\n__valu__sig__ _sigaddrs_efficiency :  2222.77777778\n__valu__sig__ _n_o_addr_total :  40979\n__valu__sig__ _ioc_total :  157632\n</pre>\n\n\n* top 18 addresses\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__list__hitnum_and_addrs__.log.v1 | sort -n  | tail -20\n1093 : [1266683272]\n1105 : [1267153912]\n1113 : [1661223296]\n\n---- top 18 addr starts below ----\n1115 : [12856320, 12857344]\n1237 : [2480]\n1686 : [1266681984]\n1693 : [1266683032]\n1730 : [1266683160]\n1732 : [2352]\n1941 : [1661223328]\n2077 : [2048]\n2110 : [1267095912]\n2188 : [327569400]\n2265 : [1266683048]\n2309 : [327568408]\n2342 : [327568936]\n2364 : [1661223320]\n2395 : [1661223136]\n2526 : [1401209744]\n7185 : [1661223128]\n</pre>\n\n\n* Bar Graph for Hits_per_Addr (MyRealTrace, Radiohead, 72h)\n<!-- [[File:Myrealtrace on radiohead 72h cdst hits per addr impulses.v01.png | 500px]] -->\n[[File:Myrealtrace on radiohead 72h cdst hits per addr impulses.v01.png]]\n\n\n===== analysis table =====\n\n{| border=\"1\"\n| address\n| # of hits\n| device node\n| process accessed\n| periodicity (1000 IOs)\n| corresponding file/dir\n| notes\n|-\n| 1661223128\n| 7185\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.964271213967\n| /home/hendrix/.config/google-chrome/Default/Cookies-journal\n| inode (39585621)\n|-\n| 1401209744\n| 2526\n| (8,1) /dev/sda1\n| BrowserBlocking\n| 0.926207876573\n| /home/hendrix/.config/google-chrome/Default/Local Storage/https_plus.google.com_0.localstorage-journal\n| inode (39714913)\n|-\n| 1661223136\n| 2395\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.957927324401\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 1661223320\n| 2364\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.957927324401\n| /home/hendrix/.config/google-chrome/Default/Cookies-journal\n| inode (39585621)\n|-\n| 327568936\n| 2342\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.938895655704\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 327568408\n| 2309\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.951583434836\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 1266683048\n| 2265\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.957927324401\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 327569400\n| 2188\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.894488428745\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 1267095912\n| 2110\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.951583434836\n| /home/hendrix/.config/google-chrome/Default/Cookies-journal\n| inode (39585621)\n|-\n| 2048\n| 2077\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.94523954527\n| INVALID BLOCK\n| inode (N/A)\n|-\n| 1661223328\n| 1941\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.957927324401\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 2352\n| 1732\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.957927324401\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 1266683160\n| 1730\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.938895655704\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 1266683032\n| 1693\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.964271213967\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 1266681984\n| 1686\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.957927324401\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 2480\n| 1237\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.951583434836\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 12856320\n| 1115\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.94523954527\n| /home/hendrix/.cache/google-chrome/Default/Cache/data_1\n| inode (39585855)\n|-\n| 12857344\n| 1115\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.94523954527\n| /home/hendrix/.cache/google-chrome/Default/Cache/index\n| inode (39585853)\n|-\n|}\n\n===== analysis result (processing output) =====\n\n <pre>\nblusjune@radiohead:[top_hot_18] $ l\ntotal 16\ndrwxrwxr-x 2 blusjune blusjune 4096 Mar 11 18:44 ./\ndrwxrwxr-x 3 blusjune blusjune 4096 Mar 11 14:49 ../\n-rwxr-xr-x 1 blusjune blusjune 2566 Mar 11 18:40 .bdx.0100.y.lba_to_name.sh*\n-rw-rw-r-- 1 blusjune blusjune 1768 Mar 11 18:44 .conf.lba_to_name.sh\n\nblusjune@radiohead:[top_hot_18] $ _BDX \nBDX[ /x/var/iowa/tdir/s05/w_ptrn_analysis/top_hot_18 ]# 0100 : lba_to_name\n\n\'_conf__target_lba_list\' is from:\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ tail -29 __simout.sigio_25.iowsz_100.t1_1000.__list__hitnum_and_addrs__.log.v2\n708 : [1795164168]\n840 : [12856336]\n841 : [1270876464]\n842 : [1661223968]\n913 : [1266682992]\n929 : [1270876456]\n943 : [1266681864]\n949 : [1266751536]\n956 : [1266751544]\n1093 : [1266683272]\n1105 : [1267153912]\n1113 : [1661223296]\n1115 : [12856320, 12857344]\n1237 : [2480]\n1686 : [1266681984]\n1693 : [1266683032]\n1730 : [1266683160]\n1732 : [2352]\n1941 : [1661223328]\n2077 : [2048]\n2110 : [1267095912]\n2188 : [327569400]\n2265 : [1266683048]\n2309 : [327568408]\n2342 : [327568936]\n2364 : [1661223320]\n2395 : [1661223136]\n2526 : [1401209744]\n7185 : [1661223128]\n\n#>> configuration started\n#<< _conf__target_dev (e.g., /dev/sda) : /dev/sda\n#<< _conf__target_dev_part (e.g., /dev/sda1) : /dev/sda1\n----\n/dev/sda1   *        2048  3907026943  1953512448   83  Linux\nBlock size:               4096\n----\n#<< _conf__lba_fs_start: 2048\n#<< _conf__fblk_size: 4096\n#<< _conf__sector_size [512]: \n#>> configuration completed\n\n#>> START Processing\n/dev/sda1: 1795164168 -> _EXCEPTION_ # inode for 224395265 is NOT FOUND -- Skip processing\n/dev/sda1: 12856336 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_1 # DEV:LBA:FBLK:INODE( /dev/sda1 : 12856336 : 1606786 : 39585855 )\n/dev/sda1: 1270876464 -> _EXCEPTION_ # inode for 158859302 is NOT FOUND -- Skip processing\n/dev/sda1: 1661223968 -> /home/blusjune/.config/google-chrome/Default/Local Storage/https_plus.google.com_0.localstorage # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223968 : 207652740 : 39714912 )\n/dev/sda1: 1266682992 -> _EXCEPTION_ # inode for 158335118 is NOT FOUND -- Skip processing\n/dev/sda1: 1270876456 -> _EXCEPTION_ # inode for 158859301 is NOT FOUND -- Skip processing\n/dev/sda1: 1266681864 -> _EXCEPTION_ # inode for 158334977 is NOT FOUND -- Skip processing\n/dev/sda1: 1266751536 -> /home/blusjune/.config/google-chrome # DEV:LBA:FBLK:INODE( /dev/sda1 : 1266751536 : 158343686 : 39585586 )\n/dev/sda1: 1266751544 -> /home/blusjune/.config/google-chrome/Default # DEV:LBA:FBLK:INODE( /dev/sda1 : 1266751544 : 158343687 : 39585590 )\n/dev/sda1: 1266683272 -> _EXCEPTION_ # inode for 158335153 is NOT FOUND -- Skip processing\n/dev/sda1: 1267153912 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_0 # DEV:LBA:FBLK:INODE( /dev/sda1 : 1267153912 : 158393983 : 39585854 )\n/dev/sda1: 1661223296 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_0 # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223296 : 207652656 : 39585854 )\n/dev/sda1: 12857344 -> /home/blusjune/.cache/google-chrome/Default/Cache/index # DEV:LBA:FBLK:INODE( /dev/sda1 : 12857344 : 1606912 : 39585853 )\n/dev/sda1: 12856320 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_1 # DEV:LBA:FBLK:INODE( /dev/sda1 : 12856320 : 1606784 : 39585855 )\n/dev/sda1: 2480 -> _EXCEPTION_ # inode for 54 is NOT FOUND -- Skip processing\n/dev/sda1: 1266681984 -> _EXCEPTION_ # inode for 158334992 is NOT FOUND -- Skip processing\n/dev/sda1: 1266683032 -> _EXCEPTION_ # inode for 158335123 is NOT FOUND -- Skip processing\n/dev/sda1: 1266683160 -> _EXCEPTION_ # inode for 158335139 is NOT FOUND -- Skip processing\n/dev/sda1: 2352 -> _EXCEPTION_ # inode for 38 is NOT FOUND -- Skip processing\n/dev/sda1: 1661223328 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223328 : 207652660 : 39585620 )\n/dev/sda1: 2048 -> _EXCEPTION_ # fsblock 0 seems INVALID BLOCK -- Skip processing\n/dev/sda1: 1267095912 -> /home/blusjune/.config/google-chrome/Default/Cookies-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1267095912 : 158386733 : 39585621 )\n/dev/sda1: 327569400 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 327569400 : 40945919 : 39585620 )\n/dev/sda1: 1266683048 -> _EXCEPTION_ # inode for 158335125 is NOT FOUND -- Skip processing\n/dev/sda1: 327568408 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 327568408 : 40945795 : 39585620 )\n/dev/sda1: 327568936 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 327568936 : 40945861 : 39585620 )\n/dev/sda1: 1661223320 -> /home/blusjune/.config/google-chrome/Default/Cookies-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223320 : 207652659 : 39585621 )\n/dev/sda1: 1661223136 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223136 : 207652636 : 39585620 )\n/dev/sda1: 1401209744 -> /home/blusjune/.config/google-chrome/Default/Local Storage/https_plus.google.com_0.localstorage-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1401209744 : 175150962 : 39714913 )\n/dev/sda1: 1661223128 -> /home/blusjune/.config/google-chrome/Default/Cookies-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223128 : 207652635 : 39585621 )\n\nblusjune@radiohead:[top_hot_18] $ \n\n\n</pre>\n\n\n=== LBA-to-name processing ===\n\n* .bd/x/exphist info.\n: bd/x/exphist/sidewinder/sidewinder-20130311_190748/tools/lba_to_name/\n\n <pre>\nblusjune@jimi-hendrix:[lba_to_name] $ pwd\n/home/blusjune/bd/x/exphist/sidewinder/sidewinder-20130311_190748/tools/lba_to_name\nblusjune@jimi-hendrix:[lba_to_name] $ l\ntotal 16\ndrwxrwxr-x 2 blusjune blusjune 4096 Mar 11 19:03 ./\ndrwxrwxr-x 4 blusjune blusjune 4096 Mar 11 19:07 ../\n-rwxr-xr-x 1 blusjune blusjune 3869 Mar 11 19:03 .bdx.0100.y.lba_to_name.sh*\n-rw-rw-r-- 1 blusjune blusjune  300 Mar 11 19:03 .conf.lba_to_name.sh\n</pre>\n\n\n* References\n: [http://www.expertslogin.com/general/how-to-fix-repair-bad-blocks-in-linux/ How To Fix / Repair Bad Blocks In Linux] (B.GOOD)\n\n\n* How to find the corresponding location (file/dir) for some LBA (e.g., 1661223128)?\n\n: step 1: We need to find the block size of the filesystem\n <pre>\nblusjune@radiohead:[google] $ sudo tune2fs -l /dev/sda1  | grep Block\nBlock count:              488378112\nBlock size:               4096\nBlocks per group:         32768\n</pre>\n\n: step 2: check the starting sector of the partition (file system)\n <pre>\nblusjune@radiohead:[google] $ sudo fdisk -lu /dev/sda\n\nDisk /dev/sda: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x00001eb9\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *        2048  3907026943  1953512448   83  Linux\n\n</pre>\n\n: step 3: File the filesystem block that contains this target LBA\n <pre>\nb = (int) ((L-S) * 512 / B)\n\n, where\nb = File system block number (we need this answer)\nB = File system block size in bytes (4096 - we got this in step 1)\nL = target LBA (1661223128 - given)\nS = Starting sector of the partition as shown by fdisk -lu (2048 - we got this in step 2)\n\n, so the answer b is 207652635 (207,652,635)\n\n</pre>\n\n: step 4: Check the file/dir name with given b (207652635)\n <pre>\nblusjune@radiohead:[s05] $ sudo debugfs /dev/sda1 \ndebugfs 1.42 (29-Nov-2011)\n\ndebugfs:  testb 207652635\nBlock 207652635 marked in use\n\ndebugfs:  icheck 207652635\nBlock	Inode number\n207652635	39585621\n\ndebugfs:  ncheck 39585621\nInode	Pathname\n39585621	/home/blusjune/.config/google-chrome/Default/Cookies-journal\n\ndebugfs:\n</pre>\n\n== ## bNote-2013-03-08 ==\n\n=== lsof command guide/examples ===\n\n* [http://www.ibm.com/developerworks/aix/library/au-lsof.html Finding open files with lsof]\n\n:- Sean A. Walberg, Senior Network Engineer\n:- Summary:  Learn more about your system by seeing which files are open. Knowing which files an application has open, or which application has a particular file open, enables you to make better decisions as a system administrator. For instance, you shouldn\'t unmount a file system while files on it are open. Using lsof, you can check for open files and stopped processes before unmounting, as needed. Likewise, if you find an unknown file, you can find the application holding it open.\n\n\n=== debugfs command guide/examples ===\n\n* [http://www.cs.montana.edu/courses/309/topics/4-disks/debugfs_example.html debugfs examples - original article from montana.edu]\n\n <pre>\ndebugfs Command Examples\n\n\n# Use debufs to prowl around a file system.\n\n> debugfs /dev/hda6\ndebugfs 1.19, 13-Jul-2000 for EXT2 FS 0.5b, 95/08/09\n\n# list files\n\ndebugfs:  ls\n2790777 (12) .   32641 (12) ..   2790778 (12) dir1   2790781 (16) file1\n2790782 (4044) file2\n\n#  List the files with a long listing\n\n#  Format is:\n# Field 1:  Inode number.\n# Field 2:  First one or two digits is the type of node:\n#    2 = Character device\n#    4 = Directory\n#    6 = Block device\n#    10 = Regular file\n#    12 = Symbolic link\n#  \n#    The Last four digits are the Linux permissions\n# 3. Owner uid\n# 4. Group gid\n# 5. Size in bytes.\n# 6. Date \n# 7. Time of last creation.\n# 8. Filename.\n\ndebugfs:  ls -l\n2790777  40700   2605   2601    4096  5-Nov-2001 15:30 .\n 32641   40755   2605   2601    4096  5-Nov-2001 14:25 ..\n2790778  40700   2605   2601    4096  5-Nov-2001 12:43 dir1\n2790781 100600   2605   2601      14  5-Nov-2001 15:29 file1\n2790782 100600   2605   2601      14  5-Nov-2001 15:30 file2\n\n# dump the contents of file1\n\ndebugfs: cat file1\nThis is file1 \n\n# dump an inode to a file (same as cat, but to a file) and using\n#  instead of the file name.\n\ndebugfs: dump <2790782> file1-debugfs\n\n# dump the contents of an inode\n\ndebugfs: stat file1 \nInode: 2790782   Type: regular    Mode:  0600   Flags: 0x0   Generation: 46520506\nUser:  2605   Group:  2601   Size: 14\nFile ACL: 0    Directory ACL: 0\nLinks: 1   Blockcount: 8\nFragment:  Address: 0    Number: 0    Size: 0\nctime: 0x3be712ea -- Mon Nov  5 15:30:02 2001\natime: 0x3be712ea -- Mon Nov  5 15:30:02 2001\nmtime: 0x3be712ea -- Mon Nov  5 15:30:02 2001\nBLOCKS:\n5603924\nTOTAL: 1\n\n# Dump an directory inode and look at it.\n\ndebugfs: dump dir1 dir1-debugfs\n\n# Leave debugfs or use another xterm to look at the contents\n# using od or xxd.  The format of a directory (ext2 version 2.0) is:\n\n# Field 1. Four byte inode number.\n# Field 2. Two byte directory entry length.\n# Field 3. Two byte file name length. \n# Field 5. Filename (1-255 characters).\n# Pad.     The filename is padded to be a multiple of 4 bytes long.\n\n\n# use -c to see the file names and single byte values\n# You can see the file names and identify the locatin of\n# the other fields.  Of importance, the length of the \n# entries (octal); . (4-5), .. (20-21), file3 (34-35),\n# file4 (54-55), .file4.swp (74-75),  ...\n\n> od -c dir1-dump  \n0000000   z 225   *  \\0  \\f  \\0 001 002   .  \\0  \\0  \\0   y 225   *  \\0\n0000020  \\f  \\0 002 002   .   .  \\0  \\0 202 225   *  \\0 020  \\0 005 001\n0000040   f   i   l   e   3  \\0  \\0  \\0 201 225   *  \\0   ? 017 005 001\n0000060   f   i   l   e   4  \\0  \\0  \\0 177 225   *  \\0   ? 017  \\n 001\n0000100   .   f   i   l   e   4   .   s   w   p  \\0  \\0 200 225   *  \\0\n0000120   ? 017 006 001   f   i   l   e   4   ~   .   s   w   p   x  \\0\n0000140  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0\n*\n0010000\n\n# use -d to see the two byte values\n\n> od -d dir1-dump  \n0000000 38266    42    12   513    46     0 38265    42\n0000020    12   514 11822     0 38274    42    16   261\n0000040 26982 25964    51     0 38273    42  4056   261\n0000060 26982 25964    52     0 38271    42  4040   266\n0000100 26158 27753 13413 29486 28791     0 38272    42\n0000120  4020   262 26982 25964 32308 29486 28791   120\n0000140     0     0     0     0     0     0     0     0\n*\n0010000\n\n# You can see that the lengths of the entries are:\n#    . = 12, .. = 12, file3 = 16, file4 = 4096\n# Whoa! what happened there.  The file .file4.swp\n# and any other files in the directory have been deleted,\n# so the length of the entry goes to the end of the block\n#\n# use -l to see the four byte values.  We can see the inode\n# values of the files.\n\n> od -l dir1-dump  \n0000000     2790778    33619980          46     2790777\n0000020    33685516       11822     2790786    17104912\n0000040  1701603686          51     2790785    17108952\n0000060  1701603686          52     2790783    17436616\n0000100  1818846766  1932407909       28791     2790784\n0000120    17174452  1701603686  1932426804     7893111\n0000140           0           0           0           0\n*\n0010000\n\n\n-------------------------------------------------------------------<\n\n\n\n#\n# You inadvertently delete a file you want back.  The file was named\n# /home/harkin/test/file2.  Immediately do the following.\n#\n\n\n> umount /home\n\n# so that you don\'t create a new file that overwrites the inode\n# or use one of the file blocks.\n\n# Execute df to find out what partition /home is on\n\n>df \nFilesystem           1k-blocks      Used Available Use% Mounted on\n/dev/hda1              1011928    507860    452664  53% /\n/dev/hda8             27364092   1890176  24083896   8% /home\n/dev/hda5              8064272   3492760   4161860  46% /usr\n/dev/hda7              1011928     87956    872568  10% /var\nclowns:/db/boze       17783240  10494056   7183568  60% /home/bozo/db\n\n# Get the data on the /home filesystem\n\ntune2fs -l /dev/hda8 | grep \"Block size\"\n\n   Block size:               4096\n\n# So the block size is 4096 bytes.\n\n# Create a file system to duplicate the /home file system in case\n# you screw up royally.  This disk should be exactly the same size\n# as the file system you are backing up.  Fortunately there is an\n# unused disk /dev/hdb.\n\n> fdisk /dev/hdb\nCommand (m for help): n\nCommand action\n   l   logical (5 or over)\n   p   primary partition (1-4)\np\n\n+27364092K\nw\n\n# copy /home to the backup location\n\ndd if=/dev/hda8 of=/dev/hdb1 bs=4096\n\n# Now use debugfs to try to fix things.  We need to try to\n# find the inode of the deleted file.  Use lsdel to \n# list all of the deleted inodes on the file system.\n\ndebugfs -w            # to allow writing\ndebugfs:  lsdel\n3061 deleted inodes found.\n Inode  Owner  Mode    Size    Blocks    Time deleted\n                     .\n                     .\n3296723   2605 100600    652    1/   1 Fri Nov  2 07:30:33 2001\n3296724   2605 100600   1545    1/   1 Fri Nov  2 07:30:33 2001\n3296725   2605 100600    355    1/   1 Fri Nov  2 07:30:33 2001\n3296731   2605 100600    440    1/   1 Fri Nov  2 07:30:33 2001\n3296732   2605 100600   3536    1/   1 Fri Nov  2 07:30:33 2001\n3296733   2605 100600   2365    1/   1 Fri Nov  2 07:30:33 2001\n3296734   2605 100600    443    1/   1 Fri Nov  2 07:30:33 2001\n3296850   2605 100600   2046    1/   1 Fri Nov  2 07:30:33 2001\n3296851   2605 100600    729    1/   1 Fri Nov  2 07:30:33 2001\n3296852   2605 100600    850    1/   1 Fri Nov  2 07:30:33 2001\n3296853   2605 100600   3251    1/   1 Fri Nov  2 07:30:33 2001\n3296854   2605 100600   3733    1/   1 Fri Nov  2 07:30:33 2001\n3296855   2605 100600   3109    1/   1 Fri Nov  2 07:30:33 2001\n3296856   2605 100600   3211    1/   1 Fri Nov  2 07:30:33 2001\n652818   2605 100600 171791   43/  43 Fri Nov  2 16:07:33 2001\n897613   2605 100600   2096    1/   1 Mon Nov  5 07:49:28 2001\n979218   2605 100600   3797    1/   1 Mon Nov  5 07:49:29 2001\n979219   2605 100600   4096    1/   1 Mon Nov  5 07:49:29 2001\n179573   2605 100600   9113    3/   3 Mon Nov  5 12:41:16 2001\n636513   2605 100600   1327    1/   1 Mon Nov  5 12:41:16 2001\n636520   2605 100600     20    1/   1 Mon Nov  5 12:41:16 2001\n1338319   2605 100600   6998    2/   2 Mon Nov  5 12:48:55 2001\n \n# Based on the time and date, the inode to restore is 179573, 636513 \n# or 636520.  Try to figure out which one.\n\ndebugfs:cat <179573> \n   .\n   .\n\ndebugfs:cat <636513>\n   .\n\n# This is rather inconvenient.  If the directory where the files were\n# deleted from still exists, use the cd command to get there and then\n# use ls -d  which lists the files in the directory only, including\n# those with the deleted flag set. \n\n1566721  (12) .    32641  (12) ..    1566788  (60) 530\n1566790 (48) file1   1566791 (24) file2\n<1566747>  (20) file3\n\nThe inode numbers in brackets are deleted files.  A better looking display\ncomes with ls -ld.\n\n\n# So now you know which inode you need to restore.\n# To restore the file, you need to modify the inode, not the \n# directory entry.  This can be done with the modify_inode (mi)\n# command.  Specifically, change the deletion time to zero\n# and the link count to 1.\n\ndebugfs: mi <636513>\ndebugfs:  mi <148003>\n                              Mode    [0100644] \n                           User ID    [510] \n                          Group ID    [510] \n                              Size    [8123] \n                     Creation time    [904216575] \n                 Modification time    [904234782] \n                       Access time    [904234782] \n                     Deletion time    [904236721] 0\n                        Link count    [0] 1\n                       Block count    [16] \n                        File flags    [0x0] \n                         Reserved1    [0] \n                          File acl    [0] \n                     Directory acl    [0] \n                  Fragment address    [0] \n                   Fragment number    [0] \n                     Fragment size    [0] \n                   Direct Block #0    [100321] \n                   Direct Block #1    [100322] \n                   Direct Block #2    [100323] \n                   Direct Block #3    [100324] \n                   Direct Block #4    [200456] \n                   Direct Block #5    [200457] \n                   Direct Block #6    [200675] \n                   Direct Block #7    [200675] \n                   Direct Block #8    [304568] \n                   Direct Block #9    [0] \n                  Direct Block #10    [0] \n                  Direct Block #11    [0] \n                    Indirect Block    [0] \n             Double Indirect Block    [0] \n             Triple Indirect Block    [0] \n\n# It has been recovered.\n\n# This won\'t work for files with indirect blocks and you might find that\n# one or more blocks have been reused already.  If so, you can\n# recover as much data as possible by dumping the blocks to a file.\n\ndebugfs: dump <100321> /tmp > file1.000\ndebugfs: dump <100322> /tmp >> file1.000\n\n# and so on. For files that are longer than 12 blocks, you have to \n# trace the indirect, double-indirect and triple-indirect blocks.\n\n</pre>\n\n=== blktrace advanced ===\n\n* legacy \'blktrace\' data output\n <pre>\nDev_ID CPU_ID   SN   Timestamp      PID   Phz Act Address   Offset ProcessName\n------------------------------------------------------------------------------------  \n  8,16   1      929  2200.865379372 26328  A   R 3188196112 + 8 <- (8,17) 3188194064\n  8,17   1      930  2200.865379890 26328  Q   R 3188196112 + 8 [mysqld]\n  8,17   1      931  2200.865380598 26328  G   R 3188196112 + 8 [mysqld]\n  8,17   1      932  2200.865381014 26328  P   N [mysqld]\n  8,17   1      933  2200.865381784 26328  I   R 3188196112 + 8 [mysqld]\n  8,17   1      934  2200.865382107 26328  U   N [mysqld] 1\n  8,17   1      935  2200.865382605 26328  D   R 3188196112 + 8 [mysqld]\n  8,17   1      936  2200.871162161     0  C   R 3188196112 + 8 [0]\n  8,16   1      937  2200.871189524 26328  A   R 3188589744 + 8 <- (8,17) 3188587696\n  8,17   1      938  2200.871190517 26328  Q   R 3188589744 + 8 [mysqld]\n  8,17   1      939  2200.871192023 26328  G   R 3188589744 + 8 [mysqld]\n  8,17   1      940  2200.871192992 26328  P   N [mysqld]\n  8,17   1      941  2200.871194468 26328  I   R 3188589744 + 8 [mysqld]\n  8,17   1      942  2200.871195233 26328  U   N [mysqld] 1\n</pre>\n\n* legacy \'lsof\' data output\n <pre>\nCOMMAND     PID            USER   FD      TYPE             DEVICE SIZE/OFF       NODE NAME\n-------------------------------------------------------------------------------------------------\ninit          1            root  cwd       DIR                8,1     4096          2 /\ninit          1            root  rtd       DIR                8,1     4096          2 /\ninit          1            root  txt       REG                8,1   163096   57147454 /sbin/init\ninit          1            root  mem       REG                8,1    52120   96997047 /lib/x86_64-linux-gnu/libnss_files-2.15.so\ninit          1            root  mem       REG                8,1    47680   96993415 /lib/x86_64-linux-gnu/libnss_nis-2.15.so\ninit          1            root  mem       REG                8,1    97248   96997056 /lib/x86_64-linux-gnu/libnsl-2.15.so\ninit          1            root  mem       REG                8,1    35680   96997048 /lib/x86_64-linux-gnu/libnss_compat-2.15.so\ninit          1            root  mem       REG                8,1  1811128   96993409 /lib/x86_64-linux-gnu/libc-2.15.so\ninit          1            root  mem       REG                8,1    31752   96993413 /lib/x86_64-linux-gnu/librt-2.15.so\ninit          1            root  mem       REG                8,1   135366   96993411 /lib/x86_64-linux-gnu/libpthread-2.15.so\ninit          1            root  mem       REG                8,1   276392   96996820 /lib/x86_64-linux-gnu/libdbus-1.so.3.5.8\ninit          1            root  mem       REG                8,1    38888   96996879 /lib/x86_64-linux-gnu/libnih-dbus.so.1.0.0\ninit          1            root  mem       REG                8,1    96240   96996881 /lib/x86_64-linux-gnu/libnih.so.1.0.0\ninit          1            root  mem       REG                8,1   149280   96997051 /lib/x86_64-linux-gnu/ld-2.15.so\ninit          1            root    0u      CHR                1,3      0t0       1029 /dev/null\ninit          1            root    1u      CHR                1,3      0t0       1029 /dev/null\ninit          1            root    2u      CHR                1,3      0t0       1029 /dev/null\ninit          1            root    3r     FIFO                0,8      0t0       3001 pipe\ninit          1            root    4w     FIFO                0,8      0t0       3001 pipe\ninit          1            root    5r     0000                0,9        0       6797 anon_inode\ninit          1            root    6r     0000                0,9        0       6797 anon_inode\ninit          1            root    7u     unix 0xffff88020e5cc680      0t0       7152 socket\ninit          1            root    8u     unix 0xffff8802127caa40      0t0      10936 socket\ninit          1            root    9u     unix 0xffff8802116623c0      0t0       1999 socket\ninit          1            root   10u     unix 0xffff880211663400      0t0      10692 socket\ninit          1            root   12w      REG               8,18     2664   12845346 /var/log/upstart/mysql.log.1 (deleted)\ninit          1            root   14u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   16u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   17u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   18u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   20w      REG               8,18     1342   12845172 /var/log/upstart/modemmanager.log.1 (deleted)\ninit          1            root   21u      CHR                5,2      0t0       7184 /dev/ptmx\nkthreadd      2            root  cwd       DIR                8,1     4096          2 /\nkthreadd      2            root  rtd       DIR                8,1     4096          2 /\nkthreadd      2            root  txt   unknown                                        /proc/2/exe\nksoftirqd     3            root  cwd       DIR                8,1     4096          2 /\nksoftirqd     3            root  rtd       DIR                8,1     4096          2 /\nksoftirqd     3            root  txt   unknown                                        /proc/3/exe\n...\napache2    2241            root  mem       REG                8,1  1852792   96996819 /lib/x86_64-linux-gnu/libcrypto.so.1.0.0\napache2    2241            root  mem       REG                8,1   374608   96996818 /lib/x86_64-linux-gnu/libssl.so.1.0.0\napache2    2241            root  mem       REG                8,1  1030512   96997045 /lib/x86_64-linux-gnu/libm-2.15.so\napache2    2241            root  mem       REG                8,1    66784   96996836 /lib/x86_64-linux-gnu/libbz2.so.1.0.4\napache2    2241            root  mem       REG                8,1  1518928   64756408 /usr/lib/x86_64-linux-gnu/libdb-5.1.so\napache2    2241            root  mem       REG                8,1   105288   96993414 /lib/x86_64-linux-gnu/libresolv-2.15.so\napache2    2241            root  mem       REG                8,1  8644728   65276931 /usr/lib/apache2/modules/libphp5.so\napache2    2241            root  mem       REG                8,1    34824   65276243 /usr/lib/apache2/modules/mod_negotiation.so\napache2    2241            root  mem       REG                8,1    18432   65276567 /usr/lib/apache2/modules/mod_mime.so\napache2    2241            root  mem       REG                8,1    10240   65276556 /usr/lib/apache2/modules/mod_env.so\napache2    2241            root  mem       REG                8,1    10240   65276178 /usr/lib/apache2/modules/mod_dir.so\napache2    2241            root  mem       REG                8,1    92720   96996948 /lib/x86_64-linux-gnu/libz.so.1.2.3.4\napache2    2241            root  mem       REG                8,1    22528   65276571 /usr/lib/apache2/modules/mod_deflate.so\napache2    2241            root  mem       REG                8,1    26624   65275788 /usr/lib/apache2/modules/mod_cgi.so\napache2    2241            root  mem       REG                8,1    34824   65276563 /usr/lib/apache2/modules/mod_autoindex.so\napache2    2241            root  mem       REG                8,1    10248   65275257 /usr/lib/apache2/modules/mod_authz_user.so\napache2    2241            root  mem       REG                8,1    10248   65276546 /usr/lib/apache2/modules/mod_authz_host.so\napache2    2241            root  mem       REG                8,1    10248   65275256 /usr/lib/apache2/modules/mod_authz_groupfile.so\napache2    2241            root  mem       REG                8,1     6152   65275193 /usr/lib/apache2/modules/mod_authz_default.so\napache2    2241            root  mem       REG                8,1    10248   65276547 /usr/lib/apache2/modules/mod_authn_file.so\napache2    2241            root  mem       REG                8,1    10248   65276545 /usr/lib/apache2/modules/mod_auth_basic.so\napache2    2241            root  mem       REG                8,1    14336   65275224 /usr/lib/apache2/modules/mod_alias.so\napache2    2241            root  mem       REG                8,1    14768   96993408 /lib/x86_64-linux-gnu/libdl-2.15.so\napache2    2241            root  mem       REG                8,1    18896   96996944 /lib/x86_64-linux-gnu/libuuid.so.1.3.0\napache2    2241            root  mem       REG                8,1   170024   96996871 /lib/x86_64-linux-gnu/libexpat.so.1.5.2\napache2    2241            root  mem       REG                8,1    43288   96997046 /lib/x86_64-linux-gnu/libcrypt-2.15.so\napache2    2241            root  mem       REG                8,1  1811128   96993409 /lib/x86_64-linux-gnu/libc-2.15.so\napache2    2241            root  mem       REG                8,1   135366   96993411 /lib/x86_64-linux-gnu/libpthread-2.15.so\napache2    2241            root  mem       REG                8,1   234720   64752153 /usr/lib/libapr-1.so.0.4.6\napache2    2241            root  mem       REG                8,1   142840   64752206 /usr/lib/libaprutil-1.so.0.3.12\napache2    2241            root  mem       REG                8,1   247896   96996910 /lib/x86_64-linux-gnu/libpcre.so.3.12.1\napache2    2241            root  mem       REG                8,1   149280   96997051 /lib/x86_64-linux-gnu/ld-2.15.so\napache2    2241            root  DEL       REG                0,4               11842 /dev/zero\napache2    2241            root    0r      CHR                1,3      0t0       1029 /dev/null\napache2    2241            root    1w      CHR                1,3      0t0       1029 /dev/null\napache2    2241            root    2w      REG               8,18    12640   12845390 /var/log/apache2/error.log\napache2    2241            root    3u     IPv4              14201      0t0        TCP *:http (LISTEN)\napache2    2241            root    4r     FIFO                0,8      0t0    7004026 pipe\napache2    2241            root    5w     FIFO                0,8      0t0    7004026 pipe\napache2    2241            root    6w      REG               8,18        0   12845132 /var/log/apache2/other_vhosts_access.log\napache2    2241            root    7w      REG               8,18   127987   12845338 /var/log/apache2/access.log\n\n</pre>\n\n:* [Q] what does \'node\' information mean in \'lsof\' output?\n\n\n* converged iotrace\n: join the information from the legacy blktrace data and lsof, inotify data\n\n\n\n=== SNIA IO Trace Data Files ===\n\n\n[http://iotta.snia.org/traces SNIA I/O Trace Data Files]\n\nThe categories (or types) of I/O traces include:\n\n* Application Traces [This category is currently empty]\n: Application Traces record calls made by a specific application.\n* Block I/O Traces\n: Block I/O Traces typically include block level (e.g., at the logical volume manager, disk driver, etc. level) and block protocol (e.g., SCSI, ATA, Fibre Channel) traces.\n* Historical Traces [This category is currently empty]\n: Historical Traces include all traces that 10 or more years of age.\n* NFS Traces\n: Network File System Traces are typically those for NFS and CIFS and which reflect the protocol used by such network file systems.\n* Parallel Traces\nParallel traces, generally taken from supercomputers, record the system calls made by multiple computers running in parallel.\nSSSI WIOCP Metrics\nSSSI WIOCP, the SNIA Solid State Storage Initiative (SSSI) Workload I/O Capture Program (WIOCP), collects already-summarized empirical metrics separately for both monitored devices and processes/applications.\nStatic Snapshots\nStatic Snapshots are traces taken statically of a file system rather than of system calls.\nSystem Call Traces\nSystem Call I/O Traces typically reflect operating system calls to the file system.\nTools\nHere you can find the tools used for reading the various trace files.\n\n\n==== MSR Cambridge Traces ====\n\n* [http://iotta.snia.org/traces/388 List of Traces]\n\n* MSR Cambridge Traces 1\n: 1-week block I/O Traces of enterprise servers at MSR Cambridge.\n: The citation for the MSRC traces can be found [http://static.usenix.org/event/fast08/tech/narayanan.html FAST 2008, \"Write Off-loading: Practical Power Management for Enterprise Storage\", Dushyanth Narayanan, Austin Donnelly, and Antony Rowstron, Microsoft Research Ltd.]\n\n\n\n==== Microsoft Enterprise Traces ====\n\nTraces collected at Microsoft using the event tracing for Windows framework.\n\n* [http://iotta.snia.org/traces/130 List of Traces]\n\n* TPCC Traces 1\n: TPC-C benchmark traces collected at Microsoft using the event tracing for Windows framework\n\n* TPCC Traces 2\n: TPC-C benchmark traces collected at Microsoft using the event tracing for Windows framework\n: These six 6-minute long traces were collected at various points during a TPC-C run, all of which were during periods of steady-state activity.\n\n* TPCE Traces\n: TPC-E benchmark traces collected at Microsoft using the event tracing for Windows framework\n: These 6 traces were collected during a ~ 84-minute TPC-E run which included a ~ 20-minute warm-up time.\n\n* Exchange Server Traces\n: Production traces collected at Microsoft using the event tracing for Windows framework\n: Collected for Exchange Server for a duration of 24 hours. The single tarball includes 96 trace files, each with a duration of 15 minutes.\n\n\n\n==== Microsoft Production Server Traces ====\n\n* [http://iotta.snia.org/traces/158 List of Traces]\n\n* BuildServer00 ~ BuildServer07\n: Traces of the 25 hours activity on the Microsoft Build Server\n* Development Tools Release\n: Collected for Developers Tools Release Server for a duration of 24 hours\n* Display Ads Data Server, Display Ads Payload Server\n: Collected over a period of 24 hours for Display Ads Data/Platform payload server\n* Live Maps Back End\n: Collected for LiveMaps back-end server for a duration of 24 hours\n* MSN Storage CFS\n: Collected for MSN Storage Metadata Server for a duration of 6 hours\n* MSN Storage File Server\n: Collected for MSN Storage file server for a duration of 6 hours\n* Radius Authentication\n: Collected for RADIUS authentication server\n* Radius Back End SQL Server\n: Collected for RADIUS back-end server\n\n=== 서울대 장병탁 교수님 세미나 ===\n\n* Hypernetwork ML/AI 기술\n:- [http://bi.snu.ac.kr/Courses/g-ai06_2/book-ch4-hypernetmemory-part3.pdf The Hypernetwork Model of Memory)]\n:- [http://bi.snu.ac.kr/Publications/Theses/BS12f_ChunHS.pdf 하이퍼네트워크 연상메모리 기반의 이미지-텍스트 교차검색 (Image-Text Crossmodal Retrieval via Hypernetwork Memory]\n:: 2nd wrong answer: (하이퍼네트워크 메모리 기반의 이미지-텍스트 교차검색)\n:: 1st wrong answer: (하이퍼네트워크 기반의 이미지-텍스트 연상 교차 검색)\n\n=== 상무님께서 보내주신 Storage 미래 관련 글들 ===\n\n\n* 기타\n\n: [http://wwpi.com/index.php?option=com_content&view=article&id=8158]\n\n: [http://lib.stanford.edu/files/pasig-jan2012/11B7%20Francis%20PASIG_2011_Francis_final.pdf]\n\n: [http://www.datarecoverygroup.com/articles/data-storage-history-and-future Data Storage History and Future]\n\n\n\n\n* 스토리지 미디어의 발전 전망\n: 우리가 스토리지 미디어를 개발하지는 않지만, 미디어의 발전 전망을 고려해서 소프트웨어의 미래를 전망해야겠지요.\n\n: [http://www-03.ibm.com/ibm/history/ibm100/us/en/icons/racetrack/ IBM100] \n\n: [http://static.usenix.org/events/fast02/coufal.pdf FAST 2002]\n\n\n\n\n* DNA를 데이터 스토리지로 이용하는 것에 관한 또 다른 글입니다. 장점/비용 이슈 언급됨.\n\n: [http://www.lifehacker.com.au/2013/01/is-dna-the-future-of-data-storage/ DNA를 data storage로 이용하기]\n\n \n* Storage의 미래\n\n: [http://blogs.computerworld.com/data-storage/20865/future-data-storage-revealed Future data storage revealed]\n\n: [http://blogs.computerworld.com/data-storage/21537/top-10-storage-predictions-back-future Top 10 storage predictions]\n\n: [http://blogs.computerworld.com/data-storage/21360/will-private-cloud-kill-storage-area-network Will Private Cloud Kill SAN?]\n\n== ## bNote-2013-03-07 ==\n\n\n=== Find X\'s ===\n\n==== System Event (esp., file system change) Tracing/Monitoring/Collecting ====\n\n* LTTng\n* DTrace\n* FTrace\n* Strace\n* SystemTap\n* inotify ***\n* FAM (File Alteration Monitor) [http://oss.sgi.com/projects/fam/]\n* Gamin (File and directory monitoring system defined to be a subset of the FAM system [http://people.gnome.org/~veillard/gamin/overview.html]\n\n----\n\n==== inotify ====\n\n* inotify - monitoring file system events\n: The inotify API provides a mechanism for monitoring file system events. Inotify can be used to monitor individual files, or to monitor directories. When a directory is monitored, inotify will return events for the directory itself, and for files inside the directory.\n: The following system calls are used with this API: inotify_init(2) (or inotify_init1(2)), inotify_add_watch(2), inotify_rm_watch(2), read(2), and close(2).\n\n* /proc interfaces\n: /proc/sys/fs/inotify/max_queued_events\n: /proc/sys/fs/inotify/max_user_instances\n: /proc/sys/fs/inotify/max_user_watches\n\n* Versions\n: Inotify was merged into the 2.6.13 Linux kernel. The required library interfaces were added to glibc in version 2.4. (IN_DONT_FOLLOW, IN_MASK_ADD, and IN_ONLYDIR were only added in version 2.5.)\n\n* Check whether inotify is enabled or not\n $ grep INOTIFY_USER /boot/config-$(uname -r)\n CONFIG_INOTIFY_USER=y\n\n* Installation on Ubuntu by apt-get\n: aptitude install inotify-tools python-inotifyx libinotifytools0-dev\n\n* inotify는 다음 event들에 대해서만 detection 가능함\n:* access\n:* modify\n:* attrib\n:: watched file에 대한 메타데이터가 변경되거나, watched directory 내의 file이 변경된 경우, \'attrib\' event가 발생\n:* close_write\n:* close_nowrite\n:* close\n:* open\n:* moved_to\n:* moved_from\n:* move\n:* move_self\n:: 이 event 이후에는 file or directory는 no longer being watched된다... 는데, delete event의 경우와 무엇이 다른가?\n:* create\n:* delete\n:* delete_self\n:* unmount\n\n\n\n\n* References\n# [http://www.infoq.com/articles/inotify-linux-file-system-event-monitoring InfoQ -- Inotify: Efficient, Real-time Linux File System Event Monitoring]\n# [http://www.ibm.com/developerworks/linux/library/l-ubuntu-inotify/index.html Monitor file system activity with inotify (B.GOOD)]\n# [http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=90586523eb4b349806887c62ee70685a49415124 git.kernel.org -- fsnotify: unified filesystem notification backend, 2009-05-21~2009-06-11]\n# [http://stackoverflow.com/questions/9614184/how-to-trace-per-file-io-operations-in-linux How to trace per-file IO operations in Linux? -- /proc/PID/fd/, systemtap, strace, fanotify]\n# [http://stackoverflow.com/questions/1835947/how-do-i-program-for-linuxs-new-fanotify-file-system-monitoring-feature Stackoverflow -- How do I program for Linux\'s new \'fanotify\' file system monitoring feature?]\n# [http://stackoverflow.com/questions/8381566/best-way-to-monitor-file-system-changes-in-linux Stackoverflow -- Best way to monitor file system changes in Linux]\n# [http://ubuntuforums.org/showthread.php?t=663950 python inotify example -- Ubuntu Forums]\n# [http://pyinotify.sourceforge.net/ Pyinotify: monitor filesystem events with Python under Linux - Brief Tutorial]\n# [http://github.com/seb-m/pyinotify pyinotify github]\n\n=== directory-file-addr spatial locality ===\n\n* Directory Hierarchy\n\n <pre>\nblusjune@jimi-hendrix:[dir_file_addr_spatial_locality] $ find r0\nr0\nr0/d1\nr0/d1/d13\nr0/d1/d11\nr0/d1/d12\nr0/d2\nr0/d2/d21\nr0/d2/d21/d212\nr0/d2/d21/d211\nr0/d2/d21/d213\nr0/d2/d22\nr0/d2/d22/d221\nr0/d2/d22/d222\n</pre>\n\n== ## bNote-2013-03-06 ==\n\n=== LBA-to-name processing (DELETEME) ===\n\nDone actually 2013-03-11 14:35. [http://www.expertslogin.com/general/how-to-fix-repair-bad-blocks-in-linux/ How To Fix / Repair Bad Blocks In Linux B.GOOD]\n\n\n* How to find the corresponding location (file/dir) for some LBA (e.g., 1661223128)?\n\n: step 1: We need to find the block size of the filesystem\n <pre>\nblusjune@radiohead:[google] $ sudo tune2fs -l /dev/sda1  | grep Block\nBlock count:              488378112\nBlock size:               4096\nBlocks per group:         32768\n</pre>\n\n: step 2: check the starting sector of the partition (file system)\n <pre>\nblusjune@radiohead:[google] $ sudo fdisk -lu /dev/sda\n\nDisk /dev/sda: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x00001eb9\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *        2048  3907026943  1953512448   83  Linux\n\n</pre>\n\n: step 3: File the filesystem block that contains this target LBA\n <pre>\nb = (int) ((L-S) * 512 / B)\n\n, where\nb = File system block number (we need this answer)\nB = File system block size in bytes (4096 - we got this in step 1)\nL = target LBA (1661223128 - given)\nS = Starting sector of the partition as shown by fdisk -lu (2048 - we got this in step 2)\n\n, so the answer b is 207652635 (207,652,635)\n\n</pre>\n\n: step 4: Check the file/dir name with given b (207652635)\n <pre>\nblusjune@radiohead:[s05] $ sudo debugfs /dev/sda1 \ndebugfs 1.42 (29-Nov-2011)\n\ndebugfs:  testb 207652635\nBlock 207652635 marked in use\n\ndebugfs:  icheck 207652635\nBlock	Inode number\n207652635	39585621\n\ndebugfs:  ncheck 39585621\nInode	Pathname\n39585621	/home/blusjune/.config/google-chrome/Default/Cookies-journal\n\ndebugfs:\n</pre>\n\n=== IOWA.sim.iox (myreal_72h) ===\n\n* base trace log on radiohead\n:- name: bpo_a.20130305_104633.real_whole_trace.log\n:- size: 197,372,058 bytes (197372058)\n\n <pre>\nblusjune@radiohead:[s05] $ pwd\n/x/var/iowa/sidewinder/iowa/iowa.sim.iox/tdir/s05\n\nblusjune@radiohead:[s05] $ l\ntotal 210572\ndrwxrwxr-x  2 blusjune blusjune      4096 Mar  6 11:07 ./\ndrwxrwxr-x 10 blusjune blusjune      4096 Mar  6 19:51 ../\nlrwxrwxrwx  1 blusjune blusjune        38 Mar  5 10:43 .bdx.0100.y.proc_after_trace_s10.sh -> ../.bdx.0100.y.proc_after_trace_s10.sh\n-rw-rw-r--  1 root     root     197372058 Mar  5 13:34 bpo_a.20130305_104633.real_whole_trace.log\n-rw-rw-r--  1 blusjune blusjune   4945483 Mar  6 11:06 tracelog.myrealtrace.log.A.addr\n-rw-rw-r--  1 blusjune blusjune   1081666 Mar  6 11:06 tracelog.myrealtrace.log.R.addr\n-rw-rw-r--  1 blusjune blusjune   3863817 Mar  6 11:06 tracelog.myrealtrace.log.W.addr\n-rw-rw-r--  1 blusjune blusjune   8339772 Mar  6 11:06 tracelog.myrealtrace.log.p1.out\n</pre>\n\n\n* R.addr analysis\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_200532.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  1\n__valu__sig__ _n_o_sigaddrs :  43683\n__valu__sig__ _sigioc_acc :  43683\n__valu__sig__ _sigaddrs_efficiency :  1.0\n__valu__sig__ _n_o_addr_total :  43683\n__valu__sig__ _ioc_total :  43683\n</pre>\n\n\n* W.addr analysis\n:- Used as weekly report item (2013-03-06), and lead to a patent\n::- just 18 addresses cover 25% of IO (40,010 IOs out of 157,632 IOs)\n::- x 2222.8 caching efficiency\n\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  1115\n__valu__sig__ _n_o_sigaddrs :  18\n__valu__sig__ _sigioc_acc :  40010\n__valu__sig__ _sigaddrs_efficiency :  2222.77777778\n__valu__sig__ _n_o_addr_total :  40979\n__valu__sig__ _ioc_total :  157632\n</pre>\n\n\n* Processes Contributed to the IO Workload\n\n <pre>\na1mjjung@secm:[myrealtrace.radiohead.v01] $ cat bpo_a.20130305_104633.real_whole_trace.log  | grep \'\\[\' | sed -e \'s/.*\\(\\[.*\\]\\)/\\1/g\' | sort -u\n[(null)]\n[(null)] 1\n[(null)] 3\n[(null)] 4\n[(null)] 6\n[(null)] 7\n[0]\n[BrowserBlocking]\n[BrowserBlocking] 1\n[Chrome_CacheThr]\n[Chrome_CacheThr] 1\n[Chrome_CacheThr] 10\n[Chrome_CacheThr] 2\n[Chrome_CacheThr] 3\n[Chrome_CacheThr] 6\n[Chrome_CacheThr] 9\n[Chrome_ChildIOT]\n[Chrome_DBThread]\n[Chrome_DBThread] 1\n[Chrome_DBThread] 2\n[Chrome_DBThread] 3\n[Chrome_DBThread] 4\n[Chrome_DBThread] 5\n[Chrome_DBThread] 6\n[Chrome_FileThre]\n[Chrome_FileThre] 1\n[Chrome_FileThre] 2\n[Chrome_FileThre] 3\n[Chrome_FileThre] 4\n[Chrome_HistoryT]\n[Chrome_HistoryT] 1a1mjjung@secm:[myrealtrace.radiohead.v01] $ cat bpo_a.20130305_104633.real_whole_trace.log  | grep \'\\[\' | sed -e \'s/.*\\(\\[.*\\]\\)/\\1/g\' | sort -u\n[(null)]\n[(null)] 1\n[(null)] 3\n[(null)] 4\n[(null)] 6\n[(null)] 7\n[0]\n[BrowserBlocking]\n[BrowserBlocking] 1\n[Chrome_CacheThr]\n[Chrome_CacheThr] 1\n[Chrome_CacheThr] 10\n[Chrome_CacheThr] 2\n[Chrome_CacheThr] 3\n[Chrome_CacheThr] 6\n[Chrome_CacheThr] 9\n[Chrome_ChildIOT]\n[Chrome_DBThread]\n[Chrome_DBThread] 1\n[Chrome_DBThread] 2\n[Chrome_DBThread] 3\n[Chrome_DBThread] 4\n[Chrome_DBThread] 5\n[Chrome_DBThread] 6\n[Chrome_FileThre]\n[Chrome_FileThre] 1\n[Chrome_FileThre] 2\n[Chrome_FileThre] 3\n[Chrome_FileThre] 4\n[Chrome_HistoryT]\n[Chrome_HistoryT] 1\n[Chrome_HistoryT] 10\n[Chrome_HistoryT] 11\n[Chrome_HistoryT] 12\n[Chrome_HistoryT] 14\n[Chrome_HistoryT] 15\n[Chrome_HistoryT] 16\n[Chrome_HistoryT] 2\n[Chrome_HistoryT] 3\n[Chrome_HistoryT] 4\n[Chrome_HistoryT] 5\n[Chrome_HistoryT] 6\n[Chrome_HistoryT] 7\n[Chrome_HistoryT] 8\n[Chrome_HistoryT] 9\n[Chrome_IOThread]\n[Chrome_IOThread] 1\n[Chrome_ProcessL]\n[Chrome_SafeBrow]\n[Chrome_SafeBrow] 1\n[Chrome_SafeBrow] 4\n[Chrome_SafeBrow] 5\n[Chrome_SyncThre]\n[Chrome_SyncThre] 1\n[Chrome_SyncThre] 10\n[Chrome_SyncThre] 11\n[Chrome_SyncThre] 12\n[Chrome_SyncThre] 13\n[Chrome_SyncThre] 14\n[Chrome_SyncThre] 15\n[Chrome_SyncThre] 16\n[Chrome_SyncThre] 2\n[Chrome_SyncThre] 3\n[Chrome_SyncThre] 4\n[Chrome_SyncThre] 5\n[Chrome_SyncThre] 6\n[Chrome_SyncThre] 7\n[Chrome_SyncThre] 8\n[Chrome_SyncThre] 9\n[Chrome_WebKitTh]\n[Chrome_WebKitTh] 1\n[MediaStreamDevi]\n[MediaStreamDevi] 1\n[NetworkChangeNo]\n[NetworkChangeNo] 1\n[WorkerPool/2786]\n[WorkerPool/2786] 1\n[WorkerPool/2787]\n[WorkerPool/2787] 1\n[WorkerPool/2788]\n[WorkerPool/2788] 1\n[WorkerPool/2839]\n[WorkerPool/2839] 1\n[WorkerPool/2840]\n[WorkerPool/2840] 1\n[WorkerPool/2840] 2\n[WorkerPool/2981]\n[WorkerPool/2981] 1\n[WorkerPool/2982]\n[WorkerPool/2982] 1\n[_BDX]\n[_BDX] 1\n[apache2]\n[apache2] 1\n[apache2ctl]\n[apache2ctl] 1\n[bash]\n[bash] 1\n[bash] 2\n[bash] 3\n[bash] 4\n[bash] 5\n[blktrace]\n[cat]\n[cat] 1\n[checkarray]\n[checkarray] 1\n[chrome]\n[chrome] 1\n[chrome] 2\n[chrome] 3\n[chrome] 4\n[console-kit-dae]\n[console-kit-dae] 1\n[console-kit-dae] 4\n[cp]\n[cp] 1\n[cron]\n[dbus-launch]\n[dbus-launch] 1\n[env]\n[env] 1\n[expr]\n[expr] 2\n[flush-8:0]\n[flush-8:0] 1\n[flush-8:0] 10\n[flush-8:0] 11\n[flush-8:0] 12\n[flush-8:0] 13\n[flush-8:0] 14\n[flush-8:0] 15\n[flush-8:0] 16\n[flush-8:0] 2\n[flush-8:0] 3\n[flush-8:0] 4\n[flush-8:0] 5\n[flush-8:0] 6\n[flush-8:0] 7\n[flush-8:0] 8\n[flush-8:0] 9\n[git-pull]\n[git-pull] 1\n[git]\n[git] 1\n[gnome-pty-helpe]\n[gnome-pty-helpe] 1\n[gnome-terminal]\n[gnome-terminal] 1\n[gnome-terminal] 2\n[gnome-terminal] 3\n[gnome-terminal] 4\n[gnome-terminal] 5\n[gnome-terminal] 7\n[google-chrome]\n[google-chrome] 1\n[inotify_reader]\n[inotify_reader] 1\n[jbd2/sda1-8]\n[jbd2/sda1-8] 1\n[jbd2/sda1-8] 16\n[jbd2/sda1-8] 2\n[jbd2/sda1-8] 3\n[jbd2/sda1-8] 4\n[jbd2/sda1-8] 5\n[kworker/0:1]\n[kworker/0:2]\n[kworker/2:1]\n[kworker/2:2]\n[kworker/3:0]\n[kworker/4:0]\n[kworker/4:1]\n[kworker/4:2]\n[kworker/5:0]\n[kworker/5:1]\n[kworker/5:2]\n[kworker/7:2]\n[libvirtd]\n[libvirtd] 1\n[libvirtd] 2\n[ls]\n[ls] 1\n[mysqld]\n[nmbd]\n[nmbd] 2\n[nmbd] 4\n[nmbd] 5\n[nmbd] 6\n[python]\n[python] 1\n[python] 2\n[python] 3\n[python] 4\n[python] 5\n[rserver]\n[rserver] 1\n[rserver] 2\n[rserver] 3\n[rserver] 6\n[rserver] 8\n[rsession]\n[rsession] 1\n[run-parts]\n[run-parts] 1\n[sh]\n[sh] 1\n[smbd]\n[smbd] 1\n[smbd] 2\n[smbd] 3\n[sshd]\n[sshd] 1\n[sshd] 4\n[sshd] 5\n[swapper/0]\n[udevd]\n[udevd] 1\n[update-motd-fsc]\n[update-motd-fsc] 1\n[update-motd-upd]\n[update-motd-upd] 1\n[upstart-udev-br]\n[upstart-udev-br] 1\n[xdg-settings]\n[xdg-settings] 1\n[Chrome_HistoryT] 10\n[Chrome_HistoryT] 11\n[Chrome_HistoryT] 12\n[Chrome_HistoryT] 14\n[Chrome_HistoryT] 15\n[Chrome_HistoryT] 16\n[Chrome_HistoryT] 2\n[Chrome_HistoryT] 3\n[Chrome_HistoryT] 4\n[Chrome_HistoryT] 5\n[Chrome_HistoryT] 6\n[Chrome_HistoryT] 7\n[Chrome_HistoryT] 8\n[Chrome_HistoryT] 9\n[Chrome_IOThread]\n[Chrome_IOThread] 1\n[Chrome_ProcessL]\n[Chrome_SafeBrow]\n[Chrome_SafeBrow] 1\n[Chrome_SafeBrow] 4\n[Chrome_SafeBrow] 5\n[Chrome_SyncThre]\n[Chrome_SyncThre] 1\n[Chrome_SyncThre] 10\n[Chrome_SyncThre] 11\n[Chrome_SyncThre] 12\n[Chrome_SyncThre] 13\n[Chrome_SyncThre] 14\n[Chrome_SyncThre] 15\n[Chrome_SyncThre] 16\n[Chrome_SyncThre] 2\n[Chrome_SyncThre] 3\n[Chrome_SyncThre] 4\n[Chrome_SyncThre] 5\n[Chrome_SyncThre] 6\n[Chrome_SyncThre] 7\n[Chrome_SyncThre] 8\n[Chrome_SyncThre] 9\n[Chrome_WebKitTh]\n[Chrome_WebKitTh] 1\n[MediaStreamDevi]\n[MediaStreamDevi] 1\n[NetworkChangeNo]\n[NetworkChangeNo] 1\n[WorkerPool/2786]\n[WorkerPool/2786] 1\n[WorkerPool/2787]\n[WorkerPool/2787] 1\n[WorkerPool/2788]\n[WorkerPool/2788] 1\n[WorkerPool/2839]\n[WorkerPool/2839] 1\n[WorkerPool/2840]\n[WorkerPool/2840] 1\n[WorkerPool/2840] 2\n[WorkerPool/2981]\n[WorkerPool/2981] 1\n[WorkerPool/2982]\n[WorkerPool/2982] 1\n[_BDX]\n[_BDX] 1\n[apache2]\n[apache2] 1\n[apache2ctl]\n[apache2ctl] 1\n[bash]\n[bash] 1\n[bash] 2\n[bash] 3\n[bash] 4\n[bash] 5\n[blktrace]\n[cat]\n[cat] 1\n[checkarray]\n[checkarray] 1\n[chrome]\n[chrome] 1\n[chrome] 2\n[chrome] 3\n[chrome] 4\n[console-kit-dae]\n[console-kit-dae] 1\n[console-kit-dae] 4\n[cp]\n[cp] 1\n[cron]\n[dbus-launch]\n[dbus-launch] 1\n[env]\n[env] 1\n[expr]\n[expr] 2\n[flush-8:0]\n[flush-8:0] 1\n[flush-8:0] 10\n[flush-8:0] 11\n[flush-8:0] 12\n[flush-8:0] 13\n[flush-8:0] 14\n[flush-8:0] 15\n[flush-8:0] 16\n[flush-8:0] 2\n[flush-8:0] 3\n[flush-8:0] 4\n[flush-8:0] 5\n[flush-8:0] 6\n[flush-8:0] 7\n[flush-8:0] 8\n[flush-8:0] 9\n[git-pull]\n[git-pull] 1\n[git]\n[git] 1\n[gnome-pty-helpe]\n[gnome-pty-helpe] 1\n[gnome-terminal]\n[gnome-terminal] 1\n[gnome-terminal] 2\n[gnome-terminal] 3\n[gnome-terminal] 4\n[gnome-terminal] 5\n[gnome-terminal] 7\n[google-chrome]\n[google-chrome] 1\n[inotify_reader]\n[inotify_reader] 1\n[jbd2/sda1-8]\n[jbd2/sda1-8] 1\n[jbd2/sda1-8] 16\n[jbd2/sda1-8] 2\n[jbd2/sda1-8] 3\n[jbd2/sda1-8] 4\n[jbd2/sda1-8] 5\n[kworker/0:1]\n[kworker/0:2]\n[kworker/2:1]\n[kworker/2:2]\n[kworker/3:0]\n[kworker/4:0]\n[kworker/4:1]\n[kworker/4:2]\n[kworker/5:0]\n[kworker/5:1]\n[kworker/5:2]\n[kworker/7:2]\n[libvirtd]\n[libvirtd] 1\n[libvirtd] 2\n[ls]\n[ls] 1\n[mysqld]\n[nmbd]\n[nmbd] 2\n[nmbd] 4\n[nmbd] 5\n[nmbd] 6\n[python]\n[python] 1\n[python] 2\n[python] 3\n[python] 4\n[python] 5\n[rserver]\n[rserver] 1\n[rserver] 2\n[rserver] 3\n[rserver] 6\n[rserver] 8\n[rsession]\n[rsession] 1\n[run-parts]\n[run-parts] 1\n[sh]\n[sh] 1\n[smbd]\n[smbd] 1\n[smbd] 2\n[smbd] 3\n[sshd]\n[sshd] 1\n[sshd] 4\n[sshd] 5\n[swapper/0]\n[udevd]\n[udevd] 1\n[update-motd-fsc]\n[update-motd-fsc] 1\n[update-motd-upd]\n[update-motd-upd] 1\n[upstart-udev-br]\n[upstart-udev-br] 1\n[xdg-settings]\n[xdg-settings] 1\n\n</pre>\n\n\n=== IOWA.sim.iox (tpcc_250gb_48h) ===\n\n* R.addr analysis\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_182112.sigio_25.iowsz_100.t1_10000] $ cat __simout.sigio_25.iowsz_100.t1_10000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  7\n__valu__sig__ _n_o_sigaddrs :  126606\n__valu__sig__ _sigioc_acc :  1169938\n__valu__sig__ _sigaddrs_efficiency :  9.24077847811\n__valu__sig__ _n_o_addr_total :  1691608\n__valu__sig__ _ioc_total :  4113312\n</pre>\n\n\n* W.addr analysis\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_190100.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  601\n__valu__sig__ _n_o_sigaddrs :  46304\n__valu__sig__ _sigioc_acc :  47940036\n__valu__sig__ _sigaddrs_efficiency :  1035.33249827\n__valu__sig__ _n_o_addr_total :  4910080\n__valu__sig__ _ioc_total :  191622453\n</pre>\n\n x39.02 = ( total_#_of_IOs / total_#_of_addrs_hit_actually )\n x1035.33 = ( 25%_sig_IOs / 25%_sig_addrs )\n\n=== R \'e1071\' package install (command line) ===\n\n <pre>\na1mjjung@secm:[e1071] $ R CMD INSTALL e1071\nJob <13412785> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura007>>\nWarning: invalid package \'e1071\'\nError: ERROR: no packages specified\na1mjjung@secm:[e1071] $ cd ..\nla1mjjung@secm:[e1071_build] $ l\ntotal 280\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:45 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\na1mjjung@secm:[e1071_build] $ R CMD INSTALL e1071\nJob <13412786> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n* installing to library \'/home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15\'\n* installing *source* package \'e1071\' ...\n** package \'e1071\' successfully unpacked and MD5 sums checked\nchecking for C++ compiler default output file name... a.out\nchecking whether the C++ compiler works... yes\nchecking whether we are cross compiling... no\nchecking for suffix of executables... \nchecking for suffix of object files... o\nchecking whether we are using the GNU C++ compiler... yes\nchecking whether g++ accepts -g... yes\n** libs\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c Rsvm.c -o Rsvm.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cmeans.c -o cmeans.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cshell.c -o cshell.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c floyd.c -o floyd.o\ng++ -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c svm.cpp -o svm.o\ng++ -shared -L/usr/local/lib64 -o e1071.so Rsvm.o cmeans.o cshell.o floyd.o svm.o\ninstalling to /home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15/e1071/libs\n** R\n** inst\n** preparing package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n   \'svmdoc.Rnw\' \n** testing if installed package can be loaded\n\n* DONE (e1071)\na1mjjung@secm:[e1071_build] $ R\nJob <13412787> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n\nR version 2.15.2 (2012-10-26) -- \"Trick or Treat\"\nCopyright (C) 2012 The R Foundation for Statistical Computing\nISBN 3-900051-07-0\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType \'license()\' or \'licence()\' for distribution details.\n\nR is a collaborative project with many contributors.\nType \'contributors()\' for more information and\n\'citation()\' on how to cite R or R packages in publications.\n\nType \'demo()\' for some demos, \'help()\' for on-line help, or\n\'help.start()\' for an HTML browser interface to help.\nType \'q()\' to quit R.\n\n> ?svm\nNo documentation for \'svm\' in specified packages and libraries:\nyou could try \'??svm\'\n> library(e1071) \nLoading required package: class\n> ?svm\n> \nSave workspace image? [y/n/c]: y\na1mjjung@secm:[e1071_build] $ pwd\n/home/X0101/a1mjjung/x/R/e1071_build\na1mjjung@secm:[e1071_build] $ ls -alF\ntotal 284\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:47 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\n-rw------- 1 a1mjjung X0101     25 Mar  6 18:47 .Rhistory\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:46 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\n\n</pre>\n\n\n\n\n=== 3rd meeting with Dr. CHOI ===\n\n\n\n=== 수퍼컴 (supercom) ===\n\n\n* account\n: ID: a1mjjung\n: PW: wjdaudwns\n: IP address: 202.20.183.10 (ssh)\n\n* password change\n: 한지연 사원 (jiyoun92.han@partner.samsung.com) (031-280-8147)\n\n* python 2.7.x from 2.6.x\n: [a1mjjung@login03 ~]$ /apps/Python/Python-2.7.3/bin/python\n\n== ## bNote-2013-03-05 ==\n\n=== 최희열 전문과 2차 미팅 ===\n\n* IO trace (궤적)의 변화 예측\n: IO 궤적의 변화를 신속 정확하게 예측하는 것의 중요성\n:: 언제 hot data가 fast-tier에 올라가야 할지,\n:: 현재의 hot data 말고 새로운 hot data는 언제 올라가야할지?\n:: 현재 fast-tier에 올라가 있는 hot data가 언제 unload되어야 할지?\n\ncached/tiered hot data가 계속 hot storage area에 \n: IO hot zone 궤적 변화의 전조로 사용될 수 있을만한 시스템 정보/상태 변화\n: IO hot zone 궤적이 정말로 변화하는 것일지 아니면 기존 궤적에서의 fluctuation인지?\n\n* Challenging Problems\n\n: 단위 시간 당 hot zone의 갯수가 항상 유지되는 것은 아님. 즉, hot zone의 갯수는 1개로 유지된 채, hot zone의 위치만 이동하는 형태는 아닐 수 있음. hot zone의 갯수가 1개였다가 4개로, 7개로, 줄어서 5개로, 다시 줄어서 2개로, 이런 식으로\n\n== ## bNote-2013-03-05 ==\n\n=== IOWA to ML Formulation ===\n\n* IO trace (궤적)의 변화 예측\n: IO 궤적의 변화를 신속 정확하게 예측하는 것의 중요성\n:: 언제 hot data가 fast-tier에 올라가야 할지,\n:: 현재의 hot data 말고 새로운 hot data는 언제 올라가야할지?\n:: 현재 fast-tier에 올라가 있는 hot data가 언제 unload되어야 할지?\n\ncached/tiered hot data가 계속 hot storage area에 \n: IO hot zone 궤적 변화의 전조로 사용될 수 있을만한 시스템 정보/상태 변화\n: IO hot zone 궤적이 정말로 변화하는 것일지 아니면 기존 궤적에서의 fluctuation인지?\n\n* Challenging Problems\n\n: 단위 시간 당 hot zone의 갯수가 항상 유지되는 것은 아님. 즉, hot zone의 갯수는 1개로 유지된 채, hot zone의 위치만 이동하는 형태는 아닐 수 있음. hot zone의 갯수가 1개였다가 4개로, 7개로, 줄어서 5개로, 다시 줄어서 2개로, 이런 식으로','utf-8'),(10,'== ## bNote-2013-04-29 ==\n\n=== 경쟁사분석 ===\n\n* [http://www.computerweekly.com/news/2240182642/Fusion-io-buys-NexGen-to-join-hybrid-flash-array-fray Fusion IO Buys NexGen to Join Hybrid Flash Array // 2013-04-24]\n\n* [http://www.storagereview.com/fusionio_announces_nexgen_storage_acquisition Fusion-io Announces NexGen Storage Acquisition // 2013-04-24]\n\n* [http://www.theregister.co.uk/2013/04/24/fusion_io_nexgen/ Fusion-io buys NexGen - Gets hybrid flash/disk array startup // 2013-04-24]\n\n* [http://venturebeat.com/2013/04/24/fusion-io-acquires-hybrid-storage-appliance-vendor-nexgen-storage-for-114m/ Fusion-io acquires hybrid storage appliance vendor NexGen Storage for $114M // 2013-04-24]\n\n* [http://www.computerweekly.com/news/2240179705/ID7-buy-takes-Fusion-io-deeper-into-software-defined-storage ID7 buy takes Fusion-IO deeper into software-defined storage]\n\n* [http://searchstorage.techtarget.com/opinion/Software-defined-storage-Is-hardware-obsolete Software-defined Storage: Is Hardware Obsolete? // SearchStorage.techtarget.com]\n\n* [http://www.storagereview.com/fusionio_acquires_id7_developers_of_scst Fusion-IO Acquires ID7, Developers of SCST (SCSI Target Subsystem)]\n\n* [http://www.theregister.co.uk/2013/04/09/blind_spot/ Mutant Array Upstarts Feast on EMC, NetApp\'s Leavings -- Nimble, Tegile, Tintri -- SSD/HDD Hybrid Array Startups re-inventing the hybrid array with new software]\n\n== ## bNote-2013-04-26 ==\n\n=== Consistent Hashing ===\n\n* [http://www.tom-e-white.com/2007/11/consistent-hashing.html Consistent Hashing Illustrated ((B.GOOD))]\n\n* [http://amix.dk/blog/post/19367 Consistent Hashing Simply in Python]\n\n=== SDN ===\n\n==== SDN의 정의 및 발생 배경 ====\n\n* [http://katesfam.blogspot.kr/2012/01/sdn.html SDN은 무엇인가? 그리고 왜 대두되었는가?]\n \n <pre>\n\nSDN에 대해 잘 정리한 글이 있어서 보내드립니다.\n\nhttp://katesfam.blogspot.kr/2012/01/sdn.html\n\n \n\n\n\n----\n\n \n\nSDN은 무엇인가? 그리고 왜 대두되었는가?\n\n \n\n \n\n1. SDN은 무엇인가?\n\n\n수년 동안 컴퓨터 과학자들은 네트워크의 속도와 안정성, 에너지 효율, 보안 등을 획기적으로 개선시킬 수 있는 방법을 꿈꿔왔다. 그러나 그 방법을 설계하거나 고안하더라도, 실제로 대규모(large-scale)로 실험하거나 검증하는 것은 불가능했다. 인터넷의 코어(core)를 구성하는 라우터나 스위치들이 이른바 완전히 닫혀 있어서 그 위에서 새로운 소프트웨어나 프로그램을 실험하는 것이 원천적으로 봉쇄되었기 때문이다.\n\n \n\n이러한 연유로 연구되어온 많은 기술 중 SDN은 Software Defined Networking을 의미하며 우리말로 소프트웨어 정의 네트워킹이라 부른다. SDN은 OpenFlow라는 기술 혹은 소프트웨어를 통하여 널리 알려졌다. OpenFlow와 SDN은 뗄레야 뗄 수 없는 관계이다. SDN이 물론 더 큰 개념으로 네트워크 구조 혹은 새로운 패러다임이며, OpenFlow는 SDN을 위한 “인터페이스 표준 기술”로 정의된다. SDN을 지원하는 기술 중에서 학교, 연구소, 기업 등으로부터 가장 관심을 받는  OpenFlow는 별도로 설명하기로 하고 (이미 많은 참고자료가 나와 있기도 하다), 여기서는 먼저 SDN이 무엇인지, 그리고 왜 필요성이 부각되었는지에 대하여 몇 가지 레퍼런스를 바탕으로 다루어 보려 한다.\n\n \n\n먼저 위키피디어의 정의를 살펴보자. Kate Greene이 2009년도3/4월 판 MIT 테크니컬 리뷰에서 소개한 용어로 알려져 있는 SDN은 네트워크 제어 기능(control plane)이 물리적 네트워크와 분리되어 있는 “네트워크 구조”를 말한다. 위키피디어에 따르면  SDN을 특징짓는 두 가지 중요한 포인트는 다음과 같다. 첫째, 네트워크 제어 기능을 데이터 전달 기능(data plane)과 분리하여 구현해야 한다. 둘째, 네트워크 제어 기능이 개발되고 실행될 수 있는 환경을 분리하여 전형적인 낮은 성능의 CPU가 장착된 하드웨어 스위치에 더 이상 위치시키지 않는다. 다시 말해서 SDN이라면 기본적으로 네트워크 제어 기능이 기존의 스위치나 라우터 등의 하드웨어와 별도로 분리되어야 하고, 데이터 전달 기능과도 역시 분리되어 개발 및 실행될 수 있는 네트워크 구조를 가져야 한다.\n\n \n\n분리된 SDN의 제어 기능은 필연적으로 네트워크 스위치(하드웨어) 상의 데이터 경로와 상호작용할 수 있는 기능을 가져야만 한다. 이러한 상호작용 혹은 통신 메커니즘 중의 하나가 바로 OpenFlow 기술이다. OpenFlow는 흔히 SDN과 동일한 것으로 혼동되기도 하지만, 사실 SDN을 구성하는 하나의 요소로 제어 기능을 가진 머쉰과 네트워킹 스위치간의 통신을 담당하는 표준 인터페이스이다. 그리고, SDN의 범주 안에서 OpenFlow를 반드시 사용해야 한다는 아무런 제약이나 요구사항도 없다. 현재 SDN과 OpenFlow의 정의, 마켓팅 등의 이슈는 개방형 네트워킹 재단(Open Networking Foundation; ONF)에서 관리되고 있다. \n\n \n\n그렇다면 ONF에서는 SDN을 어떻게 바라보고 있을까? 일단 ONF가 무엇인지부터 살펴보자. ONF는 (미연방세법을 따르며) 비영리, 상호 이익을 바탕으로 하는 국제 기구로 SDN의 개발과 활용을 촉진하는 것을 목표로 삼고 있다. ONF의 이사회는 여덟 명의 멤버로 구성되는데 여섯 개의 설립 회사가 각각 한 명씩 지정한 여섯 명의 이사와 두 명의 창립자이다. 여섯 개의 설립 회사는 대규모 네트워크 운영자 및 (잠재) 사용자 그룹을 대표하는 도이치 텔레콤(Deutsche Telecom), 페이스북(Facebook), 구글(Google), 마이크로소프트(Microsoft), 버라이즌(Verizon)과 야후(Yahoo)이며, 두 명의 창립자는 UC 버클리의 Scott Shenker와 스탠포드 대학의 Nick Mckeown이다. 그리고 이외에 사무총장(Executive Director) Dan Pitt이 ONF를 총괄 관리한다.\n\n \n\nONF가 SDN을 바라보는 관점은 크게 두 가지의 기본적인 원칙을 바탕으로 하고 있다.\n\n \n\n먼저 SDN은 소프트웨어 정의 포워딩(Software Defined Forwarding)을 해야 한다. 이것은 스위치와 같은 하드웨어가 수행하는 데이터 포워딩 기능이 반드시 개방형 인터페이스와 소프트웨어를 통해서 제어되어야만 한다는 것을 의미한다. 하드웨어는 소프트웨어로부터 [헤더 템플릿, 포워딩 액션] 셋을 받아 특정한 액션(action)을 실행한다. 예를 들면 어떤 네트워크 포트로 패킷을 “전달(forwarding)”하거나 혹은 “폐기(drop)”할 수 있다. 다만 해당 특정 액션은 [헤더 템플릿, 포워딩 액션]의 “헤더 템플릿”에 상응하는 패킷에 대해서만 실행된다. 여기서 헤더 템플릿은 “모든 패킷” 혹은 “어떤 패킷의 그룹”등을 의미하는 와일드 카드를 포함할 수 도 있다. 앞서 언급되었듯이 SDN의 소프트웨어 정의 포워딩은 반드시 개방형 인터페이스와 소프트웨어를 포함하는데, OpenFlow 기술이 “개방형 인터페이스”에 해당된다.\n\n \n\n그리고, 두 번째 원칙은 SDN이 추상화된 글로벌 관리 혹은 글로벌 관리 추상화(Global Management Abstraction)를 목표로 한다는 것이다. SDN은 기본적인 글로벌 관리 추상화를 지원함으로서 보다 선도적인 네트워크 관리 툴이 개발될 수 있도록 해야 한다. 예를 들면 이런 추상화 도구들은 네트워크의 글로벌 뷰, 네트워크 이벤트(토폴로지 변화나 새로운 플로우 생성 등)에 따른 반응, 그리고 네트워크 요소를 제어할 수 있는 기능 등을 포함할 수 있다. (네트워크 요소 제어는 해당 엔트리를 하드웨어의 포워딩 테이블에 넣는 방법을 사용한다.)\n\n \n\n따라서, ONF 가 바라보는 SDN은 두 가지, 즉,  소프트웨어 정의 포워딩과 글로벌 관리 추상화가 핵심이다. 그리고, 이를 위해서 개방형 인터페이스(예: OpenFlow), 제어 소프트웨어, 글로벌 네트워크 관리 툴 등의 세부적인 기능이 언급되었다.\n\n \n\n여기서 잠시 위키피디어로 돌아가보자. 위키피디어에서 언급한 Kate Greene (과학기술 저널리스트)이 작성한 테크니컬 리뷰의 내용을 보면 SDN이 무엇인지 개괄적으로 잘 정리되어 있다. 이 기사에 따르면 ONF의 창립자 중 하나이자 이사회 멤버이며, OpenFlow 기술과 표준을 개발한 Nick McKeown이 다음과 같이 말한다. “오늘날 보안, 라우팅, 에너지 효율 관리 등은 단지 기계덩어리인 네트워크 장비에 의해 좌지우지됩니다. 그건 정말 바꾸기 힘들지요. 이것이 바로 인터넷 인프라가 40년 동안이나 변하지 않은 이유입니다.” 일반적으로 데이터 패킷이 스위치 (혹은 라우터)에 도착하면 스위치의 펌웨어가 해당 패킷의 목적지 주소를 보고 그 패킷을 이미 정해진 규칙에 따라 포워딩(forwarding)한다. 정해진 규칙은 네트워크 운영자도 제어하기 어렵다. 같은 목적지를 갖는 모든 패킷은 같은 경로를 이용하고 언제나 같은 방식으로 다루어진다. 이것이 현대 인터넷의 일반적인 패킷 포워딩 방식이다.\n\nSDN은 무엇인가? 라는 질문의 답은 바로 현재 인터넷이 가지고 있는 “항상 같은 방식이며 제어가 어려운” 패킷 포워딩 방식을 바꾸는 것으로 부터 시작한다. OpenFlow의 예를 들면, 그 전에는 거의 아무도 손대기 어려웠던 종단간 네트워크 경로를 컴퓨터 과학자들이 쉽게 변경할 수 있도록 지원하여 e-mail보다 비디오 어플리케이션이 우선 데이터를 받을 수 있도록 하거나 다양한 트래픽을 각자 다른 경로로 보낼 수도 있고, 어떤 트래픽은 보안 목적으로 격리할 수 있도록 해준다.\n\n \n\n그렇다면 패킷 포워딩 방식을 바꾸는 것이 바로 SDN인가? 그렇지 않다. 이것은 SDN이라는 큰 구조를 구성하는 하나의 요소일 뿐이다. OpenFlow가 바로 패킷 포워딩 방식을 표준화된 방법으로 바꿀 수 있는 하나의 콤포넌트이다. SDN은 아키텍쳐 혹은 프레임을 제공하는 큰 개념이자 구조 혹은 패러다임으로, 하드웨어와 어플리케이션, 하드웨어 추상화 계층, 하드웨어와 분리된 제어 기능(control plane, controller 등으로 불리운다.), 하드웨어 추상화 계층과 통신하는 표준 기능 등을 모두 포함한다.\n\n \n\nSDN 구조에서, 하드웨어는 스위치나 라우터 등이며, 시스코, 쥬니퍼, HP, NEC 등이 개발하고 현재 인터넷 공급자들이 서비스를 제공하기 위하여 설치하고 운영하는 하드웨어 박스(box)를 의미한다. Nick이 말했듯이 오늘날의 인터넷이 거의 변화하지 못한 가장 큰 원인을 제공하는 주범들이다. SDN은 이 기계덩어리에 하드웨어 추상화를 위한 계층을 더해준다. 즉, OpenFlow와 같이 표준화된 인터페이스를 통하여 하드웨어에 접근하고, 소프트웨어에 기반하여 하드웨어를 “통제”할 수 있는 기반을 제공하는 것이다. 이 추상화 계층은 OpenFlow의 플로우 테이블과 같은 형태로 하드웨어에 구현되어야 한다. 따라서 하드웨어 벤더들의 지원과 협력이 필수적이다. (현재 약 16개의 주요 네트워크 벤더들이 구현했거나 구현중이다. SDN을 지향하는 OpenFlow가 가장 선두에서 탄력받는 기술로 주목받고 있는 배경이기도 하다.) 추상화 계층을 구현함에 있어서 가장 중요한 것은 표준화된 인터페이스를 지원하는 것이고, 두 번째는 각 개별 벤더가 원하는 “독립성”을 보장해 주는 것이다. 개별 벤더의 고유 기술이나 고유 기능은 자치적으로 보장되면서 SDN을 위한 표준화된 통로를 제공하는 것은 벤더 고유의 기술을 보호하면서도 호환성을 유지하는데 있어서 필수적이며, OpenFlow의 경우 이를 매우 잘 준수하고 있다.\n\n \n\n다음으로 무엇보다 중요한 요소는 바로 하드웨어와 분리된 제어 기능이다. Controller로 불리기도 하는 이 제어 기능은 스위치나 라우터가 아닌 별도의 머쉰 상에서 구현된다. 머쉰은 PC가 될 수도 있고 성능 좋은 서버가 될 수도 있다. 이 제어 기능은 두 가지 SDN의 다른 두 가지 콤포넌트와 상호작용한다. 한 가지가 어플리케이션이고 다른 한 가지는 하드웨어, 좀 더 정확히는 하드웨어에 구현된 추상화 계층이다. 따라서 제어 기능을 통해서 어플리케이션은 네트워크의 다양한 정보를 얻을 수 있고, 반대로 네트워크 역시 어플리케이션 요구 사항 등의 정보를 얻을 수 있다. 이러한 핵심적인 역할 때문에, 제어 기능은 종종 네트워크 운영 체제(Network OS)라고 불리우기도 한다.\n\n \n\n제어 기능은 주로 API와 같은 방법을 통해서 어플리케이션이 원하는 기능을 제공한다. 반대의 경우도 거의 같다. 어플리케이션과 제어 기능 간의 통로인 셈이다. 그렇다면 제어 기능과 하드웨어 추상화 계층의 통신은 어떻게 이루어질까? OpenFlow가 이 질문에 대한 해답이며, 이미 많은 연구가 진행되어 있다.\n\n \n\n \n\n2. SDN이 대두된 이유\n\n \n\n지금까지 위키피디어, ONF, 테크니컬 리뷰 등을 인용하고 몇 가지 살을 붙여 SDN이 무엇인지 정리해 보았다. SDN은 새로운 네트워크 구조이며 패러다임이다. 그럼 이제부터 SDN이 왜 대두되었는지 알아보기로 하자.\n\n \n\n가장 최근에 ONF가 개최한 컨퍼런스인 Open Networking Summit(2011년 10월)에서 ONF의 또 다른 창립자인 Scott Shenker가 발표한 내용을 보면, 인터넷이 이렇게 크게 성공한 가장 큰 이유가 바로 “계층화(layering)”에 있다는 것을 알 수 있다. 어플리케이션(WWW, e-mail 등)은 신뢰성/비신뢰성 트랜스포트 계층(TCP/UDP)위에서 돌아가고, 트랜스포트 계층은 최선형 글로벌 패킷 전달 계층(IP)위에서 동작되며, IP는 최선형 로컬 패킷 전달 계층(Ethernet, PPP 등)위에서, 다시 로컬 패킷 전달 계층은 물리 계층(copper, fiber, radio등) 위에서 돌아가는 것이 바로 계층화이다. 즉, 서로 다른 계층이 독립적으로 동작하되 상/하위 계층과 상호 호환되는 방식으로 일종의 혁신을 이룬 것이다. 덕분에 인터넷은 교육/연구망으로 시작되었으나, 상업적으로도 엄청난 성공을 거두었고 지금까지 가장 널리 사용되고 있다. 그러나, 초창기 개발된 인터넷 구조나 모델이 아직도 거의 그대로 사용되고 있는 형편으로, 상업적인 성공이 또 다른 인터넷의 혁신으로 이어지지 못했다. 왜 그럴까?\n\n \n\n이 질문에 답하기 전에 먼저 다른 분야를 한 번 살펴보자. 예를 들어 컴퓨터 운영체제(OS), 데이터베이스(DB), 분산 시스템 등은 소프트웨어의 연구 개발을 통해 발전해 왔다. 학교에서 배우는 기본적 원리들을 바탕으로 소프트웨어를 개발하고 진화시킨 것이다. 이러한 소프트웨어는 우리가 흔히 알고 있는 고급 프로그래밍 언어로 작성할 수 있으며, 새로운 기능이 필요한 경우 기존에 개발된 소프트웨어를 바탕으로 새로운 버젼으로 계속해서 발전할 수 있다. LINUX, Windows, Mac OS 등이 이런 방식으로 진화해온 대표적인 OS 이다. 데이터베이스나 분산 시스템도 마찬가지이다. 그리고 이러한 소프트웨어 기반의 시스템들은 대부분 편리하고 쉬운 유저 인터페이스를 통해 쉽게 관리 가능한 환경을 가지고 있다.\n\n \n\n그렇다면 네트워크는 어떤가? 일단 학교에서 OSI 7 계층 부터 시작하여 TCP, IP 등등의 여러 프로토콜에 대해서 배운다. 이들의 기본적 원리나 알고리즘에 대해서도 배우지만 대부분 동작 원리 등 실용적인 부분에 집중된다. 물론 이른바 단말 시스템(end-system)에서 돌아가는 TCP 등의 일부 프로토콜은 기능이 향상된 버젼이 개발되었거나 개발이 진행 중이다. 하지만, 단말과 단말 사이에서 데이터 전송과 전달을 담당하는 액세스/코어 네트워크의 프로토콜이나 기타 기능들은 이미 대부분 개발이 끝나서 적용 및 서비스 되고 있는 단계이기 때문에 새롭게 수정하거나 개발하기 어렵다. 물론 네트워크 벤더의 경우 추가적인 기술 개발과 적용이 가능하지만 DB나 운영체제 등과 비교할 때 많은 진보가 일어나지는 않았다. 네트워크 분야의 경우, 소프트웨어와 프로그래밍을 바탕으로 한 새로운 기술의 개발과 진화가 다른 컴퓨터 과학 분야와 비교할 때 그 발전이 더디게 진행되어 온 것은 분명한 사실이다. 네트워크 관리 환경은 어떠한가? 일부 운영자나 엔지니어에게 종속되어 있는데다가 그 마저도 사용하기가 쉽지 않다. 전문적인 지식이나 기술을 요구하는 경우도 많다. 최근 몇 몇 연구들이 이러한 관리 환경을 개선하는데 그 촛점을 맞추고 있긴 하지만 다른 분야에 비해서 부족한 상태로, 매우 불편한 사용자 인터페이스를 가지고 있다.\n\n \n\n“구조는 단순하지만 관리가 복잡하고 인터페이스는 어렵다.” 이것이 현재의 인터넷이 가지고 있는 가장 큰 문제로 귀결된다. SDN이 대두된 이유는 바로 이 화두를 타파하기 위해서이다.\n\n \n\n단순한 구조는 물론 장점이다. 덕분에 인터넷이 오늘날의 압도적 지위를 누리게 되었다. 하지만 단순성을 유지하기 위하여 새로운 기술을 적용하거나 소프트웨어를 개발하는 측면에서 다른 분야에 비해 큰 제약을 가져야만 했다. 이와 같은 단점을 보완하기 위하여 SDN은 제어 프레임워크, 혹은 제어 기능을 분리하여 소프트웨어의 개발을 촉진시키고자 한다. 즉, 소프트웨어를 기반으로 인터넷이 보다 빠른 속도로 진화할 수 있도록 하자는 것이다. 그리고, SDN은 관리의 복잡성을 해소하기 위하여 제어 기능을 기존 하드웨어에서 분리시키고, 사용자 인터페이스를 매우 단순하고 편리하게 만듦으로써 엔지니어, 운영자 뿐만 아니라 일반 사용자도 쉽게 네트워크를 관리하고, 가상 네트워크를 생성하여 이용할 수 있도록 한다.\n\n \n\n참고로, 미래인터넷 포럼(FIF)의 테스트베드 워킹그룹 이슈 분석서 #2에서 Nick의 발표 내용을 정리한 부분을 보면, SDN이 어떻게 인터넷이 가지고 있는 문제를 해결하고 혁신을 가속화할 수 있는지에 대하여 잘 알 수 있다.\n\n\nNick은 SDN을 기반으로 한 새로운 패러다임을 만들어 내어서 아래와 같은 혜택들을 누리는 네트워크 장치 생태계를 만드는 것이 필요하다고 역설한다.\n\n\n    1.     데이터 전달(Forwarding) 추상화에 따라서 OpenFlow 표준에 의해 검증된   하드웨어를 이용하고, 또한 이에 연동하여 소프트웨어 기반으로 제공되는 네트워킹 특성이 요구되는 모든 절차들에 대해서 각각의 절차마다 충분하게 증빙되어 있는 견실한 네트워킹을 실현할 수 있는 토대를 구축할 수 있다.\n    2.     장비 사용자들이 자신의 필요에 따라 네트워킹을 유연하게 구성(customize)하고 불필요한 구성요소들은 과감하게 제거하면서 자신만을 위해서 가상화된 네트워크를 생성하기 쉽도록 지원한다.\n    3.     하드웨어 추상화(abstraction)에 따라 확장성을 고려한 상태에서 공통화된(즉 commodity 형식으로) 하드웨어를 구입하고, 또한 소프트웨어도 분리해서 구입하도록 하여 기존의 폐쇄적인 네트워크 장비 공급자 체인을 벗어나서 자체 개발, 외주 개발, 오픈 소스 형식을 모두 포함하는 다변화된 공급자 체인으로 체질 개선을 유도할 수 있다.\n    4.     소프트웨어를 개발하는 속도로 혁신이 일어나도록 하고, 표준은 구현된 소프트웨어의 확산을 위해 뒤따라가는 방식을 취하고, 소프트웨어적인 개방성에 근간하여 기술의 공유 협력을 쉽도록 함으로써 혁신의 속도를 가속하도록 지원한다.\n\n \n\n다른 내용도 모두 중요하지만, 특히 4번에 주목하자. 소프트웨어를 개발하는 속도로 혁신이 일어나게 하고 표준은 이를 뒤따르게 하자는 것이야 말로 SDN이 왜 대두되었는지 설명하는 핵심 중의 핵심이라 할 수 있다.\n\n \n\n내용이 두서없이 길어진 듯 하다. 이제 정리해보자. SDN은 아직도 정의되고 있는 단계이다. 본문에서 언급한 여러 레퍼런스를 보면 분명히 주된 맥락은 있지만 모호하고 추상적인 부분도 있는 것이 사실이다. 따라서 앞으로 SDN의 모습은 계속해서 변화될 가능성이 틀림없이 존재한다. 그렇지만 주된 맥락을 고려할 때 SDN은 현재 시점에서 다음과 같이 정의될 수 있을 것이라 생각한다. \n\n\n\"SDN은 이른바 소프트웨어를 통해서 현재의 인터넷이 가지는 구조적 문제를 근본적으로 해결하고 혁신할 수 있도록 대두된 새로운 네트워크 구조 혹은 패러다임으로써, 어플리케이션, 네트워크 OS, 하드웨어 추상화, 표준화된 인터페이스 및 하드웨어를 모두 아우르는 개념\"이다.\n\n</pre>\n\n==== SDN 관련 소식 ( Open Daylight ) ====\n\n\n* [http://www.zdnet.co.kr/news/news_view.asp?artice_id=20130422135700 SDN 국면 전환을 꿈꾸다 \'오픈데이라이트\']\n\n <pre>\n\n[ Open Daylight 출범으로 본 SDN 물결의 변화 ] \n\n\n* 시스코, 주니퍼 등 벤더 중심의 SDN 연합체인 Open Daylight 출범(\'13년 4월 초)\n\n \n\n\n\n* 리눅스재단이 주도, 시스코, 주니퍼, IBM, MS, 레드햇,\n\n빅스위치, 브로케이드, 시트릭스 등등이 Platinum 스폰서로 참여.\n\n\n\n \n\n* 의미\n\n  : 학계, 구글, AT&T, NTT 등 서비스 업체에 의해 주도 되었던 기존 SDN 움직임과는 달리,\n\n    Open Daylight는 IT의 Big Vendor들이 주도하는 SDN 움직임이라는 차이점이 있음\n\n    (시스코는 오픈네트워크환경(ONE) 컨트롤러를 Open Daylight에 기증하는 등,\n\n     기존의 SDN에 대한 미온적인 입장을 버리고 적극적으로 SDN 물결에 동참)\n\n \n\n\n\n* Open Daylight의 프로젝트 구성\n\n   - Flexible 컨트롤러 프로젝트\n\n   - 가상 네트워크 프로젝트\n\n   - Java 기반 프로토콜 플러그인 프로젝트\n\n   - 프로그램 가능한 인터페이스 프로젝트\n\n   - SDN 응용 프로젝트 ++\n\n\n(++) SDN 응용의 중요성:\n\nSDN 컨트롤러 자체만으로는 의미가 크지 않음.\n\nSDN을 적극적으로 활용하는 응용들이 많아져야 SDN 생태계가 활성화될 수 있음\n\n\n\n \n\n* Open Daylight은 올해(\'13년) 3분기에 정식 코드 공개 예정\n\n\n\n \n\n* Open Daylight의 라이센스는 EPL(Eclipse Public License)로서,\n\n   코드를 수정해 사용하거나 별도 애플리케이션을 개발했더라도, 코드 자체를 공개하지 않아도 됨.\n\n\n\n* 관련 기사:\nSDN 국면전환을 꿈꾸다 \'오픈데이라이트\' [ZDnet 4/22]\nhttp://www.zdnet.co.kr/news/news_view.asp?artice_id=20130422135700\n\n\n\n \n\n \n\n--\n\n\nZDnet KR - All\n SDN 국면전환을 꿈꾸다 \'오픈데이라이트\' \n\n[지디넷코리아] 세계 IT거인들이 집결한 소프트웨어정의네트워킹(SDN) 연합체 ‘오픈데이라이트’가 출범했다. 시스코, IBM, 레드햇, 마이크로소프트(MS), 빅스위치 등 데이터센터 관련업체 대부분이 참여한 범 개방형 네트워크 연합체다. \n\n오픈데이라이트는 이달 7일 공식 출범을 선언하고 오픈소스 기반의 표준 SDN 프레임워크를 개발하겠다고 밝혔다. \n\n리눅스재단이 주도하며, 시스코, IBM, MS, 레드햇, 빅스위치, 브로케이드, 주니퍼네트웍스, 에릭스, 시트릭스 등이 플래티넘 스폰서로 참여했다. 이밖에 VM웨어, NEC 등이 골드 스폰서로, HP, 델, 아리스타, 인텔, 누아지네트웍스(알카텔루슨트), 플럼그리드 등이 실버 스폰서로 등록했다. \n\n오픈데이라이트는 오픈소스 SDN 컨트롤러와 가상 오버레이 네트워크, 프로토콜 플러그인, 애플리케이션, 아키텍처 및 프로그램 가능한 인터페이스 등의 개발프로젝트를 진행한다. 오는 3분기 정식 코드가 공개될 예정이다. \n\n그동안 SDN 분야는 학계와 구글, AT&T, NTT 등 서비스업체를 중심으로 발전했다. 벤더 종속없는 네트워크 환경을 구축해보자는 움직임에서 출발한 SDN은 오픈플로란 오픈소스 프로토콜을 탄생시키기에 이른다. SDN 바람 속에서 기존 벤더들은 끌려가는 듯한 인상을 줬다. \n\n오픈데이라이트는 그동안 주도권을 쥐지 못했던 벤더들이 뭉쳐 SDN 흐름을 주도하려는 노림수다. 때문에 기존 개방형 네트워크를 주도해온 진영으로부터 의심의 눈초리를 받는 것도 사실이다. \n \n \n■오픈데이라이트 주도 시스코 ‘태도변화 or 전략’ \n\n오픈데이라이트는 공식 출범 이전부터 화제였다. 당초 SDN 분야에 미온적인 이미지를 줬던 시스코가 주도적인 역할을 담당한다는 소문 때문이었다. 네트워크업계 독불장군의 대명사였던 시스코의 참여만으로 관심을 끌기 충분했다. \n\n시스코는 그동안 업계표준 작성을 위한 각종 연합체와 대립하면서, 독자 행보를 고집했었다. 그러던 시스코도 최근 2년 사이 아파치, 오픈스택, 리눅스 등 오픈소스 재단을 적극 지원하는 등 변화된 모습을 보이긴 했다. \n\n데이비드 옌 시스코 데이터센터그룹 수석부사장은 최근 텔레프레즌스를 통한 기자간담회에서 오픈데이라이트의 의의를 “업계리더들이 리눅스 재단 아래서 형성한 오픈소스 프로젝트로, 기업들이 SDN을 채택하고 혁신하도록 하는 걸 목표로 한다”라며 “기업의 SDN 도입을 실현하기 위해 벤더들이 지원 프레임워크를 구축하게 된다”라고 설명했다. \n\n그에 따르면, 오픈데이라이트는 업계 전문업체들이 머리를 맛대고 SDN 환경의 기업 도입을 앞당기기 위한 움직임이다. 그동안 중구난방으로 개발됐던 SDN관련 기술을 통합해 어디서나 활용가능한 개방형 표준을 만든다는 것이다. \n\n오픈소스의 정신과 이점을 살려 자유로운 참여를 보장함으로써, SDN 프레임워크 개발속도와 완성도를 빠르게 높이겠다는 의도도 있다. \n \n▲ 오픈데이라이트 프레임워크 1.0 버전 \n \n공식적으로 오픈데이라이트는 컨트롤러 프로젝트를 중심으로, 가상 네트워크, 자바 기반 프로토콜 플러그인, 애플리케이션, 아키텍처 및 프로그램 가능한 인터페이스 등의 프로젝트로 구성된다. \n\n현재 다운로드 가능한 오픈데이라이트 컨트롤러는 1.0 버전이다. 이를 기반으로 노스바운드API로 오픈스택, 클라우드스택 등과 연동되며, 사우스바운드 API로 오픈플로 네트워크 환경을 제어한다. 시스코가 자사의 오픈네트워크환경(ONE) 컨트롤러를 기증했다. \n\n데이비드 옌 부사장은 “현재 사우스바운드 API모듈을 통해 오픈플로 1.0 아키텍처에 추가함으로써 사용할 수 있다”라며 “자바 번들에 들어가는 HA 모듈을 시스코에서 기증했고, 다른 벤더와 고객사들이 여러 애플리케이션 모듈을 자유롭게 개발해 사용하고, 프로젝트에 기여하게 된다”라고 강조했다. \n\n오픈플로 컨트롤러 ‘플러드라이트’를 보유한 빅스위치도 컨트롤러 고도화에 기여한다. 시스코에서 기증한 컨트롤러 코드를 기초로 하지만, 자유롭게 코드를 수정할 수 있기 때문이다. 여기에 IBM, MS, 레드햇, HP, 델, 브로케이드, 주니퍼, 아리스타, 인텔 같은 회사의 소속 개발자가 다양한 애플리케이션과 운영사례를 개발해 기여한다. \n\n오픈데이라이트의 라이선스는 자바영역에서 주로 활용되는 EPL(Eclipse Public License)이다. 코드를 수정해 사용하거나 별도 애플리케이션을 개발했더라도, 코드 자체를 공개하지 않아도 된다. \n\n■오픈데이라이트는 ONF를 하위로 끌어내리려는 노림수? \n\n오픈네트워킹파운데이션(ONF) 주도의 오픈플로는 향후에도 별도로 존재한다. 오픈데이라이트는 사우스바운드API에 집중했던 ONF에 비해 노스바운드API와 전반적인 클라우드 매니지먼트 자동화란 큰 틀에서 접근한다. \n\n옌 부사장은 “오픈데이라이트 프로젝트의 목표는 두 가지로 첫 번째는 컨트롤러, 사우스바운드 및 노스바운드 API, 관련 툴과 서비스 기능을 포함하는 완벽한 SDN 컨트롤러 스택을 개발”이라며 “이 보다 더 광범위한 목표가 바로 애플리케이션, 툴, 서비스 전달은 물론 시장 지원도 가능한 컨트롤러 스택 전반을 구축할 수 있는 생태계를 마련하려는 것”이라고 강조했다. \n\n오픈플로진영은 그동안 컨트롤 플레인과 데이터 플레인을 구별하고, 하부 데이터 플레인의 관리분야에 집중했다. ONF가 클라우드 플랫폼 상의 네트워크 환경 관리를 위한 애플리케이션 개발에 눈을 돌리기 시작한 건 최근의 일이다. \n\n오픈데이라이트는 ONF의 오픈플로 버전 고도화를 반영하는 방향으로 접근할 것으로 예상된다. \n\n데이비드 옌 부사장은 “ONF는 오픈데이라이트 프로젝트의 공식 멤버는 아니지만, 프로젝트 태동 초기 단계부터 관여해왔다”라며 “오픈데이라이트와 ONF 모두 유사한 목표를 갖고 있기 때문인데, 오픈데이라이트 프로젝트의 초창기 목표 중 하나는 ‘사우스바운드’ 오픈플로우 플러그인이 될 것”이라고 설명했다. \n\n오픈데이라이트의 첫 코드 공개와 별도로, 시스코는 개발자 활용이 가능한 코드를 포스팅하고 있다. \n\n옌 부사장은 “오픈데이라이트 코드 자체의 가용성 측면에서만 본다면 시스코의 경우는 이미 개발자들이 활용 가능하도록 코드 포스팅을 시작했다”라며 “이에 첫 번째 공식 릴리즈, 즉 구현 가능한 완벽한 패키지는 올해 3분기 정도에는 나올 수 있을 것으로 기대하고 있다”고 전망했다. \n\n그는 “시스코는 올해 6월 시스코ONE 컨트롤러 SW 발표 후 추가적인 모듈을 공개할 계획”이라며 “트러블 슈팅, 인증, 슬라이싱 등을 위한 애플리케이션이 포함될 예정”이라고 밝혔다. \n\n6월말 개최될 ‘시스코라이브’ 행사에서 SDN 컨트롤러를 위한 모듈을 공개한다는 것이다. 그는 크게 3가지 정도의 모듈이 공개될 것으로 설명했다. \n\n구체적으로 우선, 네트워크 슬라이싱이다. 공유된 물리적 네트워크를 논리적 네트워크로 파티션하는 기술이다. 다음은 네트워크 태핑으로 모니터링, 분석, 디버깅 등을 네트워크 플로 상에서 할 수 있게 하는 기술이다. 세 번째는 세 번째는 커스텀 포워딩으로, 어떤 조건을 구체적으로 설정해놓고, 그 조건을 만족시킬 경우 네트워크 패킷이 엔지니어링된 경로 통해 바로 포워딩되는 기술이다. \n\n■벤더 중심의 SDN \'꼼수인가, 반성인가\'\n\n오픈데이라이트의 출범은 벤더 중심의 SDN이란 큰 틀로 읽힌다. 고객에 빼앗긴 시장 주도권을 되찾아야 한다는 위기감의 발로임을 부인할 수는 없다. \n\n실제로 오픈플로가 발전하면서 그 개발을 주도했던 서비스업체들이 각자 개발한 SDN 기술을 솔루션 및 서비스 형태로 사업화하려는 움직임을 보이고 있다. IT업체의 먹잇감을 고객이 취하겠다고 달려드는 형국이다.\n\n오픈데이라이트를 통해 벤더는 기술을 선도하는 건 자신들임을 증명할 수 있다. 경쟁적인 개발참여를 통해 기업에서 개발해낸 오픈데이라트 성과보다 항상 앞서가는 모습을 보이면 가능하다. \n \n▲ 오픈데이라이트 참여사 \n \n네트워크 장비업체들이 자신들의 이익을 빼앗기지 않으려 지연전략을 펴는 것이란 삐딱한 시각도 존재한다. \n\n기존 네트워크는 제공업체의 장비마다 제각각인 하드웨어에 기능이 좌우됐다. 멀티 벤더로 네트워크 환경을 구현하려는 기업은 장비업체에서 제공하는 성능과 기능이 저마다 달라 인프라 관리 자동화는커녕 통합적인 관리조차 불가능했다. \n\nSDN과 오픈플로는 네트워크 상의 하드웨어 종속에서 벗어나 SW로 모든 네트워크 환경을 구성함으로써 벤더 종속에서 탈피하자는 의도에서 발전하고 있다. 이는 기존 네트워크업체의 차별성을 무너뜨리고, 저가 장비 판매 중심으로 장비업체를 몰아넣는다. 그러므로 장비업체가 SDN과 오픈플로를 달가워할 이유는 별로 없다. \n\n그러나 벤더의 자존심을 건 밥그릇 지키기로 치부하기엔 무리가 있다. 오픈데이라이트를 통해 개발된 기술이 어느 참여업체에도 소유권이 없기 때문이다. \n\n데이비드 옌 부사장은 “어떤 기부가 프로젝트를 촉진시킬 수 있을지, 무엇을 씨드 코드로 삼을지, 누가 활동을 리드할지에 대한 궁극적인 결정은 TSC(The Technical Steering Committee)가 하게 된다”라며 “TSC가 오픈데이라이트 프로젝트를 위한 모든 기부 제안과 기술 방향에 대한 의사결정을 주관하므로, 특정 벤더 소유의 코드가 일단 기부가 되면 이는 커뮤니티의 범용 코드가 되고 커뮤니티는 어떤 방향을 취할지 또 결정하게 되는 과정을 거치게 된다”라고 설명했다. \n\n그는 “오픈소스다 보니 모든 사람들이 오리지널 코드 액세스 갖고 있으며, 어느 벤더도 이 소프트웨어의 기본을 독점할 수 없다”라며 “벤더들은 오픈소스 프레임워크에 기여하는 모듈을 제공하고, 고부가가치 앱을 만들어내면서 수익을 창출하게 될 것”이라고 덧붙였다. \n\n지연전략에 대해서도 오픈데이라이트 TSC의 결정에 따라 어느 누구도 주도하기 힘든 구조다. 한 회사가 코드를 늦게 개발하거나 일부러 헝클어 놓는 건 불가능하다는 설명이다. \n\n그는 “컨트롤러의 경우도 시스코가 기본 코드를 제공했지만, 어느 회사도 컨트롤러 코드를 개선해 기여할 수 있다”라며 “모든 컨트롤러는 서로 다른 다수의 컨트롤러로부터 얻게 되는 코드들의 종합 반영본으로 발전하게 될 것”이라고 답했다. \n\n이어 “다양한 소스에서 전문적인 기술과 경험을 뽑아내 궁극적으로는 최상의 결정과 더불어 지속적인 발전을 이뤄갈 수 있는 것이 오픈소스의 최대 이점”이라고 강조했다. \n\n현재까지 나온 발언들은 종합해보면 오픈데이라이트의 방향성 자체는 나쁘지 않다. 구체적인 개선과 성과가 이어질 경우 최종사용할 고객사 입장에선 고도화된 오픈소스 네트워크를 쉽게 구축하고, 어느 벤더에서건 지원을 받을 수 있게 되는 탓이다. \n\n또한, 오픈데이라이트에 대해 벤더가 주도권을 되찾으려 한다는 시각도 비관적인 입장을 일단 배제하는 게 옳아 보인다. \n\n확실히 현재의 SDN과 오픈플로는 점차 파편화될 기미를 보이고 있으며, 중구난방식의 개발이 이뤄지고 있다. 오랜 네트워크 기술 개발경험을 통해 능숙한 역량을 보유한 벤더가 공통의 표준을 형성한다는 건 주도권 되찾기보다 솔루션 완성도 높이기를 위한 선순환 구조 형성 측면으로 봐야할 것으로 보인다. \n\n \n\n-- 이상 --\n\n \n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-04-25 12:41 (GMT+09:00)\n\nTitle : EMC·IBM 업은 레노버, 기업시장 강자 부상?\n\n \n\n스토리지 서버에 애플리케이션을 실행시킨다는 대목이 눈에 띕니다.\n\n이 접근의 의미, 장/단점에 대해서 생각해 볼 필요가 있습니다.\n\n \n\nhttp://media.daum.net/digital/newsview?newsid=20130425090106567\n\n \n\n감사합니다.\n\n \n\n심은수 드림\n\n</pre>\n\n\n\n\n==== References ====\n\n* [http://ettrends.etri.re.kr/PDFData/27-2_129-136.pdf ETRI - SDN 미래 네트워킹 기술]\n\n=== Disk Performance / Speed Specifications ===\n\n\n\n==== Samsung SSD spec (128GB 2.5-inch SSD 830 Series) ====\n\n* Interface\n:- SATA 6Gb/s\n\n* Capacity\n: 128GB\n\n* Performance Benchmark (Random Read, SSD, echo 3 > /proc/sys/vm/drop_caches, hdparm -W0 /dev/sdf)\n\n <pre>\n$ time iozone -i 2 -+R -w -s 400000 -r 4 -f /x/iozone/test/ssd_test/400m.1\n        Iozone: Performance Test of File I/O\n                Version $Revision: 3.398 $ -- blusjune 20130502_023327\n                Compiled for 6          Build: linux-AMD64\n\n                Build: linux-AMD64\n\n        Contributors:William Norcott, Don Capps, Isom Crawford, Kirby Collins\n                     Al Slater, Scott Rhine, Mike Wisner, Ken Goss\n                     Steve Landherr, Brad Smith, Mark Kelly, Dr. Alain CYR,\n                     Randy Dunlap, Mark Montague, Dan Million, Gavin Brebner,\n                     Jean-Marc Zucconi, Jeff Blomberg, Benny Halevy, Dave Boone,\n                     Erik Habbinga, Kris Strecker, Walter Wong, Joshua Root,\n                     Fabrice Bacchella, Zhenghua Xue, Qin Li, Darren Sawyer.\n                     Ben England.\n\n        Run began: Thu May  2 09:20:02 2013\n\n        Setting no_unlink\n        File size set to 400000 KB\n        Record Size 4 KB\n        Command line used: iozone -i 2 -+R -w -s 400000 -r 4 -f /x/iozone/test/ssd_test/400m.1\n        Output is in Kbytes/sec\n        Time Resolution = 0.000001 seconds.\n        Processor cache size set to 1024 Kbytes.\n        Processor cache line size set to 32 bytes.\n        File stride size set to 17 * record size.\n                                                            random  random    bkwd   record   stride                   \n              KB  reclen   write rewrite    read    reread    read   write    read  rewrite     read   fwrite frewrite   fread  freread\n          400000       4                                     45213       0                                             \n\niozone test complete.\n\n==================================================\nblusjune performance metrics (random-read)\n==================================================\nblusjune__file_size (bytes)      : 409600000\nblusjune__record_size (bytes)    : 4096\nblusjune__time_elapsed (secs)    : 8.84687\nblusjune__num_of_ios             : 100000\n--------------------------------------------------\nblusjune__latency_per_io (usecs) : 88.4687\nblusjune__iops                   : 11303\nblusjune__throughput (bytes/sec) : 46298866\n==================================================\nreal    0m8.912s\nuser    0m0.072s\nsys     0m2.072s\n</pre>\n\n <pre>\n$ iozone -i 2 -+R -w -s 400000 -r 4 -f /x/bmt_iozone/test/ssd_test/400m.1 \n	Iozone: Performance Test of File I/O\n	        Version $Revision: 3.398 $ -- blusjune 20130502_110038\n		Compiled for 6		Build: linux-AMD64 \n\n		Build: linux-AMD64 \n\n	Contributors:William Norcott, Don Capps, Isom Crawford, Kirby Collins\n	             Al Slater, Scott Rhine, Mike Wisner, Ken Goss\n	             Steve Landherr, Brad Smith, Mark Kelly, Dr. Alain CYR,\n	             Randy Dunlap, Mark Montague, Dan Million, Gavin Brebner,\n	             Jean-Marc Zucconi, Jeff Blomberg, Benny Halevy, Dave Boone,\n	             Erik Habbinga, Kris Strecker, Walter Wong, Joshua Root,\n	             Fabrice Bacchella, Zhenghua Xue, Qin Li, Darren Sawyer.\n	             Ben England.\n\n	Run began: Thu May  2 11:31:38 2013\n\n	Setting no_unlink\n	File size set to 400000 KB\n	Record Size 4 KB\n	Command line used: iozone -i 2 -+R -w -s 400000 -r 4 -f /x/bmt_iozone/test/ssd_test/400m.1\n	Output is in Kbytes/sec\n	Time Resolution = 0.000001 seconds.\n	Processor cache size set to 1024 Kbytes.\n	Processor cache line size set to 32 bytes.\n	File stride size set to 17 * record size.\n                                                            random  random    bkwd   record   stride                                   \n              KB  reclen   write rewrite    read    reread    read   write    read  rewrite     read   fwrite frewrite   fread  freread\n          400000       4                                     41462       0                                                                                              \n\niozone test complete.\n\n==================================================\nblusjune performance metrics (random-read)\n==================================================\nblusjune__file_size (bytes)      : 409600000\nblusjune__record_size (bytes)    : 4096\nblusjune__time_elapsed (secs)    : 9.64722\nblusjune__num_of_ios             : 100000\n--------------------------------------------------\nblusjune__latency_per_io (usecs) : 96.4722\nblusjune__iops                   : 10365\nblusjune__throughput (bytes/sec) : 42457810\n==================================================\n</pre>\n\n\n\n\n* Performance Benchmark (Random Write, SSD, echo 3 > /proc/sys/vm/drop_caches, hdparm -W0 /dev/sdf)\n\n <pre>\n$ iozone -i 2 -+W -w -s 400000 -r 4 -f /x/bmt_iozone/test/ssd_test/400m.3\n	Iozone: Performance Test of File I/O\n	        Version $Revision: 3.398 $ -- blusjune 20130502_110038\n		Compiled for 6		Build: linux-AMD64 \n\n		Build: linux-AMD64 \n\n	Contributors:William Norcott, Don Capps, Isom Crawford, Kirby Collins\n	             Al Slater, Scott Rhine, Mike Wisner, Ken Goss\n	             Steve Landherr, Brad Smith, Mark Kelly, Dr. Alain CYR,\n	             Randy Dunlap, Mark Montague, Dan Million, Gavin Brebner,\n	             Jean-Marc Zucconi, Jeff Blomberg, Benny Halevy, Dave Boone,\n	             Erik Habbinga, Kris Strecker, Walter Wong, Joshua Root,\n	             Fabrice Bacchella, Zhenghua Xue, Qin Li, Darren Sawyer.\n	             Ben England.\n\n	Run began: Thu May  2 11:35:44 2013\n\n	Setting no_unlink\n	File size set to 400000 KB\n	Record Size 4 KB\n	Command line used: iozone -i 2 -+W -w -s 400000 -r 4 -f /x/bmt_iozone/test/ssd_test/400m.3\n	Output is in Kbytes/sec\n	Time Resolution = 0.000001 seconds.\n	Processor cache size set to 1024 Kbytes.\n	Processor cache line size set to 32 bytes.\n	File stride size set to 17 * record size.\n                                                            random  random    bkwd   record   stride                                   \n              KB  reclen   write rewrite    read    reread    read   write    read  rewrite     read   fwrite frewrite   fread  freread\n          400000       4                                         0  960635                                                                                              \n\niozone test complete.\n\n==================================================\nblusjune performance metrics (random-write)\n==================================================\nblusjune__file_size (bytes)      : 409600000\nblusjune__record_size (bytes)    : 4096\nblusjune__time_elapsed (secs)    : 0.416391\nblusjune__num_of_ios             : 100000\n--------------------------------------------------\nblusjune__latency_per_io (usecs) : 4.16391\nblusjune__iops                   : 240158\nblusjune__throughput (bytes/sec) : 983690602\n==================================================\n</pre>\n\n==== Samsung SSD spec (128GB 2.5-inch SSD 840 Pro Series) ====\n\n* Interface\n:- SATA 6Gb/s\n\n* Capacity\n: 128GB\n\n* Performance\n:- Sequential Read Speed: Up to 530 MB/s\n:- Sequential Write Speed: Up to 390 MB/s\n:- Random Read Speed: Up to 97,000 IOPS (4KB/IO) // Read Latency: 10.31 usec (80~200 usec, NCQ, Non-blocking) // python -c \"print (float(1 * (10 ** 6)) / float(97000))\"\n:- Random Write Speed: Up to 90,000 IOPS (4KB/IO) // Write Latency: 11.11 usec (80~200 usec, NCQ, Non-blocking) // python -c \"print (float(1 * (10 ** 6)) / float(90000))\"\n\n* Power Consumption (W)\n: 0.15W\n\n==== Intel SSD spec (320 Series) ====\n\n* Interface\n:- SATA 6Gb/s\n\n* Capacity\n: 120GB\n\n* Performance\n:- Sequential Read Speed: up to 270 MB/s \n:- Sequential Write Speed: up to 130 MB/s\n:- Random Read Speed: 38,000 IOPS (4KB/IO) // Read Latency: 26.32 usec // python -c \"print (float(1 * (10 ** 6)) / float(38000))\"\n:- Random Write Speed: 14,000 IOPS (4KB/IO) // Write Latency: 71.43 usec // python -c \"print (float(1 * (10 ** 6)) / float(14000))\"\n\n==== Seagate HDD spec (Barracuda 2TB) ====\n\n* Model Number\n:- ST2000DM001\n\n* Interface\n:- SATA 6Gb/s NCQ\n\n* Performance\n:- Spindle Speed (RPM): 7200\n:- Cache, Multisegmented (MB): 64\n:- SATA Transfer Rates Supported (Gb/s): 6.0/3.0/1.5\n:- Seek Average, Read (ms): <8.5 // 117 IOPS\n:- Seek Average, Write (ms): <9.5 // 105 IOPS\n:- Average Data Rate, Read/Write (MB/s): 156\n:- Max Sustained Data Rate, OD Read (MB/s): 210\n\n* Configuration/Organization\n:- Heads/Disks: 6/3\n:- Bytes per Sector: 4096\n\n* Reliability/Data Integrity\n:- Annualized Failure Rate (AFR): <1%\n:- Power-On Hours: 2400\n\n* Power Management\n:- Startup Power (A): 2.0\n:- Operating Mode, Typical (W): 8.0\n:- Idle2 Average (W): 5.40\n:- Idle Average (W): N/A\n:- Standby Mode (W): 0.75\n:- Sleep Mode (W): 0.75\n\n* Performance Benchmark (Random Read, HDD, echo 3 > /proc/sys/vm/drop_caches, hdparm -W0 /dev/sdd)\n <pre>\n$ time iozone -i 2 -+R -w -s 400000 -r 4 -f /x/iozone/test/hdd_test/400m\n        Iozone: Performance Test of File I/O\n                Version $Revision: 3.398 $ -- blusjune 20130502_023327\n                Compiled for 6          Build: linux-AMD64\n\n                Build: linux-AMD64\n\n        Contributors:William Norcott, Don Capps, Isom Crawford, Kirby Collins\n                     Al Slater, Scott Rhine, Mike Wisner, Ken Goss\n                     Steve Landherr, Brad Smith, Mark Kelly, Dr. Alain CYR,\n                     Randy Dunlap, Mark Montague, Dan Million, Gavin Brebner,\n                     Jean-Marc Zucconi, Jeff Blomberg, Benny Halevy, Dave Boone,\n                     Erik Habbinga, Kris Strecker, Walter Wong, Joshua Root,\n                     Fabrice Bacchella, Zhenghua Xue, Qin Li, Darren Sawyer.\n                     Ben England.\n\n        Run began: Thu May  2 08:24:47 2013\n\n        Setting no_unlink\n        File size set to 400000 KB\n        Record Size 4 KB\n        Command line used: iozone -i 2 -+R -w -s 400000 -r 4 -f /x/iozone/test/hdd_test/400m\n        Output is in Kbytes/sec\n        Time Resolution = 0.000001 seconds.\n        Processor cache size set to 1024 Kbytes.\n        Processor cache line size set to 32 bytes.\n        File stride size set to 17 * record size.\n                                                            random  random    bkwd   record   stride                   \n              KB  reclen   write rewrite    read    reread    read   write    read  rewrite     read   fwrite frewrite   fread  freread\n          400000       4                                      2116       0                                             \n\niozone test complete.\n\n==================================================\nblusjune performance metrics (random-read)\n==================================================\nblusjune__file_size (bytes)      : 409600000\nblusjune__record_size (bytes)    : 4096\nblusjune__time_elapsed (secs)    : 189.019\nblusjune__num_of_ios             : 100000\n--------------------------------------------------\nblusjune__latency_per_io (usecs) : 1890.19\nblusjune__iops                   : 529\nblusjune__throughput (bytes/sec) : 2166974\n==================================================\nreal    3m9.229s\nuser    0m0.120s\nsys     0m3.208s\n</pre>\n\n <pre>\n $ iozone -i 2 -+R -w -s 400000 -r 4 -f /x/bmt_iozone/test/hdd_test/400m.1\n	Iozone: Performance Test of File I/O\n	        Version $Revision: 3.398 $ -- blusjune 20130502_110038\n		Compiled for 6		Build: linux-AMD64 \n\n		Build: linux-AMD64 \n\n	Contributors:William Norcott, Don Capps, Isom Crawford, Kirby Collins\n	             Al Slater, Scott Rhine, Mike Wisner, Ken Goss\n	             Steve Landherr, Brad Smith, Mark Kelly, Dr. Alain CYR,\n	             Randy Dunlap, Mark Montague, Dan Million, Gavin Brebner,\n	             Jean-Marc Zucconi, Jeff Blomberg, Benny Halevy, Dave Boone,\n	             Erik Habbinga, Kris Strecker, Walter Wong, Joshua Root,\n	             Fabrice Bacchella, Zhenghua Xue, Qin Li, Darren Sawyer.\n	             Ben England.\n\n	Run began: Thu May  2 11:25:26 2013\n\n	Setting no_unlink\n	File size set to 400000 KB\n	Record Size 4 KB\n	Command line used: iozone -i 2 -+R -w -s 400000 -r 4 -f /x/bmt_iozone/test/hdd_test/400m.1\n	Output is in Kbytes/sec\n	Time Resolution = 0.000001 seconds.\n	Processor cache size set to 1024 Kbytes.\n	Processor cache line size set to 32 bytes.\n	File stride size set to 17 * record size.\n                                                            random  random    bkwd   record   stride                                   \n              KB  reclen   write rewrite    read    reread    read   write    read  rewrite     read   fwrite frewrite   fread  freread\n          400000       4                                      2123       0                                                                                              \n\niozone test complete.\n\n==================================================\nblusjune performance metrics (random-read)\n==================================================\nblusjune__file_size (bytes)      : 409600000\nblusjune__record_size (bytes)    : 4096\nblusjune__time_elapsed (secs)    : 188.403\nblusjune__num_of_ios             : 100000\n--------------------------------------------------\nblusjune__latency_per_io (usecs) : 1884.03\nblusjune__iops                   : 530\nblusjune__throughput (bytes/sec) : 2174068\n==================================================\n</pre>\n\n== ## bNote-2013-04-25 ==\n\n=== News / Articles ===\n\n* [http://www.zdnet.co.kr/news/news_view.asp?artice_id=20130422135700 SDN 국면 전환을 꿈꾸다 \'오픈데이라이트 (Open Daylight)\' // 2013-04-22]\n* [http://www.bloter.net/archives/150410?utm_source=feedly Facebook, Datacenter 운영 현황 공개 // 2013-04-19]\n\n\n=== Bayesian Inference with R (r_stat) ===\n\n* [http://books.google.co.kr/books?hl=en&lr=&id=AALhk_mt7SYC&oi=fnd&pg=PP5&dq=famous+R+package+bayesian+inference&ots=XvK-GIW-Ji&sig=TK-Xz5xQ1urvG6vge2YV9BAnlPo&redir_esc=y#v=onepage&q&f=false Book - \"Bayesian Computation with R\", 2nd Ed., Jim Albert]\n* [http://books.google.co.kr/books?hl=en&lr=&id=GTJUt8fcFx8C&oi=fnd&pg=PP1&dq=famous+R+package+bayesian+inference+example&ots=Iy1FKoP4ux&sig=y1_93HNmPIMDaRCCdzyzvSlReAU&redir_esc=y#v=onepage&q=famous%20R%20package%20bayesian%20inference%20example&f=false \"Bayesian Methods for Data Analysis\", 3rd Ed., Bradley P. Carlin, Thomas A. Louis]\n* [http://www.bayesian-inference.com/index Bayesian-Inference // LaplacesDemon - a complete environment for Bayesian inference in R // Statisticat, LLC.]\n* [http://cran.r-project.org/web/packages/LaplacesDemon/vignettes/BayesianInference.pdf \"Bayesian Inference\", Statisticat, LLC]\n* [http://cran.r-project.org/web/packages/LaplacesDemon/LaplacesDemon.pdf Package \'LaplacesDemon\']\n\n== ## bNote-2013-04-24 ==\n\n\n\n=== Patents ===\n\n* [http://www.google.com/patents/US7792882 Method and system for block allocation for hybrid drives // Oracle // US 7792882 B2]\n* [http://www.google.com/patents/US8285758 Tiering storage between multiple classes of storage on the same container file system // EMC // US 8285758 B1]\n\n=== Paper References ===\n\n\n* [http://research.microsoft.com/pubs/63596/usenix-08-ssd.pdf Design Tradeoffs for SSD Performance // USENIX 2008 // Microsoft Research, Silicon Valley; University of Wisconsin-Madison]\n* [http://research.microsoft.com/en-us/um/people/srikanth/data/imc09_dcTraffic.pdf The Nature of Datacenter Traffic: Measurements & Analysis // IMC 2009 // Microsoft Research]\n* [http://www.cs.ucsb.edu/~arijitkhan/Papers/multiple_timeseries_prediction.pdf Workload Characterization and Prediction in the Cloud: A Multiple Time Series Approach // UCSB, IBM Watson Research]\n* [http://infolab.stanford.edu/~ragho/hive-icde2010.pdf Hive - A Petabyte Scale Data Warehouse Using Hadoop // Facebook]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf Dremel: Interactive Analysis of Web-scale Datasets // VLDB 2010 // Google]\n* [http://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf PACMan: Coordinated Memory Caching for Parallel Jobs]\n* [http://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf Spark and Shark // AMPLab UC Berkeley]\n* [http://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf MR-Scope: A Real-time Tracing Tool for MapReduce]\n* [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGRID 2010]\n* [http://bnrg.eecs.berkeley.edu/~randy/Courses/CS268.F08/papers/42_osdi_08.pdf Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008 // UC Berkeley]\n* [http://webhdd.ru/library/files/RebalanceDesign6.pdf Rebalance an HDFS Cluster]\n* [http://www.ibm.com/developerworks/web/library/wa-introhdfs/ An introduction to the Hadoop Distributed File System -- HDFS Data Block Rebalancing]\n* [http://www.stanford.edu/~cdel/epic.talk.workloads.pdf Data Center Workload Characterization]\n* [http://seelab.ucsd.edu/virtualefficiency/related_papers/42_caecw05.pdf Data Center Workload Monitoring, Analysis, and Emulation]\n* [https://amplab.cs.berkeley.edu/projects/real-life-workloads/ AMPLab UC Berkeley - Real-life Workloads]\n* [http://davidmeisner.org/wp-content/uploads/2011/04/meisner-exert10.pdf Stochastic Queuing Simulation for Data Center Workloads]\n* [http://research.microsoft.com/en-us/um/people/srikanth/data/imc09_dcTraffic.pdf The Nature of Datacenter Traffic: Measurements & Analysis // MSR]\n* [http://nowlab.cse.ohio-state.edu/publications/tech-reports/2005/vaidyana-caecw05-tr.pdf Workload-driven Analysis of File Systems in Shared Multi-tier Data Centers over InfiniBand // Ohio State University]\n* [http://research.microsoft.com/pubs/81782/2009-06-19%20Data%20Center%20Tutorial%20-%20SIGMETRICS.pdf What Goes Into a Data Center? // MSR]\n* [http://delivery.acm.org/10.1145/1780000/1773400/p34-mishra.pdf?ip=202.20.193.254&acc=ACTIVE%20SERVICE&key=986B26D8D17D60C855C5A4E2351BBB67&CFID=209231705&CFTOKEN=89669612&__acm__=1366870790_f6ca461a6119bb2a2d6f017ca1bc9ce7 Towards Characterizing Cloud Backend Workloads: Insights from Google Compute Clusters // ACM SIGMETRICS Performance Evaluation Review Vol. 37, Issue 4, 2010-03 // Google]\n\n=== White Paper from EMC (Benchmark) ===\n\n* [http://www.emc.com/collateral/hardware/white-papers/h8131-storage-tiering-oracle-vmax-wp.pdf Implementing FAST VP and Storage Tiering for Oracle Database 11g and EMC Symmetrix VMAX // 2011-04 // ((B.GOOD))]\n* [http://www.emc.com/collateral/hardware/white-papers/h11210-p690-emc-vnx7500-scaling-performance-oracle-11g-vsphere-wp.pdf EMC VNX7500 Scaling Performance for Oracle 11gR2 RAC on VMware vSphere 5.1 // 2012-12 ((B.GOOD))]\n* [http://www.compucom.com/sites/default/files/Whitepaper-Deploying-Oracle-Database-Apps-on-EMC-VNX.pdf Deploying Oracle Database Applications on EMC VNX Unified Storage // 2011-04]\n* [http://www.emc.com/collateral/hardware/white-papers/h8850-oracle-performance-vnx-fastcache-wp.pdf EMC Performance for Oracle (EMC VNX, Enterprise Flash Drives, FAST Cache, VMware vSphere // 2011-12]\n* [http://www.emc.com/collateral/white-papers/h11209-p773-osc-integration-vmax-mgt-virtualized-oracle-ebs-wp.pdf EMC Oracle VM Storage Connect Integration Module - Efficient Infrastructure Management for a Virtualized Oracle E-Business Suite // 2013-03]\n* [http://www.emc.com/collateral/analyst-reports/esg-20091208-fast.pdf ESG Whitepaper - Automate and Optimize a Tiered Storage Environment - FAST // 2009-12]\n* [http://www.emc.com/collateral/hardware/white-papers/h8091-leveraging-fast-sql-wp.pdf White Paper - Leveraging EMC Fully Automated Storage Tiering (FAST) and FAST Cache for SQL Server Enterprise Deployments // 2010-11]\n* [http://www.emc.com/collateral/software/white-papers/h10938-vnx-best-practices-wp.pdf EMC VNX Unified Best Practices for Performance // 2012-08]\n* [http://www.emc.com/collateral/white-papers/h11122-vmax10k-fastvp-tiering-oracledb-wp.pdf Implementing FAST VP and Storage Tiering for Oracle Database 11g and EMC Symmetrix VMAX 10K // 2012-09]\n* [http://www.theregister.co.uk/2013/03/05/emc_xtremsf/ EMC declares war on all-flash array, server flash card rivals - Rolls out XtremIO array, renamed VFCache - XtremSF Server Flash, XtremSW Cache Software, XtremIO all-flash array // 2013-03-05]\n\n== ## bNote-2013-04-23 ==\n\n\n=== Real Trace: MS Exchange ===\n\n <pre>\na1mjjung@secm:[iowa] $ grep \'__valu__sig__\' f030.infile_A.iowa.anal_s0010 > summary\na1mjjung@secm:[iowa] $ cat summary \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  88\n__valu__sig__ _n_o_sigaddrs :  9989\n__valu__sig__ _sigioc_acc :  7266636\n__valu__sig__ _sigaddrs_efficiency :  727.463810191\n__valu__sig__ _n_o_addr_total :  8322343\n__valu__sig__ _ioc_total :  28866005\n</pre>\n\n=== Real Trace: MSN FileServer ===\n\n <pre>\na1mjjung@secm:[iowa] $ grep \'__valu__sig__\' iowa.anal_s0010.A.out > summary\na1mjjung@secm:[iowa] $ cat summary  \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  18\n__valu__sig__ _n_o_sigaddrs :  239328\n__valu__sig__ _sigioc_acc :  7571590\n__valu__sig__ _sigaddrs_efficiency :  31.6368749164\n__valu__sig__ _n_o_addr_total :  7425792\n__valu__sig__ _ioc_total :  29345085\n</pre>\n\n=== I/O Intensiveness Analysis (3D plot) ===\n\n* pre-processing for 3D-plot (access count per iomw)\n <pre>\n\na1mjjung@secm:[iowa.R] $ cat .bdx.0100.y.iowa_anal.sh \n#!/bin/sh\n\n_iowa_anal_cmd=\"bsc.iowa.lsp.anal_s0010\";\n\n_file_010=\"f010.T021.msnfs.R.out\";\n_file_020=\"f020.infile_R\";\n_file_030=\"f030.infile_R.iowa.anal_s0010\";\n_file_040=\"f040.iomw_acs_cnt.sorted_by_acs_cnt\";\n\ncat $_file_010 | awk \'{ print $5, \",\", $14}\' > $_file_020\ncat $_file_020 | $_iowa_anal_cmd -c 25 -w 600000000 -p 60000000 > $_file_030\ncat $_file_030 | grep __list__iomw_acs_cnt | awk \'{ print $9, $3, $6 }\' | sort -n > $_file_040\n\n</pre>\n\n* gnuplot command\n\n:* iowa.anal - Reads\n <pre>\n\na1mjjung@secm:[iowa.R] $ cat .plot_acs_cnt.gp \nclear\nunset key\nset title \"I/O intensiveness - Reads (MSN FileServer 6H)\"\nset xlabel \"time (10-minute)\"\nset ylabel \"address (LBA)\"\nset zlabel \"hit count per addr\"\nshow view\nset view 45, 60\nsplot \'./f040.iomw_acs_cnt.sorted_by_acs_cnt\' u 3:2:1 with points palette pointsize 1 pointtype 7\n\n</pre>\n\n:* iowa.anal - Writes\n <pre>\n\na1mjjung@secm:[iowa.W] $ cat .plot_acs_cnt.gp \nclear\nunset key\nset title \"I/O intensiveness - Writes (MSN FileServer 6H)\"\nset xlabel \"time (10-minute)\"\nset ylabel \"address (LBA)\"\nset zlabel \"hit count per addr\"\nshow view\nset view 45, 60\nsplot \'./f040.iomw_acs_cnt.sorted_by_acs_cnt\' u 3:2:1 with points palette pointsize 1 pointtype 7\n\n</pre>\n\n\n* References\n\n:* [http://psy.swansea.ac.uk/staff/carter/gnuplot/gnuplot_3d.htm]\n:* [http://lowrank.net/gnuplot/plot3d-e.html]\n\n== ## bNote-2013-04-18 ==\n\n=== EMC FAST (VP, Cache) Technology Study ===\n\n* [http://www.emc.com/collateral/hardware/white-papers/h8131-storage-tiering-oracle-vmax-wp.pdf Implementing FAST VP and Storage Tiering for Oracle Database 11g and EMC Symmetrix VMAX]\n: Shows performance improvement by FAST VP\n\n== ## bNote-2013-04-17 ==\n\n=== Proactive Data Placement Research Planning ===\n\n== ## bNote-2013-04-16 ==\n\n=== Git on the Server - Setting Up the Server ===\n\n\n* Reference\n: [http://git-scm.com/book/ch4-4.html 4.4 Git on the Server - Setting Up the Server // git-scm.com]\n\n\n* 4.4 Git on the Server - Setting Up the Server\n\'\'\' Setting Up the Server \'\'\'\n\n: Let\'s walk through setting up SSH access on the server side. In this example, you\'ll use the authorized_keys method for authenticating your users. We also assume you\'re running a standard Linux distribution like Ubuntu. First, you create a \'git\' user and a .ssh directory for that user.\n\n <pre>\n$ sudo adduser git\n$ su git\n$ cd\n$ mkdir .ssh\n</pre>\n\n: Next, you need to add some developer SSH public keys to the authorized_keys file for that user. Let\'s assume you\'ve received a few keys by e-mail and saved them to temporary files. Again, the public keys look something like this:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCB007n/ww+ouN4gSLKssMxXnBOvf9LGt4L\nojG6rs6hPB09j9R/T17/x4lhJA0F3FR1rP6kYBRsWj2aThGw6HXLm9/5zytK6Ztg3RPKK+4k\nYjh6541NYsnEAZuXz0jTTyAUfrtU3Z5E003C4oxOj6H0rfIF1kKI9MAQLMdpGW1GYEIgS9Ez\nSdfd8AcCIicTDWbqLAcU4UpkaX8KyGlLwsNuuGztobF8m72ALC/nLF6JLtPofwFBlgc+myiv\nO7TCUSBdLQlgMVOFq1I2uPWQOkOWQAHukEOmfjy2jctxSDBQ220ymjaNsHT4kgtZg2AYYgPq\ndAv8JggJICUvax2T9va5 gsg-keypair\n</pre>\n\n: You just append them to your authorized_keys file:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.josie.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.jessica.pub >> ~/.ssh/authorized_keys\n</pre>\n\n: Now, you can set up an empty repository for them by running git init with the --bare option, which initializes the repository without a working directory:\n\n <pre>\n$ cd /opt/git\n$ mkdir project.git\n$ cd project.git\n$ git --bare init\n</pre>\n\n: Then, John, Josie, or Jessica can push the first version of their project into that repository by adding it as a remote and pushing up a branch. Note that someone must shell onto the machine and create a bare repository every time you want to add a project. Let\'s use gitserver as the hostname of the server on which you\'ve set up your \'git\' user and repository. If you\'re running it internally, and you set up DNS for gitserver to point to that server, then you can use the commands pretty much as is:\n\n: # on Johns computer\n <pre>\n$ cd myproject\n$ git init\n$ git add .\n$ git commit -m \'initial commit\'\n$ git remote add origin git@gitserver:/opt/git/project.git\n$ git push origin master\n</pre>\n\nAt this point, the others can clone it down and push changes back up just as easily:\n\n <pre>\n$ git clone git@gitserver:/opt/git/project.git\n$ cd project\n$ vim README\n$ git commit -am \'fix for the README file\'\n$ git push origin master\n</pre>\n\n: With this method, you can quickly get a read/write Git server up and running for a handful of developers.\n\n: As an extra precaution, you can easily restrict the \'git\' user to only doing Git activities with a limited shell tool called git-shell that comes with Git. If you set this as your \'git\' user\'s login shell, then the \'git\' user cannot have normal shell access to your server. To use this, specify git-shell instead of bash or csh for your user\'s login shell. To do so, you will likely have to edit your /etc/passwd file:\n\n <pre>\n$ sudo vim /etc/passwd\n</pre>\n\n: At the bottom, you should find a line that looks something like this:\n\n <pre>\ngit:x:1000:1000::/home/git:/bin/sh\n</pre>\n\n: Change /bin/sh to /usr/bin/git-shell (or run which git-shell to see where it’s installed). The line should look something like this:\n\n <pre>\ngit:x:1000:1000::/home/git:/usr/bin/git-shell\n</pre>\n\nNow, the \'git\' user can only use the SSH connection to push and pull Git repositories and cannot shell onto the machine. If you try, you will see a login rejection like this:\n\n <pre>\n$ ssh git@gitserver\nfatal: What do you think I am? A shell?\nConnection to gitserver closed.\n</pre>\n\n== ## bNote-2013-04-15 ==\n\n=== Sorting Algorithms ===\n\n* [[Sorting algorithms]]\n* [http://corte.si/posts/code/visualisingsorting/index.html Visualising Sorting Algorithms]\n\n== ## bNote-2013-04-11 ==\n\n=== Failure trend in a data center ===\n\n==== 꽤 충실한 내용의 블로그 ((B.GOOD)) ====\n\n* URL: [http://bart7449.tistory.com/m/post/view/id/258 현실에서의 메모리와 디스크의 오류 // 2010-10-04]\n\n\n\n=== 수퍼컴 협력 ===\n\n* TAT 단축\n\n:- 40시간 걸리던 작업이 32분만에 완료 (75배 TAT 단축)\n (32 * 75) / 60 = 40 시간 \n\n:- 18시간 걸리던 작업이 27분만에 완료 (40배 TAT 단축)\n (27 * 40) / 60 = 18시간\n\n* 업무 협조 요청 (to 김혁호 책임)\n\n <pre>\n\nI/O Workload 분석 & 시뮬레이션 환경 병렬화\n\n ㆍ배경 및 목적\n① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n   ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n ㆍ추진 방향\n   ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n   ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n      예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n ㆍ추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                         : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform)  : 2주\n      [Step 3] I/O Workload Outlook 분석               : 4주 (2+2)\n      [Step 4] Dominant I/O 패턴 추출                   : 8주 (3+5)\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주 (3+7)\n      [Step 6] Data Placement 알고리즘 구현 및 검증       : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n        (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n      ※ Workload Analysis 작업 Set (24주): Step 2 ~ Step 5\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        : 22주 → 8주로 단축예상 (수퍼컴 이전 대비 110~200배 TAT 단축)\n\n ㆍ추진 효과\n   ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n   ② Workload에 최적화된 Data Management 알고리즘 개발로\n      데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n   ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n [참고1]\n ㆍ전체 Simulation 단계 중, Preprocessing단계에서 슈퍼컴 파일시스템의\n   Real I/O Trace Log Data가 필요하며, Analysis(Outlook, Modeling)\n   단계에서 Algorithm 병렬화를 통한 TAT 단축 필요\n \n [참고2]\n ㆍ과제 Workflow : 첨부파일 참조\n \n\n\n\n</pre>\n\n== ## bNote-2013-04-10 ==\n\n=== IOWA:: MSN FileServer I/O Trace ===\n\n==== T034 ====\n\n* # of reads/writes (total I/Os): 982166\n* # of reads: 811784\n* # of writes: 170382\n\n\n <pre>\na1mjjung@secm:[T033.discovery.full_path] $ pwd\n/home/X0101/a1mjjung/x/iowa.sidewinder/sidewinder/microsoft_msn_filesrvr_6h/MSNStorageFileServer/tdir/T033.discovery.full_path\n\na1mjjung@secm:[T033.discovery.full_path] $ cat T033.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.out | grep \'_iorw_ DiskRead\' > _t034/T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.R.out\n\na1mjjung@secm:[T033.discovery.full_path] $ cat T033.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.out | grep \'_iorw_ DiskWrite\' > _t034/T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.W.out\n</pre>\n\n\n <pre>\na1mjjung@secm:[_t034] $ pwd\n/home/X0101/a1mjjung/x/iowa.sidewinder/sidewinder/microsoft_msn_filesrvr_6h/MSNStorageFileServer/tdir/T033.discovery.full_path/_t034\n\n\na1mjjung@secm:[_t034] $ tail -10 ../../T031.msnfs.fld_dstr.full_path.out \n 382331  _path_ _Disk8__s4_s5_s6_\n 403592  _path_ _Disk9__s0_s1_s13_\n 413900  _path_ _Disk8__s0_s1_s7_\n 418544  _path_ _Disk9__s3_s4_s6_\n 428969  _path_ _Disk8__s0_s1_s9_\n 450260  _path_ _Disk9__s0_s1_s7_\n 466085  _path_ _Disk9__s3_s4_s5_\n 600647  _path_ _Disk9__s0_s1_s3_\n 811630  _path_ _Disk8__s0_s1_s2_\n 982166  _path_ _Disk9__s0_s1_s2_\n\n\na1mjjung@secm:[_t034] $ wc -l T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.R.out\n811784 T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.R.out\na1mjjung@secm:[_t034] $ wc -l T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.W.out\n170382 T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.W.out\n</pre>\n\n\n <pre>\na1mjjung@secm:[_t034] $ head -10 T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.R.out \n_iorw_ DiskRead   , _time_ 248425 , _prid_ p0_2004 , _thid_           540   , _addr_  100276969472  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 263287 , _prid_ p0_2004 , _thid_           540   , _addr_  97312505856  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 281149 , _prid_ p0_2004 , _thid_           540   , _addr_  92908216320  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 305383 , _prid_ p0_2004 , _thid_           540   , _addr_  98594537472  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 320338 , _prid_ p0_2004 , _thid_           540   , _addr_  100699373568  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 325746 , _prid_ p0_2004 , _thid_           540   , _addr_  100944601088  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 330001 , _prid_ p0_2004 , _thid_           540   , _addr_  94236082176  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 339520 , _prid_ p0_2004 , _thid_           540   , _addr_  92404563968  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 344061 , _prid_ p0_2004 , _thid_           540   , _addr_  94653767680  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 399820 , _prid_ p0_2004 , _thid_           540   , _addr_  157423190016  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n</pre>\n\n\n* _thid_ based T034 analysis\n\n <pre>\na1mjjung@secm:[_t034] $ cat T033.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.out | awk \'{print $11}\' | sort | uniq -c | sort | wc -l\n249\n\na1mjjung@secm:[_t034] $ cat T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.R.out | awk \'{print $11}\' | sort | uniq -c | sort | wc -l\n245\n\na1mjjung@secm:[_t034] $ cat T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.W.out | awk \'{print $11}\' | sort | uniq -c | sort | wc -l\n240\n</pre>\n\n <pre>\na1mjjung@secm:[_t034] $ cat T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.R.out | awk \'{ print $11 }\' | sort | uniq -c | sort | tail -10 \n   8602 540\n   9006 8716\n   9254 7792\n   9603 9440\n   9692 8788\n  10393 10948\n  10842 10000\n  11079 8172\n  12332 11240\n  51747 4060\n</pre>\n\n=== R plot ===\n\n* [http://www.cyclismo.org/tutorial/R/plotting.html plotting in R // R tutorial]\n\n=== Beautiful graphs in Gnuplot ===\n\n* [http://labs.guidolin.net/2010/03/how-to-create-beautiful-gnuplot-graphs.html How to create beautiful Gnuplot graphs (tips and tricks) // GuidoLabs]\n:- [[exemplary .gnuplot configuration file for beautiful gnuplot graphs]]\n:- [http://www.dafont.com/sv-basic-manual.font?text=fdgdfd SV Basic Manual Font]\n\n=== Summary of the ways to call external programs (and pros/cons) ===\n\n* os.system(\"some_command with args\") [http://docs.python.org/lib/os-process.html os.system() // Python tutorial]\n: passes the command and arguments to your system\'s shell. This is nice because you can actually run multiple commands at once in this manner and set up pipes and input/output redirection. For example,\n os.system(\"some_command < input_file | another_command > output_file\")\n: However, while this is convenient, you have to manually handle the escaping of shell characters such as spaces, etc. On the other hand, this also lets you run commands which are simply shell commands and not actually external programs.\n\n* stream = os.popen(\"some_command with args\") [http://docs.python.org/lib/os-newstreams.html os.popen() // Python Tutorial]\n: will do the same thing as os.system except that it gives you a file-like object that you can use to access standard input/output for that process. There are 3 other variants of popen that all handle the i/o slightly differently. If you pass everything as a string, then your command is passed to the shell; if you pass them as a list then you don\'t need to worry about escaping anything.\n\n* The Popen class of the subprocess module. [http://docs.python.org/lib/node528.html Popen class // Python Tutorial]\n: This is intended as a replacement for os.popen but has the downside of being slightly more complicated by virtue of being so comprehensive. For example, you\'d say\n print Popen(\"echo Hello World\", stdout=PIPE, shell=True).stdout.read()\n: instead of\n print os.popen(\"echo Hello World\").read()\n: but it is nice to have all of the options there in one unified class instead of 4 different popen functions.\n\n* The call function from the subprocess module. [http://docs.python.org/lib/node529.html call() // Python Toturial]\n: This is basically just like the Popen class and takes all of the same arguments, but it simply wait until the command completes and gives you the return code. For example: \n return_code = call(\"echo Hello World\", shell=True)\n\n* The last resort (not-recommended)\n: The os module also has all of the fork/exec/spawn functions that you\'d have in a C program, but I don\'t recommend using them directly.\n\n* <span style=\"color:blue\">\'\'\'The subprocess module should probably be what you use.\'\'\'</span>\n\n----\n=== Calling an external command in Python ===\n\n* How can I call an external command in Python?\n:->\n: Look at the subprocess module in the stdlib:\n\n from subprocess import call\n call([\"ls\", \"-l\"])\n\n: The advantage of subprocess vs system is that it is more flexible (you can get the stdout, stderr, the \"real\" status code, better error handling, etc...). I think os.system is deprecated, too, or will be ... [http://docs.python.org/library/subprocess.html#replacing-older-functions-with-the-subprocess-module related part of the python tutorial]\n: But for quick/dirty/one time scripts, os.system is enough, though.\n\n----\n=== String edit in python ===\n\n* sed to python [http://objectmix.com/python/387782-sed-python-replace-q.html]\n\nFor some reason I\'m unable to grok Python\'s string.replace() function.\n:-> replace() does not work with regular expressions. Try the following.\n import re\n p = re.compile(\"^.*\\[\")\n q = re.compile(\"].*$\")\n q.sub(\'\',p.sub(\'\', line))\n\nJust trying to parse a simple IP address, wrapped in square brackets, from Postfix logs.\n\nIn sed this is straightforward given:\n line = \"date process text [ip] more text\"\n sed -e \'s/^.*\\[//\' -e \'s/].*$//\'\n\nyet the following Python code does \'\'\'nothing\'\'\':\n line = line.replace(\'^.*\\[\', \'\', 1)\n line = line.replace(\'].*$\', \'\')\n\nIs there a decent description of string.replace() somewhere?\n:-> use re.sub()\n\n----\n\n=== Python string translate() method ===\n\n* [http://www.tutorialspoint.com/python/string_translate.htm Python string translate() method]\n: the method translate() returns a copy of the string in which all characters have been translated using table (constructed with the maketrans() function in the string module), optionally deleting all characters found in the string deletechars.\n\n* The following example shows the usage of translate() method. Under this every vowel in a string is replaced by its vowel position:\n\n <pre>\n#!/usr/bin/python\n\nfrom string import maketrans   # Required to call maketrans function.\n\nintab = \"aeiou\"\nouttab = \"12345\"\ntrantab = maketrans(intab, outtab)\n\nstr = \"this is string example....wow!!!\";\nprint str.translate(trantab);\n</pre>\n\n <pre>\nth3s 3s str3ng 2x1mpl2....w4w!!!\n</pre>\n\n* Following is the example to delete \'x\' and \'m\' characters from the string:\n\n <pre>\n#!/usr/bin/python\n\nfrom string import maketrans   # Required to call maketrans function.\n\nintab = \"aeiou\"\nouttab = \"12345\"\ntrantab = maketrans(intab, outtab)\n\nstr = \"this is string example....wow!!!\";\nprint str.translate(trantab, \'xm\');\n</pre>\n\n <pre>\nth3s 3s str3ng 21pl2....w4w!!!\n</pre>\n\n== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n=== 과제 목표 평가 방법 객관화/구체화 ===\n\n* 데이터 Access 성능 목표 달성 기준\n: Realistic enterprise workload 인가 시, (*)\n: Data placement 및 RACS-1 기술이 적용되지 않은 vanilla SW-RAID 대비\n: 80% 속도 향상 (in IOPS) (**)\n:: (*) Realistic enterprise workload라고 했을 때, 아래 workload 인가 방안의 Case A,B,C가 해당될 수 있겠다고 보고 있으며, \'13년 9월 쯤을 고려했을 때, 현실적으로는 Case C로 가고, workload type은 DB 혹은 fileserver 쪽으로 가는 것을 생각하고 있습니다. (\'14년 정도에는 Case B나 Case A도 생각해볼 수 있을 것 같습니다)\n:: (**) metric에 대한 부분인 \"SW-RAID 대비 속도 향상 비율: 80%\"은 MBO에 이대로 명시가 되어 수정이 어렵다고 생각하여 그대로 두었습니다.  다만, 속도를 향상시키는 요소 기술로서 동작하는 것은 RACS-1 뿐만 아니라 Data Placement도 가능하겠으므로, 이를 염두에 두고 아래 내용을 작성하였습니다.\n\n\n* Workload type\n: 대표적인 enterprise workload type인 DB, FileServer, Web 중 택일하거나\n: Scientific Computing (R, Matlab, Hadoop 등)을 선택할 수 있겠음\n: VDI workload를 고려할지에 대해서는 추가 고민 필요 (아직까지는 고려하지 않고 있음)\n\n\n* Workload 인가 방안\n:- 대표적인 enterprise workload로 성능 benchmark 필요\n::- Case A) [최선] real enterprise workload 사용\n::- Case B) [차선] enterprise workload에 대한 real trace에 기반한 real workload replay 활용 (workload replayer 구현 필요)\n::- Case C) [차차선] TPC-C, filebench 등과 같이 real workload를 모방한 realistic synthesized workload 사용\n::- Case D) [차차차선] (비추천), iozone 등과 같은 random/sequential read/write I/O pattern 생성기 사용\n\n\n\n* Workload 인가 방식에 따른 환경 구성\n: Case A, B, C 모두 storage system과 network 구성은 동일하며, client 구성 방식에서 차이 존재\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n:- Case A: real workload 인가 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case B: real workload replay 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case C: TPC-C 혹은 filebench를 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n\n* Storage system interface exposed\n: SAN, NAS 방식 중 결정 필요\n\n\n* 가상화 이슈\n: 기본적으로 Hypervisor (Xen, VMware 등)를 사용 하지 않는 환경으로 구성\n\n\n* 성능 측정 방법\n:- 1-day (case A), 1-week (case B) 동안 workload를 인가하여 계산된 IOPS로 판별\n:- 가급적 경쟁사 제품의 성능측정 tool에서 제공하는 metric과 동일하게 가는 것이 좋을 것으로 보임\n::- Real enterprise workload 인가 시, iostat, sar 등 별도의 성능 측정 모듈 구동 필요\n::- Real workload replayer의 경우, total IOPS metric를 계산할 수 있도록 구현 필요\n::- TPC-C, filebench의 경우 (물론 iozone 역시) total IOPS metric을 계산하여 benchmark 결과로서 제공\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n# [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=8&ved=0CG4QFjAH&url=http%3A%2F%2Fwww.cse.buffalo.edu%2F~okennedy%2Fcourses%2Fcse704fa2012%2F2.2-HDFS.pptx&ei=WRtkUfz4NOTwiAeq1oGoCA&usg=AFQjCNGLoDZhdDfLkPO1RKcLINvTG7iL9Q&sig2=pvXlwiUJr302_D-BnHrISg&cad=rjt The Hadoop Distributed File System // Konstantin Shvachko, Hairong Kuang, Sanjay Radia, Robert Chansler, Yahoo! Sunnyvale, California USA]\n# [http://storageconference.org/2010/Papers/MSST/Shvachko.pdf \"The Hadoop Distributed File System,\" MSST 2010]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== Convolution, Cross-Correlation, Autocorrelation ===\n\n* [http://en.wikipedia.org/wiki/Convolution Convolution ((B.GOOD))]\n \n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n\n\n=== SR-IOV, MR-IOV ===\n\n* [http://searchstorage.techtarget.com/video/I-O-virtualization-video-SR-IOV-MR-IOV-NICs-and-more I/O virtualization video: SR-IOV, MR-IOV, NICs and more ((B.GOOD)) // 2012-09-17]\n* [http://blog.scottlowe.org/2009/12/02/what-is-sr-iov/ What is SR-IOV? ((B.GOOD)) // 2009-12-02]\n\n\n\n=== Linux VM and SBC NAS ===\n\n* NAS (ETRI GloryFS 기반) 사용 방법\n\n <pre>\nFrom: Jaewook Oh [mailto:jaew00k.oh@samsung.com] \nSent: Tuesday, April 02, 2013 5:06 PM\nTo: 정명준\nSubject: Re: Re: [결재 통보][SBC 자원신청]실험 데이터 보관 및 전처리 작업 수행을 위한 Linux VM 신청\n\n----\n\n안녕하세요 전문님,\n \n우분투 12.xx 지원이 여의치 않아 일단 11.10으로 할당 진행하였습니다.\n \n첨부된 패키지 및 fuse 패키지(배포된 이미지에 설치되어 있을것으로 예상됩니다...)  설치 후,\n \nmount.ifs 75.2.251.181:/FSM2/fit_icl 마운트될곳 -obigwrite\n \n명령어로 마운트하시면 되며, 패스워드를 물어보는데 secicl123 입력하시면 됩니다.\n\n</pre>\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>\n\n=== Trace Data 저장용 Linux VM 및 대용량 스토리지 (10TB) ===\n\n\n\n\n\n==== Linux VM 및 대용량 스토리지 사용 정보 ====\n\n\n* original hostname of \'gaesung-gongdan\': SECSAITU006-021\n\n\n\n <pre>\n------- Original Message -------\n\nSender : Jaewook Oh<jaew00k.oh@samsung.com> S5/Senior Engineer/R&D Infrastructure Group/Samsung Electronics\n\nDate : 2013-04-02 17:05 (GMT+09:00)\n\nTitle : Re: Re: [결재 통보][SBC 자원신청]실험 데이터 보관 및 전처리 작업 수행을 위한 Linux VM 신청\n\n\n안녕하세요 전문님,\n\n우분투 12.xx 지원이 여의치 않아 일단 11.10으로 할당 진행하였습니다.\n\n첨부된 패키지 및 fuse 패키지(배포된 이미지에 설치되어 있을것으로 예상됩니다...)  설치 후,\n\nmount.ifs 75.2.251.181:/FSM2/fit_icl 마운트될곳 -obigwrite\n\n명령어로 마운트하시면 되며, 패스워드를 물어보는데 secicl123 입력하시면 됩니다.\n\n\n------- Original Message -------\n\nSender : Chae Hwan Lim<ch2036.lim@partner.samsung.com> Partner/Cloud Platform Operation Group/SAMSUNG SDS\n\nDate : 2013-04-02 16:57 (GMT+09:00)\n\nTitle : Re: [결재 통보][SBC 자원신청]실험 데이터 보관 및 전처리 작업 수행을 위한 Linux VM 신청\n\n안녕하세요 클라우드플랫폼운영그룹 임채환대리입니다.\n\n신청하신 VM을 할당 완료 하였습니다.\n\n리눅스의 경우 escort가 적용이 되지 않기 때문에 \n\nescort예외 신청을 하셔야 네트웍을 사용할 수 있습니다...\n\nIP및 ID, password는 아래와 같습니다...\n\n좋은하루되십시오...\n\n==============================\n\nIP : 75.2.252.71\n\nID : root\n\nPW: 개발100%#&\n\nVNC접속(5900) PW : qwe123\n\n접속 PORT : 22(SSH)\n                  3389(RDP)\n\n- 신청 메뉴 : IT4U - ESCORT예외신청 - 설치예외(삭제) - ESCORT 삭제 불필요\n\n - 사업장 : 종합기술원 - 종합기술원\n\n - 제조번호, 모델명 : VMware\n\n - 대상구분 : PC\n\n - 신청항목 : 인터넷 사용\n\n - OS 구분 : Linux 계열\n\n - 신청 항목중 IP / MAC 주소 입력하는 부분에서 확인은 \n\n - 터미널 모드에서 ifconfig  입력 -> \n   \n   MAC 주소는 HWaddr : 00:xx~~ 시작되는 값주소\n   \n   IP 주소는 inet addr : 75.2.252.X 주소값 입니다.\n\n감사합니다.\n\n</pre>','utf-8'),(11,'== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(12,'== Lessons learned ==\n\n* \"An Analysis of Traces from a Production MapReduce Cluster,\" CCGRID 2010\n\n* CCGRID 2010에 소개되었던 CMU의 연구인 [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5493490 An Analysis of Traces from a Production MapReduce Cluster // Kavulya, S. CMU // CCGRID 2010]에는 Yahoo!의 M45 Supercomputing Cluster에서의 10달 간의 MapReduce tracelog에 대한 분석 결과가 있다. 이를 보면, task type과 failure 와의 연관성이 있음을 알 수 있다.\n\n* Failure와 straggler 간의 관계는 다음과 같다. straggler는 아직 명확한 exception이 일어나서 task가 명시적인 failure로 mark가 된 상태는 아니지만, 통상적인 경우보다 꽤 느리게 task가 수행되고 있는 상태를 의미한다.\n\n\n== Excerpts from paper ==\n\n{| class=\"wikitable\" border=\"1\" \n|+ Summary M45 dataset\n! Key\n! Value\n|-\n| Log Period\n|\n: Apr 25 - Nov 12, 2008\n: Jan 19 - Apr 24, 2009\n|-\n| Hadoop versions\n|\n: 0.16: Apr 2008 -\n: 0.17: Jun 2008 -\n: 0.18: Jan 2009 -\n|-\n| Number of active users\n| 31\n|-\n| Number of jobs\n| 171079\n|-\n| Successful jobs\n| 165948 (97%)\n|-\n| Failed jobs\n| 4100 (2.4%)\n|-\n| Cancelled jobs\n| 1031 (0.6%)\n|-\n| Average maps per job\n| 154 +/- 558 sigma\n|-\n| Average reduces per job\n| 19 +/- 145 sigma\n|-\n| Average nodes per job\n| 27 +/- 22 sigma\n|-\n| Maximum nodes per job\n| 299\n|-\n| Average job duration\n| 1214 +/- 13875 seconds\n|-\n| Maximum job duration\n| 6.84 days\n|-\n| Node days used\n| 132624\n|}\n\n\n* There were 4100 failed jobs and 1031 incomplete jobs in our dataset. These failures accounted for 3% of the total jobs run.\n\n\n* We observed that:\n# <span style=\"color:red\">\'\'\'Most jobs fail within 150 seconds after the first aborted task\'\'\'</span>. Figure 3(c) shows that 90% of jobs exhibited an error latency of less than 150 seconds from the first aborted task to the last retry of that task (the default number of retries for aborted tasks was 4). We observed a maximum error latency of 4.3 days due to a copy failure in a single reduce task. Better diagnosis and recovery approaches are needed to reduce error latency in long-running tasks.\n# Most failures occurred during the map phase. Failures due to task exceptions in the <span style=\"color:red\">\'\'\'map phase\'\'\'</span> were the most prevalent - 36% of these failures were due to <span style=\"color:red\">\'\'\'array indexing errors\'\'\'</span> (see Figure 4). <span style=\"color:red\">\'\'\'IO exceptions\'\'\'</span> were common in the <span style=\"color:red\">\'\'\'reduce phase\'\'\'</span> accounting for 23% of failures. Configuration problems, such as missing files, led to failures during job initialization.\n# Application hangs and node failures were more prevalent in cancelled jobs. Task timeouts, which occur when a task hangs and fails to report progress for more than 10 minutes, and lost TaskTracker daemons due to node or process failures were more prevalent in cancelled jobs than in failed jobs.\n# Spurious exceptions exist in the logs. The logs contained spurious exceptions due to debugging statements that were turned on by default in certain versions of Hadoop. For example, a debug exception to troubleshoot a problem in the DFS client attributed to data buffering, and an exception due to a disabled feature that ignored the nonzero exit codes in Hadoop streaming, accounted for 90% of exceptions from January 21 to February 18, 2009. This motivates the need for error-log analysis tools that highlight important exceptions to users','utf-8'),(13,'== # RAM-SSD Hybrid Page Cache Architecture ==\n: cache data에 대한 tier 별 placement가 가능하도록 access 패턴을 기반으로 page class를 분류하고, 이에 따라 tier-1 page cache, tier-2 page cache 간에 page chunk 단위로 page tiering을 수행하는 아키텍쳐 및 방법\n\n<br/>\n\n== # 발명의 이용분야 ==\n\n* Server-side cache\n* Client-side cache\n* Storage cache\n* Storage tiering\n\n\n* SSD and SSD array\n* SSD-HDD hybrid storage systems\n* Enterprise Server\n* Client PC/Mobile\n\n\n<br/>\n\n== # 배경 / 기존 기술의 문제점 ==\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n\n\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n\n\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n\n\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n== # 본 발명의 특징 / 효과 ==\n\n* page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐*: page cache 레이어에서 동작하는 RAM-SSD cache 아키텍쳐를 통해, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있게 함. 또한 RAM-only page cache에 비해 안정적인 page cache 공간 확보가 가능함**.\n\n\n* page tiering 메커니즘 기반의 RAM-SSD 간 cache data 이동: I/O 특성 및 speed에서 큰 차이가 있는 RAM page cache와 SSD page cache를 같은 방식으로 사용할 경우 야기될 수 있는 speed 저하 문제를 극복할 수 있게 하는, page-tiering 방식의 RAM page cache - SSD page cache 간 cache data 이동 메커니즘. (async I/O화되며, I/O sequentialization의 장점을 얻을 수 있음)\n\n\n* re-hit potential 기반의 page classification 및 이에 따른 page cache data placement: page에 대한 hit pattern 분석을 기반으로 currently-hot한 page와 re-hit potential을 가지고 있는 page들, 그리고 그 외의 page들의 class를 지정하고, 이에 따라 tier-1 page cache에 위치시킬 것인지, tier-2 page cache에 위치시킬 것인지를 판단하도록 함. 이를 통해 RAM을 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함.\n\n\n* (*) 데이터 hit 패턴 관찰 시, 블록 레이어에서 관찰하는 것에 비해, page cache 레이어에서 관찰하게 되면 블록 레이어에서는 잡아낼 수 없는 정보들이 있으며, 특히 더욱 intensive하게 hit되는 데이터에 대한 정보를 볼 수 있다는 장점이 있음. 이를 기반으로 더욱 intelligent한 cache 메커니즘을 구현할 수 있게 됨.\n\n\n* (**) SSD page cache 도입으로 인한 안정적인 page cache 공간 확보: idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n<br/>\n\n== # 대표 청구항 ==\n\n\n* RAM-SSD hybrid page cache architecture\n** page cache layer에서 동작하고;\n** tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는;\n** page tiering 기반의 hybrid page cache 아키텍쳐.\n\n\n* rehit-potential aware page classification, and page class based cache data placement\n\n\n* page tiering operation\n\n\n* page chunk I/O in SSD\n** log-structured\n** parallelism-maximized\n\n\n* TRIM ahead (for page chunk writes)\n\n\n* victim page chunk selection\n** oldest-first\n** most-mark-first\n** upper-tier-backup\n\n\n<br/>\n\n== # 대표 도면 ==\n\n<br/>\n\n\n== * 도면 목록 ==\n\n* [Fig. 1] Hybrid page cache architecture\n* [Fig. 2] Page cache architecture in detail: RAM page cache only\n* [Fig. 3] Page cache architecture in detail: additional SSD block cache\n* [Fig. 4] Page cache architecture in detail: hybrid page cache, proposed\n* [Fig. 5] Tracelog based hit pattern analysis: TPC-C case (250GB, 48hrs)\n* [Fig. 6] Data move: RAM page cache only\n* [Fig. 7] Data move: additional SSD block cache\n* [Fig. 8] Data move: hybrid page cache, proposed\n* [Fig. 9] Page classification based cache data placement\n* [Fig. 10] Page find/get: radix tree based\n* [Fig. 11] Page find/get: mapping table based\n* [Fig. 12] Page node data structure\n* [Fig. 13] Page classification operation\n* [Fig. 14] Page tiering I/O\n* [Fig. 15] Page tiering pool & tiering buffer operation\n* [Fig. 16] Selection of victim page chunk: oldest-first\n* [Fig. 17] Selection of victim page chunk: most-mark-first\n* [Fig. 18] Selection of victim page chunk: most-mark-first, upper-tier-backup\n* [Fig. 19] Page chunk I/O: log-structured, parallelism-max\'ed\n* [Fig. 20] Page chunk I/O: TRIM ahead (for page chunk writes)\n\n\n<br/>\n\n== # 기술 상세 ==\n\n\n* page cache layer에서 동작하고, tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는 것을 특징으로 하는 RAM-SSD hybrid page cache 아키텍쳐. Fig. 1에 이 RAM-SSD hybrid page cache가 전체 시스템에서 어느 부분에 해당되는지에 대해 설명되어 있음.\n\n\n* Fig. 2는 일반적인 기존 page cache 구조임. 이는 RAM의 여유 공간을 사용하는 전통적인 page cache 구조로서, 속도가 빠른 RAM을 사용하지만, 아키텍쳐적인 한계 혹은 비용적인 문제로 인해 RAM 용량을 크게 늘리기 어렵다는 문제가 있다. 이로 인해, cache 용량 측면의 제약이 생기며, 그만큼 HDD로부터 read해야만 하는 경우가 많이 생길 수 있다는 근본적인 한계가 있다. 뿐만 아니라 이 방식은 시스템의 idle memory 양이 줄어들게 되면 그만큼 cache 공간이 줄어들기 때문에 시스템에서 구동되는 process들이 많아지거나, 각 응용에서 memory를 많이 요구할수록 cache로 인한 성능향상 효과가 감소하는 현상이 발생할 수 있다.\n\n\n* Fig. 3은 SSD를 block cache로 사용하는 구조임. 이는 RAM의 용량 확장이 제한적이라는 문제점을 해결하기 위해, HDD에 비해 속도가 빠르지만 RAM보다 용량 확보 측면에서 장점을 가지고 있는 SSD를 cache media로 사용하는 방식임. SSD의 용량만큼 cache의 용량을 확장하기 용이하다는 장점이 있으나, block layer에서 동작하는 구조적인 한계로 인해, 이 SSD 기반 block cache의 효용성을 극대화 하기 어렵다는 단점이 있다. 이 모델의 한계는 SSD를 cache로 활용함에 있어, SSD와 RAM의 근본적인 media 특성을 살려 접근하지 못했다는 측면과, block layer에서 동작했을 때 안고 갈 수 밖에 없는 근본적인 시스템 아키텍쳐적인 한계를 간과했다는 데에 있다. \n\n\n* Fig. 4는 tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager 로 구성되어 있는 RAM-SSD hybrid page cache 아키텍쳐를 나타낸다. page cache tiering manager는 page access pattern monitor, page class manager, page tiering I/O module을 포함하고 있다. tier-1 page cache와 tier-2 page cache는 page cache data 및 관련 meta info가 저장되는 곳이며, page cache tiering manager는 page의 access pattern에 따라 page class를 분류하고 그에 따라 page가 tiering될 수 있도록 tiering I/O를 관리한다. 이 RAM-SSD hybrid page cache와 기존 SSD block cache 간의 큰 차이점으로서, block layer가 아닌 page cache level에서 동작하는 구조이며, 서로 다른 media 특성을 가진 RAM과 SSD를 page cache로 사용함에 있어 page cache tiering 접근을 택하였다는 점을 들 수 있다.\n\n\n* page cache 레이어에서 동작하는 RAM-SSD hybrid page cache 아키텍쳐를 취함으로써 얻을 수 있는 이점으로, SSD가 가져다주는 cache capacity 확대의 장점을 끌어안으면서도, 블록 레이어에서의 SSD cache가 태생적으로 가지게 되는 낮은 cache hit ratio의 한계를 극복할 수 있다는 점을 들 수 있다. 또한 SSD page cache 도입으로 인한 안정적인 page cache 공간 확보가 가능해짐. 즉, idle RAM 공간에 의지하던 기존 RAM-only page cache의 경우에는, cache 공간을 지속적으로 확보하고 있기가 어려웠기 때문에 page cache 크기에 편차가 있을 수 밖에 없었으며, 이로 인해 결과적으로 시스템의 sustainable performance를 달성하기 어려웠음. 그러나 본 발명에서는 idle RAM 공간에 전적으로 의지하는 기존 page cache 아키텍처와는 달리 RAM의 일부 영역을 hybrid page cache의 영역으로 예약하고 SSD를 page cache의 전용 공간으로 활용토록 함으로써 안정적인 cache 공간을 확보하는 효과를 얻을 수 있음. 결과적으로 시스템의 idle RAM 크기가 부족하게 되더라도 기존 방식에 비해 성능 편차를 줄일 수 있게 됨.\n\n\n* hybrid page cache 구성의 대표적인 한 예로서 RAM이 tier-1 page cache media로 사용되고, SSD가 tier-2 page cache media로 사용되는 경우를 들 수 있다. 이때, tier-2 page cache media로서 단일 SSD가 사용될 수도 있지만, RAID 혹은 기타 방식으로 엮어진 SSD array가 사용될 수도 있다.\n\n\n* hybrid page cache가 tier-2 page cache media로서의 SSD와의 I/O를 직접 관리할 수도 있지만 SSD 혹은 SSD array에 대한 I/O 최적화를 담당하는 별도의 layer를 통할 수 도 있다. 그리고 일반적인 block layer를 통해서 SSD I/O가 이루어질 수도 있겠으나, 이러한 전형적인 block I/O path가 아닌 전용의 I/O path를 통해서 SSD를 tier-2 page cache로 사용할 수도 있다.\n\n\n* hybrid page cache architecture에서 핵심적인 역할을 담당하는 것이 page cache tiering manager로서, page access pattern monitor, page class manager, page tiering I/O module으로 구성되어 있다.\n\n\n* page access pattern monitor는 I/O request interface로부터 내려오는 모든 I/O request들을 보고 각 page들에 대한 access 패턴을 관찰한다. 기존의 page cache에서는 LRU (Least Recently Used) 방식이 page replacement (혹은 page eviction) 알고리즘으로 사용되고 있다. 이 LRU 메커니즘은 recency만을 보는 것으로서, 이를 통해 현재 RAM page cache에서 우선적으로 keep하고 있을 page들과, 언제든지 필요에 따라 evict될 수 있는 page들을 구분한다. 그러나 RAM이 tier-1 page cache media로 사용되고 SSD가 tier-2 page cache media로 사용되는 RAM-SSD hybrid page cache architecture에서는 RAM 기반의 tier-1 page cache에는 현재 가장 hot한 data가 cache되도록 하고, SSD 기반의 tier-2 page cache에는 비록 현재 가장 hot하지는 않더라도 잠재적으로 곧 다시 access될 수 있거나 다시 hot 해질 수 있는 page들이 cache되도록 한다. 즉, page에 대한 hit pattern 분석을 기반으로 currently-hot한 page와 re-hit potential을 가지고 있는 page들, 그리고 그 외의 page들의 class를 지정하고, 이에 따라 tier-1 page cache에 위치시킬 것인지, tier-2 page cache에 위치시킬 것인지를 판단하도록 함. 이를 통해 RAM을 currently-hot page cache로 사용하고, SSD를 potentially re-hittable page cache로 사용할 수있게 함. 이렇게 함으로써 더 많은 data들이 cache로 인한 성능 향상 혜택을 볼 수 있게 된다.\n\n\n* 이런 식으로 RAM 기반의 tier-1 page cache와 SSD 기반의 tier-2 page cache의 역할을 나누는 이유는 다음과 같다. tier-1 page cache media로 사용되는 RAM과 tier-2 page cache media로 사용되는 SSD를 비교해보았을 때, 일반적으로 RAM이 I/O의 속도 측면에서 월등히 우세하며, read와 write의 특성이 대칭적이라는 장점이 있다. 반대로 SSD는 RAM에 비해 I/O 속도는 상대적으로 느리지만 (그러나 HDD에 비해 월등히 빠른 I/O 속도를 가지고 있음), tier-1 page cache에 비해 훨씬 큰 용량을 확보하기 용이하다는 장점을 가지고 있다. 그러나 SSD의 경우 sustained I/O 속도 및 wear-out 측면에서 random write에 취약하다는 단점을 가지고 있다. RAM (tier-1 page cache)과 SSD (tier-2 page cache)로 이루어지는 hybrid page cache의 성능을 극대화 하기 위해서는 두 tier 간에 존재하는 이러한 I/O 특성 및 용량 측면의 차이점을 적극적으로 활용하는 것이 중요하다.\n\n\n* 디스크에서 access된 data는 우선적으로 tier-1 page cache에 cache된다. 가장 자주 access되는 data가 우선적으로 cache될 수 있도록 하는 LRU 정책으로 운영됨. 기존 kernel에서 page cache 관리 모듈 중, data를 page cache에서 찾고, pool에 추가하고, pool에서 제거하는 등의 page 처리 부분은 변경할 필요 없으며, 수정되는 부분은 hit count에 따라 page의 상태 정보를 update하는 모듈로서, 각 page에 대한 access 패턴에 따라 page class를 결정하고 업데이트하는 부분이 모듈에 추가된다.\n\n\n* Fig. 5는 TPC-C 라는 대표적인 workload에서 발생한 I/O access pattern을 분석한 결과이다. 실험 환경은 다음과 같다. MySQL DB 서버 노드에 online e-commerce transaction을 수행할 수 있는 250GB 크기의 DB table을 생성해둔 후, 48 시간 동안 TPC-C benchmark을 client side에서 구동하였다. 이때, DB 서버 노드의 DB partition 에서 생성된 block I/O tracelog를 분석하였다. (물리적으로 별도의 HDD를 DB partition에 통째로 할당하였으며, 해당 DB partition에는 MySQL DB 외의 다른 응용이 I/O access하지 않도록 isolation하였음)\n\n\n* block layer에서 관찰된 I/O들의 특성을 보았을 때, 생각보다 rehit 되는 비율이 크지 않았다는 것을 알 수 있다. 즉, 48시간 동안으 TPC-C workload에 의해 access되었던 모든 address들에 대해서, 몇 번씩 hit되었는지를 count해본 결과, 다음과 같은 결과를 볼 수 있었다. 1번 access되고 나서 다시 access되지 않은 address가 전체 경우 중 53.33%를 차지하는 것을 알 수 있으며, 1번에서 2번까지 access된 address를 합해보면 전체의 70.22%를 차지한다는 것을 알 수 있다. 1번 부터 6번 까지 access되었던 address들만 모아봐도 전체의 90%가 넘는 것을 알 수 있다 (정확히는 91.53%). 생각보다 rehit 비율이 높지 않다는 것을 보여주고 있다.\n\n\n* 이 상황을 설명할 수 있는 가능한 시나리오 중 하나는, 자주 access되는 hot한 data에 대해서는 block layer로 I/O request가 내려오기 전에 page cache layer에서 이미 serve되고 있기 때문에 정말 hot한 data들은 page cache에서 serve되고, block layer에까지 내려오게 된 I/O들은 상대적으로 덜 hot한 I/O들의 집합일 수 있다는 것이다. 이러한 바를 고려해보면, 만약 block layer에서 동작하는 cache를 만들었다 하더라도 이 block cache가 시스템 I/O 성능 향상에 기여할 수 있는 부분이 제한적일 수 있다는 생각을 할 수 있다. 즉, block layer에서 동작하는 cache보다는 page cache layer에서 동작하는 cache가 I/O 성능 향상에 더욱 큰 기여를 할 수 있을 것으로 보인다.\n\n\n* Fig. 6은 각각 RAM page cache only 경우, Fig. 7은 RAM page cache에 SSD block cache가 더해진 경우, Fig. 8은 RAM-SSD hybrid page cache의 경우 각각에 대해서 I/O request 및 cache의 동작을 도식화 한 것이다.\n\n\n* 특히 Fig. 7과 Fig. 8을 비교해보면, 기존 SSD block cache의 inefficiency를 볼 수 있다. RAM page cache only인 경우에 비해 SSD block cache가 cache 용량을 SSD 용량 급으로 늘려줄 수 있다는 장점으로 등장했으나, cache가 비효율적으로 운용되고 있음을 확인 가능하다. Fig. 8의 hybrid page cache에 비해, SSD block cache는 굳이 가지고 있지 않아도 되는 data들을 caching하게 되는 구조이며, 이러한 이유로, 같은 용량의 SSD가 있더라도 hybrid page cache 구조가 더욱 공간 효율적으로 cache를 운용할 수 있게 된다. 즉 block cache가 hybrid page cache에 비해 공간 비효율적으로 운영된다는 것이다.\n\n\n* SSD block cache는, 일단 I/O request가 발생하여 disk로부터 읽은 data는 SSD block cache에도 caching이 되는 구조이기 때문에, 이후 사용될 가능성이 낮은 data까지 불필요하게 caching하게 된다. 혹시 나중에 필요가 없다고 판단이 되어 해당 data를 eviction 하더라도 이미 한 번 write된 data를 erase해야 하기 때문에 SSD 입장에서는 wear-out 측면에서 손해가 발생한다.\n\n\n* 그러나 hybrid page cache에서는 I/O request 발생에 따라 disk로부터 읽어들이는 데이터를 무조건 SSD에 write하지 않는다. page access pattern 분석에 따라 tier-2 page cache에 있을만하다고 선택된 data들이 page chunk 단위로 SSD에 write되는 것이기 때문에 write의 횟수 자체도 작고, 그냥 버려질 수도 있을만한 data는 SSD에 write될 확률이 상대적으로 낮다. 따라서 hybrid page cache가 SSD block cache에 비해 공간 효율적, wear-out 효율적으로 cache 운용이 가능하게 된다. 달리 표현하면 같은 용량의 SSD가 주어졌다 하더라도, block cache 구조보다는 hybrid page cache 구조가 더 많은 유효 cache data를 가지고 있을 수 있다는 것이며, 이는 유횩 cache 성능으로 이어지게 된다. 이러한 cache space 효율의 차이는 block cache와 hybrid page cache 간의 근본적 아키텍쳐 차이에서 발생한다.\n\n\n* 예제로 도식화 된 Fig. 7와 Fig. 8의 경우를 보면, 유효 cache data serve 횟수는 두 경우 모두 t10, t12 두 번으로 볼 수 있다. 그러나 이를 위해서 사용한 cache space에서 차이가 있다. Fig. 7의 SSD block cache의 경우에는 t1, t2, t3, t5에 발생한 4번의 write이 수반되었으며 저장된 데이터도 4개 분량이다. , Fig. 8의 hybrid block cache의 경우에는 t7과 t8 사이에 발생한 1번의 write으로 데이터 2개를 저장하고 있다. 유효 cache data server 횟수는 두 경우 모두 2번으로 같지만 이를 위해 소비한 cache 공간 및 write 횟수는 이렇게 차이가 나므로 상대적으로 같은 단위 cache 공간 대비 유효 cache server 비율을 따져보면 hybrid page cache의 경우가 더욱 효율적임을 알 수 있다.\n\n\n\n* Fig. 9는 hybrid page cache에서 page access pattern monitoring을 통해 page class를 분류하고, 그에 따라 cache data를 적절한 tier의 page cache에 placement하는 방식을 나타낸다. page cache에 존재하는 page들은 크게 unevictable한 것과 evictable한 것으로 구분될 수 있으며, 다시 evictable한 것들은 file-related page와 그렇지 않은 page로 구분될 수 있다. 일반적으로 unevictable page들은 kernel 등 시스템에서 특정 목적으로 reserve하여 사용하는 page들인 경우가 많으므로 임의로 tiering을 하거나 별도의 조작을 하기에 조심스럽다. hybrid page cache에서는 evictable한 page들 중에서도 file-related page를 tiering을 통해 SSD에 caching하는 대상으로 삼는다. evictable, file-related page들은 page access pattern monitoring에 따라 다시 {\"currently-hot\"} page (class-1)와, {\"currently-not-hot\" but \"has-rehit-potential\"} page (class-2)와, 그리고 여기에 해당되지 않는 {\"the-others\"} page (class-3)로 분류된다. 전술한 바와 같이, {unevictable} page와 {evictable, non-file-related} page (class-0)들은 hybrid page cache가 다루는 대상에서 제외된다.\n\n\n* class-1 page들은 tier-1 page cache (RAM)에서 serve되며, class-2 page들은 tier-2 page cache (SSD)에서 serve되게 하기 위해서 page tiering operation을 통해 SSD로 write된다. class-3 page들은 hybrid page cache에 의한 별도의 관리를 받지 않고 언제든 kernel에 의해 evict될 수 있다.\n\n\n* hybrid page cache에서 각 page node는 radix tree 혹은 hash table, linked list 등 다양한 방법으로 구조화 되고 관리될 수 있다. Fig. 10은 radix tree 기반의 page node 관리 방식이며, Fig. 11은 hash table 기반의 page node 관리 방식을 예시로 보이고 있다. 이러한 구조를 통해 목적하는 page를 find/get할 수 있다.\n\n\n* Fig. 12에는 page node는 page find/get 및 hybrid page cache의 page tiering을 가능케 하기 위해 필요한 항목들이 나타나 있다. 이 page node가 어느 disk의 어느 위치에 있는 data를 cache하고 있는지에 대한 정보인 corresponding logical addr. field, 이 page가 evictable한 page인지 아닌지를 식별해주는 evictable-or-not flag, 이 page가 file과 관련되어 있는 page인지 아닌지를 식별해주는 file-related-or-not flag, 이 page가 내용이 변경되었는데 아직 disk에 저장되지 않은 상태인지를 나타내는 page-is-dirty flag, 이 page가 tier-2 page cache로 tiering되었는지 아닌지를 식별해주는 tiered-or-not flag, 지정된 단위 시간동안 이 page가 몇 번 access되었는지에 대한 integer counter로서 동작하는 access count field, 이 page가 어느 page class에 해당하는지를 식별해주는 class ID field, 이 page가 tier-2 page cache로 tiering된 경우 tier-2 page cache의 어느 위치에 해당 page data가 저장되어 있는지 찾아갈 수 있게 해주는 정보인 page data location at tier-2 page cache field, 이 page가 tier-2 page cache로 tiering되어 있지 않고 tier-1 page cache에 그대로 있는 경우에 해당 page data를 보유하고 있는 page data field, 그리고 보조 데이터로서, 기타 필요한 정보들을 가리키거나 포함할 수 있는 aux. data field. (한 예로, 필요 시, class-2 page들에 대한 chain을 관리하고 있는 page tiering pool로 가는 back link 정보가 포함될 수도 있다)\n\n\n* Fig. 13는 순서도로서, page가 class-1으로 분류되었을 때, class-2로 분류되었을 때, class-3로 분류되었을 때, 각각에 따라 처리되는 내용을 나타낸다. class-1으로 분류된 경우는 별다른 처리 없이 원래 RAM page cache에 의한 방식대로 처리되면 되기 때문에 별도의 커멘트는 없다. class-2로 분류된 경우는 일단 해당 page가 임의로 evict되는 것을 방지하기 위해서 우선 \'evictable-or-not\' flag를 \'_unevictable\'로 설정한 후, page tiering pool이 관리하는 class-2 page chain에 추가한다. \n\n\n* class-3로 분류된 경우를 그 직전 상태에 따라 exhaustive하게 구분해보면, (1) class-1에서 class-3로 변경된 경우, (2) class-2에서 class-3로 변경된 경우, (3) class-3에서 class-3으로 유지되는 경우, 이렇게 총 3가지로 볼 수 있는데, (3)번은 원래 class-3에서 변동된 것이 없으므로 특별히 언급할 것은 없고, 다만 class-3의 특성 상, 별도로 hybrid page cache에 의해 관리되는 것 없이, 시스템에 의해 필요 시 아무때나 evict될 수 있다는 것만 언급한다. 그 다음 trivial case는 (1)번, 즉 class-1에서 class-3으로 변경된 경우이다. 그러나 이 경우 역시 hybrid page cache에서 새롭게 특별히 관리할 부분은 없다. (2)번, 즉 class-2였다가 class-3으로 변경된 경우에는 해당 page가 이미 tier-2 page cache에 tiering된 상태일 수 있으므로 (상황에 따라 아직 tiering buffer에 의해 page tiering pool에서 tier-2 page cache로 옮겨지지 않았을 수도 있다) tier-2 page cache의 해당 page에 \'_to_be_discarded\' mark을 해둔다. 그러나 이것이 당장 해당 page를 erase해야 한다는 것은 아니며, SSD에서 말하는 소위 TRIM 명령이 전달된 것도 아니다. 상황에 따라 page chunk 단위로 erase할 일이 있을 때, 우선적으로 선정될 page chunk 판단 시 근거로 사용되거나, 구제되어야 할 valid data를 골라낼 때 사용된다. hybrid page cache에서는 tier-2 page cache에서 erase할 때 page chunk 크기 단위로 erase한다. 그러나 tier-2 page cache 내에서 봤을 때, write 당시의 page chunk 영역과 erase 당시의 page chunk 영역은 같지 않을 수 있다. 이는 _to_be_discarded mark의 분포와, 자주 access되어 tier-1 page cache에서 serve되고 있어서 표기된 _now_in_tier_1 mark의 분포에 의해 영향을 받을 수 있다. 즉 mark을 얻게 된 원인은 정 반대이지만, _now_in_tier_1 mark나 _to_be_discarded mark가 많이 포함된 page chunk는 tier-2 page cache의 space starvation 시 우선적으로 erase 될 가능성이 높다.\n\n\n* tier-2 page cache 내의 mark 관리 및 page chunk ID 관리 등에 필요한 meta info들은, update될 때마다 SSD 내의 NAND에 바로 바로 write되지 않는다. SSD 내부 시스템 operation을 위해 존재하는 SSD 내부의 memory (RAM) 영역의 일부에 저장되고 처리되도록 한다.\n\n\n* Fig. 14는 순서도로서, page tiering I/O operation을 나타낸다. page tiering I/O operation이 trigger되는 경우는 event에 의한 경우와 self-scheduling에 의한 경우로 구분될 수 있다. 일반적으로는 후자인 self-scheduling에 의해 trigger되겠지만, 시스템 shutdown 혹은 기타 특별히 다루어져야 하는 시스템 event가 발생했을 때에는 self-scheduling에 의한 시점이 아직 도래하지 않았다 하더라도 page tiering I/O operation을 시작한다.\n\n\n* class-2으로 mark되는 순간 page tiering pool에 등록되지만, class-2가 되자마자 바로 tiering I/O가 trigger 되는 것은 아님. hybrid page cache의 자체 scheduling time에 의해 tiering I/O operation이 trigger되기 때문에 class-2로 marking되는 시점과 실제 tiering 되는 시점에 차이가 있을 수 있음. 혹시 tiering되기 전에 page tiering pool에 담겨있던 class-2의 page들이 class-1이나 class-3으로 변경되게 되면, 그리고 나서 tiering이 trigger된다면, class-1이나 class-3이 된 page들이 tier-2 page cache에 write되는 일은 없는 구조임. 실제 tier-2 page cache로 tiering되기 전에 다시 class-1이나 class-3으로 class ID가 변경되는 순간, page class update function에 의해 page tiering pool에서 관리하는 list (chain)로부터의 link가 삭제 되기 때문에, class-2에서 class-1 혹은 class-3으로 변경된 page들이 class-2 page들과 함께 tiering되지 않음.\n\n\n* page tiering I/O operation이 trigger되면 우선 page tiering pool 내에 tiering되어야 하는 page chunk들이 존재하는지 확인한다. 만약 page chunk 크기를 채울 만큼의 page들이 page tiering pool 내에 모아져 있지 않으면 아무 일 없이 다음 번 scheduling 시기를 기다린다. 그러나 예외적으로 특별히 다루어져야 하는 시스템 event가 발생하여 trigger된 경우라면 미치 page chunk 크기 만큼의 page들이 page tiering pool 내에 모아져 있지 않더라도 tiering I/O operation을 수행해야한다. 이 경우 page chunk size를 맞추기 위해 zero padding 등을 이용할 수도 있다.\n\n\n* 만약 page chunk 크기를 채우고 남을 만큼의 page들이 page tiering pool 내에 모아져 있으면, page chunk write request를 구성하여 tier-2 page cache로 보낸다. page chunk write operation이 끝나면 SSD tier-2 page cache로부터 다음 data들을 return 받는다. (1) page chunk write operation에 대한 success/fail을 나타내는 status value, (2) SSD tier-2 page cache 내에서 해당 page chunk를 찾으려 할 때 사용할 수 있는 page chunk ID (혹은 page chunk address로 볼 수도 있음), (3) page chunk write 후 tier-2 page cache 내에 남은 free space 정보 등, 기타 보조 data가 담길 수 있는 aux. data 항목.\n\n\n* tier-2 page cache로부터 받은 page chunk write operation의 return data에서 free space 정보를 확인. available한 free space의 양이 page chunk write에 영향을 줄 수 있을 정도로 적게 남아 있으면, page chunk eviction request를 tier-2 page cache에 전달한다. 이때, victim page chunk를 선택하기 위해 oldest-first 방식을 사용할지 아니면 most-mark-first 방식을 사용할지는 시스템 정책에 따른다.\n\n\n* erase될 page chunk를 선택하는 기준으로서, 가장 오래 전에 SSD에 쓰였던 page chunk부터 순차적으로 erase를 시키는 oldest-first 방법, 혹은 page cache의 page find/get 메커니즘에 의해 RAM에 다시 load되어 사용되고 있거나 (_now_in_tier_1 mark), 자주 사용되지 않을 것으로 판단된 data (_to_be_discarded mark)들을 많이 포함하고 있는 page chunk부터 erase 시키는 most-mark-first 방법을 사용할 수 있음.\n\n\n* most-mark-first 방법을 사용하는 경우, 혹시 page chunk 내에 RAM page cache에 load된 부분과 load되지 않은 부분이 섞여 있는 경우, host에 RAM page cache로 사용할 수 있는 여유 RAM이 얼마나 존재하느냐와 시스템에서 정한 policy에 따라서, 다음 두 가지 방식 중 적절한 것을 이용할 수 있음. 첫 번째 방식은 RAM page cache로 사용할 수 있는 여유 RAM이 존재하는 경우, 해당 page chunk를 erase하기 전에 unloaded page data를 RAM page cache에 load시킴으로써, 해당 page chunk의 data가 모두 RAM page cache에 존재하게 한 후, 해당 page chunk를 통째로 erase하는 방식임. 두 번째 방식은 별도의 조치를 하지 않고, 그냥 해당 page chunk를 erase하는 방식임. 첫 번째 방식의 특징은 class 2로 분류되었던 page cache data의 이후 re-hit 잠재력을 더 중요하게 봄으로써 이후 re-hit이 일어났을 때 매우 빨리 cache로서 serve될 수 있게 한다는 데 있음. 두 번째 방식의 특징은 SSD page cache를 한 번 다 채울 때까지 해당 page chunk의 page data들이 access된 적이 없다는 것은 recency 측면에서 그만큼 re-hit 될 가능성이 낮다는 것으로 간주하였다는 것임.\n\n\n* erase 연산으로 인한 I/O 속도 저하를 방지하기 위해서는, page chunk write 시점이 도래하기 전에, SSD 내부적으로는 page chunk 단위의 erase 연산이 미리 수행될 필요가 있다. 미리 수행될 시점을 가장 잘 알고 있는 것은 page tiering이 발생하는 빈도 및 규모의 추세를 파악할 수 있는 hybrid page cache 모듈이므로, host-SSD 간 interface를 통해서 SSD에게 어느 page chunk에 대해서 erase를 미리 해두라고 요청할 수 있다. SATA 기반 SSD의 경우 TRIM 명령을 이용할 수 있으며, 이를 본 발명에서는 TRIM ahead라고 부른다.\n\n\n* hybrid page cache 모듈에서는 page access 패턴에 따라서 지속적으로 page classification 작업을 수행하게 되며, 그에 따라서 class-2로 분류되는 page들을 관리하는 page tiering pool의 크기가 늘어나고 tiering buffer에 의해 SSD page cache로 tiering 되는 양의 추세를 모니터링할 수 있음. 가용 SSD page cache의 크기를 알고 있기 때문에 앞으로 어느 정도 지나면 SSD page cache가 full이 될 것인지를 최근의 추세를 반영하는 간단한 선형식을 이용하여 파악 가능.\n\n\n* page chunk eviction request를 받으면 tier-2 page cache는 parameter로 전달받은 page chunk ID에 해당하는 page chunk가 포함하는 모든 data (page 혹은 erase block 단위)에 대해서 to-be-erased marking을 하고, 의도적인 delay 없이 바로 erase 작업을 실시한다. 이때 host에 의해 지정된 page chunk region 내에 valid data가 포함되어 있다 하더라도, 혹시라도 시스템 정책적으로 선택하는 upper tier backup (tier-1 page cache로 valid data를 copy하는 것) 외에는 tier-2 page cache 내에서의 data copy는 하지 않는 것으로 하기 때문에, 야기되는 write amplification은 없다.\n\n\n* 이후에 있을 수도 있는 cache bootstrap 시 cache warm-up 시간을 단축시키기 위해서 hybrid page cache가 관리하고 있던 page data들과 meta info들을 snapshot시킬 수 있다. hybrid page cache shutdown 혹은 시스템 shutdown 시에, 혹은 기타 경우에 snapshot trigger가 일어날 수 있으며 snapshot된 image는 별도의 SSD에 저장된다. hybrid page cache bootstrap 시에는 해당 snapshot image를 통째로 다 memory에 올리거나, SSD가 저장하고 있는 snapshot image중 meta info 부분만을 memory에 올림으로써 cache warm-up 시간을 단축할 수 있게 된다.\n   \n\n* Fig. 15(a), Fig. 15(b)는 tiering buffer operation에 의해 page chunk 크기 단위로 이루어지는 page tiering I/O 방식을 보이고 있다. page tiering pool은 class-2로 분류된 page들을 collect해 놓고 있다가 tiering buffer에 의해서 tier-2 page cache로 해당 page를 보내는 역할을 수행한다. 이때, tiering되는 단위는 page chunk로 불리우며, tiering buffer의 크기도 바로 이 page chunk 크기로 결정된다. 즉, tier-2 page cache로 동작하는 SSD에서는 page data가 tiering될 때 page chunk 단위로 write된다. tier-2 page cache에 write될 때는 tiering buffer에 의해 page chunk 단위로 I/O가 일어나지만, tier-2 page cache에서 read될 때는 page chunk 단위로 read되어야 하는 제약 없이 자유롭게 page 단위로 read 가능하다. tiering buffer는 page tiering pool이 관리하는 page node들의 chain을 traverse하면서 page chunk 단위로 tier-2 page cache에 page data들을 tiering 시킨다.\n\n\n* page cache tiering 시의 I/O 성능 최적화를 위해서 다음과 같은 방법이 사용될 수 있음. SSD에 전달되는 단위 I/O의 크기는 tiering buffer의 크기에 따라 결정되는데 host에서는 SSD의 channel 정보 및 erase block 크기를 감안하여 tiering buffer 크기를 결정할 수 있음. 예를 들어 어느 SSD가 N_ch개의 channel을 가지고 있고, erase block의 크기가 N_eb 바이트라면, N_ch * N_eb 값을 tiering buffer의 크기 N_tb 로 잡을 수 있음. \n\n\n* page cache tiering I/O에 의해 생성된 N_tb 크기의 page chunk에 대한 write 요청이 SSD에 들어오면, SSD는 page chunk를 N_ch 개의 N_eb 크기의 data로 분할한 후, 분할된 각 N_eb 크기 만큼의 data를 N_ch 개 존재하는 각 channel에 나누어 저장함. 따라서 N_eb * N_ch 크기 만큼의 page chunk 데이터를 SSD에 저장하는 데에 걸리는 시간은 N_eb 만큼의 데이터가 write되는데 걸리는 시간이 되며, N_ch배 만큼의 I/O 성능 향상을 얻게 됨.\n\n\n* SSD의 hardware 적인 구조 상, page chunk가 N_pc 개 만큼 있을 수 있다고 한다면 (page chunk 0 ~ page chunk N_pc -1) N_pc 번째의 page chunk까지 log-structured 방식으로 write되고 난 다음에는 기존 page chunk 중 하나를 erase한 후에 write해야 함. 이때 고려되어야 하는 것은 erase될 page chunk를 결정하는 방법이며, 또한 erase 연산으로 인한 I/O 속도 저하를 방지할 수 있는 방법임.\n\n\n* tier-2 page cache media로 사용되는 SSD의 상태에 따라 page tiering 작업을 잠시 보류할 수 있다. 이때, tier-2 page cache media로 사용되는 SSD에게는 어느 정도 크기의 page 데이터를 tiering할 것인지에 대한 정보를 전달하고, page-tiering-pended 라는 flag를 설정한다. 그러면 해당 SSD에서는 해당 크기의 데이터 writing에 필요한 공간 확보를 위해 어느 정도 기다려야 하는지에 대한 예상 시간을 응답한다. 만약 복수 개의 SSD들이 page cache media로 사용되고, host 내에 복수 개의 SSD들을 관리하는 SSD array management layer가 있다면, 그 SSD array management layer에서 해당 크기의 page tiering을 위해 어느 SSD에 저장하는 것이 좋을지를 알아서 판단하여 해당 SSD로 I/O가 일어나도록 연계해줄 수도 있다.\n\n\n* Fig. 16은 전술한 victim page chunk를 선택하는 방식 중 하나인 oldest-first 방식을 설명한다. 3개의 victim candidate들 중 어느 순서대로 선택을 하게 될지에 대한 예제에서, oldest-first 방식을 따르게 되면 tier-2 page cache에 가장 먼저 write 된 순서대로 선택하게 된다. 따라서, 1st victim은 page chunk 1, 2nd victim은 page chunk 2, 3rd victim은 page chunk 3이 된다.\n\n\n* Fig. 17은 전술한 victim page chunk를 선택하는 방식 중 하나인 most-mark-first 방식을 설명한다. 3개의 victim candidate들 중 어느 순서대로 선택을 하게 될지에 대한 예제에서, most-mark-first 방식을 따르게 되면 _to_be_discarded 혹은 _now_in_tier_1 mark가 많이 포함된 순서대로 선택하게 된다. 따라서, 1st victim은 page chunk 2, 2nd victim은 page chunk 3, 3rd victim은 page chunk 1이 된다.\n\n\n* Fig. 18은 전술한 victim page chunk를 선택하는 방식 중 하나인 most-mark-first 방식이 사용될 때, valid data에 대한 처리 방법으로서, upper-tier-backup에 대한 설명이다. 지금까지 Fig. 16, Fig. 17의 경우에서는 victim page chunk가 discard (erase) 될 때 해당 page chunk 내에 포함되어 있는 valid data에 대한 별도의 처리 없이 page chunk 통째로 discard시키는 방식이었으나, Fig. 18의 경우에서는 해당 page chunk 내에 포함된 valid data를 upper tier 인 tier-1 page cache로 migration한 후에 discard시키는 방식을 보여주고 있다.\n\n\n* Fig. 19(a)와 Fig. 19(b)는 log-structured manner로 page chunk가 write되는 방식을 설명하고 있다. 그리고 page chunk의 size를 결정함에 있어 channel 수와 erase block 크기를 감안하여 SSD 내부의 parallelism을 극대화 할 수 있도록 하는 방식에 대해서도 설명하고 있다. 그리고 연이은 page chunk write operation 시, 가급적 NAND write operation으로 인한 latency의 영향을 덜 받을 수 있게 하기 위해서 i 번째 page chunk가 way-2 열의 NAND block에 write되도록 했다면, i+1 번째 page chunk가 write될 때에는 같은 way 상에 있지 않는, 예를 들어 그림의 way-3 열의 NAND block에 write되도록 한다.\n\n\n* Fig. 20(a)와 Fig. 20(b)는 전술한 바와 같이 page chunk write을 위한 free space 확보를 위한 TRIM ahead 방식에 대한 설명이다. 그림에서 Nw 번째 way (wy_Nw)에 t(0) 시기의 page chunk가 write되는 경우, 곧 다가올 t(1) 시기의 page chunk가 0번째 way (wy_0)의 page chunk에 바로 write될 수 있도록 미리 해당 구역 (wy_0의 page chunk region)을 erase해둘 수 있도록 TRIM을 미리 보내놓는다.\n\n\n<br/>\n\n== # 선행 기술 ==\n\n다음 검색식\n (RAM OR DRAM OR hybrid) AND (NAND OR Flash OR SSD) AND (page cach*)\n으로 77건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 RAM-SSD hybrid page cache에 대한 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n== # 선행기술 조사 결과 및 검토 의견 ==\n\n의뢰발명은 Page cache layer 하에서 access pattern을 통해 rehit-potential을 분석하고, 이를 토대도 page chunk 단위로 RAM 또는 SSD로 tiering하는 것을 요지로 한다.\n\n이와 관련하여 선행문헌 1(US20120210058 A1)과 선행문헌 2(논문)은 page cache layer에서 동작하고, SSD, DISK, RAM을 포함하는 storage system에 관한 점에서 의뢰발명과 대응되고,\n특히 선행문헌 1(US20120210058 A1)은 미래에 반복적으로 액세스되는 데이터(repeatedly read random data, such that future random reads to this data will be served from the RR cache instead of the HDD storage.)를 pages의 단위로 SSD로부터 읽어 오는 점에서, 의뢰발명의 rehit-potential을 분석하여 page chunk의 단위로 tiering 하는 것과 유사하다고 판단된다.\n\n다만, 선행문헌 1은 의뢰발명과 같이 rehit-potential을 분석한 page tiering에 관하여 구체적인 실시가 기재되어 있지 않고, 의뢰발명에서 한정된 사항을 제외하고 ‘rehit-potential을 분석한 page tiering’에 대하여 검색한 결과 의뢰발명과 대응되는 문헌은 검색되지 않았다.\n\n따라서, 의뢰발명은 rehit-potential을 분석한 page tiering에 관해 구체적으로 특허청구범위에 작성하여 출원하는 경우, 선행문헌들로부터 신규성 및 진보성이 인정되어 등록 가능성이 높을 것으로 판단된다.\n\n----\n* RAM-SSD Hybrid Page Cache 발명의 경우에도 일반적인 block layer에서의 SSD cache에 대한 선행 기술은 당연히 존재하나, RAM-SSD를 hybrid로 사용하는 page cache에 대한 것은 발견되지 않은 것으로 보임.\n\n* 아래 의견에서 특별히 언급된 선행기술 1도 실제로 뜯어보면 page cache에 대한 내용과 상이함. (SSD에서 page 단위로 읽어온다는 점에서 page cache와 유사하다고 생각한 것으로 보여짐)\n\n* rehit potential 언급 역시 본 발명에서 이야기하는 것과 상이함. (선행기술 1에서 언급된 것은 일반적으로 cache 라는 것이 지향하는 바를 언급한 것으로서, random read 데이터를 SSD cache에 두겠으며, infrequently used page는 SSD cache로부터 sorted out 시키겠다는 내용이므로 이 역시 본 발명과 다름.)\n\n<br/>\n\n== # 침해 적발 ==\n\ndata의 특성을 미리 정의해둔 predefined data set에 의해 결정되는 workload에 대해서 각 data가 어느 시점(page cache 레이어 시점 혹은 블록 레이어 시점)에 어느 cache media로 전달되는지를 kernel tracer와 block tracer 등의 툴을 통해 관찰함으로써 침해 적발 가능.\n\n\n\n<br/>\n\n\n<!--\n== 청구항 ==\n\n1. RAM page cache, tiering buffer, page cache tiering manager, SSD page cache 등을 포함하는 RAM-SSD hybrid cache 아키텍쳐.\n\n  Tiering buffer는 RAM page cache와 SSD page cache 간에 tiering되는 page들을 보관하는 역할을 수행한다. page cache tiering manager는 RAM page cache로부터 SSD page cache로 tiering되기 위한 page를 선택하거나, 거꾸로 SSD page cache로부터 RAM page cache로 tiering되기 위한 page를 선택하는 역할을 수행한다.\n\n  Page cache 중 RAM page cache에 저장되는 page들의 조건은 다음과 같다. (1) 지정된 수준 H_T(N1) 이상으로 액세스가 매우 빈번하게 일어나는 page, 혹은 (2) 지정된 수준 H_T(N2) 이상, H_T(N1) 미만으로 액세스가 빈번하게 일어나는 page이면서 write-intensive 특성을 가지는 page. 여기서 H_T(N1), H_T(N2)의 값은 page cache로 사용할 수 있는 RAM 크기 및 SSD 크기를 감안하여 결정된다.\n\n  RAM에는 소정의 방법으로 분류되는 page cache의 일부 및 page lookup 자료구조 (예: table 혹은 radix tree 등), page class 분류 정보, access pattern을 표현하는 frequency 및 recency 등 hybrid page cache 관리에 필요한 메타 data가 저장되고; SSD에는 소정의 방법으로 분류되는 page cache의 일부가 저장되는; 구조.\n  \n  RAM에는 소정의 방법으로 분류된 page cache의 일부를 위한 전용 영역이 존재할 수 있음.\n\n  SSD에는 소정의 방법으로 분류되는 page cache의 일부 뿐만 아니라, hybrid page cache의 메타 data 백업이 저장될 수 있음. (fault-tolerance를 위한 recovery 혹은 fast cache warming-up을 위한 용도)\n\n  Page tiering 기반의 hybrid cache I/O 방법 ## Tiering Buffer 기반으로 RAM-SSD Hybrid Page Cache를 가져가는 것이 왜 필요한지 설명 (즉, 매 Page Access마다 SSD로 Page Writing (Tiering-down)이 일어난다면 얼만큼의 overhead가 존재할 수 있는지를 식으로 표현 가능? - 만약 이를 설명하는데 너무 많은 시간이 걸릴 것 같다면 직발서에서는 pass)\n\n\n\n2. Page Cache 중 unevictable이 아닌 file과 관련된 page들에 대해서, recency 및 frequency, 그리고 I/O read/write 패턴에 따라 sub-class들로 구분하여 관리하는 방법\n\n  available DRAM size가 모자랄 경우에도 우선적으로 버릴 class-3, reserved 영역에 두고 DRAM에서 cache serving을 하게 할 class-1, available DRAM size 와 SSD size를 계산/고려하여 DRAM에 두거나 SSD에 있게 될 class-2 (T1, T2)\n\n  디스크 상의 어떤 page가 액세스 되면 그 page는 우선 class-3으로 분류되고, 그 page에 대한 access 패턴이 모니터링되기 시작한다. class-3에 속한 page들 중 정해진 T3의 시간 동안 N3번만큼 액세스 되지 않은 page들은 class-3인 채로 유지되며, 이 class의 page들은 시스템의 상황에 따라 가장 우선적으로 page cache에서 evict될 대상임. (즉 system의 idle RAM 용량이 부족하게 되는 경우에 우선적으로 discard되는 page들이며, system shutdown 시에도 별도의 저장 operation 없이 그냥 discard된다.\n\n  class-3의 page들 중, 정해진 T3의 시간 동안 N3번만큼 액세스 된 page들은 class-2로 변경된다. 이 class-2의 page들은 system shutdown 시에 이후에 cache로서 다시 사용될 수 있게 하기 위해서 SSD page cache에 저장될 수 있으며, 설정에 의해 동작 여부가 결정된다.\n\n  class-2의 page들 중, 정해진 T2의 시간 동안...\n\n-->\n\n== # Notation ==\n\nH_T(N)\n: 지정된 T만큼의 시간 동안 N 만큼의 hit이 되었다는 것을 H_T(N)으로 표현한다. H_T(<N)은 지정된 T만큼의 시간 동안 N보다 적은 갯수 만큼 hit 되었다는 것을 의미한다.\n\n\n== Memo / Questions ==\n\n\n\n=== Comments ===\n\n* page tiering I/O operation: a time to invoke actual write operation to SSD\n*: 실제로 SSD에 page chunk 단위의 page tiering I/O를 발생시키는 시점을 언제로 하면 좋을까? 현재 발명 명세서에는 page가 class-2로 마킹되고 나서, 바로 memory에서 evict되는 것이 아니라, 존재할 수 있는 약간의 scheduling term을 거친 후에, 해당 시기가 도래하면 그 때 tier-2 page cache로 write되게 되고 tier-1 page cache에서는 아직 지워지지는 않는다. class-2 page가 tier-2 page cache로 tiering되었다 하더라도 여전히 radix tree에서 관리가 되어야 하는 이유는, access pattern에 따라서 class-3이나 class-1으로 class 변경이 있을 수 있기 때문이다. (참고로, RAM (tier-1 page cache)에 있는 page node가 class-1, 2, 3 무엇으로 mark 되었건 간에, 실제로 evict되지 않는 이상 radix tree에 해당 data는 유지시킨다. 해당 page가 evict되면, radix tree에서 관리되는 page node도 그때 같이 삭제한다.)\n\n\n*; actual page eviction time에 triggering되는 page tiering I/O 방식과 비교\n\n\n*; trigger 방식 1\n*: hybrid page cache 자체 scheduling time에 의해 tiering I/O operation이 trigger되는 방식으로서, class-2으로 mark되는 순간 page tiering pool에 등록되지만, class-2가 되자마자 바로 tiering I/O가 trigger 되는 것은 아님. hybrid page cache의 자체 scheduling time에 의해 tiering I/O operation이 trigger되기 때문에 class-2로 marking되는 시점과 실제 tiering 되는 시점에 차이가 있을 수 있음. 혹시 tiering되기 전에 page tiering pool에 담겨있던 class-2의 page들이 class-1이나 class-3으로 변경되게 되면, 그리고 나서 tiering이 trigger된다면, class-1이나 class-3이 된 page들이 tier-2 page cache에 write되는 것이 아닐까 하는 우려가 있을 수 있음. 그러나 그런 문제는 발생하지 않음. 실제 tier-2 page cache로 tiering되기 전에 다시 class-1이나 class-3으로 class ID가 변경되는 순간, page class update function에 의해 page tiering pool에서 관리하는 list (chain)로부터의 link가 삭제 되기 때문에 class-1이나 class-3의 page들이 tiering되지는 않는다.\n\n*; trigger 방식 2\n*: \n\n\n* cache warm-up time을 줄이기 위해서 cache snapshot을 뜰 수 있게 하는 방법 (block layer에서 뜰 필요는 없는 것인가? 그냥 page cache layer에서 뜨면 되는가?)\n\n\n\n\n==== Question:: Memory 용량 확장만으로 안되고 SSD를 써야 하는 이유 ====\n\n*; fast cache warming-up\n*: RAM cache only 방식으로는 fast-warm-up 이 어렵다. cache booting이 필요한 경우가 ____ 만큼 자주 발생한다. 이때 속도가 빠른 Non-volatile storage (예를 들어 SSD, 아니면 HDD에라도... sequential read의 throughput은 괜찮으므로)에 cache data (snapshot)가 저장되어 있으면 훨씬 빨리 cache warming-up이 가능하지만, 그렇지 않다면 real workload들이 계속 real 시간으로 play되는 것을 받아가면서 cache에 data를 쌓아가야한다. 이는 마치 5분짜리 노래가 있을 때, 노래를 저장해둔 file이 있다면 저장매체가 허락하는 속도로 빠르게 memory에 올릴 수 있지만, 노래를 저장해둔 file이 없다면 그 노래가 play되는 5분동안을 계속 견디면서 memory에 올려야만 한다. 즉, L 시간 동안의 workload를 받아내었던 cache snapshot이 있다면, 그 시간만큼의 time warp가 가능하다는 것이다.\n<!--\n\n-->\n*; To big data to fit in RAM\n*: Big data 시대가 되면서 cache되어야 하는 data 양이 많아짐. (RAM 만으로는 large-scale data를 fit시키기 어려움). data의 양이 지금처럼 크지 않고 (Volume, in 3V of Big Data), data가 증가하는 속도도 (Velocity, in 3V of Big Data) 지금보다 빠르지 않았던 과거에는 RAM 만으로도 data를 감당해낼 수 있었으나, data의 양이 매우 커지고, 증가하는 속도도 빠른 Big Data 시대에는 상대적으로 저비용으로 더 많은 용량의 cache capacity를 제공할 수 있게 하는 SSD cache의 필요성이 증대되고 있음.\n<!--\n\n-->\n*; Power consumption\n*: RAM+SSD 조합에 비해 RAM only 접근으로 해당 용량을 커버하도록 간다고 했을 때, power consumption의 양이 매우 클 수 있다. (실제로 계산해보면 얼마나 될까?)\n<!--\n\n-->\n*; architecture 적인 한계\n*: HW 변경 시 소요되는 비용, 뿐만 아니라 HW 변경으로 인한 SW 변경 등, 비용적인 문제로도 연결될 수 있음\n*: RAM을 많이 꽂을 수 있는 전용 hardware (board)를 제작하지 않는 이상, 한 node에 장착할 수 있는 max 용량의 RAM은 제한되어 있다. (실제로 현재 intel commodity-PC architecture 기반의 node의 경우, _____ GB 용량이 HW 수정 없이 max로 장착할 수 있는 물리적인 한계임)\n*: 2013-01-26 현재 Intel 기반 server node 구성 시 가능한 Max RAM 용량 [[server node spec]]\n<!--\n\n-->\n\n==== Question:: page cache data들이 RAM page cache 내에서 지워지는 시점? ====\n\n*: page cache 입장에서는 RAM page cache에 들어있는 cached data를 지우는 시점을 많이 늦출 수록 좋은 것 아닐까?\n*: 정확히 어떤 경우에, 어느정도 주기로 shrink (page 들이 언제 RAM에서 실제로 지워지는 것) 되는지 확인 필요\n\n\n\n<br/>\n\n=== 기술 분류 ===\n\n <nowiki>\n TA01 DMC > System Architecture \n TA02 DMC > System SW \n TA24 DMC > Terminal \n TA25 DMC > PC \n TC06 반도체 > Storage \n TA0102 DMC > System Architecture > Software System Design \n TA0103 DMC > System Architecture > Hardware System Design \n TA0201 DMC > System SW > Embedded OS \n TA0202 DMC > System SW > General Purpose OS \n TA0203 DMC > System SW > Device Driver \n</nowiki>\n\n\n=== 제품 분류 ===\n\n <nowiki>\n\n PA0502 IT_컴퓨터 > PC \n PA050202 IT_컴퓨터 > PC > Desktop Personal Computer \n PA050203 IT_컴퓨터 > PC > ALL IN ONE PC \n PA050204 IT_컴퓨터 > PC > Note Personal Computer \n PA050299 IT_컴퓨터 > PC > OTHERS \n PA0802 무선 > HHP \n PC0208 Memory (반도체)  > FLASH \n PC020804 Memory (반도체)  > FLASH > SSD \n PC05 Storage (반도체) \n\n</nowiki>\n\n=== Questions ===\n\n# page cache layer가 아닌 block layer에서 caching을 한다고 했을 때에는 어떤 algorithm을 사용해야 할까? 이때도 hybrid page cache에서 사용된 algorithm이 적용될 수 있을까?\n# 분산 환경을 고려한다고 했을 때, disk cache는 어떤 모습이 되어야 할까? 크게 아키텍쳐를 나눈다면 몇 가지로 구분될 수 있을까? 아키텍쳐를 구분하는 기준에는 어떤 것들이 올 수 있는 것일까?\n## memcached, memcachedb, hbase cache, directCache, mySQL cache?, fscache, dm-cache, NetApp flash cache, VeloBit HyperCache, Intel IRST (in ISRT), EMC VFCache, CORFU, FAWN cache 같은 것을 어떻게 구분하고 특징지을 수 있을까?\n# 내가 집중적으로 비교 분석해야 하는 주요 cache 기술에는 어떤 것이 있을까?\n# server-side cache 기술이 client-side cache 기술과 비교해서 근본적으로 달라져야 하는 부분에는 어떤 것들이 있을까?\n# cache 기술 계의 기술 트리는 어떤 모습이 될 수 있을까?\n# SSD 혹은 SSD array (in DAS), distributed SSD 를 large-scale cache로 사용한다고 했을 때 어떤 모습이 되어야 할까? 그리고 이를 위해서 고려되어야 하는 알고리즘/기술들은? 혹시 distributed hash table? consistent hashing? bloom filter? multi-node cache coherency? cache 입장에서의 name node 역할이 필요할까? 혹은 기타 well-known algorithm 들이 여기에 접목되어 어려운 문제를 풀어내는 데 도움이 될 수는 없을까? 매우 효율적이고 멋진 well-known problem (with solution)으로 매핑될 수 있는, 굉장히 현실적인 문제가 있다면 제일 좋을 것 같다.\n# CLRS 혹은 집에 있는 알고리즘 책들을 훓어보면서 알고리즘 목록들을 정리해보는 것도 좋겠음.\n# 주기성 ([[periodicity]]) 발견을 위해서 어떤 방식을 이용할 수 있을까? [[periodicity transform]] 같은 것을 참고할 수 있을까? real-time으로 이러한 transform을 하기에는 computation의 부담이 있을 수 있겠지만, 이것이 간단한 count, plus, minus, multiply 등의 연산으로 구현될 수 있다면 real-time으로 확인하기에도 좋을 것 같다. -> 그래서 내가 발명했다. 참고하기 바람. ㅎㅎㅎ\n# application에서 512-byte 만큼만 (혹은 1-byte만) 읽어온다고 했을 때에도 page cache에서는 4096-byte 단위로 find/get 하게 될까? (당연히 그럴 것 같지만... 확인 필요), 그리고 실제 block layer에 request가 전달될 때에는 어떻게 될까? 1-byte를 요청했어도 1 sector (512-byte)만큼 읽게 될까? 다시 정리하자면, application 수준에서 1-byte를 요청한 경우, page cache layer에서는 4096-byte를 읽어오게 되고, block layer에서는 512-byte를 읽어오게 되는 것일까?\n#* [http://sethares.engr.wisc.edu/downper.html What is a periodicity transform?]\n\n<br/>\n\n\n\n=== Memo ===\n\n* TPC-C full (48-hour) tracelog (tracelog_full.iowa.20121126_102808.log)\n blusjune@radiohead:[preproc] $ wc -l .tracelog.1.preproc.out\n 40534797 .tracelog.1.preproc.out\n\n blusjune@radiohead:[preproc] $ wc -l .tracelog.A.addr\n 40534797 .tracelog.A.addr\n\n blusjune@radiohead:[preproc] $ head -10 .tracelog.1.preproc.out\n _R __D ___8,17   8,17   1        3     0.000006564  4009  D   R 512 (85 08 2e 00 00 00 01 00 00 00 00 00 00 00 ec 00 ..) [udisks-helper-a]\n _R __D ___8,17   8,17   1       16     0.002812008  4009  D   R 512 (85 08 2e 00 d1 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [udisks-helper-a]\n _R __D ___8,17   8,17   1       19     0.002815361  4009  D   R 512 (85 08 2e 00 d1 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [udisks-helper-a]\n _R __D ___8,17   8,17   1       22     0.003145956     0  D   R 512 (85 08 2e 00 d1 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [kworker/0:0]\n _R __D ___8,17   8,17   1       26     0.018526921  4009  D   R 512 (85 08 2e 00 d0 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [udisks-helper-a]\n _R __D ___8,17   8,17   1       29     0.018531043  4009  D   R 512 (85 08 2e 00 d0 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [udisks-helper-a]\n _R __D ___8,17   8,17   1       32     0.019166601  3540  D   R 512 (85 08 2e 00 d0 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [kworker/1:1]\n _R __D ___8,17   8,17   1       35     0.023215607  3973  D   R 512 (85 08 2e 00 d0 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [kworker/1:2]\n _R __D ___8,17   8,17   1       38     0.027244917  3540  D   R 512 (85 08 2e 00 d0 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [kworker/1:1]\n _R __D ___8,17   8,17   1       41     0.029463260     0  D   R 512 (85 08 2e 00 d0 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [kworker/0:0]\n\n blusjune@radiohead:[preproc] $ tail -10 .tracelog.1.preproc.out\n _R __D ___8,17   8,17   3 15866364 210599.766563758 13089  D   R 512 (85 08 2e 00 00 00 01 00 00 00 00 00 00 00 ec 00 ..) [udisks-helper-a]\n _R __D ___8,17   8,17   1 513585046 210599.767409340 13089  D   R 512 (85 08 2e 00 d1 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [udisks-helper-a]\n _R __D ___8,17   8,17   0 24462895 210599.779210418 13089  D   R 512 (85 08 2e 00 d0 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [udisks-helper-a]\n _R __D ___8,17   8,17   0 24462898 210599.779214060 13089  D   R 512 (85 08 2e 00 d0 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [udisks-helper-a]\n _R __D ___8,17   8,17   1 513585048 210599.779644700     0  D   R 512 (85 08 2e 00 d0 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [kworker/0:0]\n _R __D ___8,17   8,17   1 513585061 212399.808134458 13151  D   R 512 (85 08 2e 00 00 00 01 00 00 00 00 00 00 00 ec 00 ..) [udisks-helper-a]\n _R __D ___8,17   8,17   1 513585068 212399.808957243 13151  D   R 512 (85 08 2e 00 d1 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [udisks-helper-a]\n _R __D ___8,17   8,17   1 513585072 212399.825431257 13151  D   R 512 (85 08 2e 00 d0 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [udisks-helper-a]\n _R __D ___8,17   8,17   1 513585075 212399.825435280 13151  D   R 512 (85 08 2e 00 d0 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [udisks-helper-a]\n _R __D ___8,17   8,17   1 513585078 212399.825925337     0  D   R 512 (85 08 2e 00 d0 00 01 00 00 00 4f 00 c2 00 b0 00 ..) [kworker/0:0]\n\n\n* TPC-C 7.7-sec tracelog (1/330 of full trace)\n blusjune@rolling-stones:[t] $ wc -l log.1000k_lines\n 4548 log.1000k_lines\n\n\n<br/>','utf-8'),(14,'== Low-computation-overhead and Memory-efficient Periodicity Detection Method ==\n\n\n== # 발명의 이용분야 ==\n\n* Server-side cache\n* Client-side cache\n* Storage cache\n* Storage tiering\n\n\n* SSD and SSD array\n* SSD-HDD hybrid storage systems\n* Enterprise Server\n* Client PC/Mobile\n\n\n<br/>\n\n\n== # 배경 / 기존 기술의 문제점 ==\n\n\n* page cache 혹은 block cache 등에서 cache에 남겨둘 data와 evict시킬 data를 구분하기 위한 목적으로 LRU (Least Recently Used) 알고리즘, LFU (Least Frequently Used) 혹은 이의 변종이 폭넓게 사용되고 있음. 그러나 이는 recency를 잘 잡아낼 수는 있어도 다른 property는 잡아내기 어렵다는 문제점이 있음. 이를 해결하기 위해 LRU 혹은 LFU의 변종 알고리즘들이 제안되어 왔으나 그 알고리즘들의 computation overhead 및 memory space 요구량으로 인해 실제 환경에서는 잘 받아들여지기 힘든 경우가 많음.\n\n\n* cache media가 hybrid, hetero 화 되면서 기존의 RAM 기반 cache 외에 SSD 등도 cache로 사용되는 경우가 증가하고 있음. 이에 따라 RAM cache에서 주로 사용되던 recency 기반 알고리즘 외에, 추가적으로 의미있는 property detection을 가능하게 하면서도 low computation overhead, memory space efficient한 알고리즘의 필요성이 증대되고 있음.\n\n<br/>\n\n== # 본 발명의 특징 / 효과 ==\n\n* 본 발명은 low-computation-overhead 및 memory-space-efficient 한 특징을 가지고 있는 simple count up/down 방식의 periodicity detection 방법을 다루고 있음.\n\n\n* periodicity 특성은, cache data pool 중에서 recency로 걸러내는 currently-hot data 외에 re-hit potential을 가지고 있는 data를 식별할 수 있는 방법으로서 중요한 의미를 가질 수 있음.\n\n\n* 그 한 예로서, RAM-SSD hybrid cache 혹은 tiered cache 같이, 고속이지만 용량이 상대적으로 작은 tier-1 cache와 상대적으로 저속이지만 용량이 큰 tier-2 cache로 구성되는 hierarchical cache 구조에서 tier-2 cache에 placement될 cache data를 결정할 때 사용될 수 있음. 즉, 현재 자주 access되고 있는 data (type-1 data)는 tier-1 cache에 placement하고, recency는 약하지만 주기적으로 access되는 data (type-2 data)는 그냥 evict하여 버릴 것이 아니라, tier-2 cache에 placement시킴으로써, 해당 type-2 data들이 HDD로부터 바로 읽혀질때 유발되는 I/O 성능 저하를 방지할 수 있게 함.\n\n\n* 이때 중요한 것은 low-computation-overhead 와 memory-space-efficiency임. 만약 이러한 periodicity detection 방법이 computation-intensive하거나 memory-space를 많이 요구한다면, 많은 데이터에 대해서 periodicity에 대한 빠른 판단을 해야 하는 상황에서는 사용하기 어렵다는 문제가 있음. \n\n\n* 만약 periodicity detection을 위해서 Fourier Transform 같은 방식을 쓴다면 아무리 FFT (Fast Fourier Transform) 같은 방법을 쓴다 하더라도 본 발명에서 제시하는 방법과 같은 정도의 low computation overhead 및 memory efficiency를 제공하기는 쉽지 않음.\n\n\n* 그러나 본 발명은 간단한 integer count-up/down 기반의 연산만으로 구성되기 때문에 computation overhead가 매우 작으며, 또한 counter 값을 유지하는 변수 정도만 유지하면 되기 때문에 memory space 측면에서도 매우 효율적이라고 볼 수 있음.\n\n\n<br/>\n\n== # 대표 청구항 ==\n\n* 적은 연산 비용 및 메모리 효율적으로, 주기성을 갖는 hit pattern을 찾아내기 위한, periodicity computing sieve 기반의 periodicity detection 방법\n\n\n<br/>\n\n\n== # 대표 도면 ==\n\n\n\n\n== # 도면 목록 ==\n\n* [Fig. 1] Tracelog: access counts per addr\n* [Fig. 2] Recency Vs. Periodicity\n* [Fig. 3] Periodicity Computing Sieve (PCS)\n* [Fig. 4] PCS-based periodicity detection\n* [Fig. 5] Periodicity scoring (normalized)\n* [Fig. 6] Periodicity detection result\n* [Fig. 7] Patterns of low periodicity\n* [Fig. 8] Patterns of high periodicity\n* [Fig. 9] Periodicity detection flow\n\n\n<br/>\n\n== # 기술 상세 ==\n\n\n* Fig. 1은 예시로 보이는 10개의 address 각각에 대해 각 단위 시간 (1부터 12까지의) 동안 몇 번의 access가 있었는지를 count한 hit pattern table이다. hit ptrn 1은 address 1에 대한 것이며, 왼쪽으로 갈 수록 현재 (t(0)으로 표기)를 기준으로 오래된 (t(-)) 패턴을 나타낸다.\n\n\n* Fig. 2는 기존 well-known RAM cache 알고리즘인 LRU (Least Recently Used) 알고리즘의 동작을 보여주고 있다. 그림의 10개의 hit pattern에 대해서 LRU를 적용하면 Fig. 2의 우측에 나타난 바와 같이 t(0) 시점에는 addr (hit ptrn) 1, 3, 5, 6, 7, 8이 hot하지 않다고 판단하게 되며 언제든지 evict될 수 있는 item으로 분류된다. 반대로 addr 2, 4, 9 ,10 등은 최근 hot한 access를 받고 있는 것으로 간주된다 (currentlly-hot). LRU 알고리즘이 t(-1) 시점에 적용되었다면 그림에서 처럼 addr 2, 3, 5, 6, 9가 hot하지 않은 것으로 분류되며, addr 1, 4, 7, 8, 10 등이 currently-hot한 것으로 역선택된다.\n\n\n* 이 LRU 알고리즘 적용을 통해 현재 (recently) hot한 data들은 - 즉 recency property - 쉽게 잡아낼 수 있다. 여기서 \'쉽게\'라 함은 복잡한 computation 필요 없이 간단한 integer counting 정도로 해당 property를 detection할 수 있음을 의미한다. 그러나 recency 외의 다른 측면들은 잡아내지 못하는 한계가 있다. 만약 LRU로 관측하는 window를 늘리게 되면, 정말 hot한 data들과 그렇지 않은 data들이 구분되지 않고 같이 섞이게 되어, hotness에 따라 별도의 처리가 가능하도록 분리하기 어렵게 되는 결과를 초래할 수 있다.\n\n\n* SSD와 RAM을 연계하여 cache로 구성하는 경우, 이미 page cache가 존재하는 시스템 hierarchy를 고려하고, 각 media의 특성을 감안했을 때, RAM에 두기에 적절한 data와 SSD에 두기에 적절한 data를 구별하는 것이 필요하다. 기존의 대표적인 RAM 기반 page cache의 경우처럼, RAM에는 recency가 있는, 즉 \"currently-hot\"한 data를 caching하고, SSD 기반의 cache에는 당장 currently-hot하지는 않더라도 \"rehit-potential\"이 있는 data들을 caching함으로써, RAM page cache와 상호보완적으로 동작할 수 있게 된다.\n\n\n* periodicity가 있는 data는 rehit-potential을 가진 data들의 좋은 예로 들 수 있다. 즉 kernel에서 LRU 알고리즘 기반으로 data들을 구분할 때 지정된 시간 동안 (일반적으로 매우 짧은 시간임) access가 없는 data들은 등급이 내려가게 되고 이러한 data들은 page cache shrink 시 우선적인 eviction 대상이 된다. Fig. 2에서 보는 것처럼 t(0) 시점에서 currently-hot한 것으로 걸러진 addr 2, 4, 9, 10를 제외한 addr 1, 3, 5, 6, 7, 8은 RAM page cache에서 evict될 가능성이 높아진다. RAM의 크기가 제한적이기 때문에 RAM page cache에서는 불가피한 선택이라고 볼 수 있다.\n\n\n* 한 편 addr 1, 2, 4, 7, 8, 9는 주기성이 있는 것으로 보이는데, 이들이 가지고 있는 최소 주기를 따져보았을 때, 각각 2w, 2w, 4w, 3w, 4w, 4w 정도의 값을 가지는 것처럼 보인다. 만약 periodicity 특성을 고려하지 않고 recency 특성만 가지고 evict될 item과 RAM page cache에 남겨질 item을 구분했다면, addr 1, 7, 8은 periodicity를 가지고 있었음에도 불구하고 아깝게 RAM page cache에서 evict되어 이후 다시 해당 address에 대한 access request가 발생했을 때 disk로부터 읽어들이게 되고 그만큼 I/O 성능 하락을 겪게 된다. 하지만, periodicity를 감안하여 addr 1, 7, 8을 SSD cache에 두었다면 해당 data를 disk로부터 읽지 않고 바로 SSD로부터 읽을 수 있게 되므로 그만큼의 성능 이득을 얻게 된다. latency 측면에서 예를 들어보면, read latency가 8,500 micro second인 7200rpm SATA HDD와 read latency가 25 micro second인 NAND-flash based SSD (SLC)가 있다고 했을 때, 단순히 계산을 해봐도, periodicity를 고려한 SSD로의 caching을 통해 한 번의 단위 I/O 발생 시, 약 340배의 latency 이득을 얻게 된다고 볼 수 있다.\n\n\n* 지금까지 recency 뿐만 아니라 periodicity 특성을 고려하여 RAM과 SSD에 caching하는 방식의 이점에 대한 설명이었다면, 이후로는 low-computation-overhead, memory-space-efficient 방식으로 periodicity detection을 가능하게 하는 방법에 대해 설명한다.\n\n\n* Fig. 3은 periodicity를 detection하는 방식에 대해 나타내고 있다. 가운데의 table은 Fig. 1에서부터 나왔던 hit pattern table이며, table 왼쪽에는 periodicity detection을 위한 요소 activity들인 hit count monitoring, periodicity scoring, periodicity decision-making이 아이콘으로 표현되어 있다. T1는 periodicity value를 update하는 interval을 나타내고 T2는 해당 address에 대한 hit pattern이 periodicity를 가지고 있는지에 대한 여부를 결정하는 interval을 나타낸다. Fig. 3으로 예를 들면, T1 값이 각각 1w, 2w, 3w, 4w인 periodicity computing sieve (PCS)들이 있고, T2 값이 12w이므로, 이 경우는 매 12w 만큼의 시간마다 각 address에 대해서 1w, 2w, 3w, 4w의 주기성이 있는지에 대한 판단을 하겠다는 것을 의미한다. 이 4개의 periodicity computing sieve들은 각각 PCS(T1=1w, T2=12w), PCS(T1=2w, T2=12w), PCS(T1=3w, T2=12w), PCS(T1=4w, T2=12w) 으로 표현할 수 있다.\n\n\n* Fig. 4는 T1=2w 이고 T2=12w인 경우, addr 2, 5, 6에 대한 periodicity detection 예시이다. hit count monitoring에 의해, 2w만큼의 기간 동안 hit이 한 번이라도 있었다면 periodicity value를 +1 만큼 increase시키고, hit이 전혀 없었다면 periodicity value를 -1 만큼 decrease 시킨다. 물론 periodicity detection 과정이 시작되기 전에 periodicity value의 값은 같은 값 (여기서는 zero)으로 initialize시킨다. 2w 만큼의 시간마다 periodicity value에 대해 increase/decrease를 하게 되므로 12w의 시간 동안 총 6번의 periodicity scoring이 발생하며, 12w 만큼의 시간이 지나면 periodicity 여부에 대한 decision-making을 하게 된다. 이때, T2=12w이고 T1=2w인 경우 periodicity value가 가질 수 있는 max 값은 +6이며 min 값은 -6으로 정해진다. 보기 쉽도록 max 값이 +1이 되도록 normalize를 해보면 (실제 상황에서는 굳이 normalize를 할 필요는 없음), hit ptrn 2를 가지는 addr 2는 periodicity value가 +1로서 판단 기준인 0.5 초과를 만족하므로 2w 길이의 주기성을 갖는다고 (2w-periodic) 판단할 수 있다. hit ptrn 5, 6을 갖는 addr 5, 6은 각각 periodicity value가 -0.33, +0.33을 갖게 되어 기준인 0.5 초과를 만족시키지 못하였으므로 2w-periodic 하지 않다고 판단한다. 이 periodicity value는 max값 (normalize한 경우는 +1)에 가까울 수록 T1 길이의 주기성이 크다고 볼 수 있으며, min값 (normalize한 경우는 -1)에 가까울 수록 T1 길이의 주기성이 작다고 볼 수 있다. 여기서 판단 기준으로 삼았던 periodicity value 0.5 초과는 임의로 정한 것이며, 상황과 필요에 따라 일관성 있게 지정할 수 있다.\n\n\n* 이러한 방식으로 T1 값이 각각 1w, 2w, 3w, 4w인 4개의 periodicity computing sieve들을 가지고 T2=12w인 경우, hit ptrn 1부터 10까지에 대해서 periodicity detection을 해보면 Fig. 5와 같은 periodicity value 계산 결과가 나온다. 우측 두 table의 파란 음영으로 표시된 부분은 방금 전에 Fig. 4에서 예시로 계산해보았던 값을 표시한 것이다.\n\n\n* Fig. 6은 Fig. 5에서 계산했던 normalized periodicity value 값을 graphical하게 표현한 것이다. hit ptrn 3, 5, 10은 주기성이 거의 없는 것으로 판단할 수 있다 (Fig. 7 참고). hit ptrn 1, 2는 2w, 3w, 4w 길이의 주기성이 크게 있고, hit ptrn 4, 8, 9는 4w 길이의 주기성이 크게 있고, hit ptrn 7은 3w, 4w 길이의 주기성이 크게 있다고 판단할 수 있다 (Fig. 8 참고).\n\n\n* Fig. 9는 지금까지 설명되었던, 본 발명에서 제안하는 PCS (periodicity computing sieve) 기반의 periodicity detection 방법에 대한 flowchart이다. 이 PCS 방식이 low-computation-overhead 이며 memory-space-efficient할 수 있는 이유는 다음과 같다. kernel에서는 page cache 관리를 위해 지정된 단위 시간 동안 해당 page에 대해 몇 번의 access가 있었는지에 대한 simple한 count 정보를 관리하고 있다. 각 page 별로 이 정보를 유지하기 위해 많은 memory 공간이 필요하지 않으며, 연산 또한 간단한 integer 산술연산만으로 이루어지므로 computation overhead가 매우 작다. 또한 access pattern monitoring 시간이 증가하거나 observation event가 누적됨에 따라 memory 공간이 지속적으로 늘어나야만 하는 구조가 아니다. 즉, 측정하고자 하는 periodicity의 갯수 만큼만의 정수 counter variable만 관리되면 된다. 예를 들어, 관심 있는 periodicity가 한 가지만 있다면, 해당 page에 대해서는 단 1개의 integer counter variable만 관리되면 된다. 이미 kernel의 page cache에서는 page에 대한 관리를 위해서 각 page마다 할당된 구조체들이 있으므로, 이 integer counter variable이 하나 추가되는 것은 memory space 측면에서 부담이 되지 않는다. \n\n<br/>\n\n== # 선행기술 조사 결과 및 검토 의견 ==\n\n다음 검색식\n ()\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 Low-computation-overhead and Memory-efficient Periodicity Detection Method 관련 선행 기술은 발견하지 못하였음.\n\n<br/>\n\n== # 침해 적발 ==\n\n<br/>\n\n\n== # Memo / Questions ==\n\n* page monitoring module이 T2 만큼의 시간동안 periodicity를 관찰한다는 것은 그만큼의 시간 동안 page들을 버리지 않고 meta info를 update해왔다는 의미인가? 그렇다면 그만큼 RAM에 해당 data를 keep하고 있어야 하므로 memory가 많이 필요하지는 않을까?\n\n\n<br/>','utf-8'),(15,'=== # 요약 ===\n\n* 본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n\n* \"MapReduce의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크\" 로서,\n: 시스템 리소스 사용량, Hadoop 상태 정보, I/O 패턴, Straggler 정보 등의\n: 다계층 정보 모니터링 및 학습을 통해 주요 병목 요인을 자동 판별케 함\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 각 노드의 시스템 리소스 사용량, Hadoop 상태 정보, I/O 패턴, Straggler 정보 등의 모니터링 및 학습을 통해,\n: Straggler 태스크 탐지 시 해당 태스크의 실시간 모니터링 정보를 기존 학습 결과와 연계 분석하여,\n: 성능 저하와 깊은 연관을 보이는 요소를 판단케 하는 방법 및 시스템 구조.\n\n\n* Question\n: 연계 분석 시, 실시간 분석이 가능한지? 어느 정도의 데이터에 대해 시간이 얼마나 걸렸는지?\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 특허 검색 결과\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n\n* 논문 검색 결과\n:* MROrchestrator: A Fine-grained Resource Orchestration Framework for Hadoop MapReduce (IEEE ICCC 2012)\n::- [http://www.cse.psu.edu/research/publications/tech-reports/2012/CSE%20-12-001.pdf Paper @ psu.edu]\n::- [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6253482 Paper @ IEEE Xplore]\n \n:* Predicting Execution Bottlenecks in Map-Reduce Clusters (HotCloud 2012)\n::- [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Paper home @ USENIX HotCloud 2012]\n\n:* Theius: A Streaming Visualization Suite for Hadoop Clusters\n::- [https://wiki.engr.illinois.edu/download/attachments/195766887/JAR-3rd.pdf?version=1&modificationDate=1336558393000 Paper @ illinois.edu]\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>','utf-8'),(16,'== 20130329_111903 ==\n\n=== R script file execution from command line ===\n\n$ cat > ex1.R\nlibrary(\"arules\")\ndata(\"Epub\")\nEpub\nsummary(Epub)\nquit(save=\"no\")\n\n$ R < ex1.R\n <pre>\nJob <13833365> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura009>>\n\nR version 2.15.2 (2012-10-26) -- \"Trick or Treat\"\nCopyright (C) 2012 The R Foundation for Statistical Computing\nISBN 3-900051-07-0\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType \'license()\' or \'licence()\' for distribution details.\n\nR is a collaborative project with many contributors.\nType \'contributors()\' for more information and\n\'citation()\' on how to cite R or R packages in publications.\n\nType \'demo()\' for some demos, \'help()\' for on-line help, or\n\'help.start()\' for an HTML browser interface to help.\nType \'q()\' to quit R.\n\n[Previously saved workspace restored]\n\n> Loading required package: Matrix\nLoading required package: lattice\n\nAttaching package: \'arules\'\n\nThe following object(s) are masked from \'package:base\':\n\n    %in%, write\n\n> > transactions in sparse format with\n 15729 transactions (rows) and\n 936 items (columns)\n> transactions as itemMatrix in sparse format with\n 15729 rows (elements/itemsets/transactions) and\n 936 columns (items) and a density of 0.001758755 \n\nmost frequent items:\ndoc_11d doc_813 doc_4c6 doc_955 doc_698 (Other) \n    356     329     288     282     245   24393 \n\nelement (itemset/transaction) length distribution:\nsizes\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n11615  2189   854   409   198   121    93    50    42    34    26    12    10 \n   14    15    16    17    18    19    20    21    22    23    24    25    26 \n   10     6     8     6     5     8     2     2     3     2     3     4     5 \n   27    28    30    34    36    38    41    43    52    58 \n    1     1     1     2     1     2     1     1     1     1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.646   2.000  58.000 \n\nincludes extended item information - examples:\n   labels\n1 doc_11d\n2 doc_13d\n3 doc_14c\n\nincludes extended transaction information - examples:\n      transactionID           TimeStamp\n10792  session_4795 2003-01-02 10:59:00\n10793  session_4797 2003-01-02 21:46:01\n10794  session_479a 2003-01-03 00:50:38\n> \n</pre>\n\n\n=== paste() example ===\n\n* example 1\n <pre>\n> 1:6\n[1] 1 2 3 4 5 6\n> paste(1:6)\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n> paste(\"A\", 1:6)\n[1] \"A 1\" \"A 2\" \"A 3\" \"A 4\" \"A 5\" \"A 6\"\n> paste(\"A\", 1:6, sep=\"\")\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\" \"A6\"\n> 2:7\n[1] 2 3 4 5 6 7\n> seq(8,3,by=-1)\n[1] 8 7 6 5 4 3\n> paste(\"A\", 1:6, 2:7, seq(8,3,by=-1), sep=\"\")\n[1] \"A128\" \"A237\" \"A346\" \"A455\" \"A564\" \"A673\"\n> paste(\"A\", 1:6, 2:7, seq(8,3,by=-1), sep=\"-\")\n[1] \"A-1-2-8\" \"A-2-3-7\" \"A-3-4-6\" \"A-4-5-5\" \"A-5-6-4\" \"A-6-7-3\"\n> paste(\"A\", 1:6, 2:7, seq(8,3,by=-1), sep=\"/\")\n[1] \"A/1/2/8\" \"A/2/3/7\" \"A/3/4/6\" \"A/4/5/5\" \"A/5/6/4\" \"A/6/7/3\"\n</pre>\n\n* example 2\n <pre>\n> stopifnot(identical(str1 <- paste(\"A\", 1:6, sep=\"\"), str2 <- paste0(\"A\", 1:6)))\n> str1\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\" \"A6\"\n> str2\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\" \"A6\"\n\n> paste(\"Today is\", date())\n[1] \"Today is Fri Mar 29 11:22:15 2013\"\n</pre>\n\n\n\n\n== 20130327_183541 ==\n\n=== list() examples ===\n\n <pre>\n> myl = list(apple=1, banana=2, cherry=3, durian=4, elderberry=5)\n> myl\n$apple\n[1] 1\n\n$banana\n[1] 2\n\n$cherry\n[1] 3\n\n$durian\n[1] 4\n\n$elderberry\n[1] 5\n\n> myl$apple\n[1] 1\n> myl$banana\n[1] 2\n> myl$cherry\n[1] 3\n> myl$durian\n[1] 4\n> myl$elderberry\n[1] 5\n> \n</pre>\n\n\n=== read-from/save-to a file ===\n\n <pre>\n# create a formatted transaction data\n> myd <- paste(\"apple,banana\", \"apple\", \"banana,cherry\", \"banana,cherry,durian\", sep=\"\\n\")> cat(myd)\napple,banana\napple\nbanana,cherry\n\n# write the transaction data to the file\n> write(myd, file = \"myd_basket_2\") \n\n# read the transaction data from the file, and save it to a variable\n> myt <- read.transactions(\"myd_basket_2\", format = \"basket\", sep=\",\")\n\n# inspect the transaction variable\n> inspect(myt)\n  items   \n1 {apple, \n   banana}\n2 {apple} \n3 {banana,\n   cherry}\n4 {banana,\n   cherry,\n   durian}\n</pre>\n\n\n\n\n=== clear workspace (delete data) ===\n\n* References\n* [https://stat.ethz.ch/pipermail/r-help/2007-August/137694.html Clear Workspace in R]\n* [http://stackoverflow.com/questions/11761992/remove-data-from-workspace Advanced method to clear data in R]\n\n* simply, remove three data \'data_1\', \'data_2\', \'data_3\'\n <pre>\nrm(data_1, data_2, data_3)\n</pre>\n\n* remove all the data searchable by ls()\n <pre>\nrm(list = ls())\n# \'list\' is a name of parameter to be passed into \'rm()\' function,\n# so it cannot be changed, it should be \"list\" literally.\n</pre>\n\n* remove all objects whose name begins with the string \"tmp\"\n <pre>\nrm(list = ls()[grep(\"^tmp\", ls())])\nrm(list = ls(pattern=\"^tmp\"))	# making use of the \'pattern\' argument\n</pre>\n\n== 20130313_172639 ==\n\n\n=== SVM example ===\n\n <pre>\n     data(iris)\n     attach(iris)\n     \n     ## classification mode\n     # default with factor response:\n     model <- svm(Species ~ ., data = iris)\n     \n     # alternatively the traditional interface:\n     x <- subset(iris, select = -Species)\n     y <- Species\n     model <- svm(x, y) \n     \n     print(model)\n     summary(model)\n     \n     # test with train data\n     pred <- predict(model, x)\n     # (same as:)\n     pred <- fitted(model)\n     \n     # Check accuracy:\n     table(pred, y)\n     \n     # compute decision values and probabilities:\n     pred <- predict(model, x, decision.values = TRUE)\n     attr(pred, \"decision.values\")[1:4,]\n     \n     # visualize (classes by color, SV by crosses):\n     plot(cmdscale(dist(iris[,-5])),\n          col = as.integer(iris[,5]),\n          pch = c(\"o\",\"+\")[1:150 %in% model$index + 1])\n</pre>\n\n\n=== cmdscale (Classical MultiDimensional Scaling) ===\n\n* Description\n: Classical multidimensional scaling of a data matrix. (a.k.a. principal coordinates analysis (Gower, 1966)\n\n* Usage\n: cmdscale(d, k=2, eig=FALSE, add=FALSE, x.ret=FALSE)\n\n* Arguments\n: \'\'\'d [mandatory]\'\'\': a distance structure such as that returned by \'dist\' or a full symmetric matrix containing the dissimilarities\n: k [optional]: the maximum dimension of the space which the data are to be represented in; must be in {1, 2, ..., n-1}\n: eig [optional]: indicates whether eigenvalues should be returned\n: add [optional]: logical indicating if an additive constant c* should be computed, and added to the non-diagonal dissimilarities such that the modified dissimilarities are Euclidean\n: x.ret [optional]: indicates whether the doubly centered symmetric distance matrix should be returned\n\n* Details\n: Multidimensional scaling takes a set of dissimilarities and returns a set of points such that the distances between the points are approximately equal to the dissimilarities\n\n\n=== dist (Distance matrix computation) ===\n\nThis computes and returns the distance matrix computed by using the specified distance measure to compute the distances between the rows of a data matrix\n\n* Usage\n: dist(x, method = \"euclidean\", diag = FALSE, upper = FALSE, p = 2)\n\n* Arguments\n: x\n:: a numeric matrix, data frame or \'dist\' object\n: method\n:: the distance measure to be used. this must be one of \"euclidean\", \"maximum\", \"manhattan\", \"canberra\", \"binary\", or \"minkowski\" (any unambiguous substring can be given)\n: diag\n\n* Examples (by blusjune)\n\n <pre>\n> mat_a\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    1    1\n[2,]    2    2    2    2\n[3,]    3    3    3    3\n[4,]    0    0    0    0\n> dist(mat_a)\n  1 2 3\n2 2    \n3 4 2  \n4 2 4 6\n> cmdscale(dist(mat_a))\n     [,1]          [,2]\n[1,]    1  5.809542e-08\n[2,]   -1  3.057654e-09\n[3,]   -3  9.172961e-09\n[4,]    3 -9.172961e-09\n> dist(cmdscale(dist(mat_a)))\n  1 2 3\n2 2    \n3 4 2  \n4 2 4 6\n</pre>\n\n <pre>\n> mat_b\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    2    2    2\n[3,]    0    0    0\n> dist(mat_b)\n         1        2\n2 1.732051         \n3 1.732051 3.464102\n> cmdscale(dist(mat_b))\n              [,1]         [,2]\n[1,]  7.412908e-33 1.564993e-08\n[2,] -1.732051e+00 2.477578e-09\n[3,]  1.732051e+00 2.477578e-09\n> dist(cmdscale(dist(mat_b)))\n         1        2\n2 1.732051         \n3 1.732051 3.464102\n\n> ((1-2)**2 + (1-2)**2 + (1-2)**2) ** 0.5\n[1] 1.732051\n> ((2-0)**2 + (2-0)**2 + (2-0)**2) ** 0.5\n[1] 3.464102\n</pre>\n\n\n== 20130306_185022 ==\n\n=== R package (\'e1071\') installation from command line ===\n\n: Assumes that already downloaded and unpacked properly the \'e1071_1.6-1.tar.gz\' file\n\n <pre>\n\n1mjjung@secm:[e1071] $ R CMD INSTALL e1071\nJob <13412785> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura007>>\nWarning: invalid package \'e1071\'\nError: ERROR: no packages specified\n\n\n\n\na1mjjung@secm:[e1071] $ cd ..\nla1mjjung@secm:[e1071_build] $ l\ntotal 280\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:45 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\n\n\n\n\na1mjjung@secm:[e1071_build] $ R CMD INSTALL e1071\nJob <13412786> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n* installing to library \'/home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15\'\n* installing *source* package \'e1071\' ...\n** package \'e1071\' successfully unpacked and MD5 sums checked\nchecking for C++ compiler default output file name... a.out\nchecking whether the C++ compiler works... yes\nchecking whether we are cross compiling... no\nchecking for suffix of executables... \nchecking for suffix of object files... o\nchecking whether we are using the GNU C++ compiler... yes\nchecking whether g++ accepts -g... yes\n** libs\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c Rsvm.c -o Rsvm.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cmeans.c -o cmeans.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cshell.c -o cshell.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c floyd.c -o floyd.o\ng++ -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c svm.cpp -o svm.o\ng++ -shared -L/usr/local/lib64 -o e1071.so Rsvm.o cmeans.o cshell.o floyd.o svm.o\ninstalling to /home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15/e1071/libs\n** R\n** inst\n** preparing package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n   \'svmdoc.Rnw\' \n** testing if installed package can be loaded\n\n* DONE (e1071)\n\n\n\n\na1mjjung@secm:[e1071_build] $ R\nJob <13412787> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n\nR version 2.15.2 (2012-10-26) -- \"Trick or Treat\"\nCopyright (C) 2012 The R Foundation for Statistical Computing\nISBN 3-900051-07-0\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType \'license()\' or \'licence()\' for distribution details.\n\nR is a collaborative project with many contributors.\nType \'contributors()\' for more information and\n\'citation()\' on how to cite R or R packages in publications.\n\nType \'demo()\' for some demos, \'help()\' for on-line help, or\n\'help.start()\' for an HTML browser interface to help.\nType \'q()\' to quit R.\n\n> ?svm\nNo documentation for \'svm\' in specified packages and libraries:\nyou could try \'??svm\'\n> library(e1071) \nLoading required package: class\n> ?svm\n> \nSave workspace image? [y/n/c]: y\n\n\n\n\na1mjjung@secm:[e1071_build] $ pwd\n/home/X0101/a1mjjung/x/R/e1071_build\n\na1mjjung@secm:[e1071_build] $ ls -alF\ntotal 284\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:47 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\n-rw------- 1 a1mjjung X0101     25 Mar  6 18:47 .Rhistory\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:46 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\n\n</pre>\n\n\n\n== 20130304_190007 ==\n\n\n\n=== test type of some object ===\n\n <pre>\n> x <- scan(\"/tmp/scan.txt\", what=list(NULL, name=character()))\n> x <- x[sapply(x, length) > 0]\n> is.vector(x)\n\n\n> x = mat.or.vec(100,1)\n> if (is.integer(x) == TRUE) { print (\"YES\") } else { print (\"NO\") }\n[1] \"NO\"\n> if (is.vector(x) == TRUE) { print (\"YES, vector\") } else { print (\"NO, NOT vector\") }\n[1] \"YES, vector\"\n</pre>\n\n\n\n\n=== Data import (load data from a file) ===\n\n* scan()\n <pre>\n > x1 <- scan(\"/etc/hosts\", what=character())\n\n > x1     \n [1] \"127.0.0.1\"       \"localhost\"       \"#127.0.1.1\"      \"stones\"         \n [5] \"#\"               \"The\"             \"following\"       \"lines\"          \n [9] \"are\"             \"desirable\"       \"for\"             \"IPv6\"           \n[13] \"capable\"         \"hosts\"           \"::1\"             \"ip6-localhost\"  \n[17] \"ip6-loopback\"    \"fe00::0\"         \"ip6-localnet\"    \"ff00::0\"        \n[21] \"ip6-mcastprefix\" \"ff02::1\"         \"ip6-allnodes\"    \"ff02::2\"        \n[25] \"ip6-allrouters\"  \"10.0.2.15\"       \"stones-eth0\"     \"#192.168.1.109\" \n[29] \"stones\"          \"hd-master-01\"    \"#192.168.1.110\"  \"pavement\"       \n[33] \"hd-slave-0001\"   \"192.168.1.112\"   \"stones\"          \"hd-master-01\"   \n[37] \"hd-slave-0001\"   \"kandinsky\"       \"192.168.1.110\"   \"pavement\"       \n[41] \"hd-slave-0002\"  \n</pre>\n\n* read.table()\n <pre>\n> iot_r <- read.table(\'tracelog.msn_filesrvr.R\')  \n</pre>\n\n\n\n=== function defintion, for loop in R ===\n\n <pre>\n> avg_smoothing <- function(src, srcl, sf) {\n    tgtl = srcl + 1 - sf\n    tgt <- mat.or.vec(tgtl, 1)\n    for (i in 1:tgtl) {\n        tgt[i] = mean(src[i:(i+sf-1)])\n    }\n    return (tgt)\n}\n\n> vec1 <- rnorm(100, mean=10, sd=1)\n> vec1_sf2 <- avg_smoothing(vec1, 100, 2)\n> vec1_sf4 <- avg_smoothing(vec1, 100, 4)\n> vec1_sf8 <- avg_smoothing(vec1, 100, 8)\n\n> plot(vec1, col=\"gray\", type=\"l\")\n> points(vec1_sf2, col=\"red\", type=\"l\")\n> points(vec1_sf4, col=\"blue\", type=\"l\")\n> points(vec1_sf8, col=\"green\", type=\"l\")\n</pre>\n\n\n\n\n== 20130127_225539 ==\n\n=== R Installation ===\n\n* to install R statistical computing software\n** me@matrix$ sudo apt-get install r-base\n\n* to start R from command line, just type \'R\' in your terminal\n** me@matrix$ R\n\n* RStudio download [http://www.rstudio.com/ide/download/]\n** RStudio Desktop [http://www.rstudio.com/ide/download/desktop]\n**: If you run R on your desktop\n** RStudio Server [http://www.rstudio.com/ide/download/server]\n**: If you run R on a Linux server and want to enable users to remotely access RStudio using a web browser\n*** RStudio Server Getting Started [http://www.rstudio.com/ide/docs/server/getting_started]\n\n <pre>\nsudo apt-get install r-base\nwget http://download2.rstudio.org/rstudio-server-0.97.312-amd64.deb\nsudo gdebi rstudio-server-0.97.312-amd64.deb \nbsc.adm.create_me\ngoogle-chrome http://kandinsky:8787 # type ID/PW (me?!)\n</pre>\n\n\n* Debian Packages of R Software [http://cran.r-project.org/bin/linux/debian/README.html]\n\n=== R Guide/Tutorial/Example ===\n\n* R Tutorial [http://www.r-tutor.com/]\n** R Tutorial:: Data Import [http://www.r-tutor.com/r-introduction/data-frame/data-import]\n\n\n* Example R scripts [http://people.eng.unimelb.edu.au/aturpin/R/index.html]]\n\n\n* R by example [http://www.mayin.org/ajayshah/KB/R/index.html]\n\n\n* R example:: Kmeans [http://stat.ethz.ch/R-manual/R-patched/library/stats/html/kmeans.html]\n\n\n* R package install howto\n; e1071\n: Misc Functions of the Department of Statistics (e1071), TU Wien\n:* package-installation and loading\n install.packages(\"e1071\") # installing the package \'e1071\'\n library(\"e1071\") # loading the installed package \'e1071\'\n\n\n* Importing SAS, SPSS and Stata files into R [http://staff.washington.edu/glynn/r_import.pdf]\n\n\n=== R Troubleshooting ===\n\n* Problems importing csv file/converting from integer to double in R [http://stackoverflow.com/questions/8381839/problems-importing-csv-file-converting-from-integer-to-double-in-r]\n\n\n=== Misc. ===\n\n* WEKA Tutorial [http://www.cs.utexas.edu/users/ml/tutorials/Weka-tut/]\n* weka is a metric prefix for 10^30','utf-8'),(17,'== DM-cache ==\n\n=== Reference ===\n\n[http://domino.watson.ibm.com/library/cyberdig.nsf/papers/BA52BEF8B940E7438525723C006BAFEA/$File/rc24123.pdf DM-cache - Dynamic Policy Disk Caching for Storage Networking]\n\n=== Mechanism ===\n\n\n\n== dm-cache (3.0.8) source code analysis ==\n\n* \"linux/drivers/md/dm-cache.c\"\n <nowiki>\n1762 /*\n1763  * Initiate a cache target.\n1764  */\n1765 int __init dm_cache_init(void)\n1766 {\n1767     int r;\n1768\n1769     r = jobs_init();\n1770     if (r)\n1771         return r;\n1772\n1773     _kcached_wq = create_singlethread_workqueue(\"kcached\");\n1774     if (!_kcached_wq) {\n1775         DMERR(\"failed to start kcached\");\n1776         return -ENOMEM;\n1777     }\n1778     INIT_WORK(&_kcached_work, do_work);\n1779\n1780     r = dm_register_target(&cache_target);\n1781     if (r < 0) {\n1782         DMERR(\"cache: register failed %d\", r);\n1783         destroy_workqueue(_kcached_wq);\n1784     }\n1785\n1786     return r;\n1787 }\n</nowiki>\n\n* workqueue creation\n*: create_singlethread_workqueue()\n*: create_workqueue()','utf-8'),(18,'\n== References ==\n\n* [http://www-03.ibm.com/systems/software/gpfs/ IBM General Parallel File System (GPFS) // Efficient storage management for big data applications]\n\n* [http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=%2Fcom.ibm.cluster.gpfs.doc%2Fgpfsbooks.html IBM GPFS Documentations]\n\n* [http://publib.boulder.ibm.com/epubs/pdf/c2351827.pdf IBM GPFS // Advanced Administration Guide]\n\n* [http://publib.boulder.ibm.com/epubs/pdf/a2322217.pdf IBM GPFS // Administration and Programming Reference]\n\n* [http://publib.boulder.ibm.com/epubs/pdf/a7604137.pdf IBM GPFS // Concepts, Planning, and Installation Guide]\n\n* [http://publibz.boulder.ibm.com/epubs/pdf/a7604147.pdf IBM GPFS // Data Management API Guide]\n\n* [http://publibz.boulder.ibm.com/epubs/pdf/a7604157.pdf IBM GPFS // Problem Determination Guide]\n\n== Solutions powered by GPFS ==\n\nGPFS article [http://www-03.ibm.com/systems/software/gpfs/] shows the solutions powered by GPFS as the following:\n\n* IBM Scale Out Network Attached Storage\n* IBM Smart Business Storage Cloud\n* IBM Information Archive\n* IBM Storwize V7000 Unified Storage\n* IBM Smart Analytics System\n* IBM DB2 pureScale\n* IBM Systems for SAP HANA\n* IBM SmartCloud Enterprise\n* IBM SmartCloud Archive\n* IBM Digital Media Center\n* Information Life cycle management\n* SAP BI Accelerator\n\n== Active File Management (AFM) ==\n\n=== Optimize storage utilization, maximize return on investments ===\n\nGPFS also features Active File Management (AFM) technology. AFM is a powerful remote file caching technology that ensures high performance access to file based information no matter where it might be geographically located. This important feature is useful for organizations that share large volumes of file based data and require high levels of information availability no matter where it resides. So applications demanding high SLA requirements will benefit from AFM.\n\n\n=== Data sharing? anywhere, anytime ===\n\nGPFS includes active file management, which is a scalable, high-performance remote file data caching solution that is integrated within a GPFS file system. If you have a situation where massive amounts of data is gathered at separate locations and the results are analyzed by people at other locations, you need a solution that makes it possible to transparently move file data automatically to where it is needed.  Using active file management you can implement a global namespace giving every site the same view to all of the data.  This is especially useful for collaborative projects, applications and workflows that are managed globally, but need to have access to the same files. Now applications can see the same set of files and have local read/write performance to those files no matter the location.  Active file management leverages the inherent scalability of GPFS, which results in a high-performance locationindependent solution that masks failures and hides wide-area latencies and outages. [http://public.dhe.ibm.com/common/ssi/ecm/en/pos03096usen/POS03096USEN.PDF]\n\n\n\n== GPFS Concepts, Planning, and Installation ==\n\n* [http://publib.boulder.ibm.com/epubs/pdf/a7604137.pdf Concepts, Planning, and Installation Guide // GPFS Version 3 Release 5.0.7]\n\n* Chapter 9. Configuring and tuning your system for GPFS\n\n* Configuration and tuning considerations for all systems include:\n: 1. “Clock synchronization”\n: 2. “GPFS administration security”\n: 3. “Cache usage” on page 90\n: 4. “GPFS I/O” on page 92\n: 5. “Access patterns” on page 92\n: 6. “Aggregate network interfaces” on page 92\n: 7. “Swap space” on page 93\n\n\n== GPFS Problem Determination ==\n\n=== GPFS Problem Determination Guide // GA76-0415-07 ===\n\nThis information [http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=%2Fcom.ibm.cluster.gpfs.v3r5.0.7.gpfs500.doc%2Fbl1pdg_about.htm] explains how to handle problems you may encounter with the General Parallel File System (GPFS™) licensed program.\n\n\n== GPFS Data Management API ==\n\n\n=== GPFS Data Management API Guide // GA76-0414-07 ===\n\nThis information [http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=%2Fcom.ibm.cluster.gpfs.v3r5.0.7.gpfs400.doc%2Fbl1dmp_about.htm] describes the Data Management Application Programming Interface (DMAPI) for General Parallel File System (GPFS™). This implementation is based on The Open Group\'s System Management: Data Storage Management (XDSM) API Common Applications Environment (CAE) Specification C429, The Open Group, ISBN 1-85912-190-X specification. The implementation is compliant with the standard. Some optional features are not implemented.\n\nThe XDSM DMAPI model is intended mainly for a single node environment. Some of the key concepts, such as sessions, event delivery, and recovery, required enhancements for a multiple-node environment such as GPFS.\n\nThis information applies to GPFS version 3.5.0.7 for AIX®, Linux, and Windows.\n\nTo find out which version of GPFS is running on a particular AIX node, enter:\n lslpp -l gpfs\\* \nTo find out which version of GPFS is running on a particular Linux node, enter:\n rpm -qa | grep gpfs\nTo find out which version of GPFS is running on a particular Windows node, open the Programs and Features control panel. The IBM® General Parallel File System installed program name includes the version number.\n\n== Generating GPFS Trace Reports ==\n\n=== Generating GPFS trace reports ===\n\n* GPFS Problem Determination Guide (GA76-0415-07) [http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=%2Fcom.ibm.cluster.gpfs.v3r5.0.7.gpfs500.doc%2Fbl1pdg_mmtrace.htm]\n\nUse the mmtracectl command to configure trace-related configuration variables and to start and stop the trace facility on any range of nodes in the GPFS™ cluster.\n\nTo configure and use the trace properly:\n\n* Issue the mmlsconfig dataStructureDump command to verify that a directory for dumps was created when the cluster was configured. The default location for trace and problem determination data is /tmp/mmfs. Use mmtracectl as instructed by service personnel to set trace configuration parameters as required if the default parameters are insufficient. For example, if the problem results in GPFS shutting down, set the traceRecyle variable with --trace-recycle as described in the mmtracectl command in order to ensure that GPFS traces are performed at the time the error occurs.\nIf desired, specify another location for trace and problem determination data by issuing this command:\n\n mmchconfig dataStructureDump=path for storage of dumps\n\n* To start the tracing facility on all nodes, issue this command:\n\n mmtracectl --start\n\n* Re-create the problem.\n\n* When the event to be captured occurs, stop the trace as soon as possible by issuing this command:\n mmtracectl --stop\n\n* The output of the GPFS trace facility is stored in /tmp/mmfs, unless the location was changed using the mmchconfig command in Step 1. Save this output.\n\n* If the problem results in a shutdown and restart of the GPFS daemon, set the traceRecycle variable as necessary to start tracing automatically on daemon startup and stop the trace automatically on daemon shutdown.\n\n\n* If the problem requires more detailed tracing, the IBM® Support Center personnel might ask you to modify the GPFS trace levels. Use the mmtracectl command to establish the required trace classes and levels of tracing. For example:\n\n mmtracectl --set --trace=def\n\nOnce the trace levels are established, start the tracing by issuing:\n\n mmtracectl --start\n\nAfter the trace data has been gathered, stop the tracing by issuing:\n\n mmtracectl --stop\n\nTo clear the trace settings and make sure tracing is turned off, issue:\n\n mmtracectl --off\n\n* On AIX®, the –aix-trace-buffer-size option can be used to control the size of the trace buffer in memory.\n\n* On Linux nodes only, use the mmtracectl command to change the following:\n: The trace buffer size in blocking mode\n: The raw data compression level\n: The trace buffer size in overwrite mode\n: When to overwrite the old data\n\n* For example:\n: To set the trace buffer size in blocking mode to 8K, issue:\n mmtracectl --set --tracedev-buffer-size=8K\n: To set the trace raw data compression level to the best ratio, issue:\n mmtracectl --set --tracedev-compression-level=9\n: To set the trace buffer size in overwrite mode to 32K, issue:\n mmtracectl --set --tracedev-overwrite-buffer-size=32K\n: To wait to overwrite the data until the trace data is written to the local disk and the buffer is available again, issue:\n mmtracectl --set --tracedev-write-mode=blocking\n\n* For more information about the mmtracectl command options, see the [http://publib.boulder.ibm.com/epubs/pdf/a2322217.pdf GPFS: Administration and Programming Reference].','utf-8'),(19,'== kworker in Linux ==\n\n\"kworker\" is a placeholder process for kernel worker threads, which perform most of the actual processing for the kernel, especially in cases where there are interrupts, timers, I/O, etc. These typically correspond to the vast majority of any allocated \"system\" time to running processes. It is not something that can be safely removed from the system in any way. [http://askubuntu.com/questions/33640/kworker-what-is-it-and-why-is-it-hogging-so-much-cpu]\n\n\n----\nOriginal Article:\n\n\"kworker\" is a placeholder process for kernel worker threads, which perform most of the actual processing for the kernel, especially in cases where there are interrupts, timers, I/O, etc. These typically correspond to the vast majority of any allocated \"system\" time to running processes. It is not something that can be safely removed from the system in any way, and is completely unrelated to nepomuk or KDE (except in that these programs may make system calls, which may require the kernel to do something).\n\nThere were some reports of excessing kworker activity for relatively idle systems starting during 2.6.36 development ( https://lkml.org/lkml/2011/3/29/2 is one example discussion ), and wide reports of confusion and problems with 2.6.38 (although many of these reports include the word \"Natty\", so I presume these people not to have used any kernel between 2.6.35 (distributed in Ubuntu 10.10) and 2.6.38 (distributed in Ubuntu 11.04).\n\nI\'ve found many reports of something that \"fixed\" this for one or another user. Most \"fixes\" seem to be related to updates of the kernel of various sorts. Where the update can be tracked to a specific issue, it seems to often be some driver or kernel service that has been patched to not misbehave: I have the impression that there are a very large number of things in the kernel that can cause a behaviour which is observed as excessive kworker usage.\n\nIf you find the system unusable due to excessive kworker activity, I would recommend trying to do fewer things. If you think you\'re not doing anything, try shutting down long-running services or timers (RSS readers, mail readers, file indexers, activity trackers, etc.). If this doesn\'t work, try restarting. If your system allows you to enable or disable hardware in a preboot environment, try turning off hardware you aren\'t using. If it happens on every restart before you do anything, you could try uninstalling things, but at this point you\'ll want to be running syscall profiling tools to track down specific applications that seem to be causing this overload.\n\nIt is to be hoped that your specific system will stop expressing this behaviour with a future kernel upgrade (and many of the most common causes of this have been solved).\n----','utf-8'),(20,'== Machine-or-transformation test ==\n\n* [http://en.wikipedia.org/wiki/Machine-or-transformation_test Machine-or-transformation test]\n: In United States patent law, the machine-or-transformation test is a test of patent eligibility under which a claim to a process qualifies to be considered for patenting if it (1) is implemented with a particular machine, that is, one specifically devised and adapted to carry out the process in a way that is not concededly conventional and is not trivial; or else (2) transforms an article from one thing or state to another\n\n\n== A guide to patenting software ==\n\n* [http://www.ipwatchdog.com/2013/02/16/a-guide-to-patenting-software-getting-started/id=35629/ A guide to patenting software // IP watchdog // 2013-02-16]\n: Written by Gene Quinn\n: Patent Attorney & Founder of IPWatchdog\n: Zies, Widerman & Malek\n: Follow Gene on Twitter @IPWatchdog \n: Posted: Feb 16, 2013 @ 2:17 pm\n\n\n== The Software IP Detective: Infringement Detection in a Nutshell ==\n\n* [http://www.ipwatchdog.com/2011/11/20/the-software-ip-detective-infringement-detection-in-a-nutshell/id=20495/ The Software IP Detective: Infringement Detection in a Nutshell // ]\n: Written by: Bob Zeidman\n: eidman Consulting\n: Software Analysis and Forensic Engineering\n: Posted: November 20, 2011 @ 9:00 am','utf-8'),(21,'\n=== ifconfig ===\n* 2013-01-11\n\n <nowiki>\nb@ub01:[~] $ ifconfig\neth0      Link encap:Ethernet  HWaddr 00:25:90:63:4c:fc\n          inet addr:75.2.99.89  Bcast:75.2.99.255  Mask:255.255.255.0\n          inet6 addr: fe80::225:90ff:fe63:4cfc/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:10530 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:123 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000\n          RX bytes:1520516 (1.5 MB)  TX bytes:21039 (21.0 KB)\n          Interrupt:16 Memory:fb5e0000-fb600000\n\neth1      Link encap:Ethernet  HWaddr 00:25:90:63:4c:fd\n          inet addr:10.0.3.1  Bcast:10.0.3.255  Mask:255.255.255.0\n          inet6 addr: fe80::225:90ff:fe63:4cfd/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:788 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:74 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000\n          RX bytes:420059 (420.0 KB)  TX bytes:7632 (7.6 KB)\n          Interrupt:17 Memory:fb6e0000-fb700000\n\neth2      Link encap:Ethernet  HWaddr 00:02:c9:52:c5:59\n          inet addr:10.0.1.1  Bcast:10.0.1.255  Mask:255.255.255.0\n          inet6 addr: fe80::202:c9ff:fe52:c559/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:785 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:461 errors:1 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000\n          RX bytes:71059 (71.0 KB)  TX bytes:37913 (37.9 KB)\n\nib0       Link encap:UNSPEC  HWaddr 80-00-00-59-FE-80-00-00-00-00-00-00-00-00-00-00\n          inet addr:10.0.2.1  Bcast:10.0.2.255  Mask:255.255.255.0\n          inet6 addr: fe80::202:c903:52:c559/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:2044  Metric:1\n          RX packets:4 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:17 errors:0 dropped:6 overruns:0 carrier:0\n          collisions:0 txqueuelen:256\n          RX bytes:978 (978.0 B)  TX bytes:2659 (2.6 KB)\n\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:16436  Metric:1\n          RX packets:420 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:420 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:53273 (53.2 KB)  TX bytes:53273 (53.2 KB)\n</nowiki>','utf-8'),(22,' <pre>\n\n 92 ##\n 93 ## print the collected and calculated results\n 94 ##\n 95 for kv_key, kv_val in _kv_list__addr_hit_tstamp.items():\n 96     print \"__list__addr_hit_tstamp__ \" + str(kv_key) + \" : \" + str(kv_val)\n 97 \n 98 for kv_key, kv_val in _kv_cdst__hits_per_addr.items():\n 99     print \"__cdst__hits_per_addr__ \" + str(kv_key) + \" : \" + str(kv_val)\n100 \n101 for kv_key, kv_val in _kv_cdst__addrs_per_hitnum.items():\n102     print \"__cdst__addrs_per_hitnum__ \" + str(kv_key) + \" : \" + str(kv_val)\n103 \n104 for kv_key, kv_val in _kv_cdst__addr_hit_interval.items():\n105     print \"__cdst__addr_hit_interval__ \" + str(kv_key) + \" : \" + str(kv_val)\n\n\n\nblusjune@jimi-hendrix:[sim.cache_perf] $ grep -nr __cdst__addrs_per_hitnum__ log \n3383117:__cdst__addrs_per_hitnum__ 1 : 917923\n3383118:__cdst__addrs_per_hitnum__ 2 : 288480\n3383119:__cdst__addrs_per_hitnum__ 3 : 149568\n3383120:__cdst__addrs_per_hitnum__ 4 : 94170\n3383121:__cdst__addrs_per_hitnum__ 5 : 65841\n3383122:__cdst__addrs_per_hitnum__ 6 : 48970\n3383123:__cdst__addrs_per_hitnum__ 7 : 37278\n3383124:__cdst__addrs_per_hitnum__ 8 : 26636\n3383125:__cdst__addrs_per_hitnum__ 9 : 18760\n3383126:__cdst__addrs_per_hitnum__ 10 : 13017\n3383127:__cdst__addrs_per_hitnum__ 11 : 9306\n3383128:__cdst__addrs_per_hitnum__ 12 : 6865\n3383129:__cdst__addrs_per_hitnum__ 13 : 5256\n3383130:__cdst__addrs_per_hitnum__ 14 : 3909\n3383131:__cdst__addrs_per_hitnum__ 15 : 2550\n3383132:__cdst__addrs_per_hitnum__ 16 : 1542\n3383133:__cdst__addrs_per_hitnum__ 17 : 869\n3383134:__cdst__addrs_per_hitnum__ 18 : 418\n3383135:__cdst__addrs_per_hitnum__ 19 : 137\n3383136:__cdst__addrs_per_hitnum__ 20 : 53\n3383137:__cdst__addrs_per_hitnum__ 21 : 9\n3383138:__cdst__addrs_per_hitnum__ 23 : 1\n\n</pre>','utf-8'),(23,'=== Class-based Implementation ===\n\n* References\n*# [http://stackoverflow.com/questions/4443920/python-building-a-lru-cache LRU Cache - Class-based approach]\n\n <nowiki>\n\nclass LRU_Cache(object):\n\n    def __init__(self, original_function, maxsize=1000):\n        self.original_function = original_function\n        self.maxsize = maxsize\n        self.mapping = {}\n\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        self.head = [None, None, None, None]        # oldest\n        self.tail = [self.head, None, None, None]   # newest\n        self.head[NEXT] = self.tail\n\n    def __call__(self, *key):\n        PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n        mapping, head, tail = self.mapping, self.head, self.tail\n        sentinel = object()\n\n        link = mapping.get(key, sentinel)\n        if link is sentinel:\n            value = self.original_function(*key)\n            if len(mapping) >= self.maxsize:\n                oldest = head[NEXT]\n                next_oldest = oldest[NEXT]\n                head[NEXT] = next_oldest\n                next_oldest[PREV] = head\n                del mapping[oldest[KEY]]\n            last = tail[PREV]\n            link = [last, tail, key, value]\n            mapping[key] = last[NEXT] = tail[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = tail[PREV]\n            last[NEXT] = tail[PREV] = link\n            link[PREV] = last\n            link[NEXT] = tail\n        return value\n\nif __name__ == \'__main__\':\n    p = LRU_Cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n</nowiki>\n\n=== Function-based Implementation ===\n\n* References\n*# [http://code.activestate.com/recipes/577969-simplified-highly-optimized-lru-cache/ LRU Cache - Function-based Approach]\n\n <nowiki>\n\ndef cache_generator(original_function, maxsize):\n    mapping = {}\n    mapping_get = mapping.get\n    root = [None, None]\n    root[:] = [root, root]\n    value = None\n    size = 0\n\n    PREV, NEXT, KEY, VALUE = 0, 1, 2, 3\n    while 1:\n        key = yield value\n        link = mapping_get(key, root)\n        if link is root:\n            value = original_function(key)\n            if size < maxsize:\n                size += 1\n            else:\n                old_prev, old_next, old_key, old_value = root[NEXT]\n                root[NEXT] = old_next\n                old_next[PREV] = root\n                del mapping[old_key]\n            last = root[PREV]\n            link = [last, root, key, value]\n            mapping[key] = last[NEXT] = root[PREV] = link\n        else:\n            link_prev, link_next, key, value = link\n            link_prev[NEXT] = link_next\n            link_next[PREV] = link_prev\n            last = root[PREV]\n            last[NEXT] = root[PREV] = link\n            link[PREV] = last\n            link[NEXT] = root\n\ndef make_cache(original_function, maxsize=100):\n    \'Create a cache around a function that takes a single argument\'\n    c = cache_generator(original_function, maxsize)\n    next(c)\n    return c.send\n\n\nif __name__ == \'__main__\':\n    p = make_cache(ord, maxsize=3)\n    for c in \'abcdecaeaa\':\n        print(c, p(c))\n\n\n</nowiki>','utf-8'),(24,'기술원에서의 Big Data 기술 연구\n\n2013년 1월\n\n	ICL에서 개발하려는 줄기 기술은 무엇인가?\nICL에서 개발하려는 줄기 기술은 지능형 차세대 Big Data 관리, 분석 및 처리 기술입니다. 짧게 말해 지능형 차세대 Big Data 기술입니다.\n\nIT 기술의 발전, 특히 웹, SNS, 모바일 서비스, 스토리지, 클라우드 등의 기술의 발전에 따라 디지털 데이터가 폭증하고 있습니다. 2012년 전세계 디지털 데이터의 양은 2.7 zeta bytes에 이릅니다. 단위가 tera, peta, exa, zeta 의 순이니까, zeta bytes는 tera bytes의 10의 9승입니다. 요즘 큰 하드디스크 용량이 테라바이트이니까 그런 하드디스크 27억개의 분량입니다. ‘20년에는 데이터량이 90 zeta bytes에 이를 것으로 예상되고 있습니다. 이미 당사도 ‘12년 기준으로 1.6PB의 디지털 데이터를 저장하고 있습니다.\n이러한 big data로부터 이전에는 기대하지 못했던 insight를 뽑아내고, 새로운 서비스를 제공하는 것이 가능하게 되었습니다. 예를 들면, 아마존은 고객 데이터 (구매이력)를 활용하여 자신들의 웹사이트에서 상품추천에 적용한 결과 30% 매출 증가를 이루었습니다. 마이크론사는 장비 운용 데이터를 분석하여 제품생산 시간을 10% 단축시켰고, 온타리오 병원에서는 미숙아의 생체정보를 분석하여 미숙아의 감염을 이전보다 훨씬 조기에 발견할 수 있게 되었습니다. 구글은 수 많은 웹검색기록을 분석하여 미국 질병통제센터보다 2주나 앞서서 어느 지역에서 독감이 유행하는지를 파악할 수 있게 되었습니다.\n그러나 big data의 활용은 이제 시작단계라고 할 수 있고, 모든 산업 분야, 나아가서 인류 생활의 모든 분야에서 big data의 활용 가능성은 무궁하다고 할 수 있습니다. 앞으로 big data의 활용은 모든 산업분야에서 개별 기업 경쟁력에 매우 중요한 부분이 될 것으로 전망됩니다.\n따라서 big data를 관리하고 분석하여 활용하는 기술이 당사의 현재와 미래 경쟁력에 매우 중요합니다.\nBig Data는 획득, 저장, 분석, 그리고 사용자와 interaction하는 서비스에의 활용 단계를 거치게 됩니다. 저희는 관리, 분석, 그리고 사용자와의 interaction 분야에서 차세대 지능형 기술을 확보하려고 합니다.\nBig data 관리에서는 big data의 실시간 처리를 가능케하는 대규모 분산 스토리지 기술을 핵심 기술로 identify하였고, 분석에서는 인간의 인지 과정을 모사하여 big data로부터 계층적인 패턴을 발견하고 학습하는 hierarchical machine learning 기술, 다종의 complex data를 통합하여 mining하는 heterogeneous data mining 기술, 연속하여 발생하는 스트림 이벤트에 대한 처리 및 분석 기술인 irregular stream processing 기술을 핵심 기술로 identify 하였습니다. Big data 활용에서는 사용자의 의도와 감정 등 high-end context에 기반한 지능적인 interaction 기술을 identify 하였습니다. 이러한 핵심기술들이 사업에 어떻게 기여할 수 있는지에 대하여는 관련 과제를 설명할 때 말씀드리겠습니다. \n	기술원에서 하는 Big Data 연구는 사업부랑 소프트웨어 센터에서 하는 것이랑 무엇이 다른가?\n\n사업부에서는 현업에 당장 적용할 수 있는 기술을 활용하는 것과 그것을 약간 개선하는 개발작업을 하고 있고, 사업부 연구소에서는 어느 정도 성숙된 기술을 내재화하고 발전시키는 작업을 하고 있다고 말씀드릴 수 있습니다.\n기술원은 주력사업 분야에 향후 impact가 크지만 당장에 사업에 적용하기에는 성숙하지는 않았으나 어느 정도 가능성이 검증된 기술을 심화시키고 성숙시키는 연구를 진행하고 있습니다. 또한 Medical 분야나 스토리지 분야와 같은 신제품/신사업 분야의 seeding 기술을 확보하는 것을 목적으로 하고 있습니다.\n당사에서 big data 기술이 개발되고 있는 현황을 간략하게 정리한 내용을 말씀드리겠습니다.\n당사의 Big Data의 활용은 크게 수익을 창출하는 것과 운영효율을 높이는 것으로 나눌 수 있습니다.\n수익 창출은 주력제품에 부가가치를 높이는 것, 서비스의 차별화를 이루는 것, 그리고 big data infra를 위한 신제품/신사업으로 나눌 수가 있습니다.\n서비스 차별화 분야에서는, 소프트웨어센터가 비디오 추천 기술을 개발하고 있습니다. 소프트웨어센터에서는 VD사와 협력하여 VOD 서비스에서 사용자의 view log를 이용한 추천기능을 개발하였고, 이를 MSC에서 받아서 개선하는 작업을 하고 있습니다. \n주력제품의 부가가치를 높이는 분야에서는 무선사가 context-aware 기능을 개발하고 있는데, 사용자의 위치와 캘린더에 있는 일정 정보 정도를 활용하여 룰 기반의 context-aware 서비스를 개발하고 있는 것으로 알고 있습니다. 이는 본격적인 big data의 활용이라고 하기에는 아직은 초보적인 단계라고 할 수 있습니다. \n신제품 개발 분야에서는, 본사에서 서버 사업에 진출하기로 결정한 바 있고, 이에 따라 DMC연에서 일단 웹 호스팅을 주 애플리케이션으로 하는 저전력 서버인 마이크로서버를 개발하고 있습니다. 신사업추진단에서는 스토리지 시스템 사업을 기획 중입니다.\n운영효율을 높이는 분야에서는 메모리 제조공정에서 나오는 센서데이터를 분석하여 에러의 원인을 찾는 것이 시도되고 있으나 매우 제한된 데이터만 분석하고 있고, S.LSI는 센서 데이터의 summary만 육안으로 분석하고 있는 상황입니다. 이에 따라 S.LSI는 제조공정 Big Data를 분석할 수 있는 기술을 확보하는 것을 추진하고 있습니다.\n\nICL에서 진행하는 세 가지 과제가 당사의 Big Data 활용 분야에 어떻게 접맥되는지는 보시는 표와 같습니다. Real-Time Big Data Intelligence Platform 과제는 다양한 분야에 활용될 수 있는 차세대 지능형 big data platform 기술을 확보하는 것을 목적으로 하고 있습니다. Medical Data Analytics는 의료 분야 영상진단 기기의 부가가치 증대 및 Connected Health 솔류션 및 서비스의 차별화를 위한 기술 개발을 목적으로 하고 있어, DMC연구소/소프트웨어 센터와는 분야가 다릅니다. Behavior Analytics는 스마트 기기의 부가가치 및 서비스 차별화를 위하여 behavior data 분석을 통한 사용자의 high-end context 인식 기술을 목적으로 하고 있습니다. 사용자의 high-end context를 추론하는 기술은 차세대 context-aware 서비스 및 당사 스마트기기의 1st screen 전략에 핵심 요소가 됩니다.\n\n	Real-Time Big Data Platform 과제는 무엇인가?\n\n차세대 big data 기술의 두 가지 keyword를 저희는 한 차원 높은 analytics을 위한 계층적 machine learning과 real-time 분석 이라고 봅니다.\n\n기존에 Big Data 문제는 일단 엄청난 양의 데이터에 어떻게 하면 기존의 알고리즘을 적용할 수 있느냐에 있었습니다. 1TB의 데이터를 하드 드라이브로부터 읽는 데에만 2시간여가 걸리고 100TB가 되면 메모리로 읽어들이는 데에만 수일이 걸리기 때문에, 저장과 컴퓨팅을 분산시켜서 속도를 높이고 기존에 relational database에서 수행하던 분석을 big data에 적용할 수 있도록 해 주는 기술개발이 진행되어 왔습니다. 한편으로 SNS와 같은 서비스에서 생성되는 새로운 형태의 데이터 구조 및 비정형 Big Data를 분석하는 기술 개발이 많이 이루어져 왔습니다.\n전체적으로 Big Data 분석은 크게 통계적인 평균이나 분포 정도를 보는 낮은 수준의 analytics에서 machine learning을 활용하는 고도의 analytics로 단계를 나눌 수 있습니다. 저희는 big data 분석의 미래는 사람의 인지 기능을 최대한 가깝게 모사하여, big data, 분산 시스템에 의한 대규모 컴퓨팅 power와 스토리지, 그리고 사람의 인지 기능을 모사하는 학습이론을 결합하는 것이라고 판단하였습니다.\n기존의 machine learning은 하나의 layer에서 수동으로 정의한 feature들을 사용하여 데이터 모델을 학습하기 때문에, data에 variation이 커지면 인식 성능이 매우 낮아집니다.\n예를 들면, 사람은 사람 얼굴의 특징을 파악하여 얼굴이 많은 부분이 가려져도 사람 얼굴로 인식하고 공상과학영화에서 창조해낸 외계인의 얼굴도 사람얼굴로 인식합니다. 이것은 사람이 사람 얼굴의 특징을 pixel 단위로 인식하는 것이 아니라 보다 높은 단계의 특징들로 인식하기 때문입니다. 즉 사람은 하나의 대상에 대하여 여러 단계에서 특징을 정의할 수 있습니다. 이것은 또한 개념화, 추상화로 연결됩니다. 이런 추상화/개념화는 사람으로 하여금 한번 배운 개념을 새로운 데이터를 다루는데 활용할 수 있게 해 줍니다.\nBig data 분석과 활용에 이러한 사람의 인지 기능과 유사한 학습방식을 활용하는 것, 역으로 big data를 이용하여 이러한 학습를 가능하게 하는 것은 big data 활용의 새로운 장을 열게 될 것입니다. 이것의 핵심 기술로 저희가 연구하고자 하는 것이 계층적 machine learning 다른 말로 hierarchical machine learning 기술입니다.\n기존의 machine learning 방식은 한 layer에서 데이터의 특징을 학습하는 것이라면 계층적 machine learning 방식은 사람의 브레인처럼 다수의 layer에서 계층적으로 데이터의 특징을 학습하는 방식입니다.\n일반적으로 machine learning은 알고리즘의 복잡도로 인해 높은 컴퓨팅 power와 데이터 처리 속도를 요구합니다. 계층적 machine learning 은 다수의 layer에서 학습하기 때문에 기존의 machine learning 방식보다 더욱 복잡하게 되고 훨씬 높은 컴퓨팅 power와 데이터 처리 속도를 요구합니다.\n\n앞에서 big data 분석의 또 다른 keyword로 real-time을 말씀드렸습니다. 기존 big data 분석은 주로 통계적인 트렌드를 offline으로 분석하고 있습니다. Big data를 실시간으로 분석하는 것은 새로운 단계의 big data 활용을 가져올 것입니다. 예를 들면, 백화점에서 고객의 과거 구매 이력 뿐만 아니라 당일 동선, 어떤 제품에 관심을 보였는지를 모니터링하여 고객의 위치에 따라 실시간으로 정확한 promotion을 할 수 있게 되면 추가적인 매출 향상을 얻을 수 있게 됩니다. 생산 라인에서도 장비에 부착된 수 만개의 센서에서 나오는 데이터를 쌓아놓고 지난 에러의 원인을 찾는 것이 아니라 실시간으로 새로운 데이터의 패턴을 계속 찾아 단시간에 에러를 일으키는 장비를 찾을 수 있게 되어 생산 수율의 한계를 극복할 수 있게 됩니다.\n이러한 big data의 실시간 분석은 현재 수준의 big data보다 훨씬 고성능의 스토리지, 컴퓨팅 인프라를 요구하게 됩니다. \n\n본 과제에서는 계층적인 machine learning을 실시간으로 수행할 수 있는 고성능 big data 플랫폼 기술을 확보하려고 합니다.\n이 플랫폼은 대규모 데이터를 저장하고 고속으로 access할 수 있도록 해 주는 고성능 분산 스토리지 layer와, 복잡도가 큰 계층적 machine learning 알고리즘을 빠르게 실행할 수 있게 해 주는 분산 컴퓨팅 layer, 그리고 계층적 machine learning layer의 3단계 구조를 갖습니다.\n\n이 플랫폼을 다양한 big data 기반의 지능형 서비스 기술 개발에 활용할 수 있습니다.\n\n예를 들면, 향후 스마트 안경 형태의 새로운 기기가 등장할 것으로 예상되는데, 사람 눈이 보는 것을 이 디바이스도 같이 보기 때문에, 영상 기반의 정보 서비스가 크게 발전할 것으로 예상됩니다. Augmented Reality도 사용자 UX 측면에서 현실성이 커질 것이고, 영상 기반의 정보 검색도 발전할 것입니다. 이러한 서비스에서 핵심은 소프트웨어가 다양한 영상의 내용을 이해하거나, 또는 영상으로부터 다양한 종류의 object를 인식하는 것이 핵심 기술이 되는데, 현재 이 기술의 수준이 아직은 상용화와는 거리가 꽤 있습니다.\n계층적 machine learning이 이 문제를 푸는데 유용한 방식이 될 것으로 기대됩니다. 작년 말 구글과 Standford 대학의 연구진이 2000대의 서버를 1주일 돌려서 유투브에서 캡쳐한 이미지 1200만장에 대해 deep learning이라는 계층적 machine learning 기법의 하나를 사용하여 object recognition을 시도한 결과를 발표하였습니다. 이 실험에서 사람이 가르쳐주지 않았는데도, deep learning은 고양이 이미지들을 다른 이미지들과 구별하였고, 고양이 얼굴 이미지를 형상화하였습니다.\n계층적 machine learning은 클래스의 종류가 많고 데이터의 variation이 큰 다양한 big data 분석에 효과를 낼 것으로 기대되는데, 앞서 말씀 드린 object recognition 외에 speech recognition, 자연어 번역 등의 문제에도 효과를 내는 것으로 알려져 있습니다.\n\n본 Real-Time Big Data Intelligence 플랫폼 연구에서 개발한 대규모 데이터의 고속 처리를 가능하게 하는 지능형 분산 스토리지와 컴퓨팅 기술은 big data infra 시장에서의 신제품/신사업의 seed 기술이 될 것입니다.\n이미 당사는 서버 사업에 진출하기로 결정한 바 있고, 스토리지 시스템 사업에 진출하는 것이 검토되고 있습니다. 스토리지 전체 시장은 2016년에 $106B 규모이고, big data를 위한 스토리지 시스템 시장도 CAGR 61%로 고성장을 하면서 2016년도에는 $4.2B의 시장을 형성할 것으로 예상되고 있습니다. \n또한 본 과제에서 개발한 분산 스토리지 기술 중 단일 서버 내에 국한해서 적용할 수 있는 기술 부분은 메모리사의 SSD 제품의 부가가치를 높이는데 기여할 수 있습니다. 이미 메모리사는 그 부분에 많은 관심을 가지고 있습니다. SSD 제품의 부가가치를 높이는 스토리지 SW 기술은 비교적 이른 시기에 사업 impact를 낼 수 있습니다만, 본 과제의 작은 부분으로서 저희가 개발하는 줄기기술의 적용이지, 과제의 중심은 아닙니다.\n기술원 Big Data 플랫폼 연구의 주 목적은 당사 주력제품의 부가가치 및 서비스 차별화이지만, 축적된 분석 SW 기술은 향후 서버 신사업이 저전력 마이크로 서버에서 고부가가치 High-End 솔루션인 big data appliance로 사업 영역을 넓히는데 seed 기술이 될 것입니다.\n\n \n	스토리지 소프트웨어 연구는 메모리사에서 하면 되지 않는가?\n\n메모리사는 SSD 및 SSD와 bundle할 수 있는 소프트웨어를 사업영역으로 잡고 있습니다. 메모리사가 직접 스토리지 시스템 사업을 하게 될 경우, 고객사와 충돌하는 상황이 발생하게 됩니다.  따라서 메모리사는 스토리지 시스템으로 영역을 확장하더라도 단일 서버 내에서 사용되는 디스크의 확장된 형태 정도, 혹은 Array 형태 정도가 될 것으로 예상됩니다.\n신사업 추진단에서도 스토리지 시스템 사업의 주체를 고민하면서 메모리사가 아닌 새로운 주체가 필요하다는 내부 의견을 가지고 있는 것으로 알고 있습니다.\n기술원은 Big Data로부터 계층적 패턴 발견을 가능하게 하는 플랫폼의 일부분으로서 지능형 고성능 분산 스토리지 SW 기술을 연구하고 있습니다. 이러한 영역은 메모리사의 사업 영역을 넘는 것입니다. \n저희가 개발하는 분산 스토리지 SW 기술의 일부분이 메모리사의 사업 영역에 적용되어 기존제품의 부가가치를 높이고 Array 형태 신제품의 seed 기술로 활용될 수 있습니다. 이것은 저희가 과제를 진행하면서 by-product 형태로 메모리 사업에 기여하는 것이지, 과제의 중심은 아닙니다.\n저희가 사업에 단기에 기여할 수 있는 측면을 강조하다보니, SSD Array 얘기를 강조한 바 있습니다만, 과제의 전체 목적이 그것이 아니라는 점을 말씀드리고 싶습니다.\n\n \n	Behavior Analytics 과제는 무엇인가\n\n스마트 기기가 얼마나 스마트해 질 것인가는 결국 기기가 얼마나 깊이 사용자를 이해할 수 있는가의 문제라고 할 수 있습니다.\nBehavior Analytics 과제는 디바이스의 센서 데이터, 디바이스 사용 이력, 웹 데이터 등을 분석하여 사용자의 Behavior패턴을 이해하고 dynamic하게 변하는 사용자의 감정과 의도, 필요한 정보와 서비스의 종류 등 High-end Context를 추론하는 기술을 확보하는 것을 목적으로 하고 있습니다.\n\n모바일 스마트 기기, 특히 스마트폰은 사용자의 일상생활 전 영역에서 활용이 되는 기기가 되었습니다. 디바이스의 기능도 갈수록 많아지고, 디바이스를 통해 access하는 서비스의 종류도 갈수록 늘어나고 있습니다. 나아가서 다른 기기를 제어하는 데에도 사용되고, 기기 간에 서비스가 연결되고 통합되고 있습니다. 이런 상황에서 사용자가 늘 어느 특정 통로을 통하여 다양한 기기의 기능, 앱, 서비스를 access하게 될 때, 그 통로는 디바이스 및 서비스의 생태계에서 길목 혹은 Service Gateway가 되고, 그 길목을 장악한 기업이 디바이스 및 서비스 생태계를 지배하게 됩니다. 그러한 통로, 즉, 사용자가 다양한 기기의 기능과 서비스를 이용하기 위하여 반복적으로 이용하는 통로를 1st screen이라고 부릅니다.\n가장 대표적인 예가 PC 기반의 웹 세상에서 구글의 검색 페이지입니다. PC에서 인터넷을 사용할 때 다른 웹사이트의 주소를 외우기 보다는 구글 검색을 통해 들어가는 경우가 많습니다. 구글은 검색 페이지를 장악함으로써 웹 광고 시장을 장악하였고, 현재의 구글이 될 수 있었습니다. 그러나 스마트폰의 작은 스크린 사이즈와 입력의 불편함으로 인해, 웹페이지 보다는 디바이에 설치되는 앱이 정보와 서비스의 이용수단이 되었습니다. 구글의 검색은 스마트폰에서 더 이상 1st screen이 되지 못하고 있습니다. 스마트폰에서는 사용자들은 구글 검색을 이용하기 보다는 필요한 정보를 제공해 주는 앱들을 설치해서 앱들을 통해서 정보를 제공받고 있습니다. 하지만 스마트폰에서의 1st screen을 장악하려는 시도가 메이저 player들에 의해 진행되고 있습니다. 애플의 시리는 음성 비서 기능을 제공하여 검색과 스마트폰을 통한 다양한 트랜잭션 서비스, 모바일 commerce까지 장악하려는 전략적인 시도이고, 최근 구글이 시작한 Google Now도 사용자가 필요한 정보들을 push해 줌으로써, 사용자로 하여금 타사의 앱이 아니라 구글 Now을 통해서 대부분의 정보를 받게 하려는 시도입니다.\n1st Screen을 장악하면 사용자의 royalty를 높이게 되고, 또한 1st Screen은 광고와 mobile commerce 등 새로운 revenue 창출을 가능케 합니다. 또한 1st Screen을 통해 새로운 기기와 서비스 시장도 창출할 수 있게 됩니다.\n1st Screen은 사용자가 여러 기능과 서비스를 쉽고 빠르게 access 할 수 있게 해 주는 것이 핵심기능입니다. 이것이 사용자 UX의 요체라고 할 수 있는데, 스마트기기의 UX는 4단계의 발전단계로 구별할 수 있습니다. 첫 번째는 사용자가 요구하는 것에 수동적으로 반응하는 것입니다. 음악 앱을 실행시키면 저장된 음악이 순서대로 나오는 것이 이러한 예입니다. 두 번째는 사용자의 위치나 시간 등 단순한 context 정보를 이용하여 기기의 기능과 서비스를 조정하는 것입니다. 식당을 찾으면 사용자의 위치와 가까운 식당부터 열거되는 것이 예입니다. 세 번째는 사용자가 세팅해 놓은 정보의 종류 안에서 사용자가 무엇이 필요할 지를 추론하여 정보나 서비스를 제안 혹은 push 하는 것입니다. 버스 정류장에서 구글 Now을 실행시키면 이 사용자의 집으로 가는 노선 버스가 언제 도착할 지를 보여주는 것이 이러한 예입니다. 구글 Now를 예로 든다면  rule 기반으로 push할 정보와 서비스가 정해져 있어 정확성이 높지 않고, 사용자의 behavior가 바뀌었을 때 adaptation을 하지 못한다는 문제가 있습니다. 네 번째는 복합적인 정보 소스를 활용하고, 사용자의 상태/상황에 대한 깊이 있는 이해에 기초하고, 특히 사용자의 행동 패턴의 변화에 맞추어 더욱 정교하게 기기의 기능과 서비스를 조정하거나 필요한 정보와 서비스를 push하는 것입니다. 네 번째 단계의 UX를 특징짓는 키워드는 High-end Context, Dynamic Adaptation, Proactive 입니다. 현재 스마트기기의 UX는 세 번째 단계로 진입하여 발전하고 있습니다. 본 과제는 네 번째 단계의 UX를 위한 핵심 기술을 개발하는 것이 목표입니다.\n1st Screen의 장악을 위해서는 Natural UI와 앞서 말씀드린 감정과 의도 등 High-end Context, Dynamic Adaptation, Proactive의 특성을 갖는 고도의 지능화된 UX를 제공하고 그 뒤에 많은 서비스가 연결이 되는 것이 필요합니다.\n\n요약을 한다면, Behavior Analytics에서 확보하려는 감정과 의도, 필요한 서비스의 종류 등 High-end Context를 추론하는 기술은 미래 스마트기기 시장에서 생태계의 헤게모니를 잡는데 핵심적인 1st Screen를 장악하는데 있어 필수적인 지능형 UX의 차세대 핵심 기술이라고 할 수 있습니다.\n\n\n \n	스마트폰에서 감정 인식을 해서 어디에 사용하나?\n\n앞으로 스마트기기에서의 중요한 경쟁력은 사용자를 얼마나 이해하고 그 이해에 바탕하여 사용자가 기기를 사용하는 과정을 편리하게 만들면서 사용자가 기기에 친근감을 느끼게 하는가 입니다. \n스마트폰이 사용자의 감정을 이해하고 반응하는 것은 사용자로 하여금 기기에 대한 친근감을 더욱 가지게 할 것이고, 제공되는 정보나 칸텐츠의 종류를 사용자의 감정정보까지 이용하여 사용자에게 맞춘다면, 사용자의 만족도는 배가될 것입니다.\n감정인식은 스마트기기에서 사용자가 다양한 기능과 서비스를 이용하기 위하여 반복적으로 사용하는 서비스게이트 역할을 하는 1st Screen의 주요한 기능입니다. 1st Screen를 장악하는 업체가 서비스 생태계를 장악하게 되는데, 그 이유는 사용자들이 1st Screen을 통해 각종 서비스를 이용하게 되기 때문입니다. 웹브라우저의 구글 서치 홈페이지가 PC 웹 세계에서의 1st Screen 이었습니다. 이를 이용해 구글은 웹 광고 시장의 절대강자가 되어 지금의 구글이 있게 되었습니다. Apple은 시리라는 음성 비서 기능을 스마트폰의 1st Screen으로 만들려고 하고 있습니다. 1st Screen을 장악하면 사용자의 royalty를 높이게 되고, 또한 1st Screen은 광고와 mobile commerce 등 새로운 revenue 창출을 가능케 합니다. 또한 1st Screen을 통해 새로운 기기와 서비스 시장도 창출할 수 있게 됩니다.\n1st Screen은 사용자가 여러 기능과 서비스를 쉽게 빠르게 이용할 수 있게 해 주는 것이 핵심기능입니다. 이것이 사용자 UX의 요체라고 할 수 있는데, 스마트기기의 UX는 4단계의 발전단계를 가집니다. 첫번째는 사용자가 요구하는 것에 수동적으로 반응하는 것입니다. 음악 앱을 실행시키면 저장된 음악이 순서대로 나오는 것이 이러한 예입니다. 두번째는 사용자의 위치나 시간 등 단순한 context 정보를 이용하여 기기의 기능과 서비스를 조정하는 것입니다. 식당을 찾으면 사용자의 위치와 가까운 식당부터 나오거나, 버스 정류장에서 구글 Now을 실행시키면 이 사용자의 집으로 가는 노선 버스가 언제 도착할 지를 보여주는 것이 이러한 예입니다. 세번째는 복합적인 정보 소스를 활용하고 특히 사용자의 상태/상황에 대한 깊이있는 이해에 기초하여\n\n이것을 위해서는 Natural UI와 더불어 사용자가 어떠한 상황에 있고 무엇을 원하는지를 파악하는 것이 필요합니다.\n감정인식 기술은 S-Voice와 같은 agent에 대한 사용자의 친근감을 획기적으로 높일 것입니다. 예를 들면, 사용자가 우울한 상황에서 ‘음악을 들려줘’라고 명령했을 때, 그냥 평소 좋아하는 음악을 틀어주는 것에 비해, 사용자가 우울한 상황에서 즐겨듣던 음악을 골라 틀어주거나, 한걸음 나아가 ‘아무개님 우울하시군요. 이 음악을 골라봤습니다.’라고 멘트를 하고 사용자가 우울한 상황에서 즐겨든던 음악을 골라주는 것은, 사용자에게 매우 다른 느낌을 주게 될 것입니다.\n\n\n구체적인 예를 들면, 광고 플랫폼에서 개인별 관심도를 반영하여 보여줄 광고를 선택하는데 사용될 수 있습니다. 모바일 광고 시장이 ‘20년에는 $54B 규모로 전망이 되는데, pay per click이 중심인 웹광고 모델에서 사업 경쟁력을 갖기 위해서는 사용자가 관심을 가질 만한 광고를 선택해서 display하는 것이 중요합니다. 사용자가 어떤 유형의 광고에 관심을 보이는지, 즉 광고에 대한 몰입도를 인식하여 광고 선택에 활용하는 것은 모바일 광고 플랫폼의 경쟁력을 높일 것입니다.\n결론적으로, 사용자의 감정까지 인식하고 그에 맞추어 디바이스의 기능과 서비스를 조절하는 기술은 미래 스마트 기기 UX의 중요한 경쟁력이 될 것입니다.\n\n \n	스마트폰에서 의도예측이 뭐냐?\n\n스마트폰의 기능/서비스는 사용자의 입력에 따라 일반적인 룰에 따라 반응하는 수동형에서 사용자의 위치 등과 같은 단순하고 쉽게 알 수 있는 context를 이용하여 반응의 내용을 조절하는 단순한 context aware 단계로 발전했고, 근래에는 사용자의 위치 등에 더불어 일정 정보까지 활용하여 사용자가 미리 지정한 타입의 정보를 push해 주는 능동형 서비스로 발전해왔습니다. 구글 Now가 이러한 서비스의 예가 됩니다.\n앞으로는 사용자의 context를 더욱 다양한 소스를 통해서 파악하면서 모두에게 적용되는 일반적인 룰이 아니라 개별 사용자의 특성, 습관에 맞추어 정보를 제공하거나 \n스마트폰은 정보검색, 음악 듣기, 비디오 시청, 채팅 등 다양한 용도로 사용되면서 개별 사용자들이 사용하는 앱의 종류와 수도 매우 많아지고 있고, 개별 앱에서 목적하는 기능을 실행시키기까지의 단계도 짧지 않은 상태입니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n[과제 설명]\n\nBehavior Analytics과제는 스마트 디바이스가 사람의 행동과 상황을 체계적으로 이해할 수 있도록 하는 지능형 소프트웨어 기술 확보를 목표로 하고 있습니다.\n\n현재의 스마트 디바이스는 많은 편리한 기능을 제공하고 사용자가 컨텐츠들을 골라서 소비할 수 있게 해 주는 도구이지만 그 기능들을 사용하기 위해서는 사람이 디바이스와 서비스에 대해서 배워야 하기 때문에 사람이 스마트해져야만 사용할 수 있는 디바이스라고 할 수 있습니다. 사람에게 한 차원 높은 편리함과 편안한 느낌을 주기 위해서는 기계가 정말로 스마트해지면서 사람의 행동을 배우면서 사람에게 적응하여 그 사람에게 맞추어진 기능과 서비스를 제공할 수 있어야만 합니다.\n\n지금 삼성의 스마트폰과 스마트 TV는 세계 1위이지만 미래 스마트기기를 차별화할 수 있는 기술로 사람을 이해할 수 있는 소프트웨어 기술을 준비해야 합니다.\n\n디바이스가 사용자의 상황을 알아내어 맞추어진 서비스로 Context Awareness 서비스가 시도되어 왔습니다. 주로 시간과 사용자가 있는 장소 등 확실히 알 수 있는 상황에 따라 다른 컨텐츠와 기능을 제공하는 것으로, 특정 장소에 도착하면 그 지역에서 판매하는 광고나 지역 안내가 제공되는 정도의 수준입니다. 보다 다양한 사람에 대한 컨텍스트를 활용하는 시도들도 스마트폰에서 시작되고 있으며 그 하나 예가, 사용자의 얼굴과 눈감음 등을 컨텍스트로 활용하는 Face Unlock, Smart Stay 등입니다. 이러한 기능은 이미 제품 차별화를 위한 주요 요소로 작용하고 있습니다.\n\n저희가 바라보는 Context-aware의 미래는 사람의 생활습관과, 감정, 그리고 기능과 서비스를 사용하려는 의도 등의 보다 차원 높은 컨텍스트를 활용하는 것입니다. 사용자의 감정을 정확하게 알게 되면, 사용자가 기기와 서비스를 사용할 때의 사용자 경험의 질을 평가하고 이를 활용할 수 있게 됩니다. \n\n어떤 음악을 들었을 때 어떤 App을 사용했을 때 만족도가 높아졌는지, 어떤 TV 채널에서 몰입도가 높았는지를 평가할 수 있으며 사용자들 사이에 감정을 교류할 수 있는 방법을 제공할 수 있습니다. 또한 사용자가 현재 상황에서 어떤 기능과 서비스를 이용하고 싶을지 그 의도를 예측할 수 있다면, 사용자가 일일이 메뉴를 선택하고 검색을 하지 않아도 사용자가 쓸만한 정보와 컨텐츠를 추천하고 제공함으로써 스마트 기기의 편리성을 높일 수 있을 것입니다.\n\n앞에서 말씀드렸다시피, 감정을 인식하고 의도를 예측하는 기술은 무선사, VD 사 등 스마트 기기를 개발하는 사업부에서 관심을 가지고 있습니다. 무선사업부는 컨텐츠 추천에 사용자 감정인식을 활용하기 위해서 기술 확보를 작년부터 시도 중입니다만, 정확한 감정인식 기술은 당장에 상용화할 수 있는 수준의 기술이 없어서 올해에는 적용을 하지 못했습니다. 얼굴 표정을 읽는 기술부터 단계적으로 제품에 적용할 것으로 예상됩니다.\n\n감정인식 기술은 학계나 벤쳐 업체를 제외하고는 뚜렷한 경쟁사를 찾을 수 없는 상황이어서, 저희가 연구 개발에 집중할 경우 기술을 선도해 나갈 수 있을 것으로 생각합니다. 사용자의 기기 사용 의도 예측은 검색 정보를 활용한 Google Now가 작년에 Release되었습니다. 아직은 사용자가 일일이 설정을 해야 하는 기능들이 많고, 서비스의 영역도 정보 검색과 관련된 부분에 국한되고 있습니다. 저희가 사용자의 행동 전체를 활용하여 보다 차원 높은 서비스를 제공함으로써 구글보다 앞선 지능 기술을 확보하도록 연구하겠습니다.\n\n[스마트 디바이스의 트렌드]\n스마트 디바이스 시장은 앞으로 커다란 패러다임의 변화가 예상됩니다. 현재의 스마트 디바이스는 기능과 컨텐츠를 다양하게 제공하여 사용자가 일일히 찾아서 기능을 실행해야 하지만 미래의 스마트 기기는 사용자의 현재 상태와, 필요한 기능을 미리 예측하여 알아서 제공할 수 있는 똑똑한 비서로 발전할 것입니다. 이를 위해서 감정의 인식과 사용 의도의 예측을 위한 기술 연구가 필요합니다. \n\nQuestion: 어디에다가 쓸 것인가?\n한마디로 말씀드리면 스마트폰을 똑똑한 비서로 만들 수 있습니다.\n일일이 시키지 않아도 의도를 예측하여 필요한 기능을 수행하는 능동형 서비스가 가능하고\n사용자의 성향과 선호도에 가장 알맞은 컨텐츠를 제공하는 개인 맞춤형 서비스가 가능해집니다.\n\n예를 들면 모바일 광고 시장이 2012년 640억불에서 2016년 2360억불로 증가하고 있습니다. 광고사업에서는 Pay per Click 모델 즉 사용자로 하여금 클릭을 하게 만들어야 수익이 창출되는 구조로서, 사용자가 클릭할 가능성이 가장 큰 광고를 선택하여 보여 주는 것이 핵심 기술입니다. 광고는 사용자의 감정에 호소하여 선택을 유도하는 것이므로 사용자가 어떤 감정 상태에 있는지, 광고를 보았을 때 어떤 감정이 유발되는지를 아는 것이 클릭을 유도하는 광고의 종류를 예측하는데 중요하며, 현재의 광고 플랫폼을 한 차원 높게 발전시킬 수 있는 핵심 기술로 기대합니다. \n\n사용자의 의도를 예측하는 능동형 서비스의 예로는 스마트폰을 네비게이션 용도로 사용하는 경우를 들 수 있습니다. 사용자가 자동차에 타서 운전을 하려고 하면 스마트폰은 네비게이션 용도로 사용될 것을 예측하고 네비게이션 앱을 실행합니다. 더 나아가서 학습된 사용자의 운전 경로 패턴을 토대로 지금 시점에서의 목적지를 정확하게 예측하고 이를 네비게이션의 목적지로 설정할 수 있습니다. 사용자는 여러 단계의 입력 없이도 자신이 원하는 기능을 스마트폰이 실행해 주어 편리함을 경험할 수 있습니다.\n\n무선사업부는 컨텐츠 추천의 한 요소로서 사용자의 현재 감정을 인식하는 기술을 찾고 있지만 현재 상용화된 기술을 찾지 못하고 있습니다.\nDMC연은 멀티미디어 프레임워크를 개발할 계획이며 프레임웍의 한 기능으로 제공하기 위해 사진과 동영상으로부터 인물의 감정을 찾아내는 기술을 찾고 있습니다.  \n \n\nQuestion: 왜 기술원이 해야 하는가?\n정확한 사용자의 감정인식 기술은 학계와 벤쳐 업체에서 기술 연구가 시도되고 있지만 사람마다 다른 특성을 모두 고려할 수 없는 한계에 부딪혀 있어 상용 수준의 기술이 없는 분야입니다. Affectiva는 얼굴 표정과 손목 센서를 이용해서 사람의 흥분 정도를 측정하는데 80% 정도의 정확도를 보여 주고 있습니다만, 스마트기기에서 적용할 수는 없는 기술입니다.\n의도 예측은 구글에서 연구 중입니다만 실제로 상용화된 것은 위치 정보와 미리 지정한 정보를 연결하는 제한된 수준으로 그나마도 우리가 Access할 수 없으며 그 외에 상용화된 기술은 없습니다.\n\nQuestion: 다른 업체들과의 격차는 어떠냐?\n감정인식의 기초기술이라고 할 수 있는 얼굴 표정 기반 분석의 경우 지금까지 개발한 Mobile 기기에서의 얼굴표정 기반 인식기의 성능은 높은 수준에 있으며 학계나 중소업체의 솔류션과 비교하면 더 높은 수준입니다. 의도 예측은 이제 시작하려고 합니다. \n\nQuestion: 정말로 격차를 내서 잘 할 수 있냐?\n굉장히 어려운 문제이지만 기본적인 Context인식, physical activity 등에 대한 인식 기술 연구 경험을 갖고 있어서 고차원의 context인식 연구의 바탕을 삼고, Machine Learning 분야의 새로운 이론을 적용하고 개선하여 기술을 확보하겠습니다. \n\nQuestion: 지능형 UX는 무엇인가?\n사용자의 명령에 단순히 반응하는 것이 아니라, 사용자가 과거에 했던 행동과, 사용자의 현재 상태를 이해하여 사용자에게 adaptation해서 진화하고, 사용자의 입력 없이도 알아서 기능을 수행하는 것입니다.\n예를 들면 회의실에 가면 미리 설정하지 않아도 알아서 진동으로 바뀌고, 자동차에 앉으면 네비게이션으로 바뀌고 목적지가 세팅되는 것 등입니다. 9시에 TV를 켜면 내가 켜면 뉴스가 나오고 와이프가 켜면 드라마가 나오는 등입니다.\n\nQuestion: 지금 5개 해서 80퍼센트인데, 20개를 더 해서 90퍼센트 이상이 가능한가? 그러면 얼마나 걸리는가?\n현재의 경쟁사들의 approach는 데이터를 많이 모아서 패턴을 찾는 방식이고, 사람의 인식 방법과는 근본적으로 달라, 성능향상의 가능성이 희박합니다. 그래서 우리는 사람의 인식 방법에서 착안한 Hierarchical Temporal Recognition 기술을 연구하려고 합니다.\n\nQuestion: 예상 확보 시기는?\n완벽한 기술 확보는 현재로부터 5년 후로 2017년까지 확보하겠습니다.\n\nQuestion: Affectiva가 뭐하는 회사인가? \nMIT Media Lab에서 감성 컴퓨팅이라는 분야를 개척한 Rosalind Picard라는 교수가 벤쳐로 창업을 해서 피부센서와 얼굴 분석을 통한 감정인식 기술 분야에서 가장 앞서 나가는 회사입니다. \nQuestion: 대기업은 없는가?\n음성인식의 Nuance가 음성을 사용한 감정인식을 개발중이라고 알려졌으며, 애플은 감정인식 분야는 공개된 특허나 관련회사의 M&A 활동 등이 파악되지 않았습니다. \n\nQuestion: 사업부에서 필요하다고 하는가?\n무선사에서 UX 차별화와 컨텐츠 추천을 위해 감정인식기술의 필요성을 인지하고 기술 확보를 위해 노력 중입니다. 하지만 현재는 얼굴표정 인식 등 제한된 기술만이 가용하여, 실제 활용하기에 어려운 상태입니다. 인간 수준으로 다양한 정보를 종합하여 정확한 인식을 하는 기술을 개발/확보할 필요가 있습니다.\nVD사에서는 시청자의 몰입도 등의 심리상태 파악 기술을 필요로 하고 있습니다.\nMSC에서는 Chat-On에서 사용자의 감정을 인식하고 전달하는 기술을 필요로 하고 있습니다.\n\nQuestion: 의도예측을 하고 나면 뭘 할 거냐?\n개인화된 서비스, 능동적인 기능 수행이 가능합니다.\n\nQuestion: Behavior Analytics인데 감정만 연구하는가?\n신체활동 등의 생활습관에 대한 인식을 포함하고 있으며, 보다 고차원의 컨텍스트 인식을 위해서 연구를 발전시키고 있습니다.\n\n\n\n	Big Data 란?\n-	기존 데이터베이스 관리도구의 데이터 수집•저장•관리•분석의 역량을 넘어서는 대량의 정형 또는 비정형 data set.\n-	기존 데이터베이스에서 다루던 데이터에 비해 세가지 다른 특징을 가짐.\n①	양 (volume)\n한 기업이 다루는 양이 수십 terabytes (TB)를 넘어 petabytes (PB)에 도달하는 경우도 이미 적지 않음. 분석을 위해서는 디스크로부터 읽어야 하는데, 널리 쓰이는 HDD로 1TB를 읽기 위해서는 대략 2시간이 걸림. 100TB는 읽는 데에만 8일 이상이 걸리게 됨. 따라서 데이터의 저장과 분석을 네트웍으로 연결된 다수의 서버에 분산시켜 처리해야만 의미있는 분석 속도를 얻을 수 있음.\n\n(참고)\n최고사양 PCI express 버스 전송속도 (16 lane, v3.0) : 15.75 GB/s\nNAND based SSD disk drive 속도 (Samsung 840 Pro):  read 380MB/s write 300MB/s\nHDD disk drive 속도 (7200rpm SATA):  < 170MB/s\n10G Ethernet throughput: < 1GB/s\n당사 S.LSI 제조공정에서 나오는 센서 데이터의 양: 219 TB/year (’12)\n당사 모바일 서비스 로그 양: 총 29TB (’11)\n당사 모바일 서비스 데이터 양: 0.029PB(‘11), 1.6PB(‘12)\nA사 모바일 서비스 데이터 양: 214PB(‘11), 480PB(‘12)\n전세계 digital data 양: 0.8 zettabytes (’00)  35 zettabytes (’20)\n\n②	속도 (velocity)\n생성되는 속도가 매우 빠름. 데이터를 생성하는 단말의 수, 사용자의 수, 센서의 수가 매우 크기 때문임. Internet of Things 혹은 trillion devices로 표현되듯이 네트웍에 연결된 디바이스의 수는 지속 증가가 예상됨.\n\n	(참고)\n트위터에서는 하루 2억개의 트윗이 발생 (’11)  7TB/day\nFacebook에는 2억5천만명이 매일 사진을 업로드하고 있음 (’11)  10TB/day\n삼성 모바일 계정 수: 4천만명 (’12)\n당사 S.LSI 제조공정에서 나오는 센서 데이터의 양: 0.6 TB/day (’12)\n전세계 대기업들이 2010년에 생성하고 저장한 데이터 양: 7 exabytes \n\n③	다양성 (variety)\n과거 데이터베이스에는 잘 정의된 schema에 따라 record 형태로 데이터가 저장.\n요즘 데이터의 80%는 자연어 text, 이미지, 센서 데이터, 비디오 등 비정형 데이터.\n\n\n	Big Data 기술이 중요한 이유\n-	Big Data로부터 이전에는 생각할 수 없었던 가치가 생성되며, Big Data를 활용하여 경영의 다양한 분야에서 생산성 향상과 경쟁력 강화를 달성할 수 있는 가능성이 열림  Big Data의 활용 능력이 기업 경쟁력의 중요한 요소가 됨.\n-	세계 경제 포럼은 2012년 떠오르는 10대 기술 중 그 첫 번째를 Big Data 기술로 선정하였으며, 대한민국 지식경제부 R&D 전략기획단은 IT 10대 핵심기술 가운데 하나로 Big Data 기술을 선정.\n-	스토리지 용량 증대/비용 감소, Big Data 분석 기술의 발전에 따라 이전까지는 분석할 수 없어 버리던 데이터를 모으고 분석하는 것이 가능해지고 있어, 새로운 활용 사례 및 가치 창출의 가능성이 크게 열려 있음.\n\n(참고)\n-	Big Data의 비즈니스 활용 사례\n	구글: 구글은 전세계의 웹페이지들이라는 big data와 구글 서치 사용자의 서치 로그를 종합하여 독보적인 웹서치 서비스를 구현.\n	구글: 수억건의 번역된 text 자료를 이용하여 58개 언어에 대한 자동번역 서비스 구현. (IBM은 수백만건의 자료를 이용하였으나 실패함.)\n	아마존: 구매 이력 데이터 분석을 통한 효과적인 상품 추천  매출의 30%가 추천으로 발생.\n	IBM: 자연어 텍스트로부터 Question/Answering 기능 개발  인공지능 컴퓨터 Watson  의료 및 금용 솔류션화\n	온타리오 병원: 미숙아 생체신호 분석을 통한 감염 조기 감지 (의료진보다 24시간 조기 발견)\n	마이크론: 제조 설비의 운영 데이터를 분석하여 설비 활용 효율을 10% 향상  추가 설비 없이 생산 능력 10% 향상\n\n\n	삼성이 Big Data 기술을 확보해야 하는 이유\n	사업전략 접합성\nBig Data 분석 능력을 확보하고 비즈니스의 다양한 방면에 활용하여 현재와 미래의 비즈니스 경쟁력을 강화할 수 있기 때문임.\n\n삼성에서 big data의 기존 비즈니스 활용 분야:\n-	제조 공정 big data 분석을 통한 제조 효율 향상 및 생산 비용 절감  가격경쟁력, 수익성 개선  다양한 GBM의 제조 경쟁력 강화\n-	미디어/모바일 서비스 로그 분석을 통한 서비스 향상 (검색 결과 개선, 서비스 만족도 향상, 매출 증대)  단말 생태계 강화  단말 제품 사업 경쟁력 강화\n-	타겟 광고 차별화로 모바일 광고 신사업 경쟁력 강화\n-	SNS분석, 블로그, 뉴스 분석을 통한 소비자/시장/사회 동향 분석  광고/마케팅 전략 향상\n-	단말에서의 차별화 기능 및 서비스 창출 \n예)\n	mixed reality를 위한 scene understanding 기술 (대규모 이미지 데이터에서 유사한 이미지를 찾고, 의미를 추출하는 기술)\n	사용자 상황/context에 적합한 정보 검색 기술 (예를 들면, SNS에서 관련 정보를 찾아내어 제공하는 기술)\n\n	사올 수 없는가? Analytics 측면\n-	개별 big data의 고급 분석은 데이터의 종류 및 분석 목적에 따라 데이터의 처리 및 분석 방식/알고리즘이 달라짐으로 package화된 제품을 구입하여서 해결하기 어려움. (단순 통계와 같은 간단한 분석은 package화된 제품으로 해결할 수 있음.)\n-	부분적인 요소 기술의 seed는 산학을 활용할 수 있으나, 상용 수준으로 해당 서비스/기능을 개발하는 것을 산학에 의지할 수 없음 (학교의 역량 및 연구 성격의 한계).\n-	제조 공정 big data 분석 기술을 확보한 vendor를 S.LSI에서 찾아보았으나 (MS Research, Google, Oracle, Samsung SDS/Teradata 접촉) 필요한 기술을 확보한 vendor를 찾지 못했음.\n-	따라서 Global 기업들은 관련 기술을 자체 개발하고 있음. (예, 구글, GE, 아마존, …)\n\n	사올 수 없는가? Platform 기술 측면\n-	독자적인 데이터센터를 구축해야 하는가는 현재 합의가 되지 않은 상황이나, 서비스 사용자수가 어느 threshold를 넘어서면 독자적인 데이터센터 구축이 경제적으로 합리적이 됨. \n-	Infrastructure의 규모가 매우 커지게 되면, infrastructure 구축과 운영 비용 또한 매우 커지게 되기 때문에, 구축과 운영을 효율화하기 위한 기술이 사업의 bottom line에 중요하게 됨. 구글, 아마존, 야휴 등은 cloud 구축과 운영을 위한 기술 연구에 많은 투자를 하고, 독자적인 기술을 활용하고 있음. 이들 업체들은 cloud에 들어가는 서버도 독자 설계하고 OEM으로 납품받아 설치하고 있으며, 현재 big data처리에 널리 쓰이는 open source 툴들의 핵심 기술들을 개발하였음.\n-	Big Data 플랫폼으로 가장 널리 쓰이는 open source인 hadoop 기반의 플랫폼은 여러 한계를 가짐.\n	끊임없이 생성되는 stream data의 실시간 분석을 지원하는데 한계를 보임.\n	연결된 서버 수가 어느 이상 되면 성능 저하 (scalability의 한계).\n	서버 내 이기종의 프로세서가 있어도 활용하지 못함.\n\n이러한 이유로 Big Data에서의 선진업체들 (구글, 아마존 등)은 hadoop을 넘어서는 독자적인 플랫폼 기술을 지속적으로 개발함.\n-	패키지화된 상용 솔류션들은 big data appliance라는 제품군으로 시장에 나와 있는데, 일반적인 기업 시장을 목표로 한 제품들이기 때문에 scalability 측면의 한계, 분석툴 측면에서는 R과 같은 표준화된 통계툴과 데이터베이스만 제공하는 한계를 가지고 있음. \n-	플랫폼 측면에서는 일단 open source hadoop 기반으로 구축하여 비실시간 analytics를 중심으로 활용하고, 실시간 analytics를 위한 비용과 성능이 최적화된 플랫폼을 구축하여 실시간 analytics를 구현하는 전략이 바람직함.\n\n	플랫폼 신사업의 가능성\n-	당사에서는 서버 사업을 신사업으로 준비 중임.\n-	첫번째 타겟은 저전력 서버인 마이크로 서버 시장임.\n-	혁신적인 Big Data 플랫폼은 미래 서버 사업에서 강력한 제품군이 될 수 있음.\n-	혁신적인 Big Data 플랫폼 개념 :\n	Distributed computing과 heterogeneous computing을 지원 (분산된 서버들 사이에서 GPU 및 이기종 co-processor를 지원하며 효율적으로 태스크를 자동 분배).\n	Big Data의 실시간 분석을 가능케하는 분산 데이터 저장 및 access 지원.\n	동적 distributed heterogeneous computing을 위한 프로그래밍 환경.\n	Petabytes을 넘어 Exabytes 규모의 데이터 저장과 분석을 지원.\n	Big Data에 대한 고급 analytics을 가능케하는 machine learning 알고리즘 구현.\n\n\n\n	생태계 내에서의 의미\n	기술원이 해야 할 이유\n	사내 생태계에서의 위치\n	역할 분담\n	기술 성숙도\n	핵심 경쟁력의 원천\n	그 경쟁력의 성숙도\n	사업화 예상 시기\n	예상 시장 규모\n	당사 예상 시장 점유 규모\n\n\n\n \n\n(참고)\nComputing 용량은 지속 발전하여 2018년 경에는 exascale 급 슈퍼컴퓨터 등장 예상. \n2001년                         2011년\n슈퍼컴퓨팅(tera flops)            슈퍼컴퓨팅(peta flops)\n데스크탑컴퓨팅(giga flops)        데스크탑컴퓨팅(tera flops)\n모바일컴퓨팅(mega flops)         모바일컴퓨팅(giga flops)\n\n\nExascale computing refers to a computer system capable of reaching performance of at least one exaflops. Such capacity would represent a thousandfold increase over the currently existing petascale[1] (one exaflop is a thousand petaflops). On the basis of a supercomputing conference held in December 2009, Computerworld projected its implementation by 2018.[2]\nIn January 2012 Intel purchased the InfiniBand product line from QLogic for US $125 million in order to fulfill its promise of developing Exascale technology by 2018.[3]\nThe initiative has been endorsed by two US agencies: the Department of Energy and the National Nuclear Security Administration.[4] The technology would be useful in various computation-intensive research areas, including basic research, engineering, earth science, biology, materials science, energy issues, and national security.[5]\nThe United States has put aside $126 million for exascale computing beginning in 2012. [6]\nThree projects aiming at developing technologies and software for Exascale Computing have been started in 2011 within the European Union. The CRESTA project (Collaborative Research into Exascale Systemware, Tools and Applications)[7], the DEEP project (Dynamical ExaScale Entry Platform)[8], and the project Mont-Blanc[9].\nThe National Science Foundation is responsible for initiating and funding several petascale computers in the USA, as well as DARPA who gave IBM the contract to develop the petascale PERCS (Productive, Easy-to-use, Reliable Computer System) platform.\nChina has developed two petascale computers, Nebulae and Tianhe-I.\nRussia has developed the Lomonosov petascale computer.\nOther countries, such as Germany and Japan, have plans of their own for petascale computers.\nPetascale computers are under development from manufacturers such as Sun Microsystems, Cray, IBM, Dawning, SGI, and NEC.\nActive\nAs of 2012, these are the known active petascale computers in the world.\n•	Roadrunner, built by IBM, was the first computer to go petascale, and did so on May 25, 2008, with sustained performance of 1.026 petaflops.\n•	XT5 \"Jaguar\", built by Cray, was the second, later in 2008. After an update in 2009, its performance reached 1.759 petaflops.[1]\n•	SGI Pleiades which went online in 2008, reaching 600 TFLOPS, reached petascale in 2012.\n•	Nebulae built by Dawning, was the third petascale computer and the first built by China with a performance of 1.271 petaflops in 2010.\n•	Tianhe-1A built by NUDT, at 2.566 petaflops in 2010.\n•	K computer built by Fujitsu, is the second fastest supercomputer in the world, at 8.162 petaflops in 2011.\n•	Tsubame built by NEC/HP\n•	Cielo built by Cray\n•	Hopper built by Cray\n•	Tera 100 built by Bull SA\n•	IBM Sequoia\n•	Cray Titan, an updated version of Jaguar.\nThe first 20 supercomputers on the June 2012 list are petascale.\nAs of June 2012, the Green500 list rates BlueGene/Q, Power BQC 16C as the most efficient supercomputer on the TOP500 in terms of FLOPS per watt, running at 2,100.88 MFLOPS/watt.[3]\n\nGeForce 9800 GT Green Edition  462 GFlops at 75 Watts\n	\nGeForce 9800 GT Green         504 GFlops at 125/105 Watts\n\n\nhttp://en.wikipedia.org/wiki/Google_platform\n\nAccording to Google their global data center operation electrical power ranges between 500 and 681 megawatts.[6][7] The combined processing power of these servers might reach from 20 to 100 petaflops.[8]\n\n\nhttp://news.inews24.com/php/news_view.php?g_serial=714321&g_menu=020200\n\nIBM 글로벌 비즈니스 서비스 사업부 마이클 슈록 글로벌 인포메이션 매니지먼트 리더는 \"대부분의 기업들이 빅데이터가 기업의 의사 결정과 사업 결과를 개선시킬 수 있다는 데에 동의하지만 빅데이터를 어떻게 사용할 것인가에 대해서는 여전히 어려워한다\"면서 \"빅데이터 시장은 아직 초기 단계지만 선두 기업들은 빅데이터를 통해 이미 막대한 가치를 얻기 시작했다\"고 말했다.\n\nEMC가 IDC와 공동으로 조사한 디지털 유니버스 보고서도 \'빅데이터 시장이 활성화될 것\'으로 전망하며 폭증하는 데이터를 수용하기 위한 하드웨어, 소프트웨어, 서비스, 통신 장비, IT 전문 인력에 대한 투자도 급증할 것으로 분석했다.\n\nIDC는 아시아태평양 지역의 빅데이터 기술과 서비스를 포함한 시장이 향후 5년간 연평균 46.8%의 고성장세를 기록할 것으로 전망하면서 2016년에는 17억 6천만 달러에 이를 것으로 예상했다.','utf-8'),(25,'== Formulation: IOWA based Proactive Data Placement ==\n\n== Key Modules ==\n\n=== Define: the ultimate questions ===\n\n* IO에 대한 이해가 스토리지 시스템 혹은 데이터센터 클러스터의 성능에 어떻게 기여할 수 있나?\n:- Bottleneck pinpointing & prediction: IO bottleneck 사전 방지 목적 (발생 시 신속한 해소도 가능할까?)\n::- 별도의 hardware component로 구현 가능한지? 구현해야만 하는 이유가 있는지?\n:- Future I/O Prediction: Proactive Data Placement 목적\n::- 별도의 hardware component로 구현 가능한지? 구현해야만 하는 이유가 있는지?\n\n=== Collect: Multi-modal trace data (IO trace log + other related data) ===\n\n* Multi-modal Tracer Requirements\n:- User/Process/FileType/FileName/DirectoryPath/Time/Previous&Current-IOTrace 등의 정보들이 서로 sync되는 형태로 수집될 수 있어야 함\n::- Log data presentation type SHOULD be re-configurable\n\n* Linux native file system에서의 IO trace\n:- 3달 (현재 SDS 이형주 차장 협력, 혹은 소프트웨어멤버십)\n::- Linux Kernel Module 형태로 시스템에 쉽게 삽입 가능하면 좋겠음\n\n* 분산 file system (e.g., [[IBM GPFS]])에서의 IO trace [http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=%2Fcom.ibm.cluster.gpfs.v3r5.0.7.gpfs500.doc%2Fbl1pdg_mmtrace.htm]\n:- 3달 (SDS 이형주 차장 협력)\n::- IBM GPFS에서의 IO trace log gathering 및 synchronized multi-modal information gathering 엔진 구현 필요\n\n* Notes\n:- 멤버십학생 및 SDS 두 곳을 활용할 수 있다면, 멤버십은 Linux native file system에서의 synchronized multi-modal trace 메커니즘을 개발하고, IBM GPFS에서의 multi-modal trace 메커니즘은 SDS의 이형주 차장과 함께 협력하는 것이 적절할 수 있겠음. (accessibility)\n:- IBM GPFS를 무료로 설치할 수 있는지?\n\n\n=== Determine: presentation manner ===\n\n* Data presentation issue\n:- 하나의 데이터를 여러 다른 형태의 정보로 표현할 수 있음. 그 정보의 형태에 따라서 의미있는 발견이 이루어질 수도 있고, 그렇지 않을 수도 있음.\n:- 따라서 동일한 데이터셋에 대해서 presentation을 여러 다른 형태로 변경하면서 data mining을 실행할 필요 있음\n\n=== Monitor: Multi-modal Data ===\n\n* On-the-spot Multi-modal Data Monitoring\n:- blktrace-like mechanism to relay system information to our monitoring engine (via debug file system - e.g., /sys/kernel/debug/block/xxx), but more than blktrace in terms of collecting IO-triggering user info, LBA-to-name info, ...\n\n\n=== Predict (runtime): Future IO ===\n\n* Online IO prediction (sidewinder)\n:- On-the-spot prediction of IO\n\n* Objectives of IO prediction\n:- Bottleneck prediction and then prevention\n:- Boosting IO performance (proactive data placement)\n\n=== Manipulate: IO ===\n\n* Action of proactive data placement\n:- Vertical placement in the local node (e.g., among DRAM-SSD-HDD)\n:- Horizontal placement across the nodes (e.g., among distributed nodes)\n\n== Dominant IO Pattern Mining (using Association Rules Mining) ==\n\n* IO 궤적 (X1) 정보를 포함하는, 다양한 측면의 정보 X\'s들로 구성된 I = {selected-subset-of-{X1, X2, X3, ...}} 정보가 주어졌을 때, I가 어떻게 구성될 때 가장 X1(prev)로부터 X1(next)로의 규칙성/패턴이 잘 설명될 수 있을까? 예를 들어 t1 구간에서 Zone 1에서 (어떤 패턴의) IO가 발생했다고 했을 때, 다음 t2 구간에서는 어느 위치 (Zone ?)에서 IO가 발생하게 될까?\n\n* 그냥 IO 궤적 (X1)만 봐서는 Zone 1에서 IO가 발생한 다음에 어느 곳에서 IO가 발생할지를 예측케 하는 패턴이 보이지 않을 수 있다 (즉, Zone 1 다음에 꼭 Zone 2로만 가는 것이 아니라, Zone 3으로 갈 수도 있고, Zone 17로 갈 수도 있으므로 X1만 보아서는 규칙성을 발견하기 어려울 수 있다는 의미임). 그러나, 추가 정보를 결합하여 관찰함으로써, 적은 정보만으로는 보이지 않던 패턴들을 찾아낼 수도 있다. I1 경우 (<- IO 궤적 (X1) 정보와 시간 (X2) 정보를 결합), 혹은 I2 경우 (<- IO 궤적 (X1) 정보와 user category (X3) 정보를 결합), 혹은 I3 경우 (<- IO 궤적 (X1) 정보와 process category (X4) 정보를 결합), 혹은 I4 경우 (<- IO 궤적 (X1) 정보와 시간 (X2) 정보, user category (X3) 정보를 결합), 혹은 I5 경우 (<- IO 궤적 (X1) 정보와 시간 (X2) 정보, user category (X3) 정보, 그리고 process category (X4) 정보를 결합) 등 다양한 I의 경우에 대해서 관측을 해보게 되면, 이 중에서 의미 있는 패턴을 찾아내는 데 도움이 되는 조합이 존재할 수 있다.\n\n* 이때, 의미 있는 패턴을 찾아내는 데 도움이 되는 하나 이상의 I 케이스가 찾아질 수 있는데, 그 중에서도 가장 비용효율적인 I 케이스를 선택할 수 있겠다. 물론, 이러한 고정확도의 비용효율적인 I 케이스는 IO workload 타입마다 다를 수 있다. 즉, web server workload의 경우에는 I3이 적합하다고 판단되었지만, file server workload의 경우에는 I5가 적합하다고 판단될 수도 있다. 따라서 다양한 IO workload에 따라, 그리고 이 IO workload가 발생하는 system의 architecture/status 등에 따라서 최적의 I 선택은 달라질 수 있다.\n\n* 그리고 각 정보 X\'s 들이 어떠한 형태로 presentation 되는가에 따라서 의미 있는 패턴 발견에 기여할 수도, 그렇지 않을 수도 있다. 시간 정보 (X2)를 예로 들어보면, X2r1은 elapsed seconds (from beginning of observation), X2r2는 0h-23h 형태의 daily hours, X2r3는 Mon-Sun 형태의 week days, X2r4는 1-31 형태의 month days 라고 하자. Workload W1의 경우에, X2r1으로 했을 때는 보이지 않던 패턴이 X2r2으로 했을 때 의미있는 패턴이 관측될 수도 있다. Workload W2의 경우에는 X2r2로도 보이지 않던 패턴이 X2r3으로 했을 때 보일 수도 있다.\n\n:- 우선은 규칙성을 갖는 X\'s, Y\n\n== Encoding/presentation of X\'s ==\n\n* Notation\n: \'Xirj\': presentation type j for Xi\n\n* X1: Place\n:- X1r1: logical block address (LBA)\n:- X1r2: file object\n:- X1r3: directory (full path) // e.g., \"/usr/local/bin/\"\n:- X1r4: directory (leaf node) // e.g., \"bin/\"\n:- X1r5: directory (first N depth) // e.g., \"/usr/local/\"\n:- X1r6: node\n:- X1r7: rack\n:- X1r8: zone in the data center deployment architecture\n\n* X2: Time\n:- X2r1: elapsed time in (micro) seconds from the beginning of observation(tracing)\n:- X2r2: 24-hour presentation (0h-23h)\n:- X2r3: week days (Mon-Sun)\n:- X2r4: days in each month (1-31)\n:- X2r5: month (1-12)\n:- X2r6: year (..., 2013, 2014, ...)\n:- X2r7: other presentation of time\n\n* X3: User Category\n:- X3r1: user (identifiable by UID)\n:- X3r2: group (identifiable by GID)\n:- X3r3: sub-category of user group\n:: // e.g., normal user group 중에서도 통계적 분석을 주로 하는 user들, source code build를 주로 하는 user들, excel, powerpoint 등 office 계열 작업을 주로 하는 user들, web browsing을 주로 하는 user들, 등등으로 보다 상세히 재 구분 될 수 있음\n\n* X4: Process\n:- X4r1: process ID (PID)\n:- X4r2: parent process ID (PPID)\n:- X4r3: process session (which ties related processes together)\n:- X4r4: process name\n:- X4r5: process category (to be defined by domain expert)\n:: // e.g., text parser, system message logger, DBMS, web server, mail server, print server, image editor, audio editor, video editor, image viewer, file system journaling, kernel worker, ...\n\n* X5: Data type\n:- X5r1: file type (by extension or by magic number)\n:: // X5r2 (file category)보다도 더 자세한 구분임. 예를 들어 X5r2 (file category)에서는 image file 범주로 묶였지만 다시 jpeg, png, gif, tif, 등으로 세분될 수 있음\n:- X5r2: file category (to be defined by domain expert)\n:: // e.g., configuration, script, binary executable, library, database table, database log, system log, image, audio, video, productivity, normal text, HDFS file image, HDFS metadata, ...\n\n* X6: I/O pattern\n:- X6.1: read/write\n:- X6.2: random/sequential\n:: // read-modify-write, write-read-read-read ..., write-write-write ... , 등과 같이 read와 write 간의 인과관계 혹은 연관관계를 찾아낼 수 있을까?\n:: // 최종 leaf directory는 다르지만, depth N에서는 같은 directory path를 공유하고 있는 해당 address zone에서, random read와 sequential write가 각각 70:30 비율로 동시에 발생하고 있으며, 열려 있는 file들의 갯수의 비율은 random read 중인 files, sequential write 중인 files가 80:1 정도의 비율임.\n:- X6.3: currently-hot/cold // size of time-window to define \'current\'\n:- X6.4: access periodicity\n:- X6.5: access recency\n:- X6.6: co-accessed place (by the same process)\n\n\n* 추가 고려 사항들\n:- X2에서, 시간적인 Granularity는 얼마나 fine-grained하게, 혹은 어떻게 slicing/partitioning을 해야할까?\n:- X1에서, 공간적인 Granularity는 얼마나 fine-grained하게, 혹은 어떻게 slicing/partitioning을 해야할까?\n \n\n\n\n== Issues ==\n\n=== Co-use: data mining & machine learning ===\n\nIOWA에 있어서 어떤 부분에서는 발견 (data mining)이 필요하고 어떤 부분에서는 학습 (machine learning)이 필요할 수 있다. Data mining을 통해서 숨겨진 주요 패턴들의 존재를 파악할 수 있다. Machine learning을 통해서 변화에 적응하고, 아직 오지 않은 어떤 상황을 예측할 수 있다.\n\n\n==== Observation, Modeling ====\n:- Bottleneck Prediction (BNP) 관점에서, 그리고 IO Prediction (IOP) 관점에서, multi-modal information (IO와 기타 시스템 정보들)을 어떻게 수집하고 표현하고 사용할 수 있을것인가? 우선은 IO Prediction 관점에서 본 후, 이후에 Bottleneck Prediction 관점에서 보는 것으로 하자. (bMemo-2013-03-25)\n:- Association Rules Mining을 위해서는 데이터 모델이 필요함. (Market baskset 경우를 예로 들면, I->j와 같은 association rule에서 I는 basket에 담긴 itemset, j는 그 itemset이 존재할 때 같이 bakset에 담겨질만한 item)\n\n\n==== Data mining (DM) ====\n:- Hidden dominant I/O pattern 발견\n::- by use of Association Rules Mining (Market Basket Analysis)\n\n\n==== Machine learning (ML) ====\n:- 기존 발견된 dominant I/O pattern (혹은 dominant C&E pattern)들과 정확히 매칭되지는 않지만, 본질적인 측면에서 유사하다고 볼 수 있는 pattern들을 학습 (by machine learning)','utf-8'),(26,'== Dual Inline Memory Module ==\n\n\n* [http://en.wikipedia.org/wiki/DIMM DIMM (dual in-line memory module)]\n\n\n: A DIMM or dual in-line memory module, comprises a series of dynamic random-access memory integrated circuits. These modules are mounted on a printed circuit board and designed for use in personal computers, workstations and servers. DIMMs began to replace SIMMs (single in-line memory modules) as the predominant type of memory module as Intel P5-based Pentium processors began to gain market share.\n\n\n: The main difference between SIMMs and DIMMs is that DIMMs have separate electrical contacts on each side of the module, while the contacts on SIMMs on both sides are redundant. Another difference is that standard SIMMs have a 32-bit data path, while standard DIMMs have a 64-bit data path. Since Intel\'s Pentium has (as do several other processors) a 64-bit bus width, it requires SIMMs installed in matched pairs in order to complete the data bus. The processor would then access the two SIMMs simultaneously. DIMMs were introduced to eliminate this practice.','utf-8'),(27,'== .gnuplot ==\n\n <pre>\n\nset macro\n\n#####  Color Palette by Color Scheme Designer\n#####  Palette URL: http://colorschemedesigner.com/#3K40zsOsOK-K-\n\n   blue_000 = \"#A9BDE6\" # = rgb(169,189,230)\n   blue_025 = \"#7297E6\" # = rgb(114,151,230)\n   blue_050 = \"#1D4599\" # = rgb(29,69,153)\n   blue_075 = \"#2F3F60\" # = rgb(47,63,96)\n   blue_100 = \"#031A49\" # = rgb(3,26,73)\n\n   green_000 = \"#A6EBB5\" # = rgb(166,235,181)\n   green_025 = \"#67EB84\" # = rgb(103,235,132)\n   green_050 = \"#11AD34\" # = rgb(17,173,52)\n   green_075 = \"#2F6C3D\" # = rgb(47,108,61)\n   green_100 = \"#025214\" # = rgb(2,82,20)\n\n   red_000 = \"#F9B7B0\" # = rgb(249,183,176)\n   red_025 = \"#F97A6D\" # = rgb(249,122,109)\n   red_050 = \"#E62B17\" # = rgb(230,43,23)\n   red_075 = \"#8F463F\" # = rgb(143,70,63)\n   red_100 = \"#6D0D03\" # = rgb(109,13,3)\n\n   brown_000 = \"#F9E0B0\" # = rgb(249,224,176)\n   brown_025 = \"#F9C96D\" # = rgb(249,201,109)\n   brown_050 = \"#E69F17\" # = rgb(230,159,23)\n   brown_075 = \"#8F743F\" # = rgb(143,116,63)\n   brown_100 = \"#6D4903\" # = rgb(109,73,3)\n\n   grid_color = \"#d5e0c9\"\n   text_color = \"#6a6a6a\"\n\n   my_font = \"SVBasic Manual, 12\"\n   my_font_file = \"~/local/share/fonts/defaults/LiberationMono-Regular.ttf\"\n   my_export_sz = \"1024,768\"\n\n   my_line_width = \"2\"\n   my_axis_width = \"1.5\"\n   my_ps = \"1.2\"\n   my_font_size = \"14\"\n\n# must convert font fo svg and ps\n#set term svg  size @my_export_sz fname my_font fsize my_font_size enhanced dynamic rounded\n# set term png  size @my_export_sz large font my_font\n# set term jpeg size @my_export_sz large font my_font\n#set term wxt enhanced font my_font\n\nset style data linespoints\nset style function lines\nset pointsize my_ps\n\nset style line 1  linecolor rgbcolor blue_025  linewidth @my_line_width pt 7\nset style line 2  linecolor rgbcolor green_025 linewidth @my_line_width pt 5\nset style line 3  linecolor rgbcolor red_025   linewidth @my_line_width pt 9\nset style line 4  linecolor rgbcolor brown_025 linewidth @my_line_width pt 13\nset style line 5  linecolor rgbcolor blue_050  linewidth @my_line_width pt 11\nset style line 6  linecolor rgbcolor green_050 linewidth @my_line_width pt 7\nset style line 7  linecolor rgbcolor red_050   linewidth @my_line_width pt 5\nset style line 8  linecolor rgbcolor brown_050 linewidth @my_line_width pt 9\nset style line 9  linecolor rgbcolor blue_075  linewidth @my_line_width pt 13\nset style line 10 linecolor rgbcolor green_075 linewidth @my_line_width pt 11\nset style line 11 linecolor rgbcolor red_075   linewidth @my_line_width pt 7\nset style line 12 linecolor rgbcolor brown_075 linewidth @my_line_width pt 5\nset style line 13 linecolor rgbcolor blue_100  linewidth @my_line_width pt 9\nset style line 14 linecolor rgbcolor green_100 linewidth @my_line_width pt 13\nset style line 15 linecolor rgbcolor red_100   linewidth @my_line_width pt 11\nset style line 16 linecolor rgbcolor brown_100 linewidth @my_line_width pt 7\nset style line 17 linecolor rgbcolor \"#224499\" linewidth @my_line_width pt 5\n\n#set style line 1  linecolor rgbcolor \"#a0bae9\" linewidth @my_line_width pt 7\n#set style line 2  linecolor rgbcolor \"#ff7f7f\" linewidth @my_line_width pt 5\n#set style line 3  linecolor rgbcolor \"#80c65a\" linewidth @my_line_width pt 9\n#set style line 4  linecolor rgbcolor \"#ffcc7f\" linewidth @my_line_width pt 13\n#set style line 5  linecolor rgbcolor \"#dedc06\" linewidth @my_line_width pt 11\n#set style line 6  linecolor rgbcolor \"#7711ff\" linewidth @my_line_width\n#set style line 7  linecolor rgbcolor \"#ff0000\" linewidth @my_line_width\n#set style line 8  linecolor rgbcolor \"#008000\" linewidth @my_line_width\n#set style line 9  linecolor rgbcolor \"#ff9900\" linewidth @my_line_width\n#set style line 10 linecolor rgbcolor \"#aa9900\" linewidth @my_line_width\n#set style line 11 linecolor rgbcolor \"#990066\" linewidth @my_line_width\n#set style line 12 linecolor rgbcolor \"#990000\" linewidth @my_line_width\n#set style line 13 linecolor rgbcolor \"#003971\" linewidth @my_line_width\n#set style line 14 linecolor rgbcolor \"#76a4fb\" linewidth @my_line_width\n#set style line 15 linecolor rgbcolor \"#d5e0c9\" linewidth @my_line_width\n#set style line 16 linecolor rgbcolor \"#e5ecf9\" linewidth @my_line_width\n#set style line 17 linecolor rgbcolor \"#224499\" linewidth @my_line_width\n\n## plot 1,2,3,4,5,6,7,8,9\nset style increment user\nset style arrow 1 filled\n\n## used for bar chart borders\nset style fill solid 0.5\n\n# Grey background\n#set object 1 rectangle from screen 0, screen 0 to screen 1, screen 1 behind fc  rgbcolor \"#cccccc\"\n\nset size noratio\nset samples 300\n\nset xtics textcolor rgb text_color font my_font\nset ytics textcolor rgb text_color font my_font\nset xlabel \"X Label (unit)\" textcolor rgb text_color font my_font\nset ylabel \"Y Label (unit)\" textcolor rgb text_color font my_font\nset label textcolor rgb text_color font my_font\n\nset title \"Top Title\" textcolor rgb text_color font my_font\n\nset border 31 lw @my_axis_width lc rgb text_color\n\nset grid lc rgb grid_color\nset key outside box width 2 height 2 enhanced spacing 2\n\n\n</pre>','utf-8'),(28,'== Data ==\n <pre>\n\n3282792808\n3282793000\n3282793096\n720616224\n3765476016\n728218248\n730235992\n817521776\n721421456\n3712136296\n724475672\n724475864\n724475960\n828267560\n895981360\n3277437312\n726381104\n726381496\n725469848\n726793728\n723112360\n3677713128\n3773683448\n730730224\n725235304\n828274704\n896778072\n3277440104\n725008864\n724481496\n724727104\n720987560\n545503608\n896780128\n3076725824\n3069345832\n3767545408\n3752324048\n721386480\n721386960\n725420616\n3066446120\n727735192\n726671672\n827916488\n721030800\n721030896\n721411176\n829399208\n829403504\n725068856\n3736491280\n726432144\n726432336\n3639248608\n3103097424\n3729607304\n3072645072\n3076312720\n3085619744\n3100418040\n746217392\n3628461000\n3734766024\n746217520\n3078022768\n3090797216\n3072006216\n3720237440\n721020072\n3628467984\n3736488464\n727777744\n727777840\n727777968\n726793472\n726793536\n726793760\n727357248\n727357344\n727357408\n727357536\n3768245104\n722890080\n3628534800\n720733600\n725324832\n3628536056\n726709584\n3770217696\n724038192\n721020200\n721020488\n721020616\n3088595240\n3102279880\n3078151248\n3078151312\n724524648\n3077245584\n723077128\n725323456\n730948592\n730948720\n730948888\n3744455904\n727926784\n727926880\n727926976\n576700216\n725349592\n722147208\n3754034920\n3754035272\n3795617760\n3744583192\n3770225000\n3849309928\n722095544\n721020712\n721020904\n721021000\n728448888\n728449080\n728449208\n720714864\n720967280\n723434544\n723118416\n723118512\n723118672\n723118768\n722291520\n722291776\n722118256\n722118416\n722118608\n729451608\n725560656\n3082594920\n3090865984\n720798752\n720798880\n720798944\n720799040\n721021288\n721021384\n721021480\n731305496\n728376368\n728376464\n728376656\n3753161152\n725280432\n725280496\n721743696\n721743792\n576705248\n723269440\n3315969936\n3775391456\n3761976320\n3744589296\n724979520\n3682938696\n3761977800\n722330536\n3853222504\n3081294136\n721021992\n721022312\n725422280\n722074080\n724231576\n3592182288\n3593993488\n720673768\n3311452248\n3311452472\n726802760\n3084979848\n723296264\n723296360\n723296424\n723296488\n723296552\n3705471152\n3761977928\n3853230224\n727718152\n728408784\n3686818080\n725999856\n725999920\n725999960\n726000152\n726000312\n3711565312\n722172160\n722172256\n722172448\n3083966960\n3645899472\n3316057216\n3592188376\n3068106752\n728205720\n728206176\n3705474808\n3758928848\n3747058600\n726000440\n726000504\n722302864\n722303088\n725704776\n3686697936\n3810389176\n724219632\n724219824\n575833352\n726000696\n726000888\n3316063176\n725288888\n3594635720\n725705384\n726767368\n3592188504\n723131128\n3595538344\n3719358608\n446863056\n726001080\n726001272\n726001432\n726735656\n3351322912\n3595227936\n3719363320\n724028296\n3742523600\n3728141880\n721856608\n726001624\n726001816\n3077554448\n3333261720\n722948184\n722948632\n3769593968\n724232056\n575838608\n721005848\n723420832\n725980640\n3758119208\n3754595872\n3344324224\n726001848\n726685576\n3803523760\n3333269808\n3754604040\n724013496\n3739844776\n3803526904\n453642952\n3769289240\n3749150472\n3750385816\n724234368\n725984072\n728206272\n739058912\n723328232\n723328488\n723328680\n576793184\n721715192\n523481352\n453644080\n3074037192\n3093278752\n3072984952\n723365704\n3808964824\n3066358944\n728207040\n728207360\n3808970592\n3102292208\n3075011872\n530435856\n723313912\n723314040\n723314136\n728207552\n728208000\n3091351568\n3099031544\n3068780616\n3082321072\n506416792\n3089662088\n530439288\n721855008\n721855072\n721855168\n721855264\n721855392\n721855488\n3094030144\n3101859496\n3070549232\n3072113296\n739023776\n3086880592\n3086880752\n3091896160\n3091896256\n3067674048\n506683336\n3098347672\n724899504\n721855552\n721855904\n577051912\n3078421088\n3083558088\n3083558184\n3083558312\n506684752\n721856096\n721856160\n721856288\n3087592936\n3080313272\n3080313464\n3080313560\n3080313624\n3080313752\n3088118560\n578455968\n3101019856\n3099428968\n723418784\n3092488152\n3097463416\n3097463480\n3097463576\n3097463640\n3079823752\n3093582160\n3070717512\n3073028928\n3073029120\n3066876976\n3066877264\n3095944032\n3094940448\n3083211080\n3083211400\n3074440024\n3076970168\n723418912\n723419008\n723419360\n3096875968\n723419424\n723419584\n723419680\n723419936\n723420064\n723420128\n723420288\n723420480\n723420736\n723420800\n723420864\n3103096624\n3081409592\n3097657728\n3071370264\n3067868560\n3067868816\n3076195424\n3080564504\n3100236144\n723097880\n723098040\n723098136\n3090795936\n730769368\n3080527080\n3065566936\n3065567160\n3077295904\n3091203000\n728378992\n728379056\n728379184\n3753485256\n3755592368\n722045512\n722045704\n722045800\n725222168\n725222328\n725222424\n724037904\n724801000\n727725992\n727389656\n727727696\n727727824\n727727888\n727727984\n725979424\n725979520\n725979616\n722689488\n720782352\n721381896\n721381992\n721382056\n721382120\n721382280\n723048360\n726060752\n724037008\n3076725760\n728409712\n3709153400\n3712289624\n3091876296\n3091876552\n723994984\n730973840\n3772806904\n3079262344\n728389528\n723889504\n3084687040\n3065818168\n3084040248\n3077960600\n724037552\n724037648\n724037712\n724037936\n724038032\n3072552920\n727351224\n726024040\n3354047664\n3798910256\n723060792\n3075855880\n554821352\n723998760\n3775431744\n3350278896\n722705248\n720767232\n720767296\n3751152936\n724038064\n724038160\n724038224\n724038288\n721038104\n729707744\n726083368\n726083496\n726083688\n3354053904\n554821320\n723624720\n723450848\n3257524472\n725415456\n724627672\n724706816\n725491496\n3354054552\n723624816\n723625040\n724623440\n721853184\n3832348312\n727354360\n554825128\n723995016\n3748654368\n723389312\n3711887336\n723995144\n723995240\n3596665632\n723610792\n3832350528\n731423832\n3080356808\n3596285568\n3101942200\n3066449640\n738873280\n724642240\n724642336\n724642528\n554825096\n3071377600\n3757910136\n726711440\n726711600\n3085888512\n724608192\n724608320\n792911384\n554821288\n3098411720\n3068461312\n3068461480\n721854112\n3067764712\n792919520\n3651712480\n724628696\n578670944\n554825064\n3651704440\n792952392\n576065528\n792954992\n3099060728\n3679494592\n3085496952\n554825032\n3101061336\n731049496\n724616712\n724616904\n724616968\n739028352\n3070816384\n3074504024\n3065467576\n3091939688\n3089691272\n3088145688\n3096915400\n3103534384\n3068820048\n554825000\n3094098272\n554824968\n3651704376\n554824936\n575975488\n3101210408\n3065557232\n3069431480\n3097504640\n3079865200\n3087644680\n3099464312\n3096736616\n3093320224\n554824904\n3082852560\n3096744752\n3060256272\n3082378952\n554824872\n554825256\n3066938976\n3066939104\n3078456416\n578744640\n578374320\n3095983464\n578410352\n3101903024\n3083901376\n3083901696\n3096384496\n578744192\n3092531680\n3098326528\n3085305224\n3077774584\n3076648720\n3076648912\n3076649008\n3103222744\n3077913976\n739000152\n3084142808\n739063464\n3091437992\n3782733160\n3090592648\n3091278296\n3065309904\n3094983992\n3076125344\n3076125536\n3076125696\n3611799864\n725737288\n725737576\n727476336\n3074070472\n3754376384\n3737178144\n3089100816\n3080701152\n3092635024\n3084673296\n3084673392\n3737569888\n812157800\n727730096\n727730384\n3071197232\n726749744\n3598339832\n3689823056\n3737185928\n3737570048\n3230985248\n724433000\n724433064\n724433384\n3070052328\n3071916400\n812159376\n3667509344\n816909824\n3591405784\n3231293448\n3732326576\n725387528\n3801297368\n731490104\n3591405912\n724549952\n724389344\n723675144\n723675432\n723675496\n3699431576\n3801303360\n724913376\n3766894544\n722637952\n731492704\n832084840\n3231300272\n3351322688\n835267872\n3699436864\n523479816\n730421080\n731193136\n724913600\n724913696\n724913792\n3755250296\n722638048\n722638272\n725884880\n724547288\n724581304\n724265240\n3748770992\n832091728\n836525336\n723030776\n3332470376\n523961648\n724366888\n725291136\n723439888\n3597603816\n724910432\n836529024\n3332475792\n724631864\n725650704\n725291200\n725291296\n3824177080\n523966904\n729934088\n727739576\n724261048\n724017984\n721610984\n523552856\n723443416\n836131728\n791082704\n723646248\n724018304\n724940056\n3754815048\n725735208\n731410288\n723443608\n723443704\n724679496\n725283448\n725283640\n725283736\n3795518312\n724548896\n724587456\n724587552\n724587712\n724587776\n725548384\n791189312\n836139864\n725827648\n3757832784\n835268064\n723342352\n723342672\n724555872\n725561840\n727775760\n791196168\n836672952\n725244848\n3078852032\n3078852256\n3078852352\n3731090712\n3087667712\n725682160\n3101247784\n3086554744\n3097552232\n3103584072\n3774255592\n836680704\n3073592096\n3073592320\n3100294472\n3074525016\n3066945608\n721234008\n721234136\n721234200\n721234264\n3079886176\n3094125400\n3067777496\n3067777592\n3082879944\n722082728\n3077040832\n3077040960\n3077041120\n3091972968\n3068836928\n3070823016\n3085923840\n3088187184\n3101973424\n3081690296\n3081690488\n3081690584\n3081690648\n3067880184\n3067880280\n3067880408\n3075350536\n3096956848\n3663612344\n3647830400\n3647836456\n3663243024\n721235584\n575881880\n738877856\n3065305648\n3066456272\n3068472048\n3068472368\n3099483240\n3093343256\n3101092568\n739032936\n3080960968\n722053616\n3071918680\n3086184616\n3097951888\n3080237600\n3072794032\n3080506960\n3103168296\n3103168392\n3103168456\n3091220928\n3083917200\n3087425832\n3065983496\n3065983592\n3618843608\n3094378464\n554683280\n729926304\n3060487552\n3638555672\n3618837744\n3101871920\n3638560512\n3768437000\n3077473056\n3618843768\n3638560672\n724219344\n3618964840\n3597243944\n3100711464\n3075459568\n724030152\n724030280\n727743056\n3097723776\n3097723936\n3071232368\n3667412416\n3691004216\n723400104\n3084378536\n3609964928\n3618971408\n722170400\n722170560\n722170880\n3609965800\n3259344576\n726711216\n724262232\n725239400\n3075884072\n3070201064\n725525456\n725525584\n3089301024\n3093007272\n3077526264\n3065640592\n726711248\n727419096\n575528624\n724178024\n723674688\n3806180680\n724945280\n3158428752\n724178152\n724178216\n724178344\n3092544392\n576255904\n3094617080\n725225568\n3174361960\n3806185392\n3080115472\n3088836840\n726711824\n816918792\n577786120\n727897792\n723113576\n576255544\n727387224\n3174363696\n3805985968\n726711984\n726712080\n723113640\n723113704\n723113768\n727898080\n725513152\n726712560\n3652080488\n3806229896\n726712752\n726712880\n726712944\n726713104\n3641075288\n727440496\n3806231376\n727440560\n727440784\n726713232\n3084023720\n3776380400\n3341894368\n730253960\n724750968\n494587672\n722814424\n3780166248\n722080744\n724158192\n722973904\n722080840\n722081000\n724935512\n577788648\n3341894528\n3776382264\n3101888832\n731389704\n722973968\n722974008\n722974264\n901494608\n3080576232\n730442864\n494590880\n3785138400\n721313768\n820546848\n902752048\n3318390232\n723068280\n724163576\n3081898000\n476768128\n3756137544\n791083408\n3785142824\n721313864\n721313928\n721314120\n820551048\n902759800\n3318390528\n3756145552\n727392352\n3676697320\n731218280\n721868464\n3771933160\n476773448\n791820736\n457382320\n724942624\n725482880\n3676697480\n722137760\n3072704344\n3685847712\n822014768\n791827144\n457386616\n721284552\n3078064832\n822170576\n870844152\n3637706544\n3654675584\n722055056\n725407608\n721284752\n721284976\n726128752\n728117896\n725218552\n822170744\n870855200\n3642881480\n3779707504\n3685634288\n457025608\n722055120\n722055216\n722055312\n722055376\n725157784\n721285200\n721285296\n728117992\n728118184\n725218680\n725218808\n725218936\n725219032\n\n</pre>','utf-8'),(29,'== R:: Introduction ==\n\n* R Tutorial [http://www.r-tutor.com/]\n\n\n== R:: Installation ==\n\n=== Installation of R statistical computing software ===\n\n* me@matrix$ sudo apt-get install r-base\n\n\n* to start R from command line, just type \'R\' in your terminal\n** me@matrix$ R\n\n<br/>\n\n=== RStudio ===\n\n* RStudio download [http://www.rstudio.com/ide/download/]\n** RStudio Desktop [http://www.rstudio.com/ide/download/desktop]\n**: If you run R on your desktop\n** RStudio Server [http://www.rstudio.com/ide/download/server]\n**: If you run R on a Linux server and want to enable users to remotely access RStudio using a web browser\n*** RStudio Server Getting Started [http://www.rstudio.com/ide/docs/server/getting_started]\n <pre>\nsudo apt-get install r-base\nwget http://download2.rstudio.org/rstudio-server-0.97.312-amd64.deb\nsudo gdebi rstudio-server-0.97.312-amd64.deb\nbsc.adm.create_me\ngoogle-chrome http://kandinsky:8787 # type ID/PW (me?!)\n</pre>\n\n\n* Debian Packages of R Software [http://cran.r-project.org/bin/linux/debian/README.html]\n\n<br/>\n\n== R:: Basics ==\n\n=== Factors in R ===\n* Factors in R (stat.berkeley.edu) [http://www.stat.berkeley.edu/classes/s133/factors.html]\n\nConceptually, factors are variables in R which take on a limited number of different values; such variables are often refered to as categorical variables. One of the most important uses of factors is in statistical modeling; since categorical variables enter into statistical models differently than continuous variables, storing data as factors insures that the modeling functions will treat such data correctly.\n\n <pre>\n> data = c(1,2,2,3,1,2,3,3,1,2,3,3,1)\n> fdata = factor(data)\n> fdata\n [1] 1 2 2 3 1 2 3 3 1 2 3 3 1\nLevels: 1 2 3\n> rdata = factor(data,labels=c(\"I\",\"II\",\"III\"))\n> rdata\n [1] I   II  II  III I   II  III III I   II  III III I\nLevels: I II III\n</pre>\n\n== R:: K-means Clustering in R ==\n\n* ?kmeans\n\n <pre>\nExamples\n\nrequire(graphics)\n\n# a 2-dimensional example\nx <- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),\n           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))\ncolnames(x) <- c(\"x\", \"y\")\n(cl <- kmeans(x, 2))\nplot(x, col = cl$cluster)\npoints(cl$centers, col = 1:2, pch = 8, cex=2)\n\nkmeans(x,1)$withinss # if you are interested in that\n\n## random starts do help here with too many clusters\n(cl <- kmeans(x, 5, nstart = 25))\nplot(x, col = cl$cluster)\npoints(cl$centers, col = 1:5, pch = 8)\n</pre>\n\n== R:: SVM (Support Vector Machine) in R ==\n\n\n=== SVM:: Installation ===\n\n <pre>\nme@matrix$ R\n> install.packages(\'e1071\')\n> library(e1071)\n</pre>\n\n <pre>\nme@kandinsky$ R\n> install.packages(\'e1071\')\nInstalling package(s) into ‘/home/me/R/library’\n(as ‘lib’ is unspecified)\ntrying URL \'http://cran.rstudio.com/src/contrib/e1071_1.6-1.tar.gz\'\nContent type \'application/x-gzip\' length 258910 bytes (252 Kb)\nopened URL\n==================================================\ndownloaded 252 Kb\n\n* installing *source* package ‘e1071’ ...\n** package ‘e1071’ successfully unpacked and MD5 sums checked\nchecking for C++ compiler default output file name... a.out\nchecking whether the C++ compiler works... yes\nchecking whether we are cross compiling... no\nchecking for suffix of executables... \nchecking for suffix of object files... o\nchecking whether we are using the GNU C++ compiler... yes\nchecking whether g++ accepts -g... yes\n** libs\ngcc -std=gnu99 -I/usr/share/R/include      -fpic  -O3 -pipe  -g -c Rsvm.c -o Rsvm.o\ngcc -std=gnu99 -I/usr/share/R/include      -fpic  -O3 -pipe  -g -c cmeans.c -o cmeans.o\ngcc -std=gnu99 -I/usr/share/R/include      -fpic  -O3 -pipe  -g -c cshell.c -o cshell.o\ngcc -std=gnu99 -I/usr/share/R/include      -fpic  -O3 -pipe  -g -c floyd.c -o floyd.o\ng++ -I/usr/share/R/include      -fpic  -O3 -pipe  -g -c svm.cpp -o svm.o\nsvm.cpp: In function ‘svm_model* svm_load_model(const char*)’:\nsvm.cpp:2707:24: warning: ignoring return value of ‘int fscanf(FILE*, const char*, ...)’, declared with attribute warn_unused_result [-Wunused-result]\nsvm.cpp:2711:25: warning: ignoring return value of ‘int fscanf(FILE*, const char*, ...)’, declared with attribute warn_unused_result [-Wunused-result]\nsvm.cpp:2733:25: warning: ignoring return value of ‘int fscanf(FILE*, const char*, ...)’, declared with attribute warn_unused_result [-Wunused-result]\nsvm.cpp:2754:33: warning: ignoring return value of ‘int fscanf(FILE*, const char*, ...)’, declared with attribute warn_unused_result [-Wunused-result]\nsvm.cpp:2756:33: warning: ignoring return value of ‘int fscanf(FILE*, const char*, ...)’, declared with attribute warn_unused_result [-Wunused-result]\nsvm.cpp:2758:33: warning: ignoring return value of ‘int fscanf(FILE*, const char*, ...)’, declared with attribute warn_unused_result [-Wunused-result]\nsvm.cpp:2760:36: warning: ignoring return value of ‘int fscanf(FILE*, const char*, ...)’, declared with attribute warn_unused_result [-Wunused-result]\nsvm.cpp:2762:29: warning: ignoring return value of ‘int fscanf(FILE*, const char*, ...)’, declared with attribute warn_unused_result [-Wunused-result]\nsvm.cpp:2768:36: warning: ignoring return value of ‘int fscanf(FILE*, const char*, ...)’, declared with attribute warn_unused_result [-Wunused-result]\nsvm.cpp:2775:37: warning: ignoring return value of ‘int fscanf(FILE*, const char*, ...)’, declared with attribute warn_unused_result [-Wunused-result]\nsvm.cpp:2782:38: warning: ignoring return value of ‘int fscanf(FILE*, const char*, ...)’, declared with attribute warn_unused_result [-Wunused-result]\nsvm.cpp:2789:38: warning: ignoring return value of ‘int fscanf(FILE*, const char*, ...)’, declared with attribute warn_unused_result [-Wunused-result]\nsvm.cpp:2796:35: warning: ignoring return value of ‘int fscanf(FILE*, const char*, ...)’, declared with attribute warn_unused_result [-Wunused-result]\ng++ -shared -o e1071.so Rsvm.o cmeans.o cshell.o floyd.o svm.o -L/usr/lib/R/lib -lR\ninstalling to /home/me/R/library/e1071/libs\n** R\n** inst\n** preparing package for lazy loading\n** help\n*** installing help indices\n** building package indices ...\n*** tangling vignette sources ...\n   ‘svmdoc.Rnw’ \n** testing if installed package can be loaded\n\n* DONE (e1071)\n\nThe downloaded packages are in\n	‘/tmp/RtmpXo1y7R/downloaded_packages’\n> library(e1071)\nLoading required package: class\n> library(e1071)\n> library(class)\n</pre>\n\n=== SVM:: Usage ===\n\n* [http://www.r-tutor.com/gpu-computing/svm/rpusvm-1 \"Support Vector Machine with GPU\"]]\n\n\n==== Support Vector Machines (?svm) ====\nsvm {e1071}	R Documentation\n\n===== Description =====\nsvm is used to train a support vector machine. It can be used to carry out general regression and classification (of nu and epsilon-type), as well as density-estimation. A formula interface is provided.\n\n===== Usage =====\n <pre>\n## S3 method for class \'formula\'\nsvm(formula, data = NULL, ..., subset, na.action = na.omit, scale = TRUE)\n## Default S3 method:\nsvm(\nx, y = NULL,\nscale = TRUE,\ntype = NULL,\nkernel = \"radial\",\ndegree = 3,\ngamma = if (is.vector(x)) 1 else 1 / ncol(x),\ncoef0 = 0,\ncost = 1,\nnu = 0.5,\nclass.weights = NULL,\ncachesize = 40,\ntolerance = 0.001,\nepsilon = 0.1,\nshrinking = TRUE,\ncross = 0,\nprobability = FALSE,\nfitted = TRUE,\nseed = 1L,\n...,\nsubset,\nna.action = na.omit\n)\n</pre>\n\n===== Arguments =====\n\n; formula	\n: a symbolic description of the model to be fit.\n\n; data	\n: an optional data frame containing the variables in the model. By default the variables are taken from the environment which ‘svm’ is called from.\n\n; x	\n: a data matrix, a vector, or a sparse matrix (object of class Matrix provided by the Matrix package, or of class matrix.csr provided by the SparseM package, or of class simple_triplet_matrix provided by the slam package).\n\n; y	\n: a response vector with one label for each row/component of x. Can be either a factor (for classification tasks) or a numeric vector (for regression).\n\n; scale	\n: A logical vector indicating the variables to be scaled. If scale is of length 1, the value is recycled as many times as needed. Per default, data are scaled internally (both x and y variables) to zero mean and unit variance. The center and scale values are returned and used for later predictions.\n\n; type	\n: svm can be used as a classification machine, as a regression machine, or for novelty detection. Depending of whether y is a [[factor]] or not, the default setting for type is C-classification or eps-regression, respectively, but may be overwritten by setting an explicit value.\n: Valid options are:\n::* C-classification\n::* nu-classification\n::* one-classification (for novelty detection)\n::* eps-regression\n::* nu-regression\n\n; kernel	\n: the kernel used in training and predicting. You might consider changing some of the following parameters, depending on the kernel type.\n:; linear:\n:: u\'*v\n:; polynomial:\n:: (gamma*u\'*v + coef0)^degree\n:; radial basis:\n:: exp(-gamma*|u-v|^2)\n:; sigmoid:\n:: tanh(gamma*u\'*v + coef0)\n\n; degree	\n: parameter needed for kernel of type polynomial (default: 3)\n\n; gamma	\n: parameter needed for all kernels except linear (default: 1/(data dimension))\n\n; coef0	\n: parameter needed for kernels of type polynomial and sigmoid (default: 0)\n\n; cost	\n: cost of constraints violation (default: 1)—it is the ‘C’-constant of the regularization term in the Lagrange formulation.\n\n; nu	\n: parameter needed for nu-classification, nu-regression, and one-classification\n\n; class.weights	\n: a named vector of weights for the different classes, used for asymmetric class sizes. Not all factor levels have to be supplied (default weight: 1). All components have to be named.\n\n; cachesize	\n: cache memory in MB (default 40)\n\n; tolerance	\n: tolerance of termination criterion (default: 0.001)\n\n; epsilon	\n: epsilon in the insensitive-loss function (default: 0.1)\n\n; shrinking	\n: option whether to use the shrinking-heuristics (default: TRUE)\n\n; cross	\n: if a integer value k>0 is specified, a k-fold cross validation on the training data is performed to assess the quality of the model: the accuracy rate for classification and the Mean Squared Error for regression\n\n; fitted	\n: logical indicating whether the fitted values should be computed and included in the model or not (default: TRUE)\n\n; probability	\n: logical indicating whether the model should allow for probability predictions.\n\n; seed	\n: integer seed for libsvm (used for cross-validation and probability prediction models).\n\n; ...	\n: additional parameters for the low level fitting function svm.default\n\n; subset	\n: An index vector specifying the cases to be used in the training sample. (NOTE: If given, this argument must be named.)\n\n; na.action	\n: A function to specify the action to be taken if NAs are found. The default action is na.omit, which leads to rejection of cases with missing values on any required variable. An alternative is na.fail, which causes an error if NA cases are found. (NOTE: If given, this argument must be named.)\n\n===== Details =====\n\nFor multiclass-classification with k levels, k>2, libsvm uses the ‘one-against-one’-approach, in which k(k-1)/2 binary classifiers are trained; the appropriate class is found by a voting scheme.\n\nlibsvm internally uses a sparse data representation, which is also high-level supported by the package SparseM.\n\nIf the predictor variables include factors, the formula interface must be used to get a correct model matrix.\n\nplot.svm allows a simple graphical visualization of classification models.\n\nThe probability model for classification fits a logistic distribution using maximum likelihood to the decision values of all binary classifiers, and computes the a-posteriori class probabilities for the multi-class problem using quadratic optimization. The probabilistic regression model assumes (zero-mean) laplace-distributed errors for the predictions, and estimates the scale parameter using maximum likelihood.\n\n===== Value =====\n\nAn object of class \"svm\" containing the fitted model, including:\n\nSV	\nThe resulting support vectors (possibly scaled).\n\nindex	\nThe index of the resulting support vectors in the data matrix. Note that this index refers to the preprocessed data (after the possible effect of na.omit and subset)\n\ncoefs	\nThe corresponding coefficients times the training labels.\n\nrho	\nThe negative intercept.\n\nsigma	\nIn case of a probabilistic regression model, the scale parameter of the hypothesized (zero-mean) laplace distribution estimated by maximum likelihood.\n\nprobA, probB	\nnumeric vectors of length k(k-1)/2, k number of classes, containing the parameters of the logistic distributions fitted to the decision values of the binary classifiers (1 / (1 + exp(a x + b))).\n\n===== Note =====\n\nData are scaled internally, usually yielding better results.\n\nParameters of SVM-models usually must be tuned to yield sensible results!\n\nAuthor(s)\n\nDavid Meyer (based on C/C++-code by Chih-Chung Chang and Chih-Jen Lin)\nDavid.Meyer@R-project.org\n\n===== SVM References =====\n\nChang, Chih-Chung and Lin, Chih-Jen:\nLIBSVM: a library for Support Vector Machines\nhttp://www.csie.ntu.edu.tw/~cjlin/libsvm\n\nExact formulations of models, algorithms, etc. can be found in the document:\nChang, Chih-Chung and Lin, Chih-Jen:\nLIBSVM: a library for Support Vector Machines\nhttp://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz\n\nMore implementation details and speed benchmarks can be found on: Rong-En Fan and Pai-Hsune Chen and Chih-Jen Lin:\nWorking Set Selection Using the Second Order Information for Training SVM\nhttp://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf\n\n===== See Also =====\n\npredict.svm plot.svm tune.svm matrix.csr (in package SparseM)\n</pre>\n\n\n===== Examples =====\n\n <pre>\ndata(iris)\nattach(iris)\n\n## classification mode\n# default with factor response:\nmodel <- svm(Species ~ ., data = iris)\n\n# alternatively the traditional interface:\nx <- subset(iris, select = -Species)\ny <- Species\nmodel <- svm(x, y) \n\nprint(model)\nsummary(model)\n\n# test with train data\npred <- predict(model, x)\n# (same as:)\npred <- fitted(model)\n\n# Check accuracy:\ntable(pred, y)\n\n# compute decision values and probabilities:\npred <- predict(model, x, decision.values = TRUE)\nattr(pred, \"decision.values\")[1:4,]\n\n# visualize (classes by color, SV by crosses):\nplot(cmdscale(dist(iris[,-5])),\n     col = as.integer(iris[,5]),\n     pch = c(\"o\",\"+\")[1:150 %in% model$index + 1])\n\n## try regression mode on two dimensions\n\n# create data\nx <- seq(0.1, 5, by = 0.05)\ny <- log(x) + rnorm(x, sd = 0.2)\n\n# estimate model and predict input values\nm   <- svm(x, y)\nnew <- predict(m, x)\n\n# visualize\nplot(x, y)\npoints(x, log(x), col = 2)\npoints(x, new, col = 4)\n\n## density-estimation\n\n# create 2-dim. normal with rho=0:\nX <- data.frame(a = rnorm(1000), b = rnorm(1000))\nattach(X)\n\n# traditional way:\nm <- svm(X, gamma = 0.1)\n\n# formula interface:\nm <- svm(~., data = X, gamma = 0.1)\n# or:\nm <- svm(~ a + b, gamma = 0.1)\n\n# test:\nnewdata <- data.frame(a = c(0, 4), b = c(0, 4))\npredict (m, newdata)\n\n# visualize:\nplot(X, col = 1:1000 %in% m$index + 1, xlim = c(-5,5), ylim=c(-5,5))\npoints(newdata, pch = \"+\", col = 2, cex = 5)\n\n# weights: (example not particularly sensible)\ni2 <- iris\nlevels(i2$Species)[3] <- \"versicolor\"\nsummary(i2$Species)\nwts <- 100 / table(i2$Species)\nwts\nm <- svm(Species ~ ., data = i2, class.weights = wts)\n</pre>\n\n\n== R:: Examples ==\n\n* Example R scripts [http://people.eng.unimelb.edu.au/aturpin/R/index.html]]\n\n* R by example [http://www.mayin.org/ajayshah/KB/R/index.html]\n\n* R example:: Kmeans [http://stat.ethz.ch/R-manual/R-patched/library/stats/html/kmeans.html]\n\n* Importing SAS, SPSS and Stata files into R [http://staff.washington.edu/glynn/r_import.pdf]\n\n* R Tutorial:: Data Import [http://www.r-tutor.com/r-introduction/data-frame/data-import]\n\n== R:: Troubleshooting ==\n\n\n\n=== matrix manipulation // 2013-01-28 ===\n\n\n; converting a matrix of lists to a regular matrix [http://stackoverflow.com/questions/1515193/converting-a-matrix-of-lists-to-a-regular-matrix]\n\n: Question (Problem)\n\n <pre>\n\nTake the following code:\n\nfoo <- list()\nfoo[[1]] <- list(a=1, b=2)\nfoo[[2]] <- list(a=11, b=22)\nfoo[[3]] <- list(a=111, b=222)\nresult <- do.call(rbind, foo)\nresult[,\'a\']\nIn this case, result[,\'a\'] shows a list.\n\nIs there a more elegant way such that result is a \"regular\" matrix of vectors?\nI imagine there are manual ways of going about this,\nbut I was wondering if there was an obvious step that I was missing.\n\n</pre>\n\n: Answer (Solution)\n:: do.call\n\n <pre>\ndo.call on lists is very elegant, and fast. In fact do.call(rbind, my.list) once saved my ass when I needed to combine a huge list. It was by far the fastest solution.\n\nTo solve your problem, maybe something like:\n\ndo.call(rbind, lapply(foo, unlist))\n\n\n> result.2 <- do.call(rbind, lapply(foo, unlist))\n> result.2\n       a   b\n[1,]   1   2\n[2,]  11  22\n[3,] 111 222\n> result.2[, \'a\']\n[1]   1  11 111\n>\n</pre>\n\n\n\n=== read.table() to numeric // 2013-01-27 ===\n\n; List to numeric vector or matrix\n: If you read lots of numerical data from a file using read.table(), then the type of the resultant data is set to list, which itself is not suitable type to be used in numerical calculation. So, you should flatten that list structure so that you get the numerical value, not the list structure. For this purpose, you can use \'unlist()\' function.\n <pre>\n> a1 <- read.table(\"/x/f/addr.10k.log\", header=F) # read the 1-column data from the file \"/x/f/addr.10k.log\"\n> typeof(a1)\n[1] \"list\"\n> a1_ul <- unlist(a1) # flatten (unlist) the data to be vector of numerical values\n> typeof(a1_ul)\n[1] \"double\"\n\n</pre>\n\n\n; List to numeric vector (stat.ethz.ch) [https://stat.ethz.ch/pipermail/r-help/2002-December/027879.html]\n\n <pre>\n[R] List to vector\n\nGregor Gawron gregor.gawron at rmf.ch \nThu Dec 12 15:23:03 CET 2002\nPrevious message: [R] List to vector\nNext message: [R] interfacing ranlib\nMessages sorted by: [ date ] [ thread ] [ subject ] [ author ]\nTry unlist()\n\n-----Original Message-----\nFrom: Vikentia Provizionatou [mailto:vprovi at essex.ac.uk] \nSent: Donnerstag, 12. Dezember 2002 14:51\nTo: r-help at stat.math.ethz.ch\nSubject: [R] List to vector\n\n\nHi,\n\nCan you help me to transform my list file to a numeric vector file?\n\nThanks a lot.\n\nVikentia\n\n______________________________________________\nR-help at stat.math.ethz.ch mailing list\nhttp://www.stat.math.ethz.ch/mailman/listinfo/r-help\n</pre>\n\n\n\n; List to \'integer or double\' in R [http://stackoverflow.com/questions/3814322/list-to-integer-or-double-in-r]\n\n: Questions (Problem)\n <pre>\nI have a list of about 1000 single integers. I need to be able to do some mathematical computations, but they\'re stuck in list or character form. How can I switch them so they\'re usable?\n\nsample data :\n\ny [[1]] [1] \"7\" \"3\" \"1\" \"6\" \"7\" \"1\" \"7\" \"6\" \"5\" \"3\" \"1\" \"3\" \"3\" \"0\" \"6\" \"2\" \"4\" \"9\" [19] \"1\" \"9\" \"2\" \"2\" \"5\" \"1\" \"1\" \"9\" \"6\" \"7\" \"4\" \"4\" \"2\" \"6\" \"5\" \"7\" \"4\" \"7\" [37] \"4\" \"2\" \"3\" \"5\" \"5\" \"3\" \"4\" \"9\" \"1\" \"9\" \"4\" \"9\" \"3\" \"4\" \"9\" \"6\" \"9\" \"8\" [55] \"3\" \"5\" \"2\" \"0\" \"3\" \"1\" \"2\" \"7\" \"7\" \"4\" \"5\" \"0\" \"6\" \"3\" \"2\" \"6\" \"2\" \"3\" [73] \"9\" \"5\" \"7\" \"8\" \"3\" \"1\" \"8\" \"0\" \"1\" \"6\" \"9\" \"8\" \"4\" \"8\" \"0\" \"1\" \"8\" \"6\" ...\n\nJust the first couple of lines.\n</pre>\n\n: Answers (Solution-1)\n:: see ?unlist\n <pre>\n\n> x\n[[1]]\n[1] \"1\"\n\n[[2]]\n[1] \"2\"\n\n[[3]]\n[1] \"3\"\n\n\n> y <- as.numeric(unlist(x))\n\n> y\n[1] 1 2 3\n\n</pre>\n\n\n\n: Answers (Solution-2)\n:: see ?numeric\n <pre>\n> x <- list(as.character(1:3))\n\n> x\n[[1]]\n[1] \"1\" \"2\" \"3\"\n\n\n> y <-as.numeric(x[[1]])\n\n> y\n[1] 1 2 3\n</pre>\n\n== Weka:: Introduction ==\n\n* WEKA Tutorial [http://www.cs.utexas.edu/users/ml/tutorials/Weka-tut/]\n\n\n\n== ML Related Articles ==\n\n=== Support Vector Machines (SVM) ===\n\n==== SVM concepts ====\n\n*; Support Vector Machines -- The Interface to libsvm in package e1071 [http://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf] [http://www.jstatsoft.org/v15/i09/paper]\n*: by David Meyer (Technische Universitat Wien, Austria), September 12, 2012\n*: SVMs were developed by Cortes & Vapnik (1995) for binary classification. Their approach may be roughly sketched as follows:\n*:* Class separation\n*:* Overlapping classes\n*:* Nonlinearity\n*:* Problem solution\n*: A program able to perform all these tasks is called a \'Support Vector Machine\'\n*: [[File:Svm classification fig linear separable case.png]]\n*: Several extensions have been developed; the ones currently included in libsvm are:\n*:; \'\'v\'\'-classification: allows more control over the number of support vectors by specifying an additional parameter \'\'v\'\' which approximates the fraction of support vectors\n*:; One-class-classification: tries to find the support of a distribution and thus allows for outlier/novelty detection\n*:; Multi-class classification: to allow multi-class classification, libsvm uses the one-against-one technique by fitting all binary subclassifiers and finding the correct class by a voting (basically, SVMs can only solve binary classification problems)\n*:; \'\'e\'\'-regression: the data points in between the two borders of the margin which is maximized under suitable conditions to avoid outlier inclusion\n*:; \'\'v\'\'-regression: with analogue modifications of the regression model as in the classification case\n\n==== SVM Software ====\n\n===== e1071 // TU Wien =====\n\n* e1071: Misc Functions of the Department of Statistics (e1071), TU Wien [http://cran.r-project.org/web/packages/e1071/index.html]\n\n===== SVM Software List =====\n* Review: Applications of Support Vector Machines in Chemistry, Rev. Comput. Chem. 2007, 23, 291-400 [http://www.support-vector-machines.org/SVM_soft.html]\n\n\n===== LIBSVM: A Library for Support Vector Machines =====\n\nA Library for Support Vector Machines // Chih-Chung Chang and Chih-Jen Lin [http://www.csie.ntu.edu.tw/~cjlin/libsvm/]\n\n*; Introduction\n*: LIBSVM is an integrated software for support vector classification, (C-SVC, nu-SVC), regression (epsilon-SVR, nu-SVR) and distribution estimation (one-class SVM). It supports multi-class classification.\n\n*; Goal of LIBSVM\n*: Our goal is to help users from other fields to easily use SVM as a tool. LIBSVM provides a simple interface where users can easily link it with their own programs. Main features of LIBSVM include\n*:* Different SVM formaulations\n*:* Efficient multi-class classification\n*:* Cross validation for model selection\n*:* Probability estimates\n*:* Various kernels (including precomputed kernel matrix)\n*:* Weighted SVM for unbalanced data\n*:* Both C++ and Java sources\n*:* GUI demonstrating SVM classification and regression\n*:* Python, R, MATLAB, Perl, Ruby, Weka, Common LISP, CLISP, Haskell, LabVIEW, and PHP interfaces. C# .NET code and CUDA extension is available. It\'s also included in some data mining environments: RapidMiner, PCP, and LIONsolver.\n*:* Automatic model selection which can generate contour of cross valiation accuracy.\n\n*; News\n*: Since version 2.8, it implements an SMO-type algorithm proposed in this paper:\n*: R.-E. Fan, P.-H. Chen, and C.-J. Lin. [http://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf Working set selection using second order information for training SVM]. Journal of Machine Learning Research 6, 1889-1918, 2005. You can also find a pseudo code there. ([http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f203 how to cite LIBSVM])\n*:* Version 3.16 released on January 27, 2013. We support more flexible options for the parameter selection tool grid.py. \n*:* We now have a nice page LIBSVM data sets providing problems in LIBSVM format. \n*:* A practical guide to SVM classification is available now! (mainly written for beginners) \n*:* LIBSVM tools available now! \n*:* We now have an easy script (easy.py) for users who know NOTHING about svm. It makes everything automatic--from data scaling to parameter selection. The parameter selection tool grid.py generates the following contour of cross-validation accuracy. To use this tool, you also need to install python and gnuplot.\n\n=== K-means Clustering ===\n\n\n== References ==\n\n# SVM - Support Vector Machines [http://www.support-vector-machines.org/index.html]\n# Review: Applications of Support Vector Machines in Chemistry, Rev. Comput. Chem. 2007, 23, 291-400 [http://www.ivanciuc.org/Files/Reprint/Ivanciuc_SVM_CCR_2007_23_291.pdf]\n# LIBSVM -- A Library for Support Vector Machines [http://www.csie.ntu.edu.tw/~cjlin/libsvm/]\n# R Data Import/Export [http://cran.r-project.org/doc/manuals/R-data.pdf]\n# Quick-R [http://www.statmethods.net/]\n# Quick-R -- Data Types [http://www.statmethods.net/input/datatypes.html]\n# Factors in R (stat.berkeley.edu) [http://www.stat.berkeley.edu/classes/s133/factors.html]\n# An introduction to R (cran.r-project.org/doc/manuals/R-intro) [http://cran.r-project.org/doc/manuals/R-intro.html]','utf-8'),(30,'; Linux kernel 3.0 - block device plug/unplug\n: http://nimhaplz.egloos.com/5598614\n\n\n== 개요 ==\n\nLinux의 block layer에는 block device의 성능을 향상시키기 위한 여러 가지 기법이 적용되어 있다. 대표적인 것으로 disk scheduler (I/O scheduler)가 있을 것이다.\n이 글에서는, block layer의 성능을 향상시키는 기법 중 하나인, block device plugging에 대해 설명한다. Linux 2.6.39에서 리눅스의 plugging이 대대적으로 개선되었다. 2.6.34의 plug구현을 먼저 살펴보고, 그 문제점을 어떻게 Linux 3.0에서 해결했는지 살펴본다.\n\n작성자: S-Core 김.민.규\n작성일: 2011-11-06\n\n\n\n== Plug란 무엇인가? ==\n\n먼저 디스크의 특성에 대해 간단한 설명이 필요하다. HDD의 경우 두드러지는 특성이지만, SSD도 특성은 크게 다르지 않다. \n디스크에서 I/O요청(request)를 처리하는 속도는, request의 크기에 영향을 받지만, request의 개수에도 영향을 받는다. 다시 말해, 8KB를 한번 읽는 것 보다, 4KB를 두 번 읽는 게 훨씬 느리다. 실제로 디스크가 플래터에서 정보를 읽어내는 시간에 비해, 그 외의 준비 하는 데에도 시간이 적잖게 걸리기 때문이다.\n이런 특성을 염두에 두고, 다음 그림을 살펴보자. 디스크 상에서 연속적인(contiguous) request 두 개를 처리하려고 하는 상황이다. 둘을 한 request로 합쳐서 처리한다면 빨리 처리할 수 있는 상황인 것이다. 하지만, plug가 없는 상황에서, 기본적인 동작은 다음 그림처럼 일어난다.\n\nhttp://pds24.egloos.com/pds/201111/06/90/c0080090_4eb64ebc24919.jpg\n\n그림에서 볼 수 있듯이, Req 1이 들어오자마자 디스크가 Req 1을 처리하기 시작했다. 곧 Req 2가 들어올 예정인데, 디스크는 Req 1을 처리 하고 있기 때문에, 결과적으로 Req 2는 별도로 처리돼서, 시간이 많이 걸린다.\n\n만약 plug를 이용한다면 다음 그림과 같이 Req 1,2를 한꺼번에 처리할 수 있다.\n\nhttp://pds24.egloos.com/pds/201111/06/90/c0080090_4eb64ec8ed729.jpg\n\nReq 1을 보내기 전에 plug해서, 디스크가 request를 즉석에서 처리하지 않도록 멈춰놓은 다음, Req 1,2를 보내는 것이다. Req 2를 보낸 다음 unplug를 수행하면, 디스크는 한꺼번에 req 1,2를 처리할 수 있다. 결과적으로 더 빨리 끝나게 되는 것이다.\n\n여기서 plug라는 단어의 의미를 짚고 넘어가야 하겠는데… plug라고 하면 전자제품 플러그를 생각하기 쉽다. 따라서 “plug하면 디스크가 동작한다”고 추측하게 되는 것이다. 그런데 Linux에서 말하는 플러그는 “마개”의 의미이다. 디스크 스케쥴러(Request queue)를 막아서 동작을 멈추겠다는 뜻이다. 따라서, 실제 의미는 “Plug하면 디스크가 동작을 멈춘다”가 된다.\n\n\n\n== Linux 2.6.34에서 plug구현 ==\n\n그렇다면, Linux 2.6에서의 구현을 살펴보자. 필자가 대상으로 한 버전은 2.6.34이다. 앞서 언급했듯이, plug의 구현이 개선된 것은 2.6.39이다.\n2.6.34에서는 __make_request함수에서 다음 루틴을 통해 plug한다.\n    if (queue_should_plug(q) && elv_queue_empty(q))\n        blk_plug_device(q);\n(queue_should_plug는 디스크가 SSD(non-rot)이고, command queueing을 지원하지 않는 경우에는 항상 참이다)\n쉽게 설명하자면, request queue가 비어있다가 request가 들어오면 device를 plug하는 것이다.\n\nblk_plug_device가 plug를 수행하는 함수인데, 이 함수의 내부를 보면, 크게 두 가지 일을 한다.\n1)    Request queue에 PLUGGED flag를 set한다. 이 플래그가 set되면, 디바이스 드라이버가 멈추면서 더 이상 request를 처리하지 않는다.\n다시말해, Linux 2.6.34에서는 block layer와 device driver가 함께 plugging을 구현하고 있는 것이다. MMC device driver의 코드인 mmc_queue_thread를 살펴보면, plug되지 않았을 때만 request를 처리하는 것을 볼 수 있다.\n2)    3ms Unplug timer를 세팅한다. 3ms가 지날때까지 unplug되지 않으면 자동으로 unplug를 해서 request가 처리되도록 하기 위한 것이다.\n\nUnplug되는 경우는 몇 가지 경우가 있지만, 설명의 편의상 간단히 설명하자면 다음과 같이 두 가지 경우라고 볼 수 있다.\n1)    Request queue에 request가 unplug_thresh(4개) 이상 들어오는 경우. (elv_insert)\n2)    앞서 설정한 3ms timeout이 발생하는 경우.\n\n2.6.34의 구현을 종합하자면 다음과 같다.\n* request queue가 비어있을 때 request가 들어오면 plug한다.\n* plug하고 3ms가 지나면 자동으로 unplug한다.\n* 3ms내에 request가 4개 이상 들어와도 unplug한다.\n\n\n\n== 2.6.34 plug의 문제점 ==\n\n2.6.34의 구현에는 다소 문제가 있다. 다음 request가 3ms안에 들어오지 않을 수도 있다는 것이다. Synchronous request하나만 보내놓고 응답을 기다리는 경우, 결국 3ms timer가 expire될 것이다. 그렇다면 괜히 반응성만 3ms나빠지게 되는 것이다.\n하지만 block layer에서는 다음 request가 언제 들어올지 정보가 전혀 없다. 따라서 어쩔 수 없이 기다리는 것이다.\n\n\n\n== Linux 3.0에서 plug 구현 ==\n\n앞서 Linux 2.6.34에서 plug구현의 문제점에 대해 살펴봤다. Linux 3.0에서는 plug의 구현을 수정하고, plug를 파일시스템 레벨에서 이용하게 함으로써 문제를 해결했다. 결국 request를 생성하는 것은 파일시스템인데, 파일시스템에서는 request가 산발적으로 생성되는 것이 아니라, 뭉텅이 지어서(clustered) 생성되기 때문이다. \nLinux 3.0에서는 plug를 사용하는 패턴은 다음과 같이 수정됐다.\nfunction_that_generates_requests\n{\n   struct blk_plug plug;\n   blk_start_plug(&plug);\n   for pages that need I/O\n      submit_bio\n   blk_finish_plug(&plug)\n}\n\n\n위 패턴에서 보는 바와 같이, request를 뭉텅이로 보내려고 할 때, 먼저 파일시스템이 명시적으로 plug를 요청한다.(blk_start_plug) 그 다음 submit_bio를 호출하고, 최종적으로 unplug를 하는 것이다. (blk_finish_plug)\n\nblk_start_plug는 current task(current thread)에 plug되었다고 체크를 한다. (이전에는 request queue에 체크를 했었다) 그러면 그때부터 발생하는 request는 스택에 마련된 struct blk_plug에다가 달아놓는다. 그랬다가 blk_finish_plug가 들어오면 한꺼번에 request queue에 밀어 넣음으로써, 먼저 온 request가 홀랑 먼저 처리되는 것을 방지한다.\n\n구현은 __make_request에 있다: 새로운 request가 merge가능한 경우, merge한다. merge가 불가능한데 task가 plug된 경우, current->plug에 request를 달아놓고 나간다. 그리고 blk_finish_plug가 호출되면, task가 가진 request를 모두 request queue에 집어넣는다.\n\nLinux 3.0은 위와 같은 방식으로 구현함으로써, 불필요하게 디스크가 노는 시간이 없도록 하였다. 굳이 필요 없는 timer도 설정하지 않는다.\nplug list에 있는 request를 flush하는 경우가 blk_finish_plug말고 한가지 더 있다. 바로 태스크가 sleep되는 경우이다. 이 경우 flush하지 않으면 deadlock이 발생할 수 있다고 한다: 태스크가 plug한 상태에서 memory alloc을 요청했는데, 메모리가 부족하면 memory reclaim이 발생한다. 그런데, memory reclaim을 하기 위해서 우리가 현재 plug list에 달아놓은 page를 reclaim해야 할 상황이 생길 수 있다. 이러한 dead lock을 막기 위해, sleep하기 전에 schedule함수 내에서 plug list를 처리해준다.\n\n\n\n== 결론 ==\n\n정리하자면 이렇다.\nRequest가 발생하자마자 처리 해 주는 것은, request merge를 방해해서 I/O 성능을 저하할 수 있다. 따라서 이것을 막기 위해 plug이라는 메커니즘이 적용되어 있다. 2.6 커널에서는 최대 3ms까지 디스크를 멈춰놓고 request를 기다리기 때문에, 비효율 적인 경우가 있다. block layer에서 자체적으로 plug를 구현하다 보니 그렇게 되었을 것이다.\n이러한 비효율을 해결하기 위해 Linux 3.0은 파일시스템에서 plug/unplug 동작을 수행한다. 파일시스템은 한번에 request를 얼마나 보낼지 알고 있기 때문에, 꼭 필요한 구간만 plug를 이용함으로써, 성능을 개선시킬 수 있다.\n\n\n\n== 참조 ==\n\nhttp://lwn.net/Articles/438256/\nJens Axboe님이 쓴 글이다. plug의 역사에서부터 시작해서 자세한 설명을 읽을 수 있다. Linux 3.0에서 deadlock상황은 이 글에서 인용했다.\n\nhttp://studyfoss.egloos.com/5585801\n리눅스 커널에 대한 좋은 글이 많은 블로그이다. 위에 링크된 글은 Linux의 block device I/O에 대한 전반적인 내용을 친절하게 설명하고 있다.','utf-8'),(31,'== PATENT-BRIAN-2013-Template ==\n\n\n\n\n\n=== 배경 / 기존 기술의 문제점 ===\n\n\n\n\n\n=== 본 발명의 특징 / 효과 ===\n\n\n\n\n\n=== 대표 청구항 ===\n\n\n\n\n\n=== 대표 도면 ===\n\n\n\n\n\n=== 선행 기술 ===\n\n\n\n\n\n=== 침해 적발 ===\n\n\n\n\n\n=== 기술 상세 ===\n\n\n\n\n\n=== 청구항 ===\n\n\n\n\n\n=== Notation ===\n\n\n\n\n\n=== Misc. ===','utf-8'),(32,'=== Before Abstract ===\n\n\n2월 12~15일 산호세에서 USENIX FAST 학회가 개최됩니다.\n우리팀에서 출장을 갈지는 아직 미정이지만 눈에 띄는 논문 리스트를 아래에 적어봅니다.\n아직 abstract가 올라오지 않아서 제목만으로 filtering 해봅니다.\n\nhttps://www.usenix.org/conference/fast13/tech-schedule/fast-13-program\n\n- Write Policies for Host-side Flash Caches.\n  : FusionIO 공저, host-side flash cache algorithm을 엿볼수있을듯\n\n- Warming Up Storage-Level Caches with Bonﬁre.\n\n- Unioning of the Buffer Cache and Journaling Layers with Non-volatile Memory.\n  : cache 쪽 논문은 융전문님은 필히 한번 살펴보시기 바랍니다. \n- HARDFS: Hardening HDFS with Selective and Lightweight Versioning.\n  : ? HDFS에 관해 무엇을 했을까요?\n\n- Active Flash: Towards Energy-Efficient, In-Situ Data Analytics on Extreme-Scale Machines.\n  : 저자가 많은 논문입니다. 우리 Lab이 지향해야 할 방향아닐까요? 필독요망\n\n- MixApart: Decoupled Analytics for Shared Storage Systems.\n\n- Concurrent Deletion in a Distributed Content-Addressable Storage System with Global Deduplication.\n  : global dedup?\n\n- File Recipe Compression in Data Deduplication Systems.\n- Improving Restore Speed for Backup Systems that Use Inline Chunk-Based Deduplication.\n\n  : dedup관련 논문은 신전문님 필히 체크해 주세요\n\n- Unifying File Systems and Databases Efficiently.\n  : 항상 관심을 가져왔던 주제인데 어떻게 펼쳤을지 궁금하네요\n\n- Extending the Lifetime of Flash-based Storage through Reducing Write Amplification from File Systems.\n  : File System 수준에서 WAI를 줄여주는 것인데... 결국 random IO을 sequential IO로 transform 하는걸까요?\n\n- Contention-Oblivious Disk Arrays for Cloud Storage.\n- Virtual Machine Workloads: The Case for New NAS Benchmarks.\n\n 전반적으로 flash, ssd쪽 논문이 확 줄었네요. flash session의 논문들도 그렇게 눈길을 끌지 못하고... 반면 cache, dedup 쪽이나 large-scale storage환경, big data와의 통합 이런부분은 더 부각되는 것 같습니다. 이러한 학계의 흐름도 잘 살펴야 겠습니다.\n2월 12~15일 산호세에서 USENIX FAST 학회가 개최됩니다.\n우리팀에서 출장을 갈지는 아직 미정이지만 눈에 띄는 논문 리스트를 아래에 적어봅니다.\n아직 abstract가 올라오지 않아서 제목만으로 filtering 해봅니다.\n\nhttps://www.usenix.org/conference/fast13/tech-schedule/fast-13-program\n\n- Write Policies for Host-side Flash Caches.\n  : FusionIO 공저, host-side flash cache algorithm을 엿볼수있을듯\n\n- Warming Up Storage-Level Caches with Bonﬁre.\n\n- Unioning of the Buffer Cache and Journaling Layers with Non-volatile Memory.\n  : cache 쪽 논문은 융전문님은 필히 한번 살펴보시기 바랍니다. \n- HARDFS: Hardening HDFS with Selective and Lightweight Versioning.\n  : ? HDFS에 관해 무엇을 했을까요?\n\n- Active Flash: Towards Energy-Efficient, In-Situ Data Analytics on Extreme-Scale Machines.\n  : 저자가 많은 논문입니다. 우리 Lab이 지향해야 할 방향아닐까요? 필독요망\n\n- MixApart: Decoupled Analytics for Shared Storage Systems.\n\n- Concurrent Deletion in a Distributed Content-Addressable Storage System with Global Deduplication.\n  : global dedup?\n\n- File Recipe Compression in Data Deduplication Systems.\n- Improving Restore Speed for Backup Systems that Use Inline Chunk-Based Deduplication.\n\n  : dedup관련 논문은 신전문님 필히 체크해 주세요\n\n- Unifying File Systems and Databases Efficiently.\n  : 항상 관심을 가져왔던 주제인데 어떻게 펼쳤을지 궁금하네요\n\n- Extending the Lifetime of Flash-based Storage through Reducing Write Amplification from File Systems.\n  : File System 수준에서 WAI를 줄여주는 것인데... 결국 random IO을 sequential IO로 transform 하는걸까요?\n\n- Contention-Oblivious Disk Arrays for Cloud Storage.\n- Virtual Machine Workloads: The Case for New NAS Benchmarks.\n\n 전반적으로 flash, ssd쪽 논문이 확 줄었네요. flash session의 논문들도 그렇게 눈길을 끌지 못하고... 반면 cache, dedup 쪽이나 large-scale storage환경, big data와의 통합 이런부분은 더 부각되는 것 같습니다. 이러한 학계의 흐름도 잘 살펴야 겠습니다.','utf-8'),(33,'== Introduction ==\n\nEveryone is talking about In Memory at the moment. On blogs, in tweets, in the press, in the Oracle marketing department, in books by SAP employees, even my Violin colleagues… it’s everywhere. What can I possibly add that will be of any value?\n\n\nWell, how about owning up to something: I find myself in a bit of a quandary on this subject. On the one hand it’s a new buzzword, which means that a) it’s got everyone’s attention, and b) many people with their own agenda will seek to use it to their advantage… but on the other hand, given the nature of my employment (I work for Violin Memory, purveyors of flash memory systems), it seems like something we ought to be talking about.\n\n\nAs anyone who works in the IT industry knows (and perhaps it’s the same in other industries), we love a buzzword. Cloud, Analytics, Big Data, In Memory, Transformation… all of these phrases have been used at one time or another to try and wring cash out of customers who may or may not need the services and products they imagine the phrase represents. Even back at the end of the last millenium consultants worldwide were making huge amounts of money out of exploiting the phrase “Y2K”, some with more honourable intentions than others. I remember my old school received a letter from a “Y2K conformance specialist” informing them that this person could visit and inspect their football pitches to ensure they were “Y2K compliant”… (true story!)\n\n\nSo if buzzwords are prone to misuse, maybe the first thing we need to do is explore what “In Memory” really means? In fact, rewind a step – what do we mean when we say “Memory”?\n\n<br/>\n\n== What Is Memory ? ==\n\nIt’s a basic question, but a good definition is surprisingly hard to pin down. Clearly this is an IT blog so (despite the deceiving picture above) I am only interested in talking about computer memory rather than the stuff in my head which stops working after I drink tequila. The definition of this term in the Free Online Dictionary of Computing is:\n\n\n{| width=\"80%\"\n| align=\"left\" |\n;memory\n: These days, usually used synonymously with Random Access Memory or Read-Only Memory, but in the general sense it can be any device that can hold data in machine-readable format\n|-\n|}\n\n\nSo that’s any device that can hold data in machine-readable format. So far so ambiguous. And of course that is the perfect situation for any would-be freeloader to exploit, since the less well-defined a definition is, the more room there is to manoeuvre any product into position as a candidate for that description.\n\n\nHere’s what most people think of when they talk about computer memory… DRAM:\n\n\nDynamic Random Access Memory (DRAM)\n\n\nThis is Dynamic Random Access Memory – and it’s most likely what’s in your laptop, your desktop and your servers. You know all about this stuff – it’s fast, it’s volatile (i.e. the data stored on it is lost when the power goes off) and it’s comparatively expensive to say… disk, for which many orders of magnitude more are available at the same price point.\n\n\nBut now there is a new type of “memory” on the market, NAND flash memory. Actually it’s been around for over 25 years (read this great article for more details) but it is only now that we are seeing it being adopted en mass in data-centres, as well as being prevalent in consumer devices – the chances are your phone contains NAND flash, your tablet (if you have one) and maybe your computer if you are fortunate enough to have an SSD drive in it.\n\n\nToshiba NAND Flash\n\nFlash memory, unlike DRAM, is persistent. That means when the power goes, the data remains. Flash access speeds are measured in microseconds – let’s say around 100 microseconds for a single random access. That’s significantly faster than disk, which is measured in (multiple) milliseconds - but still slower than DRAM, for which you would expect an access in around 100 nanoseconds. Flash is available in many forms, from USB devices and SSDs which fit into normal hard drive bays, through PCIe cards which connect direct to the system bus, and on to enterprise-class storage arrays such as those made by my employer like the Violin Memory 6000 series array.\n\n\nIs flash a type of memory? It certainly fits the dictionary description above. But if you run something on flash, can you describe that something as now running “in memory”? You could argue the point either way I suppose.\nSince we don’t seem to be doing well with defining what memory is, let’s change tack and talk about what it definitely isn’t. And that’s simple, because it definitely isn’t disk.\n\n\n\nDisk\n\nWhether it’s part of the formal definition or not, almost anyone would assume that memory is fast and non-mechanical, i.e. it has no moving parts. It is all semiconductors and silicon, not motors and magnets. A hard disk drive, with its rotating platters and moving actuator arm, is about the most un-memory-like way you can find to store your data, short of putting it on a big reel of tape. And, consistent with our experience of memory versus non-memory devices, it’s slow. In fact, every disk array vendor in the industry stuffs their enterprise disk arrays full of DRAM caches to make up for the slow performance of disk. So memory is something they use to mask the speed of their non-memory-based storage. Hang on then, if you have a small enough dataset so that the majority of your disk reads are coming from your disk array cache, does that mean you are running “in memory” too? No of course not, but the ambiguity is there to be exploited.\n\n<br/>\n\n== Primary Storage versus Secondary Storage ==\n\nSince we are struggling with a formal definition of memory, perhaps another way to look at it is in terms of primary storage and secondary storage. The main difference here is that primary storage is directly addressable by the CPU, whereas secondary storage is addressed through input/output channels. Is that a good way of distinguishing memory from non-memory? It certainly works with DRAM, which ends up in the primary storage category, as well as disk, which ends up in the secondary storage category. But with flash it is a less successful differentiator.\n\n\nThe first problem is that as previously mentioned flash is available in multiple different forms. PCIe flash cards are directly addressable by the CPU whilst SSDs slot into hard drive bays and are accessed using storage protocols. In fact, just looking at the Violin Memory 6000 series array around which my day job revolves, connectivity options include PCIe direct attached, fibre-channel and Infiniband, meaning it could easily fit into either of the above categories.\n\n\nWhat’s more, if you think of primary storage as somehow being faster than secondary storage, the Infiniband connectivity option of the Violin array is only about 50-100 microseconds slower than the PCIe version, yet brings a wealth of additional benefits such as high availability. It’s hard to think of a reason why you would choose the direct attached version of that with Infiniband.\n\n<br/>\n\n== Volatile versus Persistent ==\n\nMaybe this is a better method of differentiating? Perhaps we can say that memory is that which is volatile, i.e. data stored on it will be lost when power is no longer available. The alternative is persistent storage, where data exists regardless of the power state. Does that make sense?\n\n\nNot really. Think about your traditional computer, whether it’s a desktop or server. You have four high-level resources: CPU to do the work, network to communicate with the outside world, disk to store your data (the persistence layer). Why do you have memory in the form of DRAM? Why commit extra effort to managing a volatile store of data, much of which is probably duplicated on the persistence layer?\n\n\nDRAM exists to drive up CPU utilisation. Processor speeds have famously doubled every couple of years or so. Network speeds have also increased drastically since the days of the 56k modem I used to struggle with in the 1990′s. Disk hasn’t – nowhere near in fact. Sure, capacity has increased – and speeds have slowly struggled upwards until they reached the limit of the 15k RPM drive, but in comparison to CPU improvements disk has been absolutely stagnant. So your computer is stuffed full of DRAM because, if it weren’t, the processors would spend all their time waiting for I/O instead of doing any work. By keeping as much data in volatile DRAM as possible, the speed of access is increased by around five orders of magnitude, resulting in CPUs which can spend more time working and less time waiting.\n\n\nIn the world of flash memory things are slightly different. DRAM is still necessary to maintain CPU utilisation, because flash is around two-and-a-half to three orders of magnitude slower than DRAM. But does it make sense to assume that “memory” is therefore only applicable to volatile data storage? What if a hypothetical persistent flash medium arrived with DRAM access speeds? Would we refuse to say that something running on this magic new media was running “In Memory”?\n\n\nI don’t have an answer, only an opinion. My opinion is that memory is solid-state semiconductor-based storage and can be volatile or persistent. DRAM is a type of memory, but not the only type. Flash is a type of memory, while disk clearly is not.\n\n\nSo with that in mind, in the next part of this blog series I’m going to look at In Memory Database technologies and describe what I see as the three different architectures of IMDB that are currently available. As a taster, one of them is SAP HANA, one of them involves Violin Memory and the third one is the new Oracle Exadata X3 ”Database In-Memory Machine”. And as a conclusion I will have to make a decision about the quandary I mentioned at the start of this article: should we at Violin claim a piece of the “In Memory” pie?\n\n<br/>\n\n== References ==\n\n# Thoughts on In Memory Databases (Part 1) // October 9, 2012 // flashdba [http://flashdba.com/2012/10/09/in-memory-databases-part1/]','utf-8'),(34,'== What is a periodicity transform? ==\n\nIt is a way to automatically detect periodicities.\n\n[http://sethares.engr.wisc.edu/downper.html \"What is a periodicity transform?\"]\n\n\n=== What does it do? ===\n\nTo give you an idea of what the Periodicity Transform is good at, let\'s look at a simple example. Here is a data record that I constructed by adding together two periodic sequences, one with period 13 and the other with period 19. For good measure, I added about 25% noise. The result looks like this:\n\n[http://eceserv0.ece.wisc.edu/~sethares/images/xyz.gif]\n\nIt\'s not so easy to \"see\" the two periodic sequences that are buried inside, is it? If we take the Fourier transform (DFT), then we get:\n\nAgain, its pretty difficult to see any pattern here. The 13-periodic and the 19-periodic sequences are still hidden. Now let\'s apply the periodicity transform called the \"M-Best\" algorithm, which searches for the M largest periodicities. With M=10, we get:\n\nNow that\'s more like it! The two periods (at 13 and 19) are clearly visible. The other eight small periods reflect minor accidental patterns that happen to occur in the noise. The Periodicity Transforms are good at finding periodicities in data.\n\n=== How does it work? ===\n\nMost standard transforms can be interpreted as projections onto suitable subspaces, and in most cases (such as the Fourier and Wavelet transforms), the subspaces are orthogonal. Such orthogonality implies that the projection onto one subspace is independent of the projection onto others. Thus a projection onto one sinusoidal basis function (in the Fourier Transform) is independent of the projections onto others, and the Fourier decomposition can proceed by projecting onto one subspace, subtracting out the projection, and repeating. Orthogonality guarantees that the order of projection is irrelevant. This is not true for projection onto nonorthogonal subspaces such as the periodic subspaces Sp. Thus the order in which the projections occur effects the decomposition, and the Periodicity Transform does not in general provide a unique representation. Once the succession of the projections is specified, however, then the answer is unique.\n\nThe Periodicity Transform searches for the best periodic characterization of the length N signal x. The underlying technique is to project x onto some periodic subspace Sp. This periodicity is then removed from x leaving the residual r stripped of its p-periodicities. Both the projection x and the residual r may contain other periodicities, and so may be decomposed into other q-periodic components by further projection onto Sq. The trick in designing a useful algorithm is to provide a sensible criterion for choosing the order in which the successive p\'s and q\'s are chosen. The intended goal of the decomposition, the amount of computational resources available, and the measure of \"goodness-of-fit\" all influence the algorithm. Our paper discusses four ways to mind our p\'s and q\'s.\n\n=== Four flavors ===\n\n(1) The \"small to large\" algorithm assumes a threshold T and calculates the projections onto Sp beginning with p=1 and progressing through p=N/2. Whenever the projection contains at least T percent of the energy in x, then the corresponding projection is chosen as a basis element.\n\n(2) The \"M-best\" algorithm maintains a list of the M best periodicities and the corresponding basis elements. When a new (sub)periodicity is detected that removes more power from the signal than one currently on the list, the new one replaces the old, and the algorithm iterates.\n\n(3) The \"best-correlation\" algorithm projects x onto all the periodic basis elements, essentially measuring the correlation between x and the individual periodic basis elements. The p with the largest (in absolute value) correlation is then used for the projection.\n\n(4) The \"best-frequency\" algorithm determines p by Fourier methods and then projects onto Sp.\n\nMATLAB routines to calculate all of these variations are available by clicking here.\n\n\n=== References ===\n\nFor more details, see our paper:\n\nW. A. Sethares and T. W. Staley, [http://eceserv0.ece.wisc.edu/~sethares/periodic.html \"Periodicity Transforms\"], IEEE Transactions on Signal Processing, Nov 1999. You can download a slightly raw pdf version here.\n\nA [http://sethares.engr.wisc.edu/papers/rhypap.html companion paper] explores the application of Periodicity Transforms to the automatic detection of rhythm in musical performance.\n\n\n\nTo get to my homepage, click here.','utf-8'),(35,'\n\n== References ==\n\n=== Papers ===\n\n----\n\"TPC-E vs. TPC-C: Characterizing the New TPC-E Benchmark via an I/O Comparison Study\", Intel Labs Pittsburgh, EPFL, CMU [http://pandis.net/resources/sigmodrec10chen.pdf]\n\n; Abstract\n: we compare the I/O access patterns of the two benchmarks by analyzing two OLTP disk traces. We ﬁnd that (i) TPC-E is more read intensive with a 9.7:1 I/O read to write ratio, while TPC-C sees a 1.9:1 read-to-write ratio; and (ii) although TPC-E uses pseudo-realistic data, TPC-E’s I/O access pattern is as random as TPC-C. The latter suggests that like TPC-C, TPC-E can beneﬁt from SSDs, which have superior random I/O support. To verify this, we replay both disk traces on an Intel X25-E SSD and see dramatic improvements for both TPC-C and TPC-E.\n\n----','utf-8'),(36,'Trick mode (종종 Trick play로 불려짐)는 Digital Video Recorders 혹은 VoD (Video on Demand)와 같은 digital video system의 한 기능으로서, 아날로그 VCR 시스템에서 제공되던 fast-forward, rewind operation 동안의 visual feedback을 흉내내는 역할을 한다.  (video stream이 only a subset of frames만을 포함시키는 방식으로 동작)','utf-8'),(37,'== Introduction ==\n\nYou might be tempted to think that In-Memory technologies and flash are concepts which have no common ground. After all, if you can run everything in memory, why worry about the performance of your storage? However, the truth is very different: In-Memory needs flash to reach its true potential. Here I will discuss why and look at how flash memory systems can both enable In-Memory technologies as well as alleviate some of the need for them.\n\n\nNote: This is an article I wrote for a different publication recently. The brief was to discuss at a high level the concepts of In-Memory Computing. It doesn’t delve into the level of technical detail I would usually use here – and the article is more Violin marketing-orientated than those I would usually publish on my personal blog, so consider yourself warned… but In-Memory is an interesting subject so I believe the concepts are worth posting about.\n\n<br/>\n\n== In-Memory Computing (IMC) ==\n\nIn-Memory Computing (IMC) is a high-level term used to describe a number of techniques where data is processed in computer memory in order to achieve better performance. Examples of IMC include In-Memory Databases (which I’ve written about previously here and here), In-Memory Analytics and In-Memory Application Servers, all of which have been named by Gartner as technologies which are being increasingly adopted throughout the enterprise.\n\n\nTo understand why these trends are so significant, consider the volume of data being consumed by enterprises today: in addition to traditional application data, companies have an increasing exposure to – and demand for – data from Gartner’s “Nexus of Forces”: mobile, social, cloud and big data. As more and more data becomes available, competitive advantages can be won or lost through the ability to serve customers, process metrics, analyze trends and compute results. The time taken to convert source data to business-valuable output is the single most important differentiator, with the ultimate (and in my view unattainable – but that’s the subject for another blog post) goal being output that is delivered in real-time.\n\n\nBut with data volumes increasing exponentially, the goal of performance must also be delivered with a solution which is highly scalable. The control of costs is equally important – a competitive advantage can only be gained if the solution adds more value than it subtracts through its total cost of ownership.\n\n<br/>\n\n== How does In-Memory Computing Deliver Faster Performance? ==\n\nThe basic premise of In-Memory Computing is that data processed in memory is faster than data processed using storage. To understand what this means, first consider the basic elements in any computer system: CPU (Central Processing Unit), Memory, Storage and Networking. The CPU is responsible for carrying out instructions, whilst memory and storage are locations where data can be stored and retrieved. Along similar lines, networking devices allow for data to be sent or received from remote destinations.\n\n\nMemory is used as a volatile location for storing data, meaning that the data only remains in this location while power is supplied to the memory module. Storage, in contrast, is used as a persistent location for storing data i.e. once written data will remain even if power is interrupted. The question of why these two differing locations are used together in a computer system is the single most important factor to understand about In-Memory Computing: memory is used to drive up processor utilization.\n\n\nModern CPUs can perform many billions of instructions per second. However, if data must be stored or retrieved from traditional (i.e. disk) storage this results in a delay known as a “wait”. A modern disk storage system performs an input/output (I/O) operation in a time measured in milliseconds. While this may not initially seem long, when considered in the perspective of the CPU clock cycle where operations are measured in nanoseconds or less, it is clear that time spend waiting on storage will have a significant negative impact on the total time required to complete a task. In effect, the CPU is unable to continue working on the task at hand until the storage system completes the I/O, potentially resulting in periods of inactivity for the CPU. If the CPU is forced to spend time waiting rather than working then it can be considered that the efficiency of the CPU is reduced.\n\n\nUnlike disk storage, which is based on mechanical rotating magnetic disks, memory consists of semiconductor electronics with no moving parts – and for this reason access times are orders of magnitude faster. Modern computer systems use Dynamic Random Access Memory (DRAM) to store volatile copies of data in a location where they can be accessed with wait times of approximately 100 nanoseconds. The simple conclusion is therefore that memory allows CPUs to spend less time waiting and more time working, which can be considered as an increase in CPU efficiency.\n\n\nIn-Memory Computing techniques seek to extract the maximum advantage out of this conclusion by increasing the efficiency of the CPU to its limit. By removing waits for storage where possible, the CPU can execute instructions and complete tasks with the minimum of time spent waiting on I/O.\n\n\nWhile IMC technologies can offer significant performance gains through this efficient use of CPU, the obvious drawback is that data is entirely contained in volatile memory, leading to the potential for data loss in the event of an interruption to power. Two solutions exist to this problem: the acceptance that all data can be lost or the addition of a “persistence layer” where all data changes must be recorded in order that data may be reconstructed in the event of an outage. Since only the latter option guarantees business continuity the reality of most IMC systems is that data must still be written to storage, limiting the potential gains and introducing additional complexity as high availability and disaster recovery solutions are added.\n\n<br/>\n== What are the Barriers to Success with In-Memory Computing? ==\n\nThe main barriers to success in IMC are the maturity of IMC technologies, the cost of adoption and the performance impact associated with adding a persistence layer on storage. Gartner reports that IMC-enabling application infrastructure is still relatively expensive, while additional factors such as the complexity of design and implementation, as well as the new challenges associated with high availability and disaster recovery, are limiting adoption. Another significant challenge is the misperception from users that data stored using an In-Memory technology is not safe due to the volatility of DRAM. It must also be considered that as many IMC products are new to the market, many popular BI and data-manipulation tools are yet to add support for their use.\n\n\nHowever, as IMC products mature and the demand for performance and scalability increases, Gartner expects the continuing success of the NAND flash industry to be a significant factor in the adoption of IMC as a mainstream solution, with flash memory allowing customers to build IMC systems that are more affordable and have a greater impact.\n\n<br/>\n\n== NAND Flash Allows for New Possibilities ==\n \nThe introduction of NAND flash memory as a storage medium has caused a revolution in the storage industry and is now allowing for new opportunities to be considered in realms such as database and analytics. NAND flash is a persistent form of semiconductor memory which combines the speed of memory with the persistence capabilities of traditional storage. By offering speeds which are orders of magnitude faster than traditional disk systems, Violin Memory flash memory arrays allow for new possibilities. Here are just two examples:\n\n\nFirst of all, In-Memory Computing technologies such as In-Memory Databases no longer need to be held back by the performance of the persistence layer. By providing sustained ultra-low latency storage Violin Memory is able to facilitate customers in achieving previously unattainable levels of CPU efficiency when using In-Memory Computing.\n\n\nSecondly, for customers who are reticent in adopting In-Memory Computing technologies for their business-critical applications, the opportunity now exists to remove the storage bottleneck which initiated the original drive to adopt In-Memory techniques. If IMC is the concept of storing entire sets of data in memory to achieve higher processor utilization, it can be considered equally beneficial to retain the data on the storage layer if that storage can now perform at the speed of flash memory. Violin Memory flash memory arrays are able to harness the full potential of NAND flash memory and allow users of existing non-IMC technologies to experience the same performance benefits without the cost, risk and disruption of adopting an entirely new approach.\n\n<br/>\n\n== References ==\n# Why In-Memory Computing Needs Flash // November 16, 2012 // flashdba [http://flashdba.com/category/database/in-memory/]','utf-8'),(38,'== Summary ==\n\n* Date: 2012-01-22\n\n* [http://www.bloter.net/archives/92823 Bloter.net 기사 원문]\n\n== Implications ==\n\n\n== Article ==\n\n네트워크업계는 전세계 3대 빅스포츠 게임으로 올림픽, FIFA월드컵, 세계육상경기를 꼽는다. 특히 전세계의 관심이 몰린 올림픽에는 각별한 애정을 내비치기도 한다.\n\n\n국제올림픽위원회(IOC)에 따르면 IOC 공식 유튜브 계정을 통한 2008년 베이징 올림픽 디지털 콘텐츠 채널 조회수는 1650만건에 달한다. 당시 다음 스포츠섹션의 평균 트래픽은 평상시 대비 순방문자(UV)가 1.5배 늘었고, 페이지뷰(PV)가 2.7배 상승했다. 시스코는 “2008년 베이징 올림픽은 역사상 가장 많은 사람들이 관람한 올림픽이란 타이틀을 획득했다”라고 전했다.\n2010년 벤쿠버 동계올림픽 당시 상황도 마찬가지였다. 당시 NBC올림픽닷컴은 올림픽 기간 동안 7억1천만건의 페이지뷰와 4600만명에 달하는 순방문자수를 달성했다. NBC올림픽 모바일 서비스의 경우 8200만건의 페이지뷰와 190만개의 모바일 동영상 스트리밍을 기록했다.\n\n\n올림픽 경기때마다 방송사는 물론 온라인과 모바일 중계 서비스를 제공하는 기업들은 원활한 트래픽 관리와 콘텐츠 전송을 위해 노력한다.\n\n\n김선아 한국 아카마이 부장은 “기존 방송사와 미디어, 포털, 모바일 업계를 비롯해 스마트폰과 TV제조업체 모두 다가오는 2012년 7월 런던 올림픽, 2014년 브라질 월드컵, 2014년 인천 아시아게임, 평창 동계올림픽에 관심을 갖고 있다”라며 “다양한 글로벌 이벤트에서 발생하는 콘텐츠를 활용해 어떤 비즈니스 기회를 만들어낼 수 있는지 기업들이 고민하고 있다”라고 말했다.\n\n\n올림픽은 많은 네트워크업체들이 고객들에게 자사 솔루션과 장비를 뽐낼 수 있는 절호의 기회다. 아카마이, BT, 시스코를 비롯한 업체들은 벌써 올림픽 선전에 나섰으며, 곧 다가올 ‘2012 런던 올림픽’ 특수를 기대하고 있다.\n\n\n이 과정에서 모바일 중계 서비스 접속 트래픽이 증가함에 따라 발생할 수 있는 서비스 품질 저하와 지연 문제가 대두되고 있다. 네이버 프로야구 중계 서비스의 경우 경기 하나를 시청할 때마다 약 750MB 정도의 데이터 트래픽이 유발된다고 한다. 만약 3G 동시 접속자가 늘어나면 일부 지역에서 동영상이 재생이 원활하지 않는 문제가 발생할 수 있다.\n\n\n이처럼 국내 경기 시청 트래픽 관리도 어려운데 올림픽 트래픽 관리는 오죽할까.\n암르 엘라위 시스코 디지털 마케팅 프로그램 매니저는 “특히 세계 최초의 ‘모바일 디지털 올림픽’을 꿈꾸는 2012년 런던 올림픽의 경우 네트워크 트래픽 관리와 콘텐츠 전송이 가장 큰 핵심 이슈가 되고 있다”라며 “올림픽의 희로애락을 언제 어디서나 어떤 기기를 통해서든 동시다발로 공유하는 ‘모바일 디지털 올림픽’을 감당하려면 그에 걸맞은 대역폭과 네트워크와 보안 그리고 디지털 미디어 환경이 구축돼 있어야 한다”라고 말했다.\n\n\n스마트폰과 태블릿 판매율이 데스크톱이나 노트북 판매율을 넘어선지 오래다. 유튜브에는 모바일 기기를 통해 35시간 분량의 영상이 매 1분마다 올라오고 있다. 다양한 콘텐츠를 중단 없이 전송하고 분석할 수 있는 기반을 마련하는 데 온 네트워크업계의 관심이 몰린 것은 어찌보면 당연한 일이다.\n\n\n트래픽 관리와 콘텐츠 전송 외 업체들이 관심 가지고 있는 부문이 또 있다. 바로 고화질 영상을 중단 없이 전송하는 부문이다. 시스코와 아카마이는 각각 자사가 출시한 콘텐츠 전송 솔루션을 통해 문제 해결을 돕겠다고 나섰다.\n\n\n아카마이가 발표한 인터넷 현황 보고서에 따르면 2011년 2분기 상위 5~10%의 스마트폰 헤비 유저들의 온라인 비디오 시청 시간은 하루 평균 40분으로 향후 더욱 증가할 것으로 나타났다. 특히 조사된 소비자들 대부분이 온라인 비디오의 품질을 중요히 여긴다고 답했다.\n\n\nIDC가 실시한 미국 소비자 온라인 사용 행태 조사에 따르면 82% 이상의 온라인 비디오 사용자가 동영상의 선명함이나 해상도가 중요하거나 매우 중요하다고 답했으며, 67%는 동영상의 크기가 같을 경우 바로 실행 가능한지 여부가 중요하다고 답했다. 실제로 HD급의 동영상을 시청하는 사용자가 표준 품질의 영상을 시청하는 사용자보다 9.3% 더 높게 동영상 시청에 시간을 투자하는 것으로 나타났다.\n\n\n문제는 이 과정에서 고화질 영상과 실시간 중계가 서로 충돌을 일으킨다는 점이다. HDTV가 보편화되면서 방송 콘텐츠의 용량이 증가했고, 이를 전달하는 과정에서 많은 부하와 끊김 현상이 발생하곤 한다. 결국 런던 올림픽에서는 어떤 기업의 솔루션이 이 모든 현상을 해결해줄 것인지에 대한 여부가 핵심으로 작용할 것으로 보인다.\n\n\n해리 코일 NBC 방송사 감독은 “스포츠를 TV로 생중계하기 시작한 1940년대로 이때는 스포츠를 TV로 볼 수 있다는 것만으로도 소비자의 마음을 사로잡기 충분했지만, 스포츠 전문 방송 채널이 늘어나도 다양한 콘텐츠가 넘쳐나는 지금 스포츠 중계의 핵심은 경기 현장을 생생하고 원활하게 다양한 기기에서 보는데 있다”라고 답했다.\n\n\n== References ==\n\n* [[Bnote_2013#Akamai_-_CDN_Acceleration]]','utf-8'),(39,'== Summary ==\n\n* Date: 2013-01-24\n* [http://www.bloter.net/archives/141513 Bloter.net 기사 원문]\n\n\n== Implications ==\n\n\n== Article ==\n\n“아카마이를 찾아온 고객이 요구하는 건 단순합니다. 자사 웹사이트 속도를 빠르게 해 달라는 얘기지요. 웹사이트가 담고 있는 콘텐츠는 점점 풍부해졌는데, 웹사이트 뜨는 속도는 느려졌다고 생각해보세요. 고객의 손길을 잡으려면 웹사이트를 내려받는 데 2~3초의 시간도 걸리지 않아야 합니다.”\n\n\n인터넷 서비스 사업자가 가장 고민하고 있는 것은 무엇일까. 알렉스 카로 아카마이 아태지역 최고기술책임자(CTO) 겸 서비스 부문 부사장은 ‘속도’를 꼽았다. 사진, 동영상, 음악 등 웹페이지를 통해 소비자에게 보여주고 싶은 것들은 많은데, 문제는 이 콘텐츠들을 웹페이지상에서 재생하는 데 적지 않은 시간이 걸린다. 웹사이트의 속도와 사용자의 인내심은 반비례한다. 사이트가 느리면 느릴수록 많은 수의 고객들이 떠난다.\n\n\n“다양한 모바일 기기로 웹페이지에 접근하는 것도 인터넷 서비스 사업자에겐 고민입니다. 사용자가 많이 찾으면 찾을수록 웹페이지 실행 속도는 점점 느려지지요. 제대로 된 속도를 보이지 않을수록 기업이 손해보는 기회비용은 점점 더 커지기 마련입니다.”\n\n\n2005년만 해도 인터넷 익스플로러가 전체 웹브라우저 시장의 80%를 차지했다. 그러나 2012년 웹브라우저 시장은 다르다. 특정 웹브라우저가 독점하는 시대는 갔다. 대신 파이어폭스, 크롬, 사파리, 오페라 등 다양한 브라우저가 저마다 10% 이상의 점유율을 보이며 성장하고 있다. 스마트폰, 태블릿PC 등이 보편화된 덕이다.\n\n\n인터넷 서비스 사업자들의 고민은 더욱 커졌다 대응해야 할 웹브라우저가 늘어났고, 소비자의 기대치는 올라갔다. 빠른 웹서비스를 보이고 싶지만, 이는 마음처럼 쉬운 게 아니다. 웹사이트 성능을 높이기 위해서는 네트워크, 기기, 웹브라우저, 서드파티 콘텐츠 등 다양한 요소를 고려해야 한다. 웹사이트가 눈에 보이는 것처럼 물리적으로 딱 하나만 존재하는 것도 아니다. 웹사이트 안에는 또 여러 종류의 웹사이트가 유기적으로 연결돼 있다. 이 모든 요소를 파악해야만 웹사이트 속도를 좀 더 빠르게 만들 수 있다.\n\n\n이에 대해 알렉스 CTO는 자사의 노하우로 기업의 고민을 덜어줄 수 있다고 자신하는 눈치다. 알렉스 CTO는 “자사 콘텐츠 전송기술과 웹페이지 최적화 노하우가 담긴 ‘아쿠아 아이온’으로 웹페이지 성능을 획기적으로 개선할 수 있다”라고 설명했다.\n\n\n아카마이는 콘텐츠 전송 관리 기술(CDN) 분야에 특화된 기술을 갖추고 있는 업체다. 지난 2012년 런던 올림픽 인터넷 중계에도 아카마이의 ‘아쿠아 아이온’ 기술이 녹아들었다.\n아쿠아 아이온은 사용자 기기 정보와 네트워크 상황을 인식해 웹사이트를 각 상황에 맞춰 최적화해 보여준다. 이를 위해 ‘프론트 엔드 최적화’ 기술과 ‘맞춤형 이미지 압축’ 기술이 사용된다.\n\n\n프론트 엔드 최적화 기술은 웹사이트 로딩할 때 필요한 객체 수를 줄여 웹페이지를 보다 빠르게 보일 수 있게 도와준다. 여기에 웹페이지 정보량을 각 기기에 맞춰 조율한다. 웹사이트를 띄우기 위해서는 각 단말 기기에서 웹사이트로 정보를 호출하는 일이 일어난다. 이 때 프론트엔드 최적화 기술은 스마트폰, 태블릿PC, 데스크톱PC 등 웹사이트를 실행하는 기기 상황에 맞춰 서로 주고받는 정보의 양을 조율해 페이지 렌더링 자체를 가속화한다.\n\n\n웹사이트를 각 기기 상황에 맞춰 개편하기에 별도의 모바일 페이지가 필요없다는 얘기가 아니다. 웹사이트를 불러오는 과정에서 필요한 각종 정보와, 호출이 이뤄지는 객체 수를 줄여 페이지를 좀 더 빨리 띄울 수 있게 네트워크 단에서 작업한다는 얘기다.\n\n\n맞춤형 이미지 압축 기술은 웹사이트를 띄우는 각 기기가 연결된 네트워크 품질을 파악해 이미지 정보를 전달한다. 기기 상황에 맞춰 이미지를 압축해서 전달하고 보내주는 식이다. 웹사이트 자체에서 변환해 올릴 필요가 없다.\n\n\n예를 들어 DSLR 카메라로 찍은 퀄리티 100%의 사이즈 60KB 사진이 서버에 저장됐다고 하자. 맞춤형 이미지 압축 기술은 사용자가 연결 품질이 떨어지는 기기를 사용하고 있다면, 원크기의 40%를 압축해서 24Kb로 보내고, 좋은 품질이면 90%로 압축해서 사진을 보낸다.\n\n\n물론 이런 기술들은 하루아침에 완성된 게 아니다. 아카마이는 지난해 웹페이지와 모바일 앱 가속화 솔루션 업체인 코텐도, 다양한 기기에서 웹페이지를 불러들일 때 속도 저하와 지연을 방지하는 기술을 갖춘 블레이즈 인수를 통해 빠른 웹사이트 만들기에 박차를 가하고 있다. 고화질 동영상 등 리치 미디어에 대한 관심이 높은 요즘일수록 빠른 웹사이트 구축을 도와주는 시장이 앞으로 각광받을 것이라고 보는 덕분이다.\n\n\n알렉스 CTO는 ”아쿠아 아이온을 도입하면, 기존 아카마이 솔루션 사용 대비 3배 이상 빠르게 웹사이트가 뜨는 걸 확인할 수 있다”라며 “인터넷 서비스 업체는 따로 웹디자이너나 개발자를 불러 모든 상황에 최적화된 웹사이트를 만들 필요가 없이 아카마이 솔루션을 도입하면 고민 끝”이라고 말했다.\n\n== References ==\n\n* [[Bnote_2013#Akamai_-_CDN_Acceleration]]','utf-8'),(40,'== Summary ==\n\n* Date: 2011-11-28\n* [http://www.bloter.net/archives/85622 Bloter.net 기사 원문]\n\n== Implications ==\n\n\n== Article ==\n\n콘텐츠 전송 네트워크(CDN)시장에서 선두를 달리고 있는 아카마이 테크놀로지가 경쟁업체인 코텐도를 3억달러에 인수할 것이라는 설이 나오고 있다.\n\n이스라엘 경제신문 칼카리스트는 11월27일(현지기준) “아카마이 테크놀로지가 이스라엘 기업인 코텐도를 3억달러에 인수하겠다는 의향을 밝힌 것으로 알려지고 있다”라고 전했다. 또 다른 이스라엔 비즈니스 신문인 글로비스는 “코텐도 인수에 아카마이와 쥬니퍼가 뛰어들었다”라며 “아카마이는 3억달러에서 3억5천만달러를 제안한 것으로 알려졌다”라고 전했다.\n\n코텐도는 CDN 사업자로 웹페이지와 모바일 앱 가속화 솔루션을 개발하는 업체다. 지난해부터는 AT&T와 제휴를 맺고 미국 등에서 통신사업자에 맞춤화된 서비스를 선보였다. 기업들이 웹페이지에서 실시간 보고서 작성과 분석을 할 경우 이를 빠른 속도로 실행할 수 있게 지원한다. 코텐도가 서비스하는 업체로는 마이크로소프트와 구글, 페이스북이 등이 있다.\n\n아카마이는 코텐도 인수를 통해 자사 웹사이트와 모바일 사이트 가속화 솔루션을 강화하려는 것으로 보인다. 업계 관계자에 따르면 아카마이의 현재 CDN 시장 점유율은 65~70% 정도로 코텐도는 CDN시장 상위 3개 업체에는 속하지 않지만 아카마이가 코텐도를 인수하게 될 경우 파급력은 적잖을 전망이다.\n\n로이터는 “아카마이가 코텐도를 인수하게 될 경우 CDN 시장의 90%에 달하는 시장 점유율을 획득할 수 있을지도 모른다”라며 “인수를 통해 아카마이는 콘텐츠 관리 시장과 스트리밍 시장을 제어할 수 있을 것”이라고 전망했다.\n\n외신들에 따르면 코텐도는 인수설과 관련해 아직 공식적인 입장을 내놓지 않고 있는 상황이다. 그러면서 동시에 코텐도가 이번 인수설을 부정하지도 않고 있어 업계 관계자들은 아카마이의 코텐도 인수를 거의 확실한 것으로 보고 있다.\n\n== References ==\n\n* [[Bnote_2013#Akamai_-_CDN_Acceleration]]','utf-8'),(41,'== Summary ==\n\n* Date: 2011-07-13\n* [http://www.bloter.net/archives/67854 Bloter.net 기사 원문]\n\n* 현재 (2011년 7월?) 아카마이(Akamai)는 전세계 모든 웹 트래픽의 20%를 전송하고, 매일 수천억 건의 인터넷 요청을 처리 중 --> 2012월 1월 19일 블로터닷넷 기사에서는, 전세계 웹 트래픽의 30%가 아카마이 플랫폼을 거치고 있는 것으로 언급됨\n: -> [[아카마이,_\"CDN_넘어_하이퍼커넥티드로\"]]\n\n* 전 세계 72개국에 걸친 9만여대의 서버를 통해, 약 1,000개의 네트워크 기반으로 컨텐츠 전송, 애플리케이션 가속을 위한 플랫폼을 제공하고 있음. 이를 통해 클라우드 기반의 모든 서비스를 누릴 수 있게 도와주고 있음\n\n* 다음 경우에 우리가 아카마이의 서비스를 사용하고 있음\n:- 아이튠즈에서 음악 다운로드 시\n:- 인터넷에서 동영상 감상 시\n:- 인터넷으로 항공권 예약 시\n:- 인터넷으로 뉴스 볼 때\n:- 인터넷 쇼핑 시\n:- 등등\n\n* 문제점: 소비자의 인터넷 사용 패턴 변화, 서비스 복잡도 증가, 기업의 대응 역량 부족\n:- 인터넷 사용자 증가, 사용자 당 동시에 사용하는 기기 수 증가 --> 전 세계적으로 인터넷 사용량 폭증 --> 소비자들은 시간/장소/장치에 구애없이 빠른 속도로 서비스 접속을 원하지만, 기업이 이에 제대로 대응하지 못하고 있음\n:- 특히, 현재 영상 전달 서비스는 매우 복잡해서, 기업이 소비자에게 영상을 전달하기까지 디바이스, 컨텐츠포맷, 보안문제등을 해결하는 등 무수히 많은 중간 과정을 거쳐야 함. 이러한 복잡한 상황에서는 기업들이 적절한 수익을 얻기 어려움\n:- 서비스의 반응이 느리고, 버퍼링이 오래 걸리며, 화질이 낮다면, 소비자는 그 서비스를 외면하게 됨\n\n== Implications ==\n\n== Article ==\n\n아카마이. 하와이 원주민어로 \'\'\'<span style=\"color:blue\">쿨, 스마트, 인텔리전스</span>\'\'\'를 뜻하는 이 회사가 없어지면 큰일 날 정도로 우리 생활 곳곳에 퍼져 있다.\n\n\n아이튠즈에서 음악을 다운로드 받을 때, 인터넷에서 동영상을 감상할 때, 항공권을 예약할 때, 뉴스를 볼 때, 쇼핑 사이트에서 쇼핑할 때 등등 우리는 이미 다양한 서비스를 아카마이를 통해 경험하고 있다.\n\n현재 아카마이는 전세계 모든 웹 트래픽의 약 20%를 전송하며, 매일 수천억의 달하는 인터넷의 요청을 처리하고 있다. 네트워크 인프라 위에 돌아다니는 모든 애플리케이션들을 최종 사용자들까지 빠르고 안전하게 제공하겠다는 것이 이 회사의 목표다.\n\n\n정진우 아카마이 코리아 지사장은 7월12일 가진 기자간담회에서 “아카마이는 전 세계 72개국의 걸친 9만여 대의 서버를 통해 약 1000개의 네트워크 기반으로 콘텐츠 전송과 애플리케이션 가속을 위한 플랫폼을 제공하는 회사로, 클라우드 기반의 모든 서비스를 누릴 수 있게 도와준다”며 “굉장히 많은 트래픽이 지금도 아카마이를 통해 최적화 되고 있다”고 아카마이를 소개했다.\n\n\n최근 아카마이는 스트리밍 커넥티드 디바이스와 보안에 더 많은 관심을 기울이고 있다. 사용자들이 자신들이 휴대한 디바이스를 통해 언제 어디서나 콘텐츠와 애플리케이션을 접속해 끊김없이 사용할 수 있도록 서비스 업체를 지원하는 것이다. 물론 ‘안전’한 상태로.\n\n\n이날 알렉스 카로 아카마이 최고정보책임자(사진)는 새로운 서비스를 소개하기 앞서 “인터넷 사용자가 증가하고 동시에 사용자 당 사용하는 기기의 수도 증가하면서 전 세계적으로 접속 속도(Connection Speeds)가 빨라지고 있다”며 “소비자들은 이 과정에서 시간, 장소, 장치에 구애 없이 원하는 서비스에 접속하길 원하지만, 기업이 이를 대처하지 못하고 있다”고 지적했다.\n\n\n특히 카로 최고정보책임자는 스트리밍 서비스의 중요성을 강조하며 “현재 영상 전달 서비스는 굉장히 복잡해서 기업이 소비자에게 영상을 전달하기까지 디바이스, 콘텐츠 포맷, 보안 문제들을 해결하는 등 무수히 많은 중간 과정들을 거쳐야 한다”며 “이런 방식으로는 기업들이 적절한 수익을 얻지 못한다”고 말했다.\n\n\n아카마이는 오늘날의 소비자가 더 이상 버퍼링, 낮은 화질이나 심지어 느린 반응 으로 인해 서비스를 사용하는 것이 지연되거나 끊기게 되면 그걸 못참아 하고 이를 지원하는 업체로 바로 갈아탄다는 점을 기업들도 잘 알고 있다고 전했다.\n\n\n카로 최고정보책임자는 “아카마이 유니버셜 스트리밍 서비스를 이용하면 각 디바이스별로 다양한 형태의 콘텐츠를 새로 만들거나 전송 속도를 고려할 필요 없이 쉽게 접속 환경을 구현할 수 있다”며 “기업은 그저 H.264 동영상을 아카마이에 제공하고, 아카마이는 네트워크 패키징과 네트워크 트랜스 코딩을 이용해 동영상을 보안 문제도 해결하고, 콘텐츠 포맷도 다양하게 갖춰 소비자에게 전달한다”고 설명했다.\n\n\n네트워크 패키징은 포맷의 문제를, 네트워크 트랜스코딩은 비트레이트 문제를 해결해주는 아카마이 서비스다.\n\n\n물론, 이 과정에서 기업들이 겪는 불편함도 있다. 표준 동영상으로 H.264가 자리 잡지 않은 과정에서 기업들이 아카마이만 믿고 모든 동영상을 H.264로 바꿔야 한다.  이에 대해 카로 최고책임자는 “만병통치약은 없다. 선택은 결국 고객의 몫”이라며 “다만 여러 측면을 감안했을 때, H.264가 향후 오래 지속될 안정적인 포맷이라고 생각한다”고 대답했다.\n\n\n이처럼 아카마이가 스트리밍 커넥티드 분야 말고 중점을 두는 서비스가 또 있다. 바로 보안서비스다.  정진우 지사장은 “점점 더 많은 기업들의 비즈니스 중요 데이터와 운영체제가 클라우드로 옮겨가고 있는 만큼, 단순 정보 전달을 넘어서 안전하게 정보를 전달하는 것에 중점을 두고 있다”며 “최근 7.7 디도스 공격 등으로 기업들이 보안 서버를 늘리는 등 하드웨어 쪽 장비를 추가해서 보안을 강화하려고 하는데, 아카마이는 9만대 이상의 서버 엣지 플랫폼을 기반으로 유연하고 지능적인 웹 애플리케이션 방화벽 서비스를 제공한다”고 말했다.\n\n\n아카마이 엣지 플랫폼에 구축된 약 8만5000개의 달하는 웹 애플리케이션 방화벽은 HTTP와 SSL 트래픽 내의 잠재된 공격이 고객의 원 데이터 센터에 도달하지 않도록 감지하고 완화하는 성능을 지원한다. 아카마이가 고객을 대신해서 엣지플랫폼 단에서 95% 정도의 이상 트래픽을 소화하고, 5%의 트래픽 정도만 고객에게 전달해 고객 서버가 다운되는 경우를 막는다는 것.\n\n\n유니버셜 스트리밍과 엣지 토크나이제이션은 현재 베타 테스트 중이며, 2012년 1분기에 정식 출시될 예정이다.\n\n== References ==\n\n* [[Bnote_2013#Akamai_-_CDN_Acceleration]]\n* [[아카마이,_\"CDN_넘어_하이퍼커넥티드로\"]]','utf-8'),(42,'== Summary ==\n\n* Date: 2012-02-09\n* [http://www.bloter.net/archives/95561 Bloter.net 기사 원문]\n\n* 아카마이, 2012년 2월 8일 \'블레이즈 (Blaze)\' 인수 발표.  이를 통해 클라우드 기반의 서비스를 제공하는데 도움이 될 것으로 기대\n\n* 블레이즈는 2010년 설립된 캐나다 소재 서비스 업체로, 스크립트 작동을 최적화시켜 웹사이트 불러오는 시간을 단축시켜주는 기술을 보유함. (기업이 자주 찾는 데이터를 자사 서버에 저장하여 보다 빠르게 모바일 기기에서 받아볼 수 있게 지원)\n\n== Implications ==\n\n* 기업은 점점 더 Rich한 Interactive 웹 경험을 사용자에게 제공하고 있음 --> 아카마이는 이런 환경에서 동작하는 서비스를 최종 사용자에게 좀 더 원활하게 제공하려 함\n\n* 특히 사용자가 사용하는 디바이스가 PC에서 스마트폰, 태블릿 등으로 다변화하고 있음 --> 성능이 부족한 모바일 기기에서도 PC에서 경험하는 웹 환경과 똑같은 경험을 느낄 수 있도록 하는 것이 중요함 --> 이를 위해 모바일 관련 웹 가속화 기술을 보유한 업체를 인수한 것으로 보여짐 (\'블레이즈\'와 \'코텐도\'가 서로 다른 기기에서 웹 페이지를 최적화하여 볼 수 있는 기술을 지원함)\n\n* 블레이즈가 가진 FEO (Front-end Optimization) 기술은, 다양한 기기에서 웹 페이지를 불러들일 때 속도 저하는 지연을 방지함 --> 큰 규모의 소프트웨어 다운로드나 스크립트와 이미지 같이 페이지 자원을 많이 잡아먹는 일들을 빠르게 처리할 수 있게 함. (페이지 로드에 필요한 요청 수를 줄이고, 브라우저의 렌더링 경험을 개선시키는 블레이즈의 기술이 아카마이의 플랫폼과 결합하여 더 큰 속도 개선 효과를 만들어 낼 것으로 보여짐)\n\n== Article ==\n\n콘텐츠 전송망(CDN), 애플리케이션 전송망(ADN) 업체가 아닌 클라우드 플랫폼 제공자가 되겠다고 나선 아카마이의 행보가 눈에 띈다. 지난해 CDN업체이자 웹페이지와 모바일 앱 가속화 솔루션 개발하는 ‘코텐도’를 인수한데 이어 이번에도 유사한 업체인 ‘블레이즈’를 인수했다.\n\n\n아카마이는 2월8일(현지기준) 블레이즈소프트웨어를 인수했다고 [http://www.akamai.com/html/about/press/releases/2012/press_020812.html 발표]했다. 블레이즈는 2010년에 만들어진 캐나다 소재 서비스 업체로 \'\'\'<span style=\"color:blue\">스크립트 작동을 최적화시켜 웹사이트를 불러오는 시간을 단축시켜주는 기술</span>\'\'\'을 가졌다. 자사 서버를 활용해 사용자가 웹사이트 다운로드를 더 빨리하게 해주는 서비스를 제공한다.\n\n\n릭 맥코넬 아카마이 제품 개발 수석부사장은 “기업이 온라인과 각종 기기를 이용해 풍부한 대화형 웹 경험을 제공하고 있다”라며 “아카마이는 이런 기업 환경을 최종 사용자에게 좀 더 원활하게 제공할 계획으로, 이번 블레이즈 인수가 더욱 강력하고 강화된 클라우드 기반의 서비스를 제공하는데 도움이 될 것으로 기대하고 있다”라고 말했다.\n\n\n아카마이는 장소와 시간에 상관없이 어떤 유무선 환경에서 다양한 애플리케이션을 사용할 수 있게 도와주는 환경을 준비하고 있다. 특히 PC와 비교했을 때 성능이 부족한 모바일 기기에서도 PC에서 경험하는 웹 환경과 똑같이 경험을 느낄 수 있도록 방법을 모색 중인 것으로 보인다. 최근 유독 모바일 관련 웹페이지 가속화 기능을 가진 업체들을 인수했기 때문이다.\n이번에 인수한 블레이즈와 앞서 인수한 코텐도가 서로 다른 기기에서 웹 페이지를 최적화해서 볼 수 있는 기술을 지원한다.\n\n\n블레이즈는 앞단 최적화(FEO, front-end optimization)란 기술을 갖고 있다. 다양한 기기에서 웹페이지를 불러들일 때 속도 저하나 지연을 방지한다. 큰 규모의 소프트웨어 다운로드나 스크립트와 이미지 같은 페이지 자원을 많이 잡아먹는 일을 빠르게 처리할 수 있게 돕는다. 기업이 자주 찾는 데이터를 자사 서버에 저장해 보다 빠르게 모바일 기기에서 받아볼 수 있게 지원한다.\n코텐도 역시 기업들이 웹페이지에서 실시간 보고서 작성과 분석을 원할 경우 이를 빠른 속도로 불러오고 실행할 수 있는 기술을 지원한다.\n\n\n지난 1월 한국을 방한한 데이빗 리치 아카마이 APAC 부사장은 “코텐도 인수 배경에 대해서는 자세히 밝힐 순 없지만, 아카마이가 모바일 분야에 상당한 관심을 가지고 있다”라며 “인수한 기업의 기술을 인텔리전트 플랫폼에 녹여내 혁신을 강화할 것”이라고 말한 바 있다.\n\n\n인텔리전트 플랫폼은 아마카이가 기업에게 제공하는 웹, 모바일, 클라우드 네트워크 환경을 통합해 사용자와 연결될 수 있는 단일 플랫폼이다.\n\n\n당시 그는 “현재 아카마이가 주목하고 있는 부문은 핵심 애플리케이션을 모바일에서 쉽게 활용할 수 있게 옮겨오는 것”이라며 “다양한 미디어 콘텐츠를 기기에 옮겨오는 것에 대해 우리는 항상 관심을 가지고 고민하고 있다”라고 덧붙였다.\n\n\n이번 블레이즈 인수도 아카마이 인텔리전트 플랫폼 개선에 기여할 것으로 보인다. 마이클 웨이더 블레이즈 최고경영자는 “페이지를 로드하는데 필요한 요청 수를 줄이고 웹브라우저 렌더링 경험을 개선시키는 블레이즈의 기술이 아카마이의 플랫폼과 결합해 더 큰 시너지를 만들어 낼 것으로 기대한다”라고 말했다.\n\n== References ==\n\n* [[Bnote_2013#Akamai_-_CDN_Acceleration]]','utf-8'),(43,'== Summary ==\n\n* Date: 2012-01-19\n* [http://www.bloter.net/archives/92711 Bloter.net 기사 원문]\n\n== Implications ==\n\n\n== Article ==\n\n“하이퍼커넥티드 세계에서 혁신을 가속화하겠다.”\n\n아카마이가 단순한 콘텐츠 전송망(CDN) 겸 애플리케이션 전송망(ADN)업체가 아닌 클라우드 플랫폼 제공자가 되기 위해 변신을 꾀하고 있다. ‘하이퍼커넥티드’는 그 과정에서 나온 전략으로 사용자가 언제, 어디서나, 자유롭게 웹에 접근할 수 있게 돕겠다는 아카마이의 의지가 담겨 있는 말이다.\n\n지난해 6월 시스코가 발간한 ’2010~2015 시스코 비주얼 네트워킹 인덱스’ 보고서에 따르면 현재 350억개의 기기가 웹상에 연결돼 있다고 한다. 그리고 2년후가 되면 웹에 연결된 기기의 수는 기하급수적으로 늘어나 약 1조개에 달할 예정이다. 엄청나게 많은 기기들이 웹과 연결된, 아카마이가 주장하는 ‘하이퍼커넥티드’ 세상이 열리는 것이다.\n\n1월19일 한국을 방한한 데이빗 리치 아카마이 APAC 부사장은 “오늘 우리는 하이퍼커넥티드된 세계에 살고 있다”라며 “지금도 너무 많은 숫자의 기기들과 애플리케이션이 웹과 연결돼 있는데, 아카마이는 앞으로 사용자가 어떤 기기를 사용하고 어디에 위치해 있건 이를 매끄럽게 처리할 수 있게 적극 나설 계획”이라고 말했다.\n\n전세계 인터넷 사용자의 85%가 아카마이의 단일 서버인 ‘네트워크 홉(hop)’에 연결돼 있다고 한다. 전세계 웹 트래픽의 30% 이상은 아카마이 플랫폼을 거쳐가고 있다. 아카마이가 지구상의 어떤 기업보다 인터넷을 잘 이해하고 있다는 리치 부사장의 발언이 과장되게 들리지는 않는다.\n\n리치 부사장은 “지난해 아카마이는 좋은 성과를 냈는데, 그 이상을 올해 달성할 수 있을 것 같다”라고 강한 자신감을 보였다. 특히 그는 일본과 한국을 중심으로 한 아시아 태평양 지역에서의 성과를 기대했다. 인터넷 보급률, 초고속 인터넷 도입률이 아태지역을 중심으로 급격히 성장하고 있기 때문이다.\n\n리치 부사장은 이번 분기에 발표될 ‘전세계 인터넷 현황’ 보고서에 따르면 전세계에서 가장 빠른 인터넷 속도를 자랑하는 100대 도시가 모두 아태지역에 몰려있으며, 그 중 10곳이 한국에 있다고 밝혔다. 현재 아태지역 인터넷 사용자수는 8억2500만명으로 전세계 인터넷 사용자의 42%가 아태지역에 몰려있다고 덧붙였다. 아카마이의 조사에 따르면 이 인구는 2015년이 되면 7억명이 늘어난 15억2500만명에 달할 것으로 예상하고 있다.\n\n리치 부사장은 “현재 한국은 전세계에서 인터넷 접속 속도가 가장 빠른 곳으로 파악하고 있다”라며 “현재 우리가 관찰한 바에 의하면 한국의 평균 인터넷 속도는 16.7Mbps로, 이는 전년동기대비 17% 성장한 셈”이라고 밝혔다.\n\n인터넷 시장에 이해도를 바탕으로 아카마이는 올 한해 모바일, 비디오, 보안, 클라우드에 집중할 계획이다. 이날 같이 한국을 방한한 브루노 고비스 아카마이 APCA 엔터프라이즈 클라우드 이사는 “특히 모바일과 비디오는 기업들이 성공하려면 반드시 집중해야 할 분야”라고 강조했다.\n\n특히 다양한 모바일 기기를 사용해 웹 서비스를 즐기는 사용자가 점점 증가하면서 기업은 사용자들이 최고의 성능, 최적의 확장성, 최고 수준의 보안이 확보된 상태에서 인터넷에 접근할 수 있게 도와야 한다는 것이다. 특히 비디오 콘텐츠를 감상하는 사용자 수가 급격히 증가하면서 이를 관리하는 일이 기업에게 중요해질 것이라고 분석했다.\n\n고비스 이사는 “이미 2010년 전세계 500개 이상의 미디어 업체가 아카마이의 중요 솔루션인 아카마이 HD 네트워크를 채택했다”라며 “미디어 수요가 폭발적으로 증가함에 따라 아카마이는 자사 ‘TV에브리웨어솔루션’을 통해 소비자가 즐겨보는 TV프로그램을 여러 기기를 통해 시청하도록 지원할 예정”이라고 말했다.\n\n\n== References ==\n\n* [[Bnote_2013#Akamai_-_CDN_Acceleration]]','utf-8'),(44,'== Wikini Hot Pages ==\n\n* [http://kandinsky/wikini/index.php?title=Special%3ASearch&search=bnote&go=Go Bnote Search]\n* [[Bnote_2013]] | [[Bnote_patidea_2013]] | [[Bnote - References ( Papers / Patents / Articles )]]\n* [http://kandinsky/wikini/index.php/Special:AllPages Special:AllPages] | [http://kandinsky/wikini/index.php/Special:ListFiles Special:ListFiles]\n\n== Links ==\n* [https://mail.google.com/mail Gmail] | [http://www.naver.com/ Naver] | [http://github.com/ GitHub] | [http://mobile.olleh.com/index.asp?code=A000000 Olleh_SMS]\n* [http://klimt/ Klimt (Rolling-Stones)]\n\n== Getting started ==\n* [http://www.mediawiki.org/wiki/Manual:Configuration_settings Configuration settings list] | [http://www.mediawiki.org/wiki/Manual:FAQ MediaWiki FAQ] | [https://lists.wikimedia.org/mailman/listinfo/mediawiki-announce MediaWiki release mailing list] | [http://meta.wikimedia.org/wiki/Help:Contents User\'s Guide]','utf-8'),(45,'== Wikini (radiohead) Hot Pages ==\n\n* [http://kandinsky/wikini/index.php?title=Special%3ASearch&search=bnote&go=Go Bnote Search]\n* [[Bnote_2013]] | [[Bnote_patidea_2013]] | [[Bnote - References ( Papers / Patents / Articles )]]\n* [http://kandinsky/wikini/index.php/Special:AllPages Special:AllPages] | [http://kandinsky/wikini/index.php/Special:ListFiles Special:ListFiles]\n\n== Links ==\n* [https://mail.google.com/mail Gmail] | [http://www.naver.com/ Naver] | [http://github.com/ GitHub] | [http://mobile.olleh.com/index.asp?code=A000000 Olleh_SMS]\n* [http://klimt/ Klimt (Rolling-Stones)]\n\n== Getting started ==\n* [http://www.mediawiki.org/wiki/Manual:Configuration_settings Configuration settings list] | [http://www.mediawiki.org/wiki/Manual:FAQ MediaWiki FAQ] | [https://lists.wikimedia.org/mailman/listinfo/mediawiki-announce MediaWiki release mailing list] | [http://meta.wikimedia.org/wiki/Help:Contents User\'s Guide]','utf-8'),(46,'== Wikini (radiohead) Hot Pages ==\n\n* [http://kandinsky/wikini/index.php?title=Special%3ASearch&search=bnote&go=Go\nBnote Search]\n* [[Bnote_2013]] | [[Bnote_patidea_2013]] | [[Bnote - References (\nPapers / Patents / Articles )]]\n* [http://kandinsky/wikini/index.php/Special:AllPages\nSpecial:AllPages] |\n[http://kandinsky/wikini/index.php/Special:ListFiles\nSpecial:ListFiles]\n\n== Links ==\n* [https://mail.google.com/mail Gmail] | [http://www.naver.com/ Naver]\n| [http://github.com/ GitHub] |\n[http://mobile.olleh.com/index.asp?code=A000000 Olleh_SMS]\n* [http://klimt/ Klimt (Rolling-Stones)]','utf-8'),(47,'== Wikini (radiohead) Hot Pages ==\n\n* [http://kandinsky/wikini/index.php?title=Special%3ASearch&search=bnote&go=Go Bnote Search]\n* [[Bnote_2013]] | [[Bnote_patidea_2013]] | [[Bnote - References ( Papers / Patents / Articles )]]\n* [http://kandinsky/wikini/index.php/Special:AllPages Special:AllPages] | [http://kandinsky/wikini/index.php/Special:ListFiles Special:ListFiles]\n\n== Links ==\n* [https://mail.google.com/mail Gmail] | [http://www.naver.com/ Naver] | [http://github.com/ GitHub] | [http://mobile.olleh.com/index.asp?code=A000000 Olleh_SMS]\n* [http://klimt/ Klimt (Rolling-Stones)]','utf-8'),(48,'HMM: Hidden Markov Model\n\n== Reference ==\n: http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n\nhttp://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n\n----\n* R의 HMM 기능들\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n* References\n:* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n:* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n:* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n:* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n:* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n:* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n:* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n:* HTK [http://htk.eng.cam.ac.uk/register.php]\n:* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]','utf-8'),(49,'HMM: Hidden Markov Model\n\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n\nhttp://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n\n----\n* R의 HMM 기능들\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]','utf-8'),(50,'== HMM: Hidden Markov Model 이란 ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n\n\n== R packages for HMM ==\n\n=== R의 HMM 기능들 ===\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]','utf-8'),(51,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R packages for HMM ==\n\n=== R의 HMM 기능들 ===\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]','utf-8'),(52,'== ## bNote-2013-03-29 ==\n\n=== DailyTask ===\n\n* IOWA\n\n\n=== MBO 2013 (목표 최종 확정) ===\n\n* 정명준 MBO\n <pre>\n\n30%, ~10/31\n- I/O Workload Analysis 기술 연구\n  : Dominant I/O Pattern Mining 및 Machine Learning 기반의\n    I/O 패턴 모델링 및 예측\n  : I/O Pattern 분석/예측 모델 수립\n  : I/O Pattern Mining/Learning 엔진 구현 (Python, R, Shell-script)\n\n30%, ~10/31\n- Data Placement 기술 연구\n  : Workload Analysis 결과로 얻어진 I/O Insight/Prediction을\n    활용하여 Data를 적소에 미리 배치\n  : Linux Kernel Module 형태로 Tiering 기술 형태로 구현\n  : Proactive Data Placement를 통해 분산 스토리지의 I/O 성능\n    80% 이상 개선 검증 (시뮬레이션, 혹은 Real 시스템 기반)\n  \n20%, ~10/31\n- A급 특허 3건 작성 및 심의 통과\n\n20%, ~10/31\n- 논문 1편 (To be accepted)\n\n</pre>\n\n\n* 과제 MBO (이전문님)\n <pre>\n* 분산 플랫폼 관련 특허 15편 이상 특허심의 통과 (전략출원 2편 이상 심의 통과) (30%)\n* 분산 플랫폼 관련 논문 2편 이상 accept (20%)\n* I/O coordination과 Proactive Placement를 통해 분산 스토리지의 I/O 성능 80% 이상 개선 검증\n  (HW RAID 대비) (15%)\n* 분산 Deduplication 기술을 통해 분산 스토리지에서 데이터 제거효율 3배, Coverage 4 node 달성 (3x@4node) (15%)\n* 분산 I/O Coordination 관련 기술이전 1건 (20%)\n</pre>\n\n=== 과제 변경 ===\n\n <pre>\n\n과제명: Intelligent Large-scale Data Management\n (구과제명) Real-Time Big Data Platform\n\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n\n</pre>\n\n== ## bNote-2013-03-28 ==\n\n=== SW설계기술리더양성 교육 지원 ===\n* SW Architect 사전 교육\n\n==== 현업 프로젝트 기획서 ====\n\n* 지원과정\n: SW 설계 리더 양성과정\n\n* 과제명\n: Data-intensive Storage\n\n* 프로젝트 참여자\n <pre>\n이주평	전문 연구원	Project Leader\n정명준	전문 연구원	시스템 설계, 요소기술 연구, 기능모듈 구현\n유개원	전문 연구원	요소기술 연구, 기능모듈 구현\n이형주	SDS 차장	기능모듈 구현, 기능/성능 검증\n</pre>\n\n* 과제 담당 임원\n: 심은수 상무\n\n* 과제 개요\n <pre>\n[배경 및 현안]\n□ 데이터 폭증으로 데이터센터/기업의 클라우드 스토리지 니즈 증대\n□ 클라우드 스토리지의 핵심 경쟁력은 성능 및 용량 향상 기술에 있음\n□ H/W 수준을 높이거나 S/W 최적화 기반으로 시스템의 성능을 개선\n   하는 기존 접근 방식으로는 H/W 한계를 넘어서는 성능 향상은 어려움\n□ 데이터 I/O 속도와 데이터 저장 효율을 획기적으로 개선할 수 있게 하는\n   지능적 Data Management 기술은 클라우드 스토리지 시스템의 경쟁력을\n   혁신하는 핵심 S/W 기술임\n\n[목적]\n□ 본 Sub Task에서는 지능적 Data Management 기술 중,\n   데이터 I/O 속도 향상 기술을 연구/개발한다\n   * I/O Workload Analysis에 기반한 Proactive Data Placement 기술 확보\n     - Real trace data에 대한 I/O Workload Analysis를 통해\n       dominant workload 패턴 발굴 및 I/O 예측 모델 학습\n     - I/O 예측 모델에 기반한 multi-tier (horizontal - vertical) 간\n       proactive data 배치 수행\n</pre>\n\n* 목표\n <pre>\n[기능/성능/품질]\n□ I/O Workload Analysis에 기반한 Proactive Data Placement\n  - I/O Workload Analysis 모듈\n    - Real trace data 수집 기능\n    - Trace data parsing 및 transform 기능 (analysis를 위한 전처리)\n    - Dominant workload pattern 추출 및 I/O model 학습\n  - Proactive Data Placement 모듈\n    - Tier management 및 data move 기능\n	- I/O monitoring 및 hot/cold 판단 기능\n\n[중간 산출물]\n□ Hot/Cold Data Placement 모듈\n  - 핵심적인 automated tiering 기능 구현\n    : Data access 패턴 관찰을 통해, hot data는 고속의 storage tier에,\n      cold data는 상대적으로 느린 속도의 storage tier에, 주기적 배치\n  - 동일 수준의 시스템에서, 동일 workload에 대해,\n    경쟁사 (E사) 수준의 성능 달성 (metric: average IOPS)\n\n[최종 결과물]\n□ Proactive Data placement 모듈\n  - I/O 예측 모델에 기반한 proactive data placement\n    : Dominant workload 패턴 분석 및 I/O 예측 모델에 기반한\n	  multi-tier 간 선제적 data 배치를 통해 I/O 성능 향상\n  - 동일 수준의 시스템에서, 동일 workload에 대해,\n    경쟁사 (E사) 대비 100% 이상의 성능 향상 달성 (metric: average IOPS)\n</pre>\n\n* 기대 효과\n <pre>\n□ 지능적 Data Management 기술은 Big Data를 다루는\n   클라우드 스토리지 서비스의 핵심 기술로 활용 가능\n   - Big Data 시장에서는 특히 스토리지 분야가 연간 61.4%의 성장율로\n     전체 시장 성장을 주도\n</pre>\n\n* 과제 구성\n <pre>\n[전체 Architecture]\n□ Proactive Data Placement 시스템은\n   Workload Analysis 모듈과 Data Placement 모듈로 구성됨\n  * Workload Analysis 모듈은 Trace Log 데이터에 대한\n    Off-line I/O Analysis를 수행하여 I/O 패턴에 대한 Insight을 확보함\n    (e.g., 어느 위치의 Data가 언제쯤 Access될 것인지를 예측)\n  * Data Placement 모듈은 I/O 패턴에 대한 Insight 정보와, 실시간으로\n    모니터링되는 시스템 상태 정보를 이용하여, Data를 미리 적소에 배치함\n\n[과제 적용부 기술항목]\n□ Workload Analysis 모듈: I/O Prediction Model Optimization 이슈\n  - 응용 및 시스템 특성에 따라 Workload 특성이 다를 수 있음\n    Workload 별로 Prediction Model을 구성하는 주요 X\'s 의 최적화 필요\n□ Data Placement 모듈: Overhead 최소화 및 Tiering 구현 최적화 이슈 \n  - Real-time Monitoring으로 인해 시스템에 가해지는 Overhead 최소화 필요\n  - Tiering 기능 구현 시 I/O 특성 및 시스템 구조를 반영한 최적화 필요\n</pre>\n\n\n==== 입과 추천서 ====\n\n[본인 업무 이력]\n\n* 2004.08 ~ 2007.09 : Security & Trusted Computing 기술 연구/개발\n:- 휴대폰 Content/Right Protection 기술인 OMA DRM S/W 개발, 무선사에 기술 이전\n:- Secure MMC를 위한 Crypto Engine 개발 참여 및 MMC IOP T/F 활동, 메모리사에 기여\n:- System의 무결성 보장 기술인 Trusted Computing 기술 연구 주도, Mandatory Access Control 기술을 무선사에 이전, LiMo (Linux Mobile) Security 표준에 반영 (SubPL)\n:- A급 특허 6건 출원, 논문 2건 (ACM SACMAT \'08 등)\n\n* 2007.10 ~ 2008.05 : 전사 6시그마 MBB (Master Black Belt) 양성 과정\n:- 제 16기 6시그마 MBB 과정에 입과하여 6시그마 이론 연구 및 실습 과제를 진행하고 BB 교육 과정 강의를 진행하였음. MBB 인증 시험 통과\n\n* 2008.06 ~ 2010.10 : Virtualization 및 Operating System 기술 연구/개발\n:- H/W가상화 기술인 Xen Hypervisor의 Security 연구 참여\n:- OS가상화 기술 기반의 State Migration S/W 개발, 스토리지사업부로 기술이전(SubPL)\n:- Russia연구소와 협력, Android 부팅속도를 향상시키는 FastBoot 기술 연구 (SubPL)\n:- 본사 사업지원팀 Vision 2020 T/F에 핵심 멤버로 참여, 15개 미래 기술 테마 발굴\n:- A급 특허 6건 출원 (전략 출원 2건), 논문 1건 (MobiCom \'09)\n\n* 2010.11 ~ 2013.현재 : Data-intensive Storage 기술 연구/개발\n:- I/O Workload Analysis에 기반한 Proactive Data Placement 기술 연구 주도\n::- Workload Analysis에서 획득한 I/O에 대한 근본적인 이해를 바탕으로 Data Management 알고리즘을 혁신, 스토리지 시스템 성능을 향상시키는 기술임\n:- 본 과제는 메모리사의 사업영역 확장 및 \'클라우드 스토리지 서비스\'를 위한 스토리지 시스템 기술 확보에 기여하고 있음\n:- A급 특허 6건 출원, 논문 3건 (ICCE 등)\n\n[소속부서장 추천 사유]\n\n* (양성 후 활용계획)\n:- 스토리지 시스템 설계/구현 시 S/W Architect로 활용\n* (인물평 및 추천사유)\n:- 정명준 전문은 시스템 분야에 대한 깊은 기술적 이해와 원만한 커뮤니케이션 능력을 바탕으로한 성공적인 프로젝트 발굴/주도 경험을 가지고 있습니다.\n:- 향후 Architect로서, 해당 과제의 S/W 설계 리딩을 통해 스토리지 시스템의 차별화된 기술 경쟁력을 만들어 내는 데에 기여할 수 있을 것으로 판단되어, 금번 S/W 설계 리더 과정에 추천합니다.\n\n== ## bNote-2013-03-26 ==\n\n=== DailyTask ===\n\n* IOWA Proactive Data Placement Formulation\n* Data Representation (as a pre-processing for association rules mining)\n\n* Patentization\n:- Distributed Multi-level Caching\n:- IO Pattern-optimal Data Placement for Tiering\n:- Virtualization-aware Caching/Tiering/Placement\n\n* Study\n:- Btier\n:- Bcache\n:- Fusion IO Caching Technology (directCache, ioTurbine)\n:- EMC FAST (Fully Automated Storage Tiering)\n:- OpenStack\n:- Xen\n:- VASA, VAAI (VMware의 storage virtualization 기술들)\n:- PCIe fabric switching\n:- Software Defined Storage\n:- Virstore? (VMware가 인수?)\n\n* 심상무님께 주간보고 내용\n:- Tiering Test SW Platform 구축 건 (Open source 활용, SDS 이형주 차장님과 함께)\n:- Real Trace Log Data 확보 진행 건 (수퍼컴센터의 Analytics Workload Trace, VDI Trace)\n\n=== Patentization ===\n\n* Access Pattern Aware Tiering\n\n\n=== Memo ===\n\n* Turbine: <기계> 높은 압력의 유체를 날개바퀴의 날개에 부딪치게 함으로써 회전하는 힘을 얻는 원동기. 사용하는 유체의 종류에 따라 수력 터빈, 증기 터빈, 가스 터빈 따위가 있다.\n\n== ## bNote-2013-03-25 ==\n\n=== DailyTask ===\n\n* 업무 File 정리\n* IOWA Proactive Data Placement Formulation\n\n=== Patidea ===\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n:- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [http://www-03.ibm.com/systems/software/gpfs/][http://public.dhe.ibm.com/common/ssi/ecm/en/pos03096usen/POS03096USEN.PDF]\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n* advanced tiering: access pattern-aware optimal placement (APOP)\n:- Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n:: 예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n:- 이에 필요한 data access pattern 모니터링/분석 방법\n:: 데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\n::: NIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n:- 이를 위해 필요한 system architecture 구조\n:: 기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n=== Formulation: IOWA based Proactive Data Placement ===\n\n* Formulating [[IOWA]] or [[I/O Workload Analysis]]\n:- to clarify the total amount of work\n:- to clarify the sub-tasks (can be modularized)\n:- to clarify the area to get focused\n\n\n=== Information: S사, K사 ===\n\n* Kaminario, Solidfire\n: From 서정민 전문\n: SSIC 미팅노트로부터 FACT를 각색한 정보를 공유합니다.  제 의견은 반영하지 않았습니다.\n\n* SolidFire (SSIC 초기미팅 결과)\n:* 1. Company Overview\n::- 3 years, 82 people\n::- $37M in funding (현재 Series B 단계로, 2013년 Series C 가능성 있음)\n::- 12 customers, 4 announced: 2 private cloud enterprise customers. \n::- Multi-tenancy가 기반인 Cloud 시장을 타겟으로 제품 제작 (OpenStack, CloudStack 연동)\n:* 2. Technology: QoS, Scalability, Inline deduplication/Compression\n:: (a) QoS\n::: OS 내에서 QoS를 Volume 단위로 관리 \n::: IOPS/latency QoS support (No R/W separate QoS) \n:: (b) Scalability\n::: Full data distribution across all the nodes \n::: All the nodes contributes to rebuilds\n:* 3. Current Arch./Tech. (GA)\n::- System configuration: 5~100 nodes (they have 40 nodes in test)\n::- H/W Configuration\n:: (a) CPU: Dual 2.5GHz Sandy Bridge with 6 cores each. \n::: 10 Cores는 mostly compute intensive work including dedup and compression. \n::: 2 Core는 handles IO to SSDs\n:: (b) SSD: Viking for boot/metadata, Intel SATA SSDs (relies on supercap in SSD)\n:: (c) Network: iSCSI (FC/NFS in the future, NFS just for small filer)\n::- Performance\n::: Latency Avg is .5ms to 2ms. Worst is 20 to 30 ms. \n:* 4. 금년도 추가 개발계획 (일부)\n::- Remote replication, sync and async, coming in Q3\n::- Encryption is also on the roadmap. \n\n* Kaminario (SSIC 초기 미팅 결과)\n:* 1. Company Overview\n::- Found in 2008.3, Sequoia(VC) funded\n::- 30 patents (the engineers have 76 from the previous jobs)\n::- Target: general-purpose storage system (OLTP, OLAP, VDI)\n::: focusing Latency, Throughput, IOPS all\n::- Shipping scale-out systems for the last 2 and 1/2 years\n::- Competitors: XtremIO, SolidFire\n::- 엔터프라이즈 기능 포커스: resiliency, self-healing, automation 중심\n:* 2. Technology\n::- Core 기술에 대한 파악 결과 없음\n:* 3. Previous Arch.\n::- Dell Blade 서버 방식으로 Fusion-IO 탑재\n:* 4. Current Arch. (개발 중)\n::- 1U rack server 기반 SMART or STEC SAS SSDs 사용\n::: low cost SSDs, low end Xeon, 32GB memory 등 Cost를 줄이는 방식 채용\n::: \"They use LSI SAS controller but don’t use dual port functionality.\"\n::: No SATA SSD (SATA SSD는 신뢰성 문제 야기하는 것으로 판단)\n::: -> SSIC 전문가는 SAS Dual port 기술 개발을 실패하지 않았는가 하는 의문 제기\n::- Performance is about 100,000 IOPS/node.\n::- No Dedup/compression \n::- \"Their SPC-1 result has 20x better price performance than previous SPC results. \"\n::- Currently focus on reducing long tail numbers.  (already has good IOPS)\n::: Performance degradation: < 25% at loss of data node\n\n=== References ===\n\n* [http://www.kaseya.com/download/en-us/white_papers/KaseyaBuyersGuidePaper.pdf IT Systems Management Buyers’ Guide // Kaseya]\n* [http://www.sata-io.org/technology/6Gbdetails.asp SATA-IO Revision 3.1 Specification // Queued Trim Command]\n\n----\n\n== ## bNote-2013-03-22 ==\n\n <pre>\n(EMC (Forum OR World) VNX) ((performance OR \"iops\") AND (\"per dollar\" OR \"dollar per\" OR \"per $\" OR \"/$\" OR \"$/\")) \"vs\" (filetype:pdf OR filetype:ppt OR filetype:pptx)\n</pre>\n\n=== DailyTask ===\n\n----\n==== Books of Machine Learning / Data Mining ====\n* \"Machine Learning\" // Tom Mitchell, McGraw Hill, 1997 ((B.GOOD))\n:- [http://www.cs.cmu.edu/~tom/mlbook.html Book]\n:- [http://www.cs.cmu.edu/~tom/mlbook-chapter-slides.html Slides]\n\n* \"Mining of Massive Datasets\" // Anand Rajaraman, Jeffrey David Ullman ((B.GOOD))\n:- [http://i.stanford.edu/~ullman/mmds.html Book - Online Version]\n:- [http://i.stanford.edu/~ullman/mmds/book.pdf Download the latest book (PDF, 415 pages, approximately 2.5MB)]\n\n----\n\n==== Machine Learning / Data Mining ====\n\n* [http://en.wikipedia.org/wiki/Gradient_descent Gradient Descent]\n* [http://ko.wikipedia.org/wiki/%EC%9D%8C%ED%95%A8%EC%88%98]\n* [http://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95%EC%82%AC%EC%83%81]\n* [http://ko.wikipedia.org/wiki/%EB%AF%B8%EC%A0%81%EB%B6%84%ED%95%99]\n* [http://ko.wikipedia.org/wiki/%ED%8E%B8%EB%AF%B8%EB%B6%84]\n* [http://ko.wikipedia.org/wiki/%ED%8F%89%EA%B7%A0%EA%B0%92_%EC%A0%95%EB%A6%AC]\n* [http://www.iiswc.org/iiswc2008/Papers/012.pdf] Characterization of Storage Workload Traces from Production Windows Servers // Swaroop Kavalanekar, Bruce Worthington, Qi Zhang, Vishal Sharda\n\n----\n\n== ## bNote-2013-03-21 ==\n\n=== Official Death of ... ===\n* What to do? why?\n\n=== DailyTask ===\n\n* list of candidate tasks\n\n:- (~) IOWA DM/ML Planning\n::- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n::- Hadoop case 고려 (in-local-node vertical placement between DRAM-SSD-HDD 뿐만 아니라 horizontal placement (among distributed nodes in Hadoop cluster)도 고려)\n\n:- IOWA ML Study\n::- IOWA:: Bayesian Network study\n::- IOWA:: Neural Network study\n::- IOWA:: HMM study\n::- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n::- (~) IOWA:: Association Rule Mining, Frequent Item Sets Mining (Market Basket Problem)\n\n:- page cache to be revisited\n\n:- Real IO trace 확보 작업\n::- 김혁호 책임이 보내준 업무요청 사례 기반, 업무요청 문서 작성할 것\n::- (V) NetApp (김주영 과장)에게 연락 -> 스토리지 모델 정보 요청 -> 추가 문의 (Management Console에서 보다 자세한 정보를 볼 수는 없는지? NetApp OnCommand System Manager 같은 Tool을 사용해서라도..) -> 김주영 과장과 통화 (오전 10:20 ~ 46), 이번 주 중에 찾아뵙고 말씀 나누는 걸로 ...\n::- EMC 연락\n::- (~) Dell (지근영 대리)에게 연락\n:::- Dell EqualLogic PS6100XV iSCSI arrays [http://www.dell.com/us/enterprise/p/equallogic-ps6100xv/pd] [http://www.equallogic.com/products/default.aspx?id=10661]\n\n:- (~) Distributed Computing Problem Study\n::- CAP theorem (CAP: Consistency, Availability, Partition tolerance) [http://en.wikipedia.org/wiki/CAP_theorem]\n::- Paxos [http://en.wikipedia.org/wiki/Paxos_(computer_science)]\n::- State Machine Replication [http://en.wikipedia.org/wiki/State_machine_replication]\n\n----\n=== Formulation: IOWA Proactive Data Placement (moved to ## bNote-2013-03-25) ===\n----\n\n== ## bNote-2013-03-20 ==\n\n=== DailyTask ===\n\n* list of candidate tasks\n\n:- (~) IOWA DM/ML Planning\n::- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n::- Hadoop case 고려 (in-local-node vertical placement between DRAM-SSD-HDD 뿐만 아니라 horizontal placement (among distributed nodes in Hadoop cluster)도 고려)\n\n:- IOWA ML Study\n::- IOWA:: Bayesian Network study\n::- IOWA:: Neural Network study\n::- IOWA:: HMM study\n::- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n::- (~) IOWA:: Association Rule Mining, Frequent Item Sets Mining (Market Basket Problem)\n\n:- page cache to be revisited\n\n:- Real IO trace 확보 작업\n::- 김혁호 책임이 보내준 업무요청 사례 기반, 업무요청 문서 작성할 것\n::- (V) NetApp (김주영 과장)에게 연락 -> 스토리지 모델 정보 요청 -> 추가 문의 (Management Console에서 보다 자세한 정보를 볼 수는 없는지? NetApp OnCommand System Manager 같은 Tool을 사용해서라도..) -> 김주영 과장과 통화 (오전 10:20 ~ 46), 이번 주 중에 찾아뵙고 말씀 나누는 걸로 ...\n::- EMC 연락\n::- (~) Dell (지근영 대리)에게 연락\n:::- Dell EqualLogic PS6100XV iSCSI arrays [http://www.dell.com/us/enterprise/p/equallogic-ps6100xv/pd] [http://www.equallogic.com/products/default.aspx?id=10661]\n\n:- (V) 2013년 MBO 작성\n::- IOWA PDP (I/O Workload Analysis based Proactive Data Placement) 와 IOBA (I/O Bottleneck Analysis) 두 아이템으로 작성\n\n:- (~) Distributed Computing Problem Study\n::- CAP theorem (CAP: Consistency, Availability, Partition tolerance) [http://en.wikipedia.org/wiki/CAP_theorem]\n::- Paxos [http://en.wikipedia.org/wiki/Paxos_(computer_science)]\n::- State Machine Replication [http://en.wikipedia.org/wiki/State_machine_replication]\n\n----\n\n=== Formulation: IOWA Proactive Data Placement (moved to #bNote-2013-03-21) ===\n\n* [[http://kandinsky/wikini/index.php/Bnote_2013#Formulation:_IOWA_Proactive_Data_Placement]]\n\n=== Supercom Usage Statistics ===\n\n <pre>\nblusjune@jimi-hendrix:[~] $ ssh a1mjjung@supercom\na1mjjung@supercom\'s password:\nLast login: Tue Mar 19 19:28:27 2013 from 75.2.93.158\n----------------------------------------------------------\n| During : 20130311 ~ 20130317                            |\n| Username : a1mjjung , Application(Total jobs) : unix(3)\n----------------------------------------------------------\nTotal RUN time : 2 min 36 secs\nAverage RUN time : 52 secs\nMaximum RUN time : 1 min 14 secs\nAverage Wait time 1 secs\nMaximum Wait time 1 secs\n ---------------------------------------------------------\n</pre>\n\n----\n== ## bNote-2013-03-19 ==\n\n\n\n=== The Market-Basket Model ===\n\n\n=== IOWA::Outlook (MSN FileServer IO Trace // msnfs) ===\n\n* # of IOs (Read/Write/All)\n<pre>\na1mjjung@secm:[microsoft_msn_filesrvr_6h] $ wc -l tracelog.msn_filesrvr.[ARW]\n\n  29345085 tracelog.msn_filesrvr.A\n  19729611 tracelog.msn_filesrvr.R\n   9615474 tracelog.msn_filesrvr.W\n</pre>\n\n* Reads Outlook\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130313_154605.sigio_25.iowsz_2000.t1_2000] $ cat __simout.sigio_25.iowsz_2000.t1_2000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  17\n__valu__sig__ _n_o_sigaddrs :  173401\n__valu__sig__ _sigioc_acc :  5005517\n__valu__sig__ _sigaddrs_efficiency :  28.8667135714\n__valu__sig__ _n_o_addr_total :  5053016\n__valu__sig__ _ioc_total :  19729611\n</pre>\n\n* Writes Outlook\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130313_155325.sigio_25.iowsz_2000.t1_2000] $ cat __simout.sigio_25.iowsz_2000.t1_2000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  8\n__valu__sig__ _n_o_sigaddrs :  211100\n__valu__sig__ _sigioc_acc :  2989803\n__valu__sig__ _sigaddrs_efficiency :  14.1629701563\n__valu__sig__ _n_o_addr_total :  4506823\n__valu__sig__ _ioc_total :  9615474\n</pre>\n\n* Microsoft Production Workload Trace - Related Articles\n\n:- \"Characterization of Storage Workload Traces from Production Windows Servers\", IISWC 2008, Swaroop Kavalanekar, Bruce Worthington, Qi Zhang, Vishal Sharda, Microsoft Corporation [http://www.iiswc.org/iiswc2008/sildes/4_3.pdf Slides], [http://www.iiswc.org/iiswc2008/Papers/012.pdf Papers]\n\n:- \"Write Off-Loading: Practical Power Management for Enterprise Storage\" [http://static.usenix.org/event/fast08/tech/full_papers/narayanan/narayanan.pdf FAST 2008]\n\n=== R Tutorial (Data Frame, Preview) ===\n\n----\n==== Data Frame ====\nA data frame is used for storing data tables. It is a list of vectors of equal length. For example, the following variable df is a data frame containing three vectors n, s, b.\n\n <pre>\n> n = c(2, 3, 5) \n> s = c(\"aa\", \"bb\", \"cc\") \n> b = c(TRUE, FALSE, TRUE) \n> df = data.frame(n, s, b)       # df is a data frame\n</pre>\n\n----\n==== Built-in Data Frame ====\nWe use built-in data frames in R for our tutorials. For example, here is a built-in data frame in R, called mtcars.\n\n <pre>\n> mtcars \n               mpg cyl disp  hp drat   wt ... \nMazda RX4     21.0   6  160 110 3.90 2.62 ... \nMazda RX4 Wag 21.0   6  160 110 3.90 2.88 ... \nDatsun 710    22.8   4  108  93 3.85 2.32 ... \n               ............\n</pre>\n\nThe top line of the table, called the header, contains the column names. Each horizontal line afterward denotes a data row, which begins with the name of the row, and then followed by the actual data. Each data member of a row is called a cell.\n\nTo retrieve data in a cell, we would enter its row and column coordinates in the single square bracket \"[]\" operator. The two coordinates are separated by a comma. In other words, the coordinates begins with row position, then followed by a comma, and ends with the column position. The order is important.\n\nHere is the cell value from the first row, second column of mtcars.\n\n <pre>\n> mtcars[1, 2] \n[1] 6\n</pre>\n\nMoreover, we can use the row and column names instead of the numeric coordinates.\n\n <pre>\n> mtcars[\"Mazda RX4\", \"cyl\"] \n[1] 6\n</pre>\n\nLastly, the number of data rows in the data frame is given by the nrow function.\n\n <pre>\n> nrow(mtcars)    # number of data rows \n[1] 32\n</pre>\n\nAnd the number of columns of a data frame is given by the ncol function.\n\n <pre>\n> ncol(mtcars)    # number of columns \n[1] 11\n</pre>\n\nFurther details of the mtcars data set is available in the R documentation.\n\n <pre>\n> help(mtcars)\n</pre>\n\n----\n\n==== Preview ====\n\nPreview\nInstead of printing out the entire data frame, it is often desirable to preview it with the head function beforehand.\n\n <pre>\n> head(mtcars) \n               mpg cyl disp  hp drat   wt ... \nMazda RX4     21.0   6  160 110 3.90 2.62 ... \n               ............\n</pre>\n\n----\n\n==== Data Import ====\n\n\n\n\n\n\n\n\nIt is necessary to import the sample textbook data into R before you start working on your homework.\n\n* Excel File\n: Quite often, the sample data is in Excel format, and needs to be imported into R prior to use. For this, we use the read.xls function from the gdata package. It reads from an Excel spreadsheet and returns a data frame. The following shows how to load an Excel spreadsheet named \"mydata.xls\". As the package is not in the core R library, it has to be installed and loaded into the R workspace.\n\n <pre>\n> library(gdata)                   # load the gdata package \n> help(read.xls)                   # documentation \n> mydata = read.xls(\"mydata.xls\")  # read from first sheet\n</pre>\n\n* Minitab File\n: If the data file is in Minitab Portable Worksheet format, it can be opened with the read.mtp function from the foreign package. It returns a list of components in the Minitab worksheet.\n\n <pre>\n> library(foreign)                 # load the foreign package \n> help(read.mtp)                   # documentation \n> mydata = read.mtp(\"mydata.mtp\")  # read from .mtp file\n</pre>\n\n* SPSS File\n: For the data files in SPSS format, it can be opened with the read.spss function from the foreign package. There is a \"to.data.frame\" option for choosing whether a data frame is to be returned.\n\n <pre>\n> library(foreign)                 # load the foreign package \n> help(read.spss)                  # documentation \n> mydata = read.spss(\"myfile\", to.data.frame=TRUE)\n</pre>\n\n* Table File\n: A data table can resides in a text file. The cells inside the table are separated by blank characters. Here is an example of a table with 4 rows and 3 columns.\n\n <pre>\n100   a1   b1 \n200   a2   b2 \n300   a3   b3 \n400   a4   b4\n</pre>\n\nNow copy and paste the table above in a file named \"mydata.txt\" with a text editor. Then load the data into the workspace with the read.table function.\n\n <pre>\n> mydata = read.table(\"mydata.txt\")  # read text file \n> mydata                             # print data frame \n   V1 V2 V3 \n1 100 a1 b1 \n2 200 a2 b2 \n3 300 a3 b3 \n4 400 a4 b4\n</pre>\n\nFor further detail of the read.table function, please consult the R documentation.\n\n <pre>\n> help(read.table)\n</pre>\n\n* CSV File\nThe sample data can also be in comma separated values (CSV) format. Each cell inside such data file is separated by a special character, which usually is a comma, although other characters can be used as well.\n\nThe first row of the data file should contain the column names instead of the actual data. Here is a sample of the expected format.\n\n <pre>\nCol1,Col2,Col3 \n100,a1,b1 \n200,a2,b2 \n300,a3,b3\n</pre>\n\nAfter we copy and paste the data above in a file named \"mydata.csv\" with a text editor, we can read the data with the read.csv function.\n\n <pre>\n> mydata = read.csv(\"mydata.csv\")  # read csv file \n> mydata                           # print data frame \n  Col1 Col2 Col3 \n1  100   a1   b1 \n2  200   a2   b2 \n3  300   a3   b3\n</pre>\n\nIn various European locales, as the comma character serves as decimal point, the read.csv2 function should be used instead. For further detail of the read.csv and read.csv2 functions, please consult the R documentation.\n\n <pre>\n> help(read.csv)\n</pre>\n\n----\n\n== ## bNote-2013-03-18 ==\n\n=== DailyPlan ===\n\n* list of candidate tasks\n\n:- Real IO trace 확보 작업\n::- 김혁호 책임이 보내준 업무요청 사례 기반, 업무요청 문서 작성할 것\n::- [V://j] NetApp (김주영 과장)에게 연락 -> 스토리지 모델 정보 요청 -> 추가 문의 (Management Console에서 보다 자세한 정보를 볼 수는 없는지? NetApp OnCommand System Manager 같은 Tool을 사용해서라도..) -> 김주영 과장과 통화 (오전 10:20 ~ 46), 이번 주 중에 찾아뵙고 말씀 나누는 걸로 ...\n::- EMC 연락\n::- [~] Dell (지근영 대리)에게 연락\n:::- Dell EqualLogic PS6100XV iSCSI arrays [http://www.dell.com/us/enterprise/p/equallogic-ps6100xv/pd] [http://www.equallogic.com/products/default.aspx?id=10661]\n\n:- IOWA ML Planning\n::- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n::- Hadoop case 고려 (in-local-node vertical placement between DRAM-SSD-HDD 뿐만 아니라 horizontal placement (among distributed nodes in Hadoop cluster)도 고려)\n\n:- IOWA ML Study\n::- IOWA:: Bayesian Network study\n::- IOWA:: Neural Network study\n::- IOWA:: HMM study\n::- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n\n:- Netflix의 Cloud Computing Challenge 내용 파악\n(심상무님 지시: 거기가 우리보다 앞서 있으니, 어떤 기술들이 필요한지, 이슈가 무엇인지에 대한 힌트를 얻을 수 있을 것임)\n\n:- page cache to be revisited\n\n\n----\n\n=== Linux File Systems: Ext2 vs. Ext3 vs. Ext4 ===\n\n* [http://www.thegeekstuff.com/2011/05/ext2-ext3-ext4/]\n\next2, ext3 and ext4 are all filesystems created for Linux. This article explains the following:\n\n:- High level difference between these filesystems.\n:- How to create these filesystems.\n:- How to convert from one filesystem type to another.\n\n==== Ext2 ====\n\n* Ext2 stands for second extended file system.\n* It was introduced in 1993. Developed by Remy Card.\n* This was developed to overcome the limitation of the original ext file system.\n* Ext2 does not have journaling feature.\n* On flash drives, usb drives, ext2 is recommended, as it doesn’t need to do the over head of journaling.\n* Maximum individual file size can be from 16 GB to 2 TB\n* Overall ext2 file system size can be from 2 TB to 32 TB\n\n\n==== Ext3 ====\n\n* Ext3 stands for third extended file system.\n* It was introduced in 2001. Developed by Stephen Tweedie.\n* Starting from Linux Kernel 2.4.15 ext3 was available.\n* The main benefit of ext3 is that it allows journaling.\n* Journaling has a dedicated area in the file system, where all the changes are tracked. When the system crashes, the possibility of file system corruption is less because of journaling.\n* Maximum individual file size can be from 16 GB to 2 TB\n* Overall ext3 file system size can be from 2 TB to 32 TB\n* There are three types of journaling available in ext3 file system.\n:- Journal ? Metadata and content are saved in the journal.\n:- Ordered ? Only metadata is saved in the journal. Metadata are journaled only after writing the content to disk. This is the default.\n:- Writeback ? Only metadata is saved in the journal. Metadata might be journaled either before or after the content is written to the disk.\n* You can convert a ext2 file system to ext3 file system directly (without backup/restore).\n\n\n==== Ext4 ====\n\n* Ext4 stands for fourth extended file system.\n* It was introduced in 2008.\n* Starting from Linux Kernel 2.6.19 ext4 was available.\n* Supports huge individual file size and overall file system size.\n* Maximum individual file size can be from 16 GB to 16 TB\n* Overall maximum ext4 file system size is 1 EB (exabyte). 1 EB = 1024 PB (petabyte). 1 PB = 1024 TB (terabyte).\n* Directory can contain a maximum of 64,000 subdirectories (as opposed to 32,000 in ext3)\n* You can also mount an existing ext3 fs as ext4 fs (without having to upgrade it).\n* Several other new features are introduced in ext4: multiblock allocation, delayed allocation, journal checksum. fast fsck, etc. All you need to know is that these new features have improved the performance and reliability of the filesystem when compared to ext3.\n* In ext4, you also have the option of turning the journaling feature “off”.\n\n\n----\n\n=== Frequent patterns/items mining examples with R::arules ===\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n* References\n:# [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))]\n:# [http://www.analysis-of-patterns.net/files/bgoethals.pdf Frequent Pattern Mining Slides - Universiteit Antwerpen]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning - Wikipedia]\n:# [http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf Frequent Pattern Mining in Web Log Data]\n:# [http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010 Temporal Pattern Mining Tutorial KDD 2010]\n:# [http://cran.r-project.org/web/packages/arules/index.html arules: Mining Association Rules and Frequent Itemsets]\n:# [http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf Introduction to arules - A computational environment for mining association rules and frequent item sets -- http://cran.r-project.org/]\n:# [http://lyle.smu.edu/IDA/arules/ Intelligent Data Analysis Lab @ SMU]\n:# [http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))]\n:# [http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/ Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning]\n:# [http://en.wikipedia.org/wiki/Sequence_mining Sequence mining]\n:# [http://en.wikipedia.org/wiki/GSP_Algorithm GSP Algorithm]\n:# [http://en.wikipedia.org/wiki/Apriori_algorithm Apriori algorithm]\n:# [http://en.wikipedia.org/wiki/Market_basket_analysis Market basket analysis]\n:# [http://en.wikipedia.org/wiki/Time_series Time series]\n\n\n----\n\n=== R ===\n\n* png file output work-around against \'plot() error\' in R\n:- [http://www.mail-archive.com/r-help@r-project.org/msg40658.html]\n <pre>\nThe png() device does not need an X server to connect to. I think it\nused to in versions gone by, but not any more. Here I\'ve disabled X so\nthat X11() doesn\'t work, but png() still does:\n\n > x11()\n Error in X11(d$display, d$width, d$height, d$pointsize, d$gamma,\nd$colortype,  :\n   unable to start device X11cairo\n In addition: Warning message:\n In x11() : unable to open connection to X11 display \'\'\n > png(file=\"foo2.png\")\n > plot(1:10)\n > dev.off()\n null device\n          1\n\n I suspect your R was compiled without png support. What does the\n\'capabilities()\' function in R tell you?\n\n > capabilities()\n    jpeg      png     tiff    tcltk      X11     aqua http/ftp  sockets\n    TRUE     TRUE     TRUE     TRUE    FALSE    FALSE     TRUE     TRUE\n  libxml     fifo   cledit    iconv      NLS  profmem    cairo\n    TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE\n</pre>\n\n\n\n==== An Introduction to R ====\n\n* [http://cran.r-project.org/doc/manuals/R-intro.html#The-read_002etable_0028_0029-function An Introduction to R - Table of Contents]\n\n\n==== R Tutorial - (http://www.r-tutor.com/) ====\n\n* [http://www.r-tutor.com/gpu-computing/gaussian-process/rvbm Bayesian Classification with Gaussian Process]\n* [http://www.r-tutor.com/content/r-tutorial-ebook R Tutorial with Bayesian Statistics Using OpenBUGS]\n* [http://www.r-tutor.com/bayesian-statistics/openbugs Bayesian Inference Using OpenBUGS]\n* [http://www.r-tutor.com/gpu-computing/svm/rpusvm-2 Support Vector Machine with GPU, Part II]\n\n==== R: Input and output: scripts, saving and loading data ((B.GOOD)) ====\n\n* [http://egret.psychol.cam.ac.uk/statistics/R/savingloading.html Cambridge University]\n\n\n* General file-handling commands\n <pre>\nsetwd(\"c:/myfiles\") # use / or \\\\ to separate directories under Windows (\\\\ becomes \\ once processed through the escape character mechanism)\ndir() # list the contents of the current directory\n</pre>\n\n\n* Running scripts\n <pre>\nsource(\"myfile.R\") # load and execute a script of R commands\n</pre>\n\n* For a startup script\n: edit \".Rprofile\" in your home directory (for details see ?Startup). Here\'s an example\n <pre>\n# RNC ~/.Rprofile\n\n# auto width adjustment\n.adjustWidth <- function(...){\n       options(width=Sys.getenv(\"COLUMNS\"))\n       TRUE\n}\n.adjustWidthCallBack <- addTaskCallback(.adjustWidth)\n\n.First <- function() cat(\"\\n   Script ~/.Rprofile executed.\\n\\n\")\n.Last <- function()  cat(\"\\n   Goodbye!\\n\\n\")\n</pre>\n\n\n* Redirecting output\n <pre>\nsink(\"myfile.txt\") # redirect console output to a file\nsink() # restore output to the screen\n\npdf(\"mygraph.pdf\") # subsequent graphical output will go to a PDF\npng(\"mygraph.png\") # subsequent graphical output will go to a PNG\njpeg(\"mygraph.jpeg\") # subsequent graphical output will go to a JPEG\nbmp(\"mygraph.bmp\") # subsequent graphical output will go to a BMP\npostscript(\"mygraph.ps\") # subsequent graphical output will go to a PostScript file\ndev.off() # back to the screen\n</pre>\n\n\n* Text files\n <pre>\nmy.data = read.csv(filename)\nmy.data = read.csv(file.choose())\n# Note: (1) = and <- are synonymous, and are the assignment operator (while == tests for equality)\n#       (2) file.choose() pops up a live filename picker\n#       (3) The default is to assume a header row with variable names (header=TRUE),\n#           and no row names, but you can change all these defaults (e.g. row.names=1 reads\n#           row names from the first column).\n\nattach(my.data) # you might then want to attach the new data to the path, though this is optional\n\nwrite.csv(my.data, filename2) # Write the data to a new file. There are several options available; see the help (use ?write.csv)\nwrite.csv(my.data, file=\"d:/temp/newfile.csv\", row.names=FALSE) # Here\'s one: turn off row names to avoid creating a spurious additional column.\n\nread.table(...)  # } A more generic way to read/write tabular data from/to disk\nwrite.table(...) # } (read.csv and write.csv are specialized versions of read.table and write.table)\n</pre>\n\n\n* Microsoft Excel spreadsheets\n <pre>\nlibrary(RODBC)\nchannel <- odbcConnectExcel(\"Osteomalacia_data.xls\") # specify the filename\npatientdata <- sqlFetch(channel, \"Vitamin_D_levels\") # specify a sheet within the spreadsheet\nindexcasedata <- sqlFetch(channel, \"Sheet2\") # by default Excel names individual sheets Sheet1, Sheet2, ..., though you may have renamed them something more informative\nodbcClose(channel)\n</pre>\n\n\n* SPSS data\n <pre>\nlibrary(foreign)\nmydata <- data.frame(read.spss(\"filename.sav\"))\n# Remember you can also use file.choose() in place of the filename, as above.\n</pre>\n\n\n* ODBC data sources (databases)\n <pre>\n# 1. Connect\nlibrary(RODBC)\nchannel <- odbcConnect(\"my_DSN\") # specify your DSN here\n# if you need to specify a username/password, use:\n#  channel <-odbcConnect(\"mydsn\", uid=\"username\", pwd=\"password\")\n\n# 2. List all tables\nsqlTables(channel)\n\n# 3. Fetch a whole table into a data frame\nmydataframe <- sqlFetch(channel, \"my_table_name\") # fetch a table from the database in its entirety\nclose(channel)\n\n# 4. Fetch the results of a query into a data frame. Example:\nmydf2 <- sqlQuery(channel, \"SELECT * FROM MonkeyCantab_LOOKUP_TaskTypes WHERE TaskType < 6\")\n</pre>\n\nIf you\'re using MySQL, you can talk to the database directly:\n <pre>\nlibrary(RMySQL) # use install.packages(\"RMySQL\") if this produces an error\n# if the install.packages() command produces an error, under Ubuntu:\n# use \"sudo apt-get install libmysql++-dev\" (in addition to MySQL itself, i.e. the\n# \"mysql-server mysql-client mysql-navigator mysql-admin\" packages)\ncon <- dbConnect(MySQL(), host=\"localhost\", port=3306, dbname=\"mydatabase\", user=\"myuser\", password=\"mypassword\")\ndbListTables(con)\ndbListFields(con, \"table_name\")\nd <- dbReadTable(con, \"table_name\")\ne <- dbGetQuery(con, \"SELECT COUNT(*) FROM table_name\")\n# and much more possible\n</pre>\n\n\n* R native format\n <pre>\nsave(myobject1, myobject2, ..., file=\"D:/temp/mydata.rda\")\nload(file=\"D:/temp/mydata.rda\")\n# note that the load command recreates the \"mydata\" object without prompting\n# you can also use save.image() to save a whole workspace\n</pre>\n\n\n* Other data-moving techniques\nTo export the definition of an R object (which you can then re-import using \"object = THISTHING\"):\n <pre>\ndput(object, \"\")\n</pre>\n\nTo read a tabular object with a header row from the clipboard\n <pre>\nobject = read.table(\"clipboard\", header=T)\n</pre>\n\n----\n\n=== Samsung SSD 840 Series Information ===\n\n* [http://thessdreview.com/our-reviews/samsung-840-series-240gb-ssd-review-the-worlds-first-tlc-ssd-takes-the-stage/4/ Samsung 840 Series 250GB SSD Review ? The Worlds First TLC SSD Takes Center Stage]\n\n* [http://www.techspot.com/review/578-samsung-840-pro-ssd/ Samsung 840 Pro SSD Review]\n\n----\n\n=== Gnuplot Tips ===\n\n\n* How to unset key [http://people.duke.edu/~hpgavin/gnuplot.html]\n <pre>\n      Create a title:                  > set title \"Force-Deflection Data\" \n      Put a label on the x-axis:       > set xlabel \"Deflection (meters)\"\n      Put a label on the y-axis:       > set ylabel \"Force (kN)\"\n      Change the x-axis range:         > set xrange [0.001:0.005]\n      Change the y-axis range:         > set yrange [20:500]\n      Have Gnuplot determine ranges:   > set autoscale\n      Move the key:                    > set key 0.01,100\n      Delete the key:                  > unset key\n      Put a label on the plot:         > set label \"yield point\" at 0.003, 260 \n      Remove all labels:               > unset label\n      Plot using log-axes:             > set logscale\n      Plot using log-axes on y-axis:   > unset logscale; set logscale y \n      Change the tic-marks:            > set xtics (0.002,0.004,0.006,0.008)\n      Return to the default tics:      > unset xtics; set xtics auto\n</pre>\n\n* Mouse and hotkey support in interactive terminals\n\n: Interaction with the current plot via mouse and hotkeys is supported for the X11, OS/2 Presentation Manager, ggi and Windows terminals. See `mouse input` for more information on mousing. See help for bind for information on hotkeys. Also see the documentation for individual mousing terminals `ggi`, `pm`, `windows` and `x11`.\n\n: Here are briefly some useful hotkeys. Hit \'h\' in the interactive interval for help. Hit \'m\' to switch mousing on/off. Hit \'g\' for grid, \'l\' for log and \'e\' for replot. Hit \'r\' for ruler to measure peak distances (linear scale) or peak ratios (log scale), and \'5\' for polar coordinates inside a map. Zoom by mouse (MB3), and move in the zoom history by \'p\', \'u\', \'n\'; hit \'a\' for autoscale. Use other mouse buttons to put current mouse coordinates to clipboard (double click of MB1), add temporarily or permanently labels to the plot (middle mouse button MB2). Rotate a 3D surface by mouse. Hit spacebar to switch to the gnuplot command window.\n\n: Sample script: mousevariables.dem\n\n* [http://www.gnuplot.info/docs_4.0/gnuplot.html#Mouse_and_hotkey_support_in_interactive_terminals Mouse and hotkey support in interactive terminals -- Gnuplot info]\n\n=== NetApp Storage System Management Software ===\n\n* NetApp OnCommand System Manager [http://www.netapp.com/us/products/management-software/system-manager.aspx]\n:\n\n== ## bNote-2013-03-15 ==\n\n=== DailyPlanning 2013-03-15 ===\n\n* list of candidate tasks\n\n:- [V] HML basic concept study, 오늘 AP 주제에 대해 lightreading\n\n:- [V] 엄교수님께 특허 일정 전달\n\n:- Real IO trace 확보 작업\n::- [V] 김혁호 책임과 미팅 > 13:30 미팅 수행 (업무요청하기로 함)\n::- NetApp, Dell, EMC 측과 연락\n::- 지근영 대리에게 연락\n\n:- IOWA:: Bayesian Network study\n:- IOWA:: Neural Network study\n:- IOWA:: HMM study\n:- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n\n:- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n\n:- page cache to be revisited\n\n:- Data Placement 이슈: Media Difference (RAM,SSD,HDD) 외에 어떤 이슈가 있는가? Hadoop 같은 경우 노드 간 수평적 이동 이슈 있음. 매우 중요.\n\n:- 계획 외 업무들\n::- 이주평전문님의 본사팀과의 Conference Call 위해, 상무님 회의 대신 참석 (차주 화요일: 소장님께보고, 목요일: 부원장님께보고), 액션아이템 이전문님과 팀원께 전달.\n::- AP 세미나 참석 (최희열 전문)\n::- 팀 미팅: 소장님보고 자료 대응 방안 논의 -> IO Prediction 기반의 time 차원 제어로 공간적인 IO 속도 제약 극복 (마치 SS랩의 cooperative caching case처럼)\n\n\n=== HML Study:: \"Reducing the Dimensionality of Data with Neural Networks\" ===\n\n* Gradient descent [http://en.wikipedia.org/wiki/Gradient_descent]\n\n:- Gradient descent is a first-order optimization algorithm [http://en.wikipedia.org/wiki/First-order_approximation]\n\n:- Gradient descent to find the local minimum, gradient ascent to find the local maximum\n:: Gradient descent를 이용하여 function의 local \'\'\'minimum\'\'\'을 찾아내기 위해서는, 현재 지점에서의 function의 \'\'\'negative\'\'\' of the gradient (or of the approximate gradient)에 비례하는 taking steps를 한다. 만약 \'\'\'positive\'\'\' of the gradient에 비례하여 taking step한다면 그 function의 local \'\'\'maximum\'\'\'에 다가가게 된다. 이러한 절차는 gradient ascent라고 한다.\n\n\n* Gradient [http://en.wikipedia.org/wiki/Gradient]\n: Vector calculus에서, scalar field의 gradient는 다음 조건을 만족하는 vector field이다.\n:: direction은 scalar field의 증가분 (rate of increase)이 가장 최대가 되는 방향이다\n:: magnitude는 그 증가분이 된다 {{ In vector calculus, the gradient of a scalar field is a vector field that points in the direction of the greatest rate of increase of the scalar field, and whose magnitude is that rate of increase. }}\n\n\n* Orders of approximation [http://en.wikipedia.org/wiki/First-order_approximation]\n: terms for how precise an approximation is.\n: to indicate progressively more refined approximations: in increasing order of precision, a zeroth order approximation, a first order approximation, a second order approximation, and so forth\n: (Formally) an nth order of approximation\n:: one where the order of magnitude of the error is at most x^n, 혹은 big O notation으로 나타낸다면, error는 O(x^n) 이다.\n: detailed explanation with examples\n::- Zeroth-order (constant; a flat line with no slope; a polynomial of degree 0)\n::: x = [0, 1, 2]\n::: y = [3, 3, 5]\n::: y ~ f(x) = 3.67\n::- First-order (a linear approximation; straight line with a slope; a polynomial of degree 1)\n::: x = [0, 1, 2]\n::: y = [3, 3, 5]\n::: y ~ f(x) = x + 2.67\n::- Second-order (a quadratic polynomial; geometrically, a parabola; a polynomial of degree 2)\n::: x = [0, 1, 2]\n::: y = [3, 3, 5]\n::: y ~ f(x) = x^2 - x + 3\n\n=== Memo ===\n\n* 엄교수님과 연락 내용\n\n:- 엄교수님께 특허 일정 전달 (2013-03-15, 10\n:: 교수님, 안녕하세요? 기술원의 정명준전문입니다. 특허일정을 알아본 결과 3월 29일까지 직무발명서 시스템 등록을 하면 된다고 합니다. 앞으로 2주 정도 여유가 있네요 ^^ 그동안 실험결과를 정리하고 핵심아이디어 및 청구항을 잘 정리하면 될 것 같습니다. 차주 목요일 쯤에 한 번 조박사와 통화하여 기술상세/청구항/기존특허비교/침해적발등을 같이 논의해보면 어떨까합니다만 교수님 보시기에는 어떠신지요? 오늘도 멋진 하루 보내시구요, 항상 감사합니다. 정명준 드림.\n\n=== 연락처 (자주 사용하는) ===\n\n* 기술원 이주평 전문 : 01025984182, 010-2598-4182, #9956 : jupyung.lee@samsung.com\n* 기술원 신현정 전문 : 0173249294, 017-324-9294, #9747 : pharoah@samsung.com\n* 기술원 서정민 전문 : 01025441231, 010-2544-1231, #9817 : tony.seo@samsung.com\n* 기술원 구본철 전문 : 01091905907, 010-9190-5907, #9704 : bc.gu@samsung.com \n* 기술원 유개원 전문 : : gaewon.you@samsung.com\n* 기술원 최희열 전문 : 01096236578, 010-9623-6578, #9692 : heeyoul.choi@samsung.com\n* 기술원 문민영 전문 : , , #9716 :\n* 기술원 최영상 전문 : , , #9951 :\n* 기술원 박상도 전문 : , , #9586 :\n* 기술원 전바롬 전문 : , , #9547 :\n* 기술원 송인철 전문 : , , #9962 :\n* 기술원 박정현 연구원 : , , #9238 :\n\n* 기술원 심은수 상무 : 01020518077, 010-2051-8077, #9950 : eunsoo.shim@samsung.com\n* 기술원 서영완 전문 : 01030020208, 010-3002-0208, #9843 : sywpro@samsung.com\n* 기술원 유연아 사원 : 01090338452, 010-9033-8452, #9858 : yeonah78.yu@samsung.com\n\n* 삼성 SDS ESDM 인프라그룹 이형주 차장님: _ : hj001.lee@partner.samsung.com\n\n* 기술원 에어컨 안나올 때 (기술원 통합 방재 센터, 과장, 지원팀 > 환경안전그룹): #9120 :\n* VDI (SBC) 문제 있을 때 (VDI HelpDesk): #8272 : \n* 네트워크 안될 때 (방화벽 등) 어디로?: :\n* CLMS 시스템 문의 - 한지연 선임 / 서초 인사 CI 그룹: :\n\n* 서울대 컴퓨터공학부 엄현상 교수님 : 0162324667, 016-232-4667, 02-880-6755 : hseom@cse.snu.ac.kr\n* 서울대 컴퓨터공학부 조인순 박사 : 01051317886, 010-5131-7886, 02-880-9330 : insoonjo@gmail.com\n* 서울대 컴퓨터공학부 성민영 석사과정 : 01047245304, 010-4724-5304 : mysung@dcslab.snu.ac.kr\n\n=== HML (Hierarchical Machine Learning) AP (Advanced Program) ===\n\n* 세미나 일정\n\n{| border=\"1\"\n| 이름\n| 논문제목\n| 날짜\n|-\n| 최희열\n| Reducing the dimensionality of data with neural networks [http://www.cs.toronto.edu/~hinton/science.pdf]\n| 03월 15일 \n|-\n| 민윤홍	\n| A fast learning algorithm for deep belief nets [http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf]\n| 03월 22일 \n|-\n| 성재모	\n| Graphical Models \n| 03월 29일 \n|-\n| 정명준	\n| Hierarchical Temporal Memory including HTM Cortical Learning Algorithms \n| 04월 05일 \n|-\n| 박상도 	\n| How to Grow a Mind: Statistics, Structure, and Abstraction	\n| 04월 12일\n|-\n| 전바롬	\n| Learning Hierarchical Models of Scenes, Objects, and Parts\n| 04월 19일\n|-\n| 이호섭\n| Building high-level features using large scale unsupervised learning\n| 4월 26일\n|-\n| 박정현\n| High-Performance Neural Networks for Visual Object Classification\n| 05월 03일\n|-\n| 이호식\n| Deep Neural Networks for Acoustic Modeling in Speech Recognition\n| 5월 10일\n|-\n| 이예하\n| Unsupervised feature learning for audio classification using convolutioinal deep belief networks\n| 05월 24일\n|-\n| 송인철\n| Multimodal Deep Learning\n| 05월 31일\n|-\n|}\n\n== ## bNote-2013-03-14 ==\n\n\n=== Hidden Markov model (HMM) ===\n\n[[Hidden Markov model (HMM)]]\n\n== ## bNote-2013-03-13 ==\n\n\n=== Supercom Usage Statistics ===\n\n <pre>\n----------------------------------------------------------\n| During : 20130304 ~ 20130310                            |\n| Username : a1mjjung , Application(Total jobs) : matlab(1)\n----------------------------------------------------------\nTotal RUN time : 2 min 16 secs\nAverage RUN time : 2 min 16 secs\nMaximum RUN time : 2 min 16 secs\nAverage Wait time 1 secs\nMaximum Wait time 1 secs\n ---------------------------------------------------------\n|                       Application(Total jobs) : unix(14)\n----------------------------------------------------------\nTotal RUN time : 13 min 32 secs\nAverage RUN time : 58 secs\nMaximum RUN time : 3 min 23 secs\nAverage Wait time 1 secs\nMaximum Wait time 2 secs\n ---------------------------------------------------------\n\n</pre>\n\n=== ACM Transactions on Storage ===\n\n삼성 SDS 강석우 상무님 요청으로 우리 팀이 Review하게 됨.\n\n* [http://mc.manuscriptcentral.com/tos Welcome to the ACM Transactions on Storage manuscript submission site]\n\n== ## bNote-2013-03-12 ==\n\n\n=== SNIA Real IO Traces ===\n\n\n----\n==== Microsoft Production MSNStorageFileServer ( msnfs ) ====\n\n* Summary (Reads/Writes - All)\n: 2008-03-10 01:00 + 6 hours\n: Total # of IOs\n:: = 29,345,085 (total)\n:: = 19,729,611 (reads) + 9,615,474 (writes)\n:: = 29345085 = 19729611 + 9615474\n: Average IOPS\n:: = 1358.56 (= 29345085 / (6 * 3600))\n: Average interval time between IOs\n:: = 736 micro-seconds (= (6 * 3600 * 10^6 ) / 29345085)\n\n\n* Summary (Reads)\n <pre>\na1mjjung@secm:[R] $ pwd\n/home/X0101/a1mjjung/x/iowa.sidewinder/sidewinder/microsoft_msn_filesrvr_6h/MSNStorageFileServer/tdir/iowa-mw30m/R\n\na1mjjung@secm:[R] $ grep __valu__sig__ f030.infile_R.iowa.anal_s0010 \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  17\n__valu__sig__ _n_o_sigaddrs :  173401\n__valu__sig__ _sigioc_acc :  5005517\n__valu__sig__ _sigaddrs_efficiency :  28.8667135714\n__valu__sig__ _n_o_addr_total :  5053016\n__valu__sig__ _ioc_total :  19729611\n</pre>\n\n\n* Trace Log Field Information\n <pre>\n\n       1,         2,                 3,        4,      5,          6,      7,           8,       9,       10,          11,      12,       13,         14,       15\nDiskRead, TimeStamp, ProcessName (PID), ThreadID, IrpPtr, ByteOffset, IOSize, ElapsedTime, DiskNum, IrpFlags, DiskSvcTime, I/O Pri,  VolSnap, FileObject, FileName\n\nDiskWrite, TimeStamp, ProcessName (PID), ThreadID, IrpPtr, ByteOffset, IOSize, ElapsedTime, DiskNum, IrpFlags, DiskSvcTime, I/O Pri, VolSnap, FileObject, FileName\n</pre>\n\n\n* [[Trace Data Preprocessing Screenshot - Microsoft Production Trace - MSN FileServer]]\n\n\n----\n\n==== MSR Cambridge IO Traces ====\n\n\n* Summary\n: 22.4GB Trace Data from Data center servers\n:: \'\'\'13 servers, 36 volumes, 179 disks, 1 week\'\'\'\n\n\n* Backgrounds\n: Many enterprise servers are less I/O intensive than TPC benchmarks, which are specifically designed to stress the system under test. Enterprise workloads also show significant variation in usage over time, for example due to diurnal patterns.\n: In order to understand better the I/O patterns generated by standard data center servers, we instrumented the core servers in our building\'s data center to generate per volume block-level traces for one week.\n\n\n* References\n:* \"Write Off-Loading: Practical Power Management for Enterprise Storage\" - FAST 2008 [http://www.usenix.org/event/fast08/tech/narayanan.html]\n::- Messages from this paper\n::: The traditional view is that enterprise workloads make spinning disks down ineffective because idle periods are too short. We analyzed block-level traces from 36 volumes in an enterprise data center for one week and concluded that significant idle periods exist, and that they can be further increased by modifying the read/write patterns using write off-loading. Write off-loading allows write requests on spun-down disks to be temporarily redirected to persistent storage elsewhere in the data center.\n:* \"Self-organizing Storage (SOS) Project - Software\" \n::- Tools: nfsdump/nfsscan [http://www.eecs.harvard.edu/sos/software/index.html]\n::- SOS Project Traces [http://www.eecs.harvard.edu/sos/traces.html]\n\n\n* Trace Log Field Names\n: Timestamp, Hostname, DiskNumber, Type(Read/Write), Offset, Size, ResponseTime\n:- Timestamp: the time the I/O was issued in \"Windows filetime\"\n:- Hostname: the hostname (should be the same as that in the trace file name)\n:- DiskNumber: the disknumber (should be the same as in the trace file name)\n:- Type: \"Read\" or \"Write\"\n:- Offset: starting offset of the I/O in bytes (from the start of the logical disk)\n:- Size: transfer size of the I/O request in bytes\n:- ResponseTime: time taken by the I/O to complete, in \"Windows filetime\"\n\n\n* Trace Log Example\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ head -100 rsrch_0.csv \n\n128166372003061629,rsrch,0,Read,7014609920,24576,41286\n128166372016382155,rsrch,0,Write,1317441536,8192,1963\n128166372026382245,rsrch,0,Write,2436440064,4096,1835\n128166372036348580,rsrch,0,Write,3196526592,57344,35436\n128166372036379390,rsrch,0,Write,3154132992,4096,4626\n128166372036382264,rsrch,0,Write,3154124800,4096,1752\n128166372053100669,rsrch,0,Write,7609925632,10240,2053\n128166372053101032,rsrch,0,Write,15282630656,16384,1691\n128166372053101054,rsrch,0,Write,7612473344,16384,1668\n</pre>\n\n\n* IO Trace Nodes\n{| border=\"1\"\n| Node\n| Description\n| # of volumes\n| # of IOs\n|-\n| usr\n| User home directories\n| 3\n|-\n| proj\n| Project directories\n| 5\n|-\n| prn\n| Print server\n| 2\n|-\n| hm\n| Hardware monitoring\n| 2\n|-\n| rsrch\n| Research projects\n| 3\n|-\n| prxy\n| Firewall/WebProxy\n| 2\n|-\n| src1\n| Source control\n| 3\n|-\n| src2\n| Source control\n| 3\n|-\n| stg\n| Web staging\n| 2\n|-\n| ts\n| Terminal server\n| 1\n|-\n| web\n| Web/SQL server\n| 4\n|-\n| mds\n| Media server\n| 2\n|-\n| wdev\n| Test web server\n| 4\n|-\n|}\n\n\n* # of IOs\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ wc -l *.csv\n    3993316 hm_0.csv\n     609311 hm_1.csv\n    1211034 mds_0.csv\n    1637711 mds_1.csv\n    5585886 prn_0.csv\n   11233411 prn_1.csv\n    4224524 proj_0.csv\n   23639742 proj_1.csv\n   29266482 proj_2.csv\n    2244644 proj_3.csv\n    6465639 proj_4.csv\n   12518968 prxy_0.csv\n  168638964 prxy_1.csv\n    1433655 rsrch_0.csv\n      13780 rsrch_1.csv\n     207587 rsrch_2.csv\n   37415613 src1_0.csv\n   45746222 src1_1.csv\n    1907773 src1_2.csv\n    1557814 src2_0.csv\n     657774 src2_1.csv\n    1156885 src2_2.csv\n    2030915 stg_0.csv\n    2196861 stg_1.csv\n    1801734 ts_0.csv\n    2237889 usr_0.csv\n   45283980 usr_1.csv\n   10570046 usr_2.csv\n    1143261 wdev_0.csv\n       1055 wdev_1.csv\n     181266 wdev_2.csv\n        682 wdev_3.csv\n    2029945 web_0.csv\n     160891 web_1.csv\n    5175368 web_2.csv\n      31380 web_3.csv\n  434212008 total\n</pre>\n\n\n* List of traces\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ l\ntotal 22956880\ndrwxrwxr-x 2 a1mjjung X0101       4096 Mar 12 16:07 ./\ndrwxrwxr-x 3 a1mjjung X0101       4096 Mar 12 16:04 ../\n-r--r--r-- 1 a1mjjung X0101       1262 Oct 31  2008 DISCLAIMER.txt\n-r--r--r-- 1 a1mjjung X0101       1712 Oct 31  2008 MD5.txt\n-r--r--r-- 1 a1mjjung X0101       1815 Oct 31  2008 README.txt\n-r--r--r-- 1 a1mjjung X0101  202270441 Oct 30  2008 hm_0.csv\n-r--r--r-- 1 a1mjjung X0101   29765989 Oct 30  2008 hm_1.csv\n-r--r--r-- 1 a1mjjung X0101   63955502 Oct 30  2008 mds_0.csv\n-r--r--r-- 1 a1mjjung X0101   88337038 Oct 30  2008 mds_1.csv\n-r--r--r-- 1 a1mjjung X0101  291595716 Oct 30  2008 prn_0.csv\n-r--r--r-- 1 a1mjjung X0101  597136927 Oct 30  2008 prn_1.csv\n-r--r--r-- 1 a1mjjung X0101  233038754 Oct 30  2008 proj_0.csv\n-r--r--r-- 1 a1mjjung X0101 1305533029 Oct 30  2008 proj_1.csv\n-r--r--r-- 1 a1mjjung X0101 1614727432 Oct 30  2008 proj_2.csv\n-r--r--r-- 1 a1mjjung X0101  119913539 Oct 30  2008 proj_3.csv\n-r--r--r-- 1 a1mjjung X0101  350117046 Oct 30  2008 proj_4.csv\n-r--r--r-- 1 a1mjjung X0101  658840568 Oct 30  2008 prxy_0.csv\n-r--r--r-- 1 a1mjjung X0101 9043988744 Oct 30  2008 prxy_1.csv\n-r--r--r-- 1 a1mjjung X0101   77717781 Oct 31  2008 rsrch_0.csv\n-r--r--r-- 1 a1mjjung X0101     755814 Oct 31  2008 rsrch_1.csv\n-r--r--r-- 1 a1mjjung X0101   11154823 Oct 31  2008 rsrch_2.csv\n-r--r--r-- 1 a1mjjung X0101 2077380082 Oct 31  2008 src1_0.csv\n-r--r--r-- 1 a1mjjung X0101 2536095762 Oct 31  2008 src1_1.csv\n-r--r--r-- 1 a1mjjung X0101  101236500 Oct 31  2008 src1_2.csv\n-r--r--r-- 1 a1mjjung X0101   82511780 Oct 31  2008 src2_0.csv\n-r--r--r-- 1 a1mjjung X0101   35607343 Oct 31  2008 src2_1.csv\n-r--r--r-- 1 a1mjjung X0101   63026546 Oct 31  2008 src2_2.csv\n-r--r--r-- 1 a1mjjung X0101  105682669 Oct 31  2008 stg_0.csv\n-r--r--r-- 1 a1mjjung X0101  116358242 Oct 31  2008 stg_1.csv\n-r--r--r-- 1 a1mjjung X0101   93309044 Oct 31  2008 ts_0.csv\n-r--r--r-- 1 a1mjjung X0101  118478959 Oct 31  2008 usr_0.csv\n-r--r--r-- 1 a1mjjung X0101 2451360295 Oct 31  2008 usr_1.csv\n-r--r--r-- 1 a1mjjung X0101  574047026 Oct 31  2008 usr_2.csv\n-r--r--r-- 1 a1mjjung X0101   60262085 Oct 31  2008 wdev_0.csv\n-r--r--r-- 1 a1mjjung X0101      56014 Oct 31  2008 wdev_1.csv\n-r--r--r-- 1 a1mjjung X0101    9593588 Oct 31  2008 wdev_2.csv\n-r--r--r-- 1 a1mjjung X0101      35650 Oct 31  2008 wdev_3.csv\n-r--r--r-- 1 a1mjjung X0101  107571350 Oct 31  2008 web_0.csv\n-r--r--r-- 1 a1mjjung X0101    8507311 Oct 31  2008 web_1.csv\n-r--r--r-- 1 a1mjjung X0101  276121116 Oct 31  2008 web_2.csv\n-r--r--r-- 1 a1mjjung X0101    1649607 Oct 31  2008 web_3.csv\n</pre>\n\n\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ wc -l *.csv\n    3993316 hm_0.csv\n     609311 hm_1.csv\n    1211034 mds_0.csv\n    1637711 mds_1.csv\n    5585886 prn_0.csv\n   11233411 prn_1.csv\n    4224524 proj_0.csv\n   23639742 proj_1.csv\n   29266482 proj_2.csv\n    2244644 proj_3.csv\n    6465639 proj_4.csv\n   12518968 prxy_0.csv\n  168638964 prxy_1.csv\n    1433655 rsrch_0.csv\n      13780 rsrch_1.csv\n     207587 rsrch_2.csv\n   37415613 src1_0.csv\n   45746222 src1_1.csv\n    1907773 src1_2.csv\n    1557814 src2_0.csv\n     657774 src2_1.csv\n    1156885 src2_2.csv\n    2030915 stg_0.csv\n    2196861 stg_1.csv\n    1801734 ts_0.csv\n    2237889 usr_0.csv\n   45283980 usr_1.csv\n   10570046 usr_2.csv\n    1143261 wdev_0.csv\n       1055 wdev_1.csv\n     181266 wdev_2.csv\n        682 wdev_3.csv\n    2029945 web_0.csv\n     160891 web_1.csv\n    5175368 web_2.csv\n      31380 web_3.csv\n  434212008 total\n</pre>\n\n=== Real IO Trace 수집 (삼성 SDS 강석우 상무) ===\n\n* 삼성SDS 강석우 상무 (클라우드 플랫폼 팀장)\n\n* 삼성 SDS 박성록 수석보 (클라우드 플랫폼 운영그룹)\n\n* 진행 현황\n\n:* EMC\n::- 박정원 과장에게 연락함. 담당자인 이임호 부장 소개해줌.\n::- 이임호 부장은 아직 연락 못함\n\n:* Dell\n::- 지근영 대리와 통화/메일 (TraceLog 요청사항을 Dell에게 전달하겠다고 함)\n:::- Trace Log 데이터 예제 전달\n\n:* NetApp\n::- 김주영 과장과 통화/메일\n:::- Trace Log 데이터 예제 전달\n\n:* Supercom 센터\n\n\n* 스토리지 상주 지원 인력\n:* EMC\n::- 노영변부장 : 010-5384-6426 (EMC KOREA 삼성 PMO)\n::- 박정원과장 : 010-9052-7805 (EMC KOREA 센터 상주지원)(연락하였음)\n:::- Phone call, IO trace 수집에 대해 설명 -> 담당자 연결 시켜줌 (이임호 부장)\n::- 이임호 부장: 010-3203-7823 (EMC 삼성전담, 프리세일즈 기술컨설턴트)\n:::- Not yet connected (another phone call)\n:* Dell\n::- 이정민차장 : 010-2908-0759 \n::- 지근영대리 : 010-9530-2819 (한빛마이크로 센터상주지원)\n:::- DELL 기술지원 업무(수원ICT센터) 상주 : (6층 전산실 DELL EQL스토리지 기술지원)\n:::- 연락처: <dell.korea@samsung.com> (443-803  경기?수원시?영통구?매탄3동 410-1 삼성SDS 수원ICT S/W연구소 4층)\n:::- 3/12 연락하였음. (전화/메신저/메일로 상황 설명 하였으며, 현재 Dell에 요청 전달된 상태임)\n:* NetApp(상주지원 없음)\n::- 최병석이사 : 010-8998-7138(NetApp Korea)\n::- 김주영과장 : 010-9577-4272 (아리라)\n:::- Email sent, Phone call\n\n <pre>\n\n\n------- Original Message -------\n\nSender : 강석우<sukwkang@samsung.com> 상무/팀장/클라우드플랫폼팀/삼성SDS\n\nDate : 2013-03-08 14:06 (GMT+09:00)\n\nTitle : Re: Re: Fwd: 스토리지 벤더 현황입니다.\n\n \n\n아마 스토리지 벤더 별로 자체 테스트 시스템이 있기 때문에 테스트 시스템에서 로그수집이 가능할 겁니다. 아니면 본사에서 이미 가지고 있을수도 있구요.\n\n \n\n강석우 드림\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-03-08 14:02 (GMT+09:00)\n\nTitle : Re: Fwd: 스토리지 벤더 현황입니다.\n\n \n\n대단히 감사합니다, 강 상무님.\n\n \n\n심은수 드림\n\n \n\n------- Original Message -------\n\nSender : 강석우<sukwkang@samsung.com> 상무/팀장/클라우드플랫폼팀/삼성SDS\n\nDate : 2013-03-08 13:58 (GMT+09:00)\n\nTitle : Fwd: 스토리지 벤더 현황입니다.\n\n \n\n심상무님,\n\n \n\n아래의 스토리지 벤더에 연락해서 요청을 하시면 됩니다. SDS의 클라우드 팀 강석우 상무 소개로 연락했다고 말씀하시구요. 만약 협조를 잘 안하면 저에게 다시 연락주세요. 제가 협조하도록 만들겠습니다. :-)\n\n \n\n강석우 드림\n\n \n\n------- Original Message -------\n\nSender : 박성록<rocky@samsung.com> 수석보/클라우드플랫폼운영그룹/삼성SDS\n\nDate : 2013-03-08 13:48 (GMT+09:00)\n\nTitle : 스토리지 벤더 현황입니다.\n\n \n\n \n\n안녕하십니까?  클라우드플랫폼운영그룹 박성록수석보입니다.\n\n \n\n스토리지 상주 지원 인력입니다.\n\n1. EMC \n\n  - 노영변부장 : 010-5384-6426 (EMC KOREA 삼성 PMO)\n\n  - 박정원과장 : 010-9052-7805  (EMC KOREA 센터 상주지원)\n\n \n\n2. Dell\n\n  - 이정민차장 : 010-2908-0759 \n\n  - 지근영대리 : 010-9530-2819 (한빛마이크로 센터상주지원)\n\n \n\n3. NetApp(상주지원 없음)\n\n - 최병석이사 : 010-8998-7138(NetApp Korea)\n\n - 김주영과장 : 010-9577-4272 (아리라)\n  \n</pre>\n\n=== Multimedia Streaming vs. IOWA-based PDP ===\n\n* eMBMS: Evolved Multimedia Broadcast Multicast Service\n: Expway\'s eMBMS [http://blog.expway.com/ Expway\'s eMBMS Solution Allows Mobile Operators to off-Load Mobile Traffic by 20%]\n:- Key Technical Features\n::- FLUTE (File Delivery over Unidirectional Transport) protocol\n::- Forward Error Correction\n::- File Repair\n::- Service Announcement\n::- DASH Video Protocol\n:- Mobile Traffic Prediction (in 2016)\n::- 70% of mobile traffic will be video\n::- 10% of all TV viewing will be on tablets\n::- This equauls to 25 million DVDs sent every single hour\n:- Expway Company\n::- 7 years of experience focused on mobile broadcast software\n:::- Robust and Mature Products\n:::- Optimized Bandwidth Usage\n:::- Low Footprint Terminal Stack\n\n\n\n* DASH: Dynamic Adaptive Streaming over HTTP (a.k.a MPEG-DASH)\n: Internet 상으로 media content에 대한 고품질 streaming을 가능하게 하는 기술 (기존 HTTP 웹서버들로부터 deliver됨)\n: 컨텐츠를 small HTTP-based file segement들로 쪼개어 다룬다는 점에서 Apple의 HTTP Live Streaming (HLS)와 유사하다고 볼 수 있음. (컨텐츠 예: movie, sports event의 live broadcast 등)\n\n\n* HTTP Live Streaming (HLS): HTTP-based media streaming communications protocol (QuickTime과 iOS software의 일부로 Apple이 구현함)\n:- 동작원리\n:: overall stream을 작은 HTTP-based file downloads로 쪼개어 다룬다 (각각의 download는 overall potentially unbounded transport stream의 하나의 short chunk를 담당). stream 세션 시작 시에는, available한 variouis sub-stream들에 대한 metadata를 포함하고 있는 extended M2U (m3u8) playlist를 download한다.\n:- 장점\n:: 표준화된 HTTP transaction만을 사용하기 때문에, HLS는 일반 HTTP traffic을 허용하는 firewall, proxy server들은 모두 통과 가능 (RTP와 같은 UDP 기반 프로토콜은 그렇지 못함)하며, 널리 사용 가능한 CDN 인프라를 통해서 쉽게 deliver될 수 있음.\n:- 특징\n:: AES와 같은 암호화 메커니즘 및 HTTPS 기반의 secure key distribution 방법, simple DRM 시스템을 제공함\n:: HLS의 이후 버전에서는 [[trick mode]][http://en.wikipedia.org/wiki/Trick_mode] 기반의 fast-forward/rewind 및 subtitle의 통합도 지원할 예정임 (2013-03-12 현재)\n:- 표준화\n:: Apple에서는 HLS (HTTP Live Streaming)를 Internet Draft로 작성하였음. (first stage in the process of submitting it to the IETF, as an Informational Request For Comments)\n\n\n\n=== Technical Articles ===\n\n* 상무님께서 보내주신 \"Future of Cloud Storage\" 메일에 대한 신전문님 정리\n\n아래 글들을 읽은 소감 or 요약입니다.\n글들이 이것저것 다양하네요.\n\n* [http://gigaom.com/2011/10/29/the-future-beyond-the-cloud-is-in-our-hands/ The future beyond the cloud is in our hands]\n\n: \"Cloud is all\"의 관점은 mobile device가 ubiquitous, unlimited, low-cost conntection 인데요.. 이런 생각은 현재의 mobile network의 한계와 mobile device의 빠른 발전까지 고려치 않은 비전이라는 비판입니다.\n\n:# 첫째, LTE 같은 mobile network이 향후 cloud를 모두 책임지지는 못한다는 것이고요. (WAN에서의 Bandwidth란 이미 wireless network을 사용하고 있는 user가 쓰는 용량으로 계산된 것이므로)\n:# 둘째, 향후 mobile은 n-core의 multi-gigaherts procesor와 1TB 이상의 local storage 이므로 이를 cloud에서 활용하자는 얘기입니다.\n\n: 예전의 장수석님의 Mobile Cloud 과제가 생각납니다. ^^; Mobile의 능력과 wireless network를 활용하자는 겁니다.\n::- what if those devices can talk to one another in a peer-to-peer or mesh network? \n::- What’s the aggregate power and capability of billions of these things, especially if there will be ways for them to work with and talk to one another both alone and in conjunction with cloud-based services?\n\n: 예로 든 것이 Amazon Silk과 Google의 Offline Mail입니다.\n::- Silk는 클라우드를 이용해 acceleration하는 브라우저입니다. 클라우드에서 사용자의 웹 패턴을 분석하여 미리 preloading하고 mobile에 최적화하여 속도를 높이는 건데요.역으로 mobile에 맞게 웹페이지를 작게 축소해서 mobile에 가져오기 때문에 클라우드가 동작하지 않더라도 속도를 유지시킬 수 있다고 합니다. 클라우드와 mobile간에 서로 보완하는 거지요.\n::- offline Mail은 말그대로 메일서버가 되지 않아도 전송외에 모든 메일 관리가 가능하도록 mobile에 데이터를 미리 다 가져다 놓는 겁니다.\n \n\n* [http://blog.cloudhq.net/post/36559564253/the-future-of-cloud-storage The Future of Cloud Storage]\n\n: 정확히는 이런 문제점이 나올 거다라는 cloud issue에 관한 prediction이라고 볼 수 있습니다.\n\n:# End of Files and Folders : 클라우드 서비스들이 전통적인 File이나 folder 개념을 사용하지 않고 자기만의 interface, 데이터 분류 방식을 쓰므로 나중에 cloud 간의 호환성 문제가 발생함\n:# End of Free Storage: cloud storage 시장이 mature된다면 결국 free storage는 없어질 거다. 지금은 홍보용인 거다.\n:# Data ownership Troubles : 클라우드에 데이터가 올라간 순간 사용자는 data에 대한 control를 잃어버린다.\n:# Encryption will become necessary : encryption이 필수..\n\n\n* [http://www.cs.berkeley.edu/~kubitron/talks/printable/CISCO-cloudstore.pdf Lessons from the OceanStore Project - UC Berkeley]\n\n: 첫번째 글에서 좀 더 나아간 형태..Cloud 시대를 맞이한 P2P의 재조명 되겠습니다. 과거 P2P가 공짜 미디어를 훔치는 수단이었다면 새로운 P2P는 extreme scale를 제공할 수 잇는 system의 새 디자인으로 사용할 수 있다는 얘기입니다. 여러 Client뿐 아니라 여러 Cloue Storage Provider들도 같이 참여하면 서로 윈윈할 수 있다는 얘기입니다.\n\n \n* [http://www.cloudsigma.com/blog/13-the-future-of-cloud-storage The Future of Cloud Storage (and what is wrong with the present)]\n: SAN도 local Storage도 이제 끝났다.  Distributed Replicated Block Device(DRBD) 라고 얘기하고 있습니다만, converged server+storage 형태로 Server 노드에 Storage까지 합체한 형태로 죽 붙이고 replication을 다른 node에 함으로써 latency, fail over 등의 Converged architecture장점을 얘기하고 있네요. Open Solution으로는 sheepdog이 있고 상용화버전으로는 Amplidata가 있다고 합니다.\n::>> open source로서 Linux Kernel (2.6.33 version 부터) 에 구현되어 있는 DRBD도 있음. (아래 그림 참고) HA (High Availability) 제공에 초점이 맞춰져 있고, replication mode도 fully-synchronous와 asynchronous mode, 그리고, 그 사이에, protection level과 performance 간의 tradeoff를 고려한, semi-synchronous mode (memory synchronous mode 라고도 합니다)가 지원됨.  이 Linux의 DRBD는, 우리 RACS 1 의 기술이 networked storage로 확장될 때 매우 유용한 기술적 base가 될 수 있을 것 같습니다. (마치 Local Disk Array에 대한 RACS 1이 Linux MD를 활용하여 구현되고 있는 것처럼, Networked Replicated Disk Array 기술 구현 시에 Linux DRBD를 잘 활용할 수도 있을 것임) [http://www.ibm.com/developerworks/linux/library/l-drbd/index.html High availability with the Distributed Replicated Block Device]\n\n <pre>\n\n------- Original Message -------\n\nDate : 2013-03-11 10:44 (GMT+09:00)\nTitle : Fwd: future of cloud storage\n\nFYI, \n\n저희 궁극의 시나리오가 \'무한대의 local 저장용량을 제공하는\' pervasive storage 쪽으로 잡히면서,\n상무님이 cloud storage의 핵심기술에 대한 깊은 이해를 요구하고 계십니다.\n아래 메일도 참고하시고 시간을 내어 관련 기술에 대한 study를 진행하도록 하겠습니다.\n\n\n------- Original Message -------\nDate : 2013-03-08 20:55 (GMT+09:00)\nTitle : future of cloud storage\n\n이 전문,\n\n\n아마 이 전문도 구글 검색하면 금방 찾을 글들일텐데, 하여간 내가 본 것들입니다.\n바로 아래 것은 많은 시사점을 주는 것 같습니다.\n\nhttp://gigaom.com/2011/10/29/the-future-beyond-the-cloud-is-in-our-hands/\n\n \n그 외의 글들.\n\nhttp://blog.cloudhq.net/post/36559564253/the-future-of-cloud-storage\n\nhttp://www.cs.berkeley.edu/~kubitron/talks/printable/CISCO-cloudstore.pdf\n\nhttp://www.cloudsigma.com/blog/13-the-future-of-cloud-storage\n\n \n우리가 pervasive storage로 서비스 시나리오를 잡은 만큼, 그 분야의 서비스/기술 발전 전망을 할 수 있어야겠습니다.\n</pre>\n\n== ## bNote-2013-03-11 ==\n\n\n=== Akamai - CDN Acceleration ===\n\n* 관련 기사들\n:# [[아카마이, \"쌩쌩 웹사이트 만들려면\"]] [http://www.bloter.net/archives/141513 bloter.net, 2013-01-24]\n:# [[아카마이, \'블레이즈\' 인수 - 인텔리전트 플랫폼 강화]] [http://www.bloter.net/archives/95561 bloter.net, 2012-02-09]\n:# [[아카마이, \"CDN 넘어 하이퍼커넥티드로\"]] [http://www.bloter.net/archives/92711 bloter.net, 2012-01-19]\n:# [[아카마이, CDN 장악 가속화 ... 코텐도 인수설]] [http://www.bloter.net/archives/85622 bloter.net, 2011-11-28]\n:# [[아카마이, \"스트리밍/보안 서비스를 빠르고 안전하게\"]] [http://www.bloter.net/archives/67854 bloter.net, 2011-07-13]\n:# [[네트워크 업계 \"기다렸다, 런던올림픽\"]] [http://www.bloter.net/archives/92823 bloter.net, 2012-01-22]\n\n\n=== 과제 목표 설정 ===\n\n* IOWA-based PDP 과제 목표 metric\n: 경쟁사, 기술원 현수준, 기술원 목표수준, 접근방식 등\n:- 필수 고려 사항\n:: 어째서 그러한 목표 수준을 잡았는지?\n:- 점검 사항\n:: EMC FAST 등 Automatic Tiering 기술의 현수준 파악 필요\n:- 접근 방식의 독창성/진보성\n::- ProactiveDP가 MWC 2013에 언급된 eMBMS, DASH 등과 어떤 차별점을 갖는가?\n:::- eMBMS (Evolved Multimedia Broadcast Multicast Service): LTE를 이용해 수많은 사용자에게 방송 컨텐츠를 동시에 효과적으로 배포하는 기술임. 스트리밍 전송 및 비 피크타임에 전송해 단말에 저장된 형태로 있다가 사용자가 원할 때 시청. <span style=\"color:blue\">level of intelligence</span>가 중요한 비교점이 될 수 있음.\n:::- DASH (Dynamic Adaptive Streaming over HTTP)\n::- CDN (Content Delivery Network)에서 Akamai와는 어떻게 차별되나?\n\n* 기술 진화 고민\n: evolution of technology as a driving force from old-age to the pervasive storage\n\n=== IOWA: bpo_a.20130305_104633.real_whole_trace.log ===\n\n* Trace information\n:- Machine under IOTracing: radiohead (Linux 3.2.0-34)\n:- Tracing time: 72 hours\n:- Trace log file: /x/var/iowa/sidewinder/iowa/preproc/tdir/myrealtrace/bpo_a.20130305_104633.real_whole_trace.log\n\n==== further Write pattern analysis (LBA-to-name processing, for top 18 addresses) ====\n\n* IO statistical summary\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  1115\n__valu__sig__ _n_o_sigaddrs :  18\n__valu__sig__ _sigioc_acc :  40010\n__valu__sig__ _sigaddrs_efficiency :  2222.77777778\n__valu__sig__ _n_o_addr_total :  40979\n__valu__sig__ _ioc_total :  157632\n</pre>\n\n\n* top 18 addresses\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__list__hitnum_and_addrs__.log.v1 | sort -n  | tail -20\n1093 : [1266683272]\n1105 : [1267153912]\n1113 : [1661223296]\n\n---- top 18 addr starts below ----\n1115 : [12856320, 12857344]\n1237 : [2480]\n1686 : [1266681984]\n1693 : [1266683032]\n1730 : [1266683160]\n1732 : [2352]\n1941 : [1661223328]\n2077 : [2048]\n2110 : [1267095912]\n2188 : [327569400]\n2265 : [1266683048]\n2309 : [327568408]\n2342 : [327568936]\n2364 : [1661223320]\n2395 : [1661223136]\n2526 : [1401209744]\n7185 : [1661223128]\n</pre>\n\n\n* Bar Graph for Hits_per_Addr (MyRealTrace, Radiohead, 72h)\n<!-- [[File:Myrealtrace on radiohead 72h cdst hits per addr impulses.v01.png | 500px]] -->\n[[File:Myrealtrace on radiohead 72h cdst hits per addr impulses.v01.png]]\n\n\n===== analysis table =====\n\n{| border=\"1\"\n| address\n| # of hits\n| device node\n| process accessed\n| periodicity (1000 IOs)\n| corresponding file/dir\n| notes\n|-\n| 1661223128\n| 7185\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.964271213967\n| /home/hendrix/.config/google-chrome/Default/Cookies-journal\n| inode (39585621)\n|-\n| 1401209744\n| 2526\n| (8,1) /dev/sda1\n| BrowserBlocking\n| 0.926207876573\n| /home/hendrix/.config/google-chrome/Default/Local Storage/https_plus.google.com_0.localstorage-journal\n| inode (39714913)\n|-\n| 1661223136\n| 2395\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.957927324401\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 1661223320\n| 2364\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.957927324401\n| /home/hendrix/.config/google-chrome/Default/Cookies-journal\n| inode (39585621)\n|-\n| 327568936\n| 2342\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.938895655704\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 327568408\n| 2309\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.951583434836\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 1266683048\n| 2265\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.957927324401\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 327569400\n| 2188\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.894488428745\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 1267095912\n| 2110\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.951583434836\n| /home/hendrix/.config/google-chrome/Default/Cookies-journal\n| inode (39585621)\n|-\n| 2048\n| 2077\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.94523954527\n| INVALID BLOCK\n| inode (N/A)\n|-\n| 1661223328\n| 1941\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.957927324401\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 2352\n| 1732\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.957927324401\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 1266683160\n| 1730\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.938895655704\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 1266683032\n| 1693\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.964271213967\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 1266681984\n| 1686\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.957927324401\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 2480\n| 1237\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.951583434836\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 12856320\n| 1115\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.94523954527\n| /home/hendrix/.cache/google-chrome/Default/Cache/data_1\n| inode (39585855)\n|-\n| 12857344\n| 1115\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.94523954527\n| /home/hendrix/.cache/google-chrome/Default/Cache/index\n| inode (39585853)\n|-\n|}\n\n===== analysis result (processing output) =====\n\n <pre>\nblusjune@radiohead:[top_hot_18] $ l\ntotal 16\ndrwxrwxr-x 2 blusjune blusjune 4096 Mar 11 18:44 ./\ndrwxrwxr-x 3 blusjune blusjune 4096 Mar 11 14:49 ../\n-rwxr-xr-x 1 blusjune blusjune 2566 Mar 11 18:40 .bdx.0100.y.lba_to_name.sh*\n-rw-rw-r-- 1 blusjune blusjune 1768 Mar 11 18:44 .conf.lba_to_name.sh\n\nblusjune@radiohead:[top_hot_18] $ _BDX \nBDX[ /x/var/iowa/tdir/s05/w_ptrn_analysis/top_hot_18 ]# 0100 : lba_to_name\n\n\'_conf__target_lba_list\' is from:\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ tail -29 __simout.sigio_25.iowsz_100.t1_1000.__list__hitnum_and_addrs__.log.v2\n708 : [1795164168]\n840 : [12856336]\n841 : [1270876464]\n842 : [1661223968]\n913 : [1266682992]\n929 : [1270876456]\n943 : [1266681864]\n949 : [1266751536]\n956 : [1266751544]\n1093 : [1266683272]\n1105 : [1267153912]\n1113 : [1661223296]\n1115 : [12856320, 12857344]\n1237 : [2480]\n1686 : [1266681984]\n1693 : [1266683032]\n1730 : [1266683160]\n1732 : [2352]\n1941 : [1661223328]\n2077 : [2048]\n2110 : [1267095912]\n2188 : [327569400]\n2265 : [1266683048]\n2309 : [327568408]\n2342 : [327568936]\n2364 : [1661223320]\n2395 : [1661223136]\n2526 : [1401209744]\n7185 : [1661223128]\n\n#>> configuration started\n#<< _conf__target_dev (e.g., /dev/sda) : /dev/sda\n#<< _conf__target_dev_part (e.g., /dev/sda1) : /dev/sda1\n----\n/dev/sda1   *        2048  3907026943  1953512448   83  Linux\nBlock size:               4096\n----\n#<< _conf__lba_fs_start: 2048\n#<< _conf__fblk_size: 4096\n#<< _conf__sector_size [512]: \n#>> configuration completed\n\n#>> START Processing\n/dev/sda1: 1795164168 -> _EXCEPTION_ # inode for 224395265 is NOT FOUND -- Skip processing\n/dev/sda1: 12856336 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_1 # DEV:LBA:FBLK:INODE( /dev/sda1 : 12856336 : 1606786 : 39585855 )\n/dev/sda1: 1270876464 -> _EXCEPTION_ # inode for 158859302 is NOT FOUND -- Skip processing\n/dev/sda1: 1661223968 -> /home/blusjune/.config/google-chrome/Default/Local Storage/https_plus.google.com_0.localstorage # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223968 : 207652740 : 39714912 )\n/dev/sda1: 1266682992 -> _EXCEPTION_ # inode for 158335118 is NOT FOUND -- Skip processing\n/dev/sda1: 1270876456 -> _EXCEPTION_ # inode for 158859301 is NOT FOUND -- Skip processing\n/dev/sda1: 1266681864 -> _EXCEPTION_ # inode for 158334977 is NOT FOUND -- Skip processing\n/dev/sda1: 1266751536 -> /home/blusjune/.config/google-chrome # DEV:LBA:FBLK:INODE( /dev/sda1 : 1266751536 : 158343686 : 39585586 )\n/dev/sda1: 1266751544 -> /home/blusjune/.config/google-chrome/Default # DEV:LBA:FBLK:INODE( /dev/sda1 : 1266751544 : 158343687 : 39585590 )\n/dev/sda1: 1266683272 -> _EXCEPTION_ # inode for 158335153 is NOT FOUND -- Skip processing\n/dev/sda1: 1267153912 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_0 # DEV:LBA:FBLK:INODE( /dev/sda1 : 1267153912 : 158393983 : 39585854 )\n/dev/sda1: 1661223296 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_0 # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223296 : 207652656 : 39585854 )\n/dev/sda1: 12857344 -> /home/blusjune/.cache/google-chrome/Default/Cache/index # DEV:LBA:FBLK:INODE( /dev/sda1 : 12857344 : 1606912 : 39585853 )\n/dev/sda1: 12856320 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_1 # DEV:LBA:FBLK:INODE( /dev/sda1 : 12856320 : 1606784 : 39585855 )\n/dev/sda1: 2480 -> _EXCEPTION_ # inode for 54 is NOT FOUND -- Skip processing\n/dev/sda1: 1266681984 -> _EXCEPTION_ # inode for 158334992 is NOT FOUND -- Skip processing\n/dev/sda1: 1266683032 -> _EXCEPTION_ # inode for 158335123 is NOT FOUND -- Skip processing\n/dev/sda1: 1266683160 -> _EXCEPTION_ # inode for 158335139 is NOT FOUND -- Skip processing\n/dev/sda1: 2352 -> _EXCEPTION_ # inode for 38 is NOT FOUND -- Skip processing\n/dev/sda1: 1661223328 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223328 : 207652660 : 39585620 )\n/dev/sda1: 2048 -> _EXCEPTION_ # fsblock 0 seems INVALID BLOCK -- Skip processing\n/dev/sda1: 1267095912 -> /home/blusjune/.config/google-chrome/Default/Cookies-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1267095912 : 158386733 : 39585621 )\n/dev/sda1: 327569400 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 327569400 : 40945919 : 39585620 )\n/dev/sda1: 1266683048 -> _EXCEPTION_ # inode for 158335125 is NOT FOUND -- Skip processing\n/dev/sda1: 327568408 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 327568408 : 40945795 : 39585620 )\n/dev/sda1: 327568936 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 327568936 : 40945861 : 39585620 )\n/dev/sda1: 1661223320 -> /home/blusjune/.config/google-chrome/Default/Cookies-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223320 : 207652659 : 39585621 )\n/dev/sda1: 1661223136 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223136 : 207652636 : 39585620 )\n/dev/sda1: 1401209744 -> /home/blusjune/.config/google-chrome/Default/Local Storage/https_plus.google.com_0.localstorage-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1401209744 : 175150962 : 39714913 )\n/dev/sda1: 1661223128 -> /home/blusjune/.config/google-chrome/Default/Cookies-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223128 : 207652635 : 39585621 )\n\nblusjune@radiohead:[top_hot_18] $ \n\n\n</pre>\n\n\n=== LBA-to-name processing ===\n\n* .bd/x/exphist info.\n: bd/x/exphist/sidewinder/sidewinder-20130311_190748/tools/lba_to_name/\n\n <pre>\nblusjune@jimi-hendrix:[lba_to_name] $ pwd\n/home/blusjune/bd/x/exphist/sidewinder/sidewinder-20130311_190748/tools/lba_to_name\nblusjune@jimi-hendrix:[lba_to_name] $ l\ntotal 16\ndrwxrwxr-x 2 blusjune blusjune 4096 Mar 11 19:03 ./\ndrwxrwxr-x 4 blusjune blusjune 4096 Mar 11 19:07 ../\n-rwxr-xr-x 1 blusjune blusjune 3869 Mar 11 19:03 .bdx.0100.y.lba_to_name.sh*\n-rw-rw-r-- 1 blusjune blusjune  300 Mar 11 19:03 .conf.lba_to_name.sh\n</pre>\n\n\n* References\n: [http://www.expertslogin.com/general/how-to-fix-repair-bad-blocks-in-linux/ How To Fix / Repair Bad Blocks In Linux] (B.GOOD)\n\n\n* How to find the corresponding location (file/dir) for some LBA (e.g., 1661223128)?\n\n: step 1: We need to find the block size of the filesystem\n <pre>\nblusjune@radiohead:[google] $ sudo tune2fs -l /dev/sda1  | grep Block\nBlock count:              488378112\nBlock size:               4096\nBlocks per group:         32768\n</pre>\n\n: step 2: check the starting sector of the partition (file system)\n <pre>\nblusjune@radiohead:[google] $ sudo fdisk -lu /dev/sda\n\nDisk /dev/sda: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x00001eb9\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *        2048  3907026943  1953512448   83  Linux\n\n</pre>\n\n: step 3: File the filesystem block that contains this target LBA\n <pre>\nb = (int) ((L-S) * 512 / B)\n\n, where\nb = File system block number (we need this answer)\nB = File system block size in bytes (4096 - we got this in step 1)\nL = target LBA (1661223128 - given)\nS = Starting sector of the partition as shown by fdisk -lu (2048 - we got this in step 2)\n\n, so the answer b is 207652635 (207,652,635)\n\n</pre>\n\n: step 4: Check the file/dir name with given b (207652635)\n <pre>\nblusjune@radiohead:[s05] $ sudo debugfs /dev/sda1 \ndebugfs 1.42 (29-Nov-2011)\n\ndebugfs:  testb 207652635\nBlock 207652635 marked in use\n\ndebugfs:  icheck 207652635\nBlock	Inode number\n207652635	39585621\n\ndebugfs:  ncheck 39585621\nInode	Pathname\n39585621	/home/blusjune/.config/google-chrome/Default/Cookies-journal\n\ndebugfs:\n</pre>\n\n== ## bNote-2013-03-08 ==\n\n=== lsof command guide/examples ===\n\n* [http://www.ibm.com/developerworks/aix/library/au-lsof.html Finding open files with lsof]\n\n:- Sean A. Walberg, Senior Network Engineer\n:- Summary:  Learn more about your system by seeing which files are open. Knowing which files an application has open, or which application has a particular file open, enables you to make better decisions as a system administrator. For instance, you shouldn\'t unmount a file system while files on it are open. Using lsof, you can check for open files and stopped processes before unmounting, as needed. Likewise, if you find an unknown file, you can find the application holding it open.\n\n\n=== debugfs command guide/examples ===\n\n* [http://www.cs.montana.edu/courses/309/topics/4-disks/debugfs_example.html debugfs examples - original article from montana.edu]\n\n <pre>\ndebugfs Command Examples\n\n\n# Use debufs to prowl around a file system.\n\n> debugfs /dev/hda6\ndebugfs 1.19, 13-Jul-2000 for EXT2 FS 0.5b, 95/08/09\n\n# list files\n\ndebugfs:  ls\n2790777 (12) .   32641 (12) ..   2790778 (12) dir1   2790781 (16) file1\n2790782 (4044) file2\n\n#  List the files with a long listing\n\n#  Format is:\n# Field 1:  Inode number.\n# Field 2:  First one or two digits is the type of node:\n#    2 = Character device\n#    4 = Directory\n#    6 = Block device\n#    10 = Regular file\n#    12 = Symbolic link\n#  \n#    The Last four digits are the Linux permissions\n# 3. Owner uid\n# 4. Group gid\n# 5. Size in bytes.\n# 6. Date \n# 7. Time of last creation.\n# 8. Filename.\n\ndebugfs:  ls -l\n2790777  40700   2605   2601    4096  5-Nov-2001 15:30 .\n 32641   40755   2605   2601    4096  5-Nov-2001 14:25 ..\n2790778  40700   2605   2601    4096  5-Nov-2001 12:43 dir1\n2790781 100600   2605   2601      14  5-Nov-2001 15:29 file1\n2790782 100600   2605   2601      14  5-Nov-2001 15:30 file2\n\n# dump the contents of file1\n\ndebugfs: cat file1\nThis is file1 \n\n# dump an inode to a file (same as cat, but to a file) and using\n#  instead of the file name.\n\ndebugfs: dump <2790782> file1-debugfs\n\n# dump the contents of an inode\n\ndebugfs: stat file1 \nInode: 2790782   Type: regular    Mode:  0600   Flags: 0x0   Generation: 46520506\nUser:  2605   Group:  2601   Size: 14\nFile ACL: 0    Directory ACL: 0\nLinks: 1   Blockcount: 8\nFragment:  Address: 0    Number: 0    Size: 0\nctime: 0x3be712ea -- Mon Nov  5 15:30:02 2001\natime: 0x3be712ea -- Mon Nov  5 15:30:02 2001\nmtime: 0x3be712ea -- Mon Nov  5 15:30:02 2001\nBLOCKS:\n5603924\nTOTAL: 1\n\n# Dump an directory inode and look at it.\n\ndebugfs: dump dir1 dir1-debugfs\n\n# Leave debugfs or use another xterm to look at the contents\n# using od or xxd.  The format of a directory (ext2 version 2.0) is:\n\n# Field 1. Four byte inode number.\n# Field 2. Two byte directory entry length.\n# Field 3. Two byte file name length. \n# Field 5. Filename (1-255 characters).\n# Pad.     The filename is padded to be a multiple of 4 bytes long.\n\n\n# use -c to see the file names and single byte values\n# You can see the file names and identify the locatin of\n# the other fields.  Of importance, the length of the \n# entries (octal); . (4-5), .. (20-21), file3 (34-35),\n# file4 (54-55), .file4.swp (74-75),  ...\n\n> od -c dir1-dump  \n0000000   z 225   *  \\0  \\f  \\0 001 002   .  \\0  \\0  \\0   y 225   *  \\0\n0000020  \\f  \\0 002 002   .   .  \\0  \\0 202 225   *  \\0 020  \\0 005 001\n0000040   f   i   l   e   3  \\0  \\0  \\0 201 225   *  \\0   ? 017 005 001\n0000060   f   i   l   e   4  \\0  \\0  \\0 177 225   *  \\0   ? 017  \\n 001\n0000100   .   f   i   l   e   4   .   s   w   p  \\0  \\0 200 225   *  \\0\n0000120   ? 017 006 001   f   i   l   e   4   ~   .   s   w   p   x  \\0\n0000140  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0\n*\n0010000\n\n# use -d to see the two byte values\n\n> od -d dir1-dump  \n0000000 38266    42    12   513    46     0 38265    42\n0000020    12   514 11822     0 38274    42    16   261\n0000040 26982 25964    51     0 38273    42  4056   261\n0000060 26982 25964    52     0 38271    42  4040   266\n0000100 26158 27753 13413 29486 28791     0 38272    42\n0000120  4020   262 26982 25964 32308 29486 28791   120\n0000140     0     0     0     0     0     0     0     0\n*\n0010000\n\n# You can see that the lengths of the entries are:\n#    . = 12, .. = 12, file3 = 16, file4 = 4096\n# Whoa! what happened there.  The file .file4.swp\n# and any other files in the directory have been deleted,\n# so the length of the entry goes to the end of the block\n#\n# use -l to see the four byte values.  We can see the inode\n# values of the files.\n\n> od -l dir1-dump  \n0000000     2790778    33619980          46     2790777\n0000020    33685516       11822     2790786    17104912\n0000040  1701603686          51     2790785    17108952\n0000060  1701603686          52     2790783    17436616\n0000100  1818846766  1932407909       28791     2790784\n0000120    17174452  1701603686  1932426804     7893111\n0000140           0           0           0           0\n*\n0010000\n\n\n-------------------------------------------------------------------<\n\n\n\n#\n# You inadvertently delete a file you want back.  The file was named\n# /home/harkin/test/file2.  Immediately do the following.\n#\n\n\n> umount /home\n\n# so that you don\'t create a new file that overwrites the inode\n# or use one of the file blocks.\n\n# Execute df to find out what partition /home is on\n\n>df \nFilesystem           1k-blocks      Used Available Use% Mounted on\n/dev/hda1              1011928    507860    452664  53% /\n/dev/hda8             27364092   1890176  24083896   8% /home\n/dev/hda5              8064272   3492760   4161860  46% /usr\n/dev/hda7              1011928     87956    872568  10% /var\nclowns:/db/boze       17783240  10494056   7183568  60% /home/bozo/db\n\n# Get the data on the /home filesystem\n\ntune2fs -l /dev/hda8 | grep \"Block size\"\n\n   Block size:               4096\n\n# So the block size is 4096 bytes.\n\n# Create a file system to duplicate the /home file system in case\n# you screw up royally.  This disk should be exactly the same size\n# as the file system you are backing up.  Fortunately there is an\n# unused disk /dev/hdb.\n\n> fdisk /dev/hdb\nCommand (m for help): n\nCommand action\n   l   logical (5 or over)\n   p   primary partition (1-4)\np\n\n+27364092K\nw\n\n# copy /home to the backup location\n\ndd if=/dev/hda8 of=/dev/hdb1 bs=4096\n\n# Now use debugfs to try to fix things.  We need to try to\n# find the inode of the deleted file.  Use lsdel to \n# list all of the deleted inodes on the file system.\n\ndebugfs -w            # to allow writing\ndebugfs:  lsdel\n3061 deleted inodes found.\n Inode  Owner  Mode    Size    Blocks    Time deleted\n                     .\n                     .\n3296723   2605 100600    652    1/   1 Fri Nov  2 07:30:33 2001\n3296724   2605 100600   1545    1/   1 Fri Nov  2 07:30:33 2001\n3296725   2605 100600    355    1/   1 Fri Nov  2 07:30:33 2001\n3296731   2605 100600    440    1/   1 Fri Nov  2 07:30:33 2001\n3296732   2605 100600   3536    1/   1 Fri Nov  2 07:30:33 2001\n3296733   2605 100600   2365    1/   1 Fri Nov  2 07:30:33 2001\n3296734   2605 100600    443    1/   1 Fri Nov  2 07:30:33 2001\n3296850   2605 100600   2046    1/   1 Fri Nov  2 07:30:33 2001\n3296851   2605 100600    729    1/   1 Fri Nov  2 07:30:33 2001\n3296852   2605 100600    850    1/   1 Fri Nov  2 07:30:33 2001\n3296853   2605 100600   3251    1/   1 Fri Nov  2 07:30:33 2001\n3296854   2605 100600   3733    1/   1 Fri Nov  2 07:30:33 2001\n3296855   2605 100600   3109    1/   1 Fri Nov  2 07:30:33 2001\n3296856   2605 100600   3211    1/   1 Fri Nov  2 07:30:33 2001\n652818   2605 100600 171791   43/  43 Fri Nov  2 16:07:33 2001\n897613   2605 100600   2096    1/   1 Mon Nov  5 07:49:28 2001\n979218   2605 100600   3797    1/   1 Mon Nov  5 07:49:29 2001\n979219   2605 100600   4096    1/   1 Mon Nov  5 07:49:29 2001\n179573   2605 100600   9113    3/   3 Mon Nov  5 12:41:16 2001\n636513   2605 100600   1327    1/   1 Mon Nov  5 12:41:16 2001\n636520   2605 100600     20    1/   1 Mon Nov  5 12:41:16 2001\n1338319   2605 100600   6998    2/   2 Mon Nov  5 12:48:55 2001\n \n# Based on the time and date, the inode to restore is 179573, 636513 \n# or 636520.  Try to figure out which one.\n\ndebugfs:cat <179573> \n   .\n   .\n\ndebugfs:cat <636513>\n   .\n\n# This is rather inconvenient.  If the directory where the files were\n# deleted from still exists, use the cd command to get there and then\n# use ls -d  which lists the files in the directory only, including\n# those with the deleted flag set. \n\n1566721  (12) .    32641  (12) ..    1566788  (60) 530\n1566790 (48) file1   1566791 (24) file2\n<1566747>  (20) file3\n\nThe inode numbers in brackets are deleted files.  A better looking display\ncomes with ls -ld.\n\n\n# So now you know which inode you need to restore.\n# To restore the file, you need to modify the inode, not the \n# directory entry.  This can be done with the modify_inode (mi)\n# command.  Specifically, change the deletion time to zero\n# and the link count to 1.\n\ndebugfs: mi <636513>\ndebugfs:  mi <148003>\n                              Mode    [0100644] \n                           User ID    [510] \n                          Group ID    [510] \n                              Size    [8123] \n                     Creation time    [904216575] \n                 Modification time    [904234782] \n                       Access time    [904234782] \n                     Deletion time    [904236721] 0\n                        Link count    [0] 1\n                       Block count    [16] \n                        File flags    [0x0] \n                         Reserved1    [0] \n                          File acl    [0] \n                     Directory acl    [0] \n                  Fragment address    [0] \n                   Fragment number    [0] \n                     Fragment size    [0] \n                   Direct Block #0    [100321] \n                   Direct Block #1    [100322] \n                   Direct Block #2    [100323] \n                   Direct Block #3    [100324] \n                   Direct Block #4    [200456] \n                   Direct Block #5    [200457] \n                   Direct Block #6    [200675] \n                   Direct Block #7    [200675] \n                   Direct Block #8    [304568] \n                   Direct Block #9    [0] \n                  Direct Block #10    [0] \n                  Direct Block #11    [0] \n                    Indirect Block    [0] \n             Double Indirect Block    [0] \n             Triple Indirect Block    [0] \n\n# It has been recovered.\n\n# This won\'t work for files with indirect blocks and you might find that\n# one or more blocks have been reused already.  If so, you can\n# recover as much data as possible by dumping the blocks to a file.\n\ndebugfs: dump <100321> /tmp > file1.000\ndebugfs: dump <100322> /tmp >> file1.000\n\n# and so on. For files that are longer than 12 blocks, you have to \n# trace the indirect, double-indirect and triple-indirect blocks.\n\n</pre>\n\n=== blktrace advanced ===\n\n* legacy \'blktrace\' data output\n <pre>\nDev_ID CPU_ID   SN   Timestamp      PID   Phz Act Address   Offset ProcessName\n------------------------------------------------------------------------------------  \n  8,16   1      929  2200.865379372 26328  A   R 3188196112 + 8 <- (8,17) 3188194064\n  8,17   1      930  2200.865379890 26328  Q   R 3188196112 + 8 [mysqld]\n  8,17   1      931  2200.865380598 26328  G   R 3188196112 + 8 [mysqld]\n  8,17   1      932  2200.865381014 26328  P   N [mysqld]\n  8,17   1      933  2200.865381784 26328  I   R 3188196112 + 8 [mysqld]\n  8,17   1      934  2200.865382107 26328  U   N [mysqld] 1\n  8,17   1      935  2200.865382605 26328  D   R 3188196112 + 8 [mysqld]\n  8,17   1      936  2200.871162161     0  C   R 3188196112 + 8 [0]\n  8,16   1      937  2200.871189524 26328  A   R 3188589744 + 8 <- (8,17) 3188587696\n  8,17   1      938  2200.871190517 26328  Q   R 3188589744 + 8 [mysqld]\n  8,17   1      939  2200.871192023 26328  G   R 3188589744 + 8 [mysqld]\n  8,17   1      940  2200.871192992 26328  P   N [mysqld]\n  8,17   1      941  2200.871194468 26328  I   R 3188589744 + 8 [mysqld]\n  8,17   1      942  2200.871195233 26328  U   N [mysqld] 1\n</pre>\n\n* legacy \'lsof\' data output\n <pre>\nCOMMAND     PID            USER   FD      TYPE             DEVICE SIZE/OFF       NODE NAME\n-------------------------------------------------------------------------------------------------\ninit          1            root  cwd       DIR                8,1     4096          2 /\ninit          1            root  rtd       DIR                8,1     4096          2 /\ninit          1            root  txt       REG                8,1   163096   57147454 /sbin/init\ninit          1            root  mem       REG                8,1    52120   96997047 /lib/x86_64-linux-gnu/libnss_files-2.15.so\ninit          1            root  mem       REG                8,1    47680   96993415 /lib/x86_64-linux-gnu/libnss_nis-2.15.so\ninit          1            root  mem       REG                8,1    97248   96997056 /lib/x86_64-linux-gnu/libnsl-2.15.so\ninit          1            root  mem       REG                8,1    35680   96997048 /lib/x86_64-linux-gnu/libnss_compat-2.15.so\ninit          1            root  mem       REG                8,1  1811128   96993409 /lib/x86_64-linux-gnu/libc-2.15.so\ninit          1            root  mem       REG                8,1    31752   96993413 /lib/x86_64-linux-gnu/librt-2.15.so\ninit          1            root  mem       REG                8,1   135366   96993411 /lib/x86_64-linux-gnu/libpthread-2.15.so\ninit          1            root  mem       REG                8,1   276392   96996820 /lib/x86_64-linux-gnu/libdbus-1.so.3.5.8\ninit          1            root  mem       REG                8,1    38888   96996879 /lib/x86_64-linux-gnu/libnih-dbus.so.1.0.0\ninit          1            root  mem       REG                8,1    96240   96996881 /lib/x86_64-linux-gnu/libnih.so.1.0.0\ninit          1            root  mem       REG                8,1   149280   96997051 /lib/x86_64-linux-gnu/ld-2.15.so\ninit          1            root    0u      CHR                1,3      0t0       1029 /dev/null\ninit          1            root    1u      CHR                1,3      0t0       1029 /dev/null\ninit          1            root    2u      CHR                1,3      0t0       1029 /dev/null\ninit          1            root    3r     FIFO                0,8      0t0       3001 pipe\ninit          1            root    4w     FIFO                0,8      0t0       3001 pipe\ninit          1            root    5r     0000                0,9        0       6797 anon_inode\ninit          1            root    6r     0000                0,9        0       6797 anon_inode\ninit          1            root    7u     unix 0xffff88020e5cc680      0t0       7152 socket\ninit          1            root    8u     unix 0xffff8802127caa40      0t0      10936 socket\ninit          1            root    9u     unix 0xffff8802116623c0      0t0       1999 socket\ninit          1            root   10u     unix 0xffff880211663400      0t0      10692 socket\ninit          1            root   12w      REG               8,18     2664   12845346 /var/log/upstart/mysql.log.1 (deleted)\ninit          1            root   14u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   16u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   17u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   18u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   20w      REG               8,18     1342   12845172 /var/log/upstart/modemmanager.log.1 (deleted)\ninit          1            root   21u      CHR                5,2      0t0       7184 /dev/ptmx\nkthreadd      2            root  cwd       DIR                8,1     4096          2 /\nkthreadd      2            root  rtd       DIR                8,1     4096          2 /\nkthreadd      2            root  txt   unknown                                        /proc/2/exe\nksoftirqd     3            root  cwd       DIR                8,1     4096          2 /\nksoftirqd     3            root  rtd       DIR                8,1     4096          2 /\nksoftirqd     3            root  txt   unknown                                        /proc/3/exe\n...\napache2    2241            root  mem       REG                8,1  1852792   96996819 /lib/x86_64-linux-gnu/libcrypto.so.1.0.0\napache2    2241            root  mem       REG                8,1   374608   96996818 /lib/x86_64-linux-gnu/libssl.so.1.0.0\napache2    2241            root  mem       REG                8,1  1030512   96997045 /lib/x86_64-linux-gnu/libm-2.15.so\napache2    2241            root  mem       REG                8,1    66784   96996836 /lib/x86_64-linux-gnu/libbz2.so.1.0.4\napache2    2241            root  mem       REG                8,1  1518928   64756408 /usr/lib/x86_64-linux-gnu/libdb-5.1.so\napache2    2241            root  mem       REG                8,1   105288   96993414 /lib/x86_64-linux-gnu/libresolv-2.15.so\napache2    2241            root  mem       REG                8,1  8644728   65276931 /usr/lib/apache2/modules/libphp5.so\napache2    2241            root  mem       REG                8,1    34824   65276243 /usr/lib/apache2/modules/mod_negotiation.so\napache2    2241            root  mem       REG                8,1    18432   65276567 /usr/lib/apache2/modules/mod_mime.so\napache2    2241            root  mem       REG                8,1    10240   65276556 /usr/lib/apache2/modules/mod_env.so\napache2    2241            root  mem       REG                8,1    10240   65276178 /usr/lib/apache2/modules/mod_dir.so\napache2    2241            root  mem       REG                8,1    92720   96996948 /lib/x86_64-linux-gnu/libz.so.1.2.3.4\napache2    2241            root  mem       REG                8,1    22528   65276571 /usr/lib/apache2/modules/mod_deflate.so\napache2    2241            root  mem       REG                8,1    26624   65275788 /usr/lib/apache2/modules/mod_cgi.so\napache2    2241            root  mem       REG                8,1    34824   65276563 /usr/lib/apache2/modules/mod_autoindex.so\napache2    2241            root  mem       REG                8,1    10248   65275257 /usr/lib/apache2/modules/mod_authz_user.so\napache2    2241            root  mem       REG                8,1    10248   65276546 /usr/lib/apache2/modules/mod_authz_host.so\napache2    2241            root  mem       REG                8,1    10248   65275256 /usr/lib/apache2/modules/mod_authz_groupfile.so\napache2    2241            root  mem       REG                8,1     6152   65275193 /usr/lib/apache2/modules/mod_authz_default.so\napache2    2241            root  mem       REG                8,1    10248   65276547 /usr/lib/apache2/modules/mod_authn_file.so\napache2    2241            root  mem       REG                8,1    10248   65276545 /usr/lib/apache2/modules/mod_auth_basic.so\napache2    2241            root  mem       REG                8,1    14336   65275224 /usr/lib/apache2/modules/mod_alias.so\napache2    2241            root  mem       REG                8,1    14768   96993408 /lib/x86_64-linux-gnu/libdl-2.15.so\napache2    2241            root  mem       REG                8,1    18896   96996944 /lib/x86_64-linux-gnu/libuuid.so.1.3.0\napache2    2241            root  mem       REG                8,1   170024   96996871 /lib/x86_64-linux-gnu/libexpat.so.1.5.2\napache2    2241            root  mem       REG                8,1    43288   96997046 /lib/x86_64-linux-gnu/libcrypt-2.15.so\napache2    2241            root  mem       REG                8,1  1811128   96993409 /lib/x86_64-linux-gnu/libc-2.15.so\napache2    2241            root  mem       REG                8,1   135366   96993411 /lib/x86_64-linux-gnu/libpthread-2.15.so\napache2    2241            root  mem       REG                8,1   234720   64752153 /usr/lib/libapr-1.so.0.4.6\napache2    2241            root  mem       REG                8,1   142840   64752206 /usr/lib/libaprutil-1.so.0.3.12\napache2    2241            root  mem       REG                8,1   247896   96996910 /lib/x86_64-linux-gnu/libpcre.so.3.12.1\napache2    2241            root  mem       REG                8,1   149280   96997051 /lib/x86_64-linux-gnu/ld-2.15.so\napache2    2241            root  DEL       REG                0,4               11842 /dev/zero\napache2    2241            root    0r      CHR                1,3      0t0       1029 /dev/null\napache2    2241            root    1w      CHR                1,3      0t0       1029 /dev/null\napache2    2241            root    2w      REG               8,18    12640   12845390 /var/log/apache2/error.log\napache2    2241            root    3u     IPv4              14201      0t0        TCP *:http (LISTEN)\napache2    2241            root    4r     FIFO                0,8      0t0    7004026 pipe\napache2    2241            root    5w     FIFO                0,8      0t0    7004026 pipe\napache2    2241            root    6w      REG               8,18        0   12845132 /var/log/apache2/other_vhosts_access.log\napache2    2241            root    7w      REG               8,18   127987   12845338 /var/log/apache2/access.log\n\n</pre>\n\n:* [Q] what does \'node\' information mean in \'lsof\' output?\n\n\n* converged iotrace\n: join the information from the legacy blktrace data and lsof, inotify data\n\n\n\n=== SNIA IO Trace Data Files ===\n\n\n[http://iotta.snia.org/traces SNIA I/O Trace Data Files]\n\nThe categories (or types) of I/O traces include:\n\n* Application Traces [This category is currently empty]\n: Application Traces record calls made by a specific application.\n* Block I/O Traces\n: Block I/O Traces typically include block level (e.g., at the logical volume manager, disk driver, etc. level) and block protocol (e.g., SCSI, ATA, Fibre Channel) traces.\n* Historical Traces [This category is currently empty]\n: Historical Traces include all traces that 10 or more years of age.\n* NFS Traces\n: Network File System Traces are typically those for NFS and CIFS and which reflect the protocol used by such network file systems.\n* Parallel Traces\nParallel traces, generally taken from supercomputers, record the system calls made by multiple computers running in parallel.\nSSSI WIOCP Metrics\nSSSI WIOCP, the SNIA Solid State Storage Initiative (SSSI) Workload I/O Capture Program (WIOCP), collects already-summarized empirical metrics separately for both monitored devices and processes/applications.\nStatic Snapshots\nStatic Snapshots are traces taken statically of a file system rather than of system calls.\nSystem Call Traces\nSystem Call I/O Traces typically reflect operating system calls to the file system.\nTools\nHere you can find the tools used for reading the various trace files.\n\n\n==== MSR Cambridge Traces ====\n\n* [http://iotta.snia.org/traces/388 List of Traces]\n\n* MSR Cambridge Traces 1\n: 1-week block I/O Traces of enterprise servers at MSR Cambridge.\n: The citation for the MSRC traces can be found [http://static.usenix.org/event/fast08/tech/narayanan.html FAST 2008, \"Write Off-loading: Practical Power Management for Enterprise Storage\", Dushyanth Narayanan, Austin Donnelly, and Antony Rowstron, Microsoft Research Ltd.]\n\n\n\n==== Microsoft Enterprise Traces ====\n\nTraces collected at Microsoft using the event tracing for Windows framework.\n\n* [http://iotta.snia.org/traces/130 List of Traces]\n\n* TPCC Traces 1\n: TPC-C benchmark traces collected at Microsoft using the event tracing for Windows framework\n\n* TPCC Traces 2\n: TPC-C benchmark traces collected at Microsoft using the event tracing for Windows framework\n: These six 6-minute long traces were collected at various points during a TPC-C run, all of which were during periods of steady-state activity.\n\n* TPCE Traces\n: TPC-E benchmark traces collected at Microsoft using the event tracing for Windows framework\n: These 6 traces were collected during a ~ 84-minute TPC-E run which included a ~ 20-minute warm-up time.\n\n* Exchange Server Traces\n: Production traces collected at Microsoft using the event tracing for Windows framework\n: Collected for Exchange Server for a duration of 24 hours. The single tarball includes 96 trace files, each with a duration of 15 minutes.\n\n\n\n==== Microsoft Production Server Traces ====\n\n* [http://iotta.snia.org/traces/158 List of Traces]\n\n* BuildServer00 ~ BuildServer07\n: Traces of the 25 hours activity on the Microsoft Build Server\n* Development Tools Release\n: Collected for Developers Tools Release Server for a duration of 24 hours\n* Display Ads Data Server, Display Ads Payload Server\n: Collected over a period of 24 hours for Display Ads Data/Platform payload server\n* Live Maps Back End\n: Collected for LiveMaps back-end server for a duration of 24 hours\n* MSN Storage CFS\n: Collected for MSN Storage Metadata Server for a duration of 6 hours\n* MSN Storage File Server\n: Collected for MSN Storage file server for a duration of 6 hours\n* Radius Authentication\n: Collected for RADIUS authentication server\n* Radius Back End SQL Server\n: Collected for RADIUS back-end server\n\n=== 서울대 장병탁 교수님 세미나 ===\n\n* Hypernetwork ML/AI 기술\n:- [http://bi.snu.ac.kr/Courses/g-ai06_2/book-ch4-hypernetmemory-part3.pdf The Hypernetwork Model of Memory)]\n:- [http://bi.snu.ac.kr/Publications/Theses/BS12f_ChunHS.pdf 하이퍼네트워크 연상메모리 기반의 이미지-텍스트 교차검색 (Image-Text Crossmodal Retrieval via Hypernetwork Memory]\n:: 2nd wrong answer: (하이퍼네트워크 메모리 기반의 이미지-텍스트 교차검색)\n:: 1st wrong answer: (하이퍼네트워크 기반의 이미지-텍스트 연상 교차 검색)\n\n=== 상무님께서 보내주신 Storage 미래 관련 글들 ===\n\n\n* 기타\n\n: [http://wwpi.com/index.php?option=com_content&view=article&id=8158]\n\n: [http://lib.stanford.edu/files/pasig-jan2012/11B7%20Francis%20PASIG_2011_Francis_final.pdf]\n\n: [http://www.datarecoverygroup.com/articles/data-storage-history-and-future Data Storage History and Future]\n\n\n\n\n* 스토리지 미디어의 발전 전망\n: 우리가 스토리지 미디어를 개발하지는 않지만, 미디어의 발전 전망을 고려해서 소프트웨어의 미래를 전망해야겠지요.\n\n: [http://www-03.ibm.com/ibm/history/ibm100/us/en/icons/racetrack/ IBM100] \n\n: [http://static.usenix.org/events/fast02/coufal.pdf FAST 2002]\n\n\n\n\n* DNA를 데이터 스토리지로 이용하는 것에 관한 또 다른 글입니다. 장점/비용 이슈 언급됨.\n\n: [http://www.lifehacker.com.au/2013/01/is-dna-the-future-of-data-storage/ DNA를 data storage로 이용하기]\n\n \n* Storage의 미래\n\n: [http://blogs.computerworld.com/data-storage/20865/future-data-storage-revealed Future data storage revealed]\n\n: [http://blogs.computerworld.com/data-storage/21537/top-10-storage-predictions-back-future Top 10 storage predictions]\n\n: [http://blogs.computerworld.com/data-storage/21360/will-private-cloud-kill-storage-area-network Will Private Cloud Kill SAN?]\n\n== ## bNote-2013-03-07 ==\n\n\n=== Find X\'s ===\n\n==== System Event (esp., file system change) Tracing/Monitoring/Collecting ====\n\n* LTTng\n* DTrace\n* FTrace\n* Strace\n* SystemTap\n* inotify ***\n* FAM (File Alteration Monitor) [http://oss.sgi.com/projects/fam/]\n* Gamin (File and directory monitoring system defined to be a subset of the FAM system [http://people.gnome.org/~veillard/gamin/overview.html]\n\n----\n\n==== inotify ====\n\n* inotify - monitoring file system events\n: The inotify API provides a mechanism for monitoring file system events. Inotify can be used to monitor individual files, or to monitor directories. When a directory is monitored, inotify will return events for the directory itself, and for files inside the directory.\n: The following system calls are used with this API: inotify_init(2) (or inotify_init1(2)), inotify_add_watch(2), inotify_rm_watch(2), read(2), and close(2).\n\n* /proc interfaces\n: /proc/sys/fs/inotify/max_queued_events\n: /proc/sys/fs/inotify/max_user_instances\n: /proc/sys/fs/inotify/max_user_watches\n\n* Versions\n: Inotify was merged into the 2.6.13 Linux kernel. The required library interfaces were added to glibc in version 2.4. (IN_DONT_FOLLOW, IN_MASK_ADD, and IN_ONLYDIR were only added in version 2.5.)\n\n* Check whether inotify is enabled or not\n $ grep INOTIFY_USER /boot/config-$(uname -r)\n CONFIG_INOTIFY_USER=y\n\n* Installation on Ubuntu by apt-get\n: aptitude install inotify-tools python-inotifyx libinotifytools0-dev\n\n* inotify는 다음 event들에 대해서만 detection 가능함\n:* access\n:* modify\n:* attrib\n:: watched file에 대한 메타데이터가 변경되거나, watched directory 내의 file이 변경된 경우, \'attrib\' event가 발생\n:* close_write\n:* close_nowrite\n:* close\n:* open\n:* moved_to\n:* moved_from\n:* move\n:* move_self\n:: 이 event 이후에는 file or directory는 no longer being watched된다... 는데, delete event의 경우와 무엇이 다른가?\n:* create\n:* delete\n:* delete_self\n:* unmount\n\n\n\n\n* References\n# [http://www.infoq.com/articles/inotify-linux-file-system-event-monitoring InfoQ -- Inotify: Efficient, Real-time Linux File System Event Monitoring]\n# [http://www.ibm.com/developerworks/linux/library/l-ubuntu-inotify/index.html Monitor file system activity with inotify (B.GOOD)]\n# [http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=90586523eb4b349806887c62ee70685a49415124 git.kernel.org -- fsnotify: unified filesystem notification backend, 2009-05-21~2009-06-11]\n# [http://stackoverflow.com/questions/9614184/how-to-trace-per-file-io-operations-in-linux How to trace per-file IO operations in Linux? -- /proc/PID/fd/, systemtap, strace, fanotify]\n# [http://stackoverflow.com/questions/1835947/how-do-i-program-for-linuxs-new-fanotify-file-system-monitoring-feature Stackoverflow -- How do I program for Linux\'s new \'fanotify\' file system monitoring feature?]\n# [http://stackoverflow.com/questions/8381566/best-way-to-monitor-file-system-changes-in-linux Stackoverflow -- Best way to monitor file system changes in Linux]\n# [http://ubuntuforums.org/showthread.php?t=663950 python inotify example -- Ubuntu Forums]\n# [http://pyinotify.sourceforge.net/ Pyinotify: monitor filesystem events with Python under Linux - Brief Tutorial]\n# [http://github.com/seb-m/pyinotify pyinotify github]\n\n=== directory-file-addr spatial locality ===\n\n* Directory Hierarchy\n\n <pre>\nblusjune@jimi-hendrix:[dir_file_addr_spatial_locality] $ find r0\nr0\nr0/d1\nr0/d1/d13\nr0/d1/d11\nr0/d1/d12\nr0/d2\nr0/d2/d21\nr0/d2/d21/d212\nr0/d2/d21/d211\nr0/d2/d21/d213\nr0/d2/d22\nr0/d2/d22/d221\nr0/d2/d22/d222\n</pre>\n\n== ## bNote-2013-03-06 ==\n\n=== LBA-to-name processing (DELETEME) ===\n\nDone actually 2013-03-11 14:35. [http://www.expertslogin.com/general/how-to-fix-repair-bad-blocks-in-linux/ How To Fix / Repair Bad Blocks In Linux B.GOOD]\n\n\n* How to find the corresponding location (file/dir) for some LBA (e.g., 1661223128)?\n\n: step 1: We need to find the block size of the filesystem\n <pre>\nblusjune@radiohead:[google] $ sudo tune2fs -l /dev/sda1  | grep Block\nBlock count:              488378112\nBlock size:               4096\nBlocks per group:         32768\n</pre>\n\n: step 2: check the starting sector of the partition (file system)\n <pre>\nblusjune@radiohead:[google] $ sudo fdisk -lu /dev/sda\n\nDisk /dev/sda: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x00001eb9\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *        2048  3907026943  1953512448   83  Linux\n\n</pre>\n\n: step 3: File the filesystem block that contains this target LBA\n <pre>\nb = (int) ((L-S) * 512 / B)\n\n, where\nb = File system block number (we need this answer)\nB = File system block size in bytes (4096 - we got this in step 1)\nL = target LBA (1661223128 - given)\nS = Starting sector of the partition as shown by fdisk -lu (2048 - we got this in step 2)\n\n, so the answer b is 207652635 (207,652,635)\n\n</pre>\n\n: step 4: Check the file/dir name with given b (207652635)\n <pre>\nblusjune@radiohead:[s05] $ sudo debugfs /dev/sda1 \ndebugfs 1.42 (29-Nov-2011)\n\ndebugfs:  testb 207652635\nBlock 207652635 marked in use\n\ndebugfs:  icheck 207652635\nBlock	Inode number\n207652635	39585621\n\ndebugfs:  ncheck 39585621\nInode	Pathname\n39585621	/home/blusjune/.config/google-chrome/Default/Cookies-journal\n\ndebugfs:\n</pre>\n\n=== IOWA.sim.iox (myreal_72h) ===\n\n* base trace log on radiohead\n:- name: bpo_a.20130305_104633.real_whole_trace.log\n:- size: 197,372,058 bytes (197372058)\n\n <pre>\nblusjune@radiohead:[s05] $ pwd\n/x/var/iowa/sidewinder/iowa/iowa.sim.iox/tdir/s05\n\nblusjune@radiohead:[s05] $ l\ntotal 210572\ndrwxrwxr-x  2 blusjune blusjune      4096 Mar  6 11:07 ./\ndrwxrwxr-x 10 blusjune blusjune      4096 Mar  6 19:51 ../\nlrwxrwxrwx  1 blusjune blusjune        38 Mar  5 10:43 .bdx.0100.y.proc_after_trace_s10.sh -> ../.bdx.0100.y.proc_after_trace_s10.sh\n-rw-rw-r--  1 root     root     197372058 Mar  5 13:34 bpo_a.20130305_104633.real_whole_trace.log\n-rw-rw-r--  1 blusjune blusjune   4945483 Mar  6 11:06 tracelog.myrealtrace.log.A.addr\n-rw-rw-r--  1 blusjune blusjune   1081666 Mar  6 11:06 tracelog.myrealtrace.log.R.addr\n-rw-rw-r--  1 blusjune blusjune   3863817 Mar  6 11:06 tracelog.myrealtrace.log.W.addr\n-rw-rw-r--  1 blusjune blusjune   8339772 Mar  6 11:06 tracelog.myrealtrace.log.p1.out\n</pre>\n\n\n* R.addr analysis\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_200532.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  1\n__valu__sig__ _n_o_sigaddrs :  43683\n__valu__sig__ _sigioc_acc :  43683\n__valu__sig__ _sigaddrs_efficiency :  1.0\n__valu__sig__ _n_o_addr_total :  43683\n__valu__sig__ _ioc_total :  43683\n</pre>\n\n\n* W.addr analysis\n:- Used as weekly report item (2013-03-06), and lead to a patent\n::- just 18 addresses cover 25% of IO (40,010 IOs out of 157,632 IOs)\n::- x 2222.8 caching efficiency\n\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  1115\n__valu__sig__ _n_o_sigaddrs :  18\n__valu__sig__ _sigioc_acc :  40010\n__valu__sig__ _sigaddrs_efficiency :  2222.77777778\n__valu__sig__ _n_o_addr_total :  40979\n__valu__sig__ _ioc_total :  157632\n</pre>\n\n\n* Processes Contributed to the IO Workload\n\n <pre>\na1mjjung@secm:[myrealtrace.radiohead.v01] $ cat bpo_a.20130305_104633.real_whole_trace.log  | grep \'\\[\' | sed -e \'s/.*\\(\\[.*\\]\\)/\\1/g\' | sort -u\n[(null)]\n[(null)] 1\n[(null)] 3\n[(null)] 4\n[(null)] 6\n[(null)] 7\n[0]\n[BrowserBlocking]\n[BrowserBlocking] 1\n[Chrome_CacheThr]\n[Chrome_CacheThr] 1\n[Chrome_CacheThr] 10\n[Chrome_CacheThr] 2\n[Chrome_CacheThr] 3\n[Chrome_CacheThr] 6\n[Chrome_CacheThr] 9\n[Chrome_ChildIOT]\n[Chrome_DBThread]\n[Chrome_DBThread] 1\n[Chrome_DBThread] 2\n[Chrome_DBThread] 3\n[Chrome_DBThread] 4\n[Chrome_DBThread] 5\n[Chrome_DBThread] 6\n[Chrome_FileThre]\n[Chrome_FileThre] 1\n[Chrome_FileThre] 2\n[Chrome_FileThre] 3\n[Chrome_FileThre] 4\n[Chrome_HistoryT]\n[Chrome_HistoryT] 1a1mjjung@secm:[myrealtrace.radiohead.v01] $ cat bpo_a.20130305_104633.real_whole_trace.log  | grep \'\\[\' | sed -e \'s/.*\\(\\[.*\\]\\)/\\1/g\' | sort -u\n[(null)]\n[(null)] 1\n[(null)] 3\n[(null)] 4\n[(null)] 6\n[(null)] 7\n[0]\n[BrowserBlocking]\n[BrowserBlocking] 1\n[Chrome_CacheThr]\n[Chrome_CacheThr] 1\n[Chrome_CacheThr] 10\n[Chrome_CacheThr] 2\n[Chrome_CacheThr] 3\n[Chrome_CacheThr] 6\n[Chrome_CacheThr] 9\n[Chrome_ChildIOT]\n[Chrome_DBThread]\n[Chrome_DBThread] 1\n[Chrome_DBThread] 2\n[Chrome_DBThread] 3\n[Chrome_DBThread] 4\n[Chrome_DBThread] 5\n[Chrome_DBThread] 6\n[Chrome_FileThre]\n[Chrome_FileThre] 1\n[Chrome_FileThre] 2\n[Chrome_FileThre] 3\n[Chrome_FileThre] 4\n[Chrome_HistoryT]\n[Chrome_HistoryT] 1\n[Chrome_HistoryT] 10\n[Chrome_HistoryT] 11\n[Chrome_HistoryT] 12\n[Chrome_HistoryT] 14\n[Chrome_HistoryT] 15\n[Chrome_HistoryT] 16\n[Chrome_HistoryT] 2\n[Chrome_HistoryT] 3\n[Chrome_HistoryT] 4\n[Chrome_HistoryT] 5\n[Chrome_HistoryT] 6\n[Chrome_HistoryT] 7\n[Chrome_HistoryT] 8\n[Chrome_HistoryT] 9\n[Chrome_IOThread]\n[Chrome_IOThread] 1\n[Chrome_ProcessL]\n[Chrome_SafeBrow]\n[Chrome_SafeBrow] 1\n[Chrome_SafeBrow] 4\n[Chrome_SafeBrow] 5\n[Chrome_SyncThre]\n[Chrome_SyncThre] 1\n[Chrome_SyncThre] 10\n[Chrome_SyncThre] 11\n[Chrome_SyncThre] 12\n[Chrome_SyncThre] 13\n[Chrome_SyncThre] 14\n[Chrome_SyncThre] 15\n[Chrome_SyncThre] 16\n[Chrome_SyncThre] 2\n[Chrome_SyncThre] 3\n[Chrome_SyncThre] 4\n[Chrome_SyncThre] 5\n[Chrome_SyncThre] 6\n[Chrome_SyncThre] 7\n[Chrome_SyncThre] 8\n[Chrome_SyncThre] 9\n[Chrome_WebKitTh]\n[Chrome_WebKitTh] 1\n[MediaStreamDevi]\n[MediaStreamDevi] 1\n[NetworkChangeNo]\n[NetworkChangeNo] 1\n[WorkerPool/2786]\n[WorkerPool/2786] 1\n[WorkerPool/2787]\n[WorkerPool/2787] 1\n[WorkerPool/2788]\n[WorkerPool/2788] 1\n[WorkerPool/2839]\n[WorkerPool/2839] 1\n[WorkerPool/2840]\n[WorkerPool/2840] 1\n[WorkerPool/2840] 2\n[WorkerPool/2981]\n[WorkerPool/2981] 1\n[WorkerPool/2982]\n[WorkerPool/2982] 1\n[_BDX]\n[_BDX] 1\n[apache2]\n[apache2] 1\n[apache2ctl]\n[apache2ctl] 1\n[bash]\n[bash] 1\n[bash] 2\n[bash] 3\n[bash] 4\n[bash] 5\n[blktrace]\n[cat]\n[cat] 1\n[checkarray]\n[checkarray] 1\n[chrome]\n[chrome] 1\n[chrome] 2\n[chrome] 3\n[chrome] 4\n[console-kit-dae]\n[console-kit-dae] 1\n[console-kit-dae] 4\n[cp]\n[cp] 1\n[cron]\n[dbus-launch]\n[dbus-launch] 1\n[env]\n[env] 1\n[expr]\n[expr] 2\n[flush-8:0]\n[flush-8:0] 1\n[flush-8:0] 10\n[flush-8:0] 11\n[flush-8:0] 12\n[flush-8:0] 13\n[flush-8:0] 14\n[flush-8:0] 15\n[flush-8:0] 16\n[flush-8:0] 2\n[flush-8:0] 3\n[flush-8:0] 4\n[flush-8:0] 5\n[flush-8:0] 6\n[flush-8:0] 7\n[flush-8:0] 8\n[flush-8:0] 9\n[git-pull]\n[git-pull] 1\n[git]\n[git] 1\n[gnome-pty-helpe]\n[gnome-pty-helpe] 1\n[gnome-terminal]\n[gnome-terminal] 1\n[gnome-terminal] 2\n[gnome-terminal] 3\n[gnome-terminal] 4\n[gnome-terminal] 5\n[gnome-terminal] 7\n[google-chrome]\n[google-chrome] 1\n[inotify_reader]\n[inotify_reader] 1\n[jbd2/sda1-8]\n[jbd2/sda1-8] 1\n[jbd2/sda1-8] 16\n[jbd2/sda1-8] 2\n[jbd2/sda1-8] 3\n[jbd2/sda1-8] 4\n[jbd2/sda1-8] 5\n[kworker/0:1]\n[kworker/0:2]\n[kworker/2:1]\n[kworker/2:2]\n[kworker/3:0]\n[kworker/4:0]\n[kworker/4:1]\n[kworker/4:2]\n[kworker/5:0]\n[kworker/5:1]\n[kworker/5:2]\n[kworker/7:2]\n[libvirtd]\n[libvirtd] 1\n[libvirtd] 2\n[ls]\n[ls] 1\n[mysqld]\n[nmbd]\n[nmbd] 2\n[nmbd] 4\n[nmbd] 5\n[nmbd] 6\n[python]\n[python] 1\n[python] 2\n[python] 3\n[python] 4\n[python] 5\n[rserver]\n[rserver] 1\n[rserver] 2\n[rserver] 3\n[rserver] 6\n[rserver] 8\n[rsession]\n[rsession] 1\n[run-parts]\n[run-parts] 1\n[sh]\n[sh] 1\n[smbd]\n[smbd] 1\n[smbd] 2\n[smbd] 3\n[sshd]\n[sshd] 1\n[sshd] 4\n[sshd] 5\n[swapper/0]\n[udevd]\n[udevd] 1\n[update-motd-fsc]\n[update-motd-fsc] 1\n[update-motd-upd]\n[update-motd-upd] 1\n[upstart-udev-br]\n[upstart-udev-br] 1\n[xdg-settings]\n[xdg-settings] 1\n[Chrome_HistoryT] 10\n[Chrome_HistoryT] 11\n[Chrome_HistoryT] 12\n[Chrome_HistoryT] 14\n[Chrome_HistoryT] 15\n[Chrome_HistoryT] 16\n[Chrome_HistoryT] 2\n[Chrome_HistoryT] 3\n[Chrome_HistoryT] 4\n[Chrome_HistoryT] 5\n[Chrome_HistoryT] 6\n[Chrome_HistoryT] 7\n[Chrome_HistoryT] 8\n[Chrome_HistoryT] 9\n[Chrome_IOThread]\n[Chrome_IOThread] 1\n[Chrome_ProcessL]\n[Chrome_SafeBrow]\n[Chrome_SafeBrow] 1\n[Chrome_SafeBrow] 4\n[Chrome_SafeBrow] 5\n[Chrome_SyncThre]\n[Chrome_SyncThre] 1\n[Chrome_SyncThre] 10\n[Chrome_SyncThre] 11\n[Chrome_SyncThre] 12\n[Chrome_SyncThre] 13\n[Chrome_SyncThre] 14\n[Chrome_SyncThre] 15\n[Chrome_SyncThre] 16\n[Chrome_SyncThre] 2\n[Chrome_SyncThre] 3\n[Chrome_SyncThre] 4\n[Chrome_SyncThre] 5\n[Chrome_SyncThre] 6\n[Chrome_SyncThre] 7\n[Chrome_SyncThre] 8\n[Chrome_SyncThre] 9\n[Chrome_WebKitTh]\n[Chrome_WebKitTh] 1\n[MediaStreamDevi]\n[MediaStreamDevi] 1\n[NetworkChangeNo]\n[NetworkChangeNo] 1\n[WorkerPool/2786]\n[WorkerPool/2786] 1\n[WorkerPool/2787]\n[WorkerPool/2787] 1\n[WorkerPool/2788]\n[WorkerPool/2788] 1\n[WorkerPool/2839]\n[WorkerPool/2839] 1\n[WorkerPool/2840]\n[WorkerPool/2840] 1\n[WorkerPool/2840] 2\n[WorkerPool/2981]\n[WorkerPool/2981] 1\n[WorkerPool/2982]\n[WorkerPool/2982] 1\n[_BDX]\n[_BDX] 1\n[apache2]\n[apache2] 1\n[apache2ctl]\n[apache2ctl] 1\n[bash]\n[bash] 1\n[bash] 2\n[bash] 3\n[bash] 4\n[bash] 5\n[blktrace]\n[cat]\n[cat] 1\n[checkarray]\n[checkarray] 1\n[chrome]\n[chrome] 1\n[chrome] 2\n[chrome] 3\n[chrome] 4\n[console-kit-dae]\n[console-kit-dae] 1\n[console-kit-dae] 4\n[cp]\n[cp] 1\n[cron]\n[dbus-launch]\n[dbus-launch] 1\n[env]\n[env] 1\n[expr]\n[expr] 2\n[flush-8:0]\n[flush-8:0] 1\n[flush-8:0] 10\n[flush-8:0] 11\n[flush-8:0] 12\n[flush-8:0] 13\n[flush-8:0] 14\n[flush-8:0] 15\n[flush-8:0] 16\n[flush-8:0] 2\n[flush-8:0] 3\n[flush-8:0] 4\n[flush-8:0] 5\n[flush-8:0] 6\n[flush-8:0] 7\n[flush-8:0] 8\n[flush-8:0] 9\n[git-pull]\n[git-pull] 1\n[git]\n[git] 1\n[gnome-pty-helpe]\n[gnome-pty-helpe] 1\n[gnome-terminal]\n[gnome-terminal] 1\n[gnome-terminal] 2\n[gnome-terminal] 3\n[gnome-terminal] 4\n[gnome-terminal] 5\n[gnome-terminal] 7\n[google-chrome]\n[google-chrome] 1\n[inotify_reader]\n[inotify_reader] 1\n[jbd2/sda1-8]\n[jbd2/sda1-8] 1\n[jbd2/sda1-8] 16\n[jbd2/sda1-8] 2\n[jbd2/sda1-8] 3\n[jbd2/sda1-8] 4\n[jbd2/sda1-8] 5\n[kworker/0:1]\n[kworker/0:2]\n[kworker/2:1]\n[kworker/2:2]\n[kworker/3:0]\n[kworker/4:0]\n[kworker/4:1]\n[kworker/4:2]\n[kworker/5:0]\n[kworker/5:1]\n[kworker/5:2]\n[kworker/7:2]\n[libvirtd]\n[libvirtd] 1\n[libvirtd] 2\n[ls]\n[ls] 1\n[mysqld]\n[nmbd]\n[nmbd] 2\n[nmbd] 4\n[nmbd] 5\n[nmbd] 6\n[python]\n[python] 1\n[python] 2\n[python] 3\n[python] 4\n[python] 5\n[rserver]\n[rserver] 1\n[rserver] 2\n[rserver] 3\n[rserver] 6\n[rserver] 8\n[rsession]\n[rsession] 1\n[run-parts]\n[run-parts] 1\n[sh]\n[sh] 1\n[smbd]\n[smbd] 1\n[smbd] 2\n[smbd] 3\n[sshd]\n[sshd] 1\n[sshd] 4\n[sshd] 5\n[swapper/0]\n[udevd]\n[udevd] 1\n[update-motd-fsc]\n[update-motd-fsc] 1\n[update-motd-upd]\n[update-motd-upd] 1\n[upstart-udev-br]\n[upstart-udev-br] 1\n[xdg-settings]\n[xdg-settings] 1\n\n</pre>\n\n\n=== IOWA.sim.iox (tpcc_250gb_48h) ===\n\n* R.addr analysis\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_182112.sigio_25.iowsz_100.t1_10000] $ cat __simout.sigio_25.iowsz_100.t1_10000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  7\n__valu__sig__ _n_o_sigaddrs :  126606\n__valu__sig__ _sigioc_acc :  1169938\n__valu__sig__ _sigaddrs_efficiency :  9.24077847811\n__valu__sig__ _n_o_addr_total :  1691608\n__valu__sig__ _ioc_total :  4113312\n</pre>\n\n\n* W.addr analysis\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_190100.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  601\n__valu__sig__ _n_o_sigaddrs :  46304\n__valu__sig__ _sigioc_acc :  47940036\n__valu__sig__ _sigaddrs_efficiency :  1035.33249827\n__valu__sig__ _n_o_addr_total :  4910080\n__valu__sig__ _ioc_total :  191622453\n</pre>\n\n x39.02 = ( total_#_of_IOs / total_#_of_addrs_hit_actually )\n x1035.33 = ( 25%_sig_IOs / 25%_sig_addrs )\n\n=== R \'e1071\' package install (command line) ===\n\n <pre>\na1mjjung@secm:[e1071] $ R CMD INSTALL e1071\nJob <13412785> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura007>>\nWarning: invalid package \'e1071\'\nError: ERROR: no packages specified\na1mjjung@secm:[e1071] $ cd ..\nla1mjjung@secm:[e1071_build] $ l\ntotal 280\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:45 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\na1mjjung@secm:[e1071_build] $ R CMD INSTALL e1071\nJob <13412786> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n* installing to library \'/home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15\'\n* installing *source* package \'e1071\' ...\n** package \'e1071\' successfully unpacked and MD5 sums checked\nchecking for C++ compiler default output file name... a.out\nchecking whether the C++ compiler works... yes\nchecking whether we are cross compiling... no\nchecking for suffix of executables... \nchecking for suffix of object files... o\nchecking whether we are using the GNU C++ compiler... yes\nchecking whether g++ accepts -g... yes\n** libs\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c Rsvm.c -o Rsvm.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cmeans.c -o cmeans.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cshell.c -o cshell.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c floyd.c -o floyd.o\ng++ -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c svm.cpp -o svm.o\ng++ -shared -L/usr/local/lib64 -o e1071.so Rsvm.o cmeans.o cshell.o floyd.o svm.o\ninstalling to /home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15/e1071/libs\n** R\n** inst\n** preparing package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n   \'svmdoc.Rnw\' \n** testing if installed package can be loaded\n\n* DONE (e1071)\na1mjjung@secm:[e1071_build] $ R\nJob <13412787> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n\nR version 2.15.2 (2012-10-26) -- \"Trick or Treat\"\nCopyright (C) 2012 The R Foundation for Statistical Computing\nISBN 3-900051-07-0\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType \'license()\' or \'licence()\' for distribution details.\n\nR is a collaborative project with many contributors.\nType \'contributors()\' for more information and\n\'citation()\' on how to cite R or R packages in publications.\n\nType \'demo()\' for some demos, \'help()\' for on-line help, or\n\'help.start()\' for an HTML browser interface to help.\nType \'q()\' to quit R.\n\n> ?svm\nNo documentation for \'svm\' in specified packages and libraries:\nyou could try \'??svm\'\n> library(e1071) \nLoading required package: class\n> ?svm\n> \nSave workspace image? [y/n/c]: y\na1mjjung@secm:[e1071_build] $ pwd\n/home/X0101/a1mjjung/x/R/e1071_build\na1mjjung@secm:[e1071_build] $ ls -alF\ntotal 284\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:47 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\n-rw------- 1 a1mjjung X0101     25 Mar  6 18:47 .Rhistory\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:46 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\n\n</pre>\n\n\n\n\n=== 3rd meeting with Dr. CHOI ===\n\n\n\n=== 수퍼컴 (supercom) ===\n\n\n* account\n: ID: a1mjjung\n: PW: wjdaudwns\n: IP address: 202.20.183.10 (ssh)\n\n* password change\n: 한지연 사원 (jiyoun92.han@partner.samsung.com) (031-280-8147)\n\n* python 2.7.x from 2.6.x\n: [a1mjjung@login03 ~]$ /apps/Python/Python-2.7.3/bin/python\n\n== ## bNote-2013-03-05 ==\n\n=== 최희열 전문과 2차 미팅 ===\n\n* IO trace (궤적)의 변화 예측\n: IO 궤적의 변화를 신속 정확하게 예측하는 것의 중요성\n:: 언제 hot data가 fast-tier에 올라가야 할지,\n:: 현재의 hot data 말고 새로운 hot data는 언제 올라가야할지?\n:: 현재 fast-tier에 올라가 있는 hot data가 언제 unload되어야 할지?\n\ncached/tiered hot data가 계속 hot storage area에 \n: IO hot zone 궤적 변화의 전조로 사용될 수 있을만한 시스템 정보/상태 변화\n: IO hot zone 궤적이 정말로 변화하는 것일지 아니면 기존 궤적에서의 fluctuation인지?\n\n* Challenging Problems\n\n: 단위 시간 당 hot zone의 갯수가 항상 유지되는 것은 아님. 즉, hot zone의 갯수는 1개로 유지된 채, hot zone의 위치만 이동하는 형태는 아닐 수 있음. hot zone의 갯수가 1개였다가 4개로, 7개로, 줄어서 5개로, 다시 줄어서 2개로, 이런 식으로\n\n== ## bNote-2013-03-05 ==\n\n=== IOWA to ML Formulation ===\n\n* IO trace (궤적)의 변화 예측\n: IO 궤적의 변화를 신속 정확하게 예측하는 것의 중요성\n:: 언제 hot data가 fast-tier에 올라가야 할지,\n:: 현재의 hot data 말고 새로운 hot data는 언제 올라가야할지?\n:: 현재 fast-tier에 올라가 있는 hot data가 언제 unload되어야 할지?\n\ncached/tiered hot data가 계속 hot storage area에 \n: IO hot zone 궤적 변화의 전조로 사용될 수 있을만한 시스템 정보/상태 변화\n: IO hot zone 궤적이 정말로 변화하는 것일지 아니면 기존 궤적에서의 fluctuation인지?\n\n* Challenging Problems\n\n: 단위 시간 당 hot zone의 갯수가 항상 유지되는 것은 아님. 즉, hot zone의 갯수는 1개로 유지된 채, hot zone의 위치만 이동하는 형태는 아닐 수 있음. hot zone의 갯수가 1개였다가 4개로, 7개로, 줄어서 5개로, 다시 줄어서 2개로, 이런 식으로','utf-8'),(53,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R packages for HMM ==\n\n=== R의 HMM 기능들 ===\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]','utf-8'),(54,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R packages for HMM ==\n\n=== R의 HMM 기능들 ===\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]','utf-8'),(55,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R packages for HMM ==\n\n=== R의 HMM 기능들 ===\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]','utf-8'),(56,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R packages for HMM ==\n\n=== R의 HMM 기능들 ===\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n* http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n: Chapter 11 Markov Chains (Dartmouth College)\n\n\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n\n* http://www.jstatsoft.org/v36/i07/paper\n: Journal of Statistical Software, August 2010, Vol. 36, Issue 7\n: depmixS4: An R Package for Hidden Markov Models\n\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n* http://www.tjdb.org/123456789/7390/1/IJRSP%2039(1)%2039-44.pdf\n: Higher order Markov chain models for monsoon rainfall over West Bengal, India','utf-8'),(57,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R packages for HMM ==\n\n=== R의 HMM 기능들 ===\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n* http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n: Chapter 11 Markov Chains (Dartmouth College)\n\n\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n\n* http://www.jstatsoft.org/v36/i07/paper\n: Journal of Statistical Software, August 2010, Vol. 36, Issue 7\n: depmixS4: An R Package for Hidden Markov Models\n\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n* http://www.tjdb.org/123456789/7390/1/IJRSP%2039(1)%2039-44.pdf\n: Higher order Markov chain models for monsoon rainfall over West Bengal, India','utf-8'),(58,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R packages for HMM ==\n\n=== R의 HMM 기능들 ===\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n\n* http://www.jstatsoft.org/v36/i07/paper\n: Journal of Statistical Software, August 2010, Vol. 36, Issue 7\n: depmixS4: An R Package for Hidden Markov Models\n\n\n* http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n: Chapter 11 Markov Chains (Dartmouth College)\n\n\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n* http://www.tjdb.org/123456789/7390/1/IJRSP%2039(1)%2039-44.pdf\n: Higher order Markov chain models for monsoon rainfall over West Bengal, India','utf-8'),(59,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R packages for HMM ==\n\n=== R의 HMM 기능들 ===\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n\n* http://www.jstatsoft.org/v36/i07/paper\n: Journal of Statistical Software, August 2010, Vol. 36, Issue 7\n: depmixS4: An R Package for Hidden Markov Models\n\n* http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf\n: depmixS4: An R Package for Hidden Markov Models\n\n* http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n: Chapter 11 Markov Chains (Dartmouth College)\n\n\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n* http://www.tjdb.org/123456789/7390/1/IJRSP%2039(1)%2039-44.pdf\n: Higher order Markov chain models for monsoon rainfall over West Bengal, India','utf-8'),(60,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R packages for HMM ==\n\n=== R의 HMM 기능들 ===\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n\n* http://www.jstatsoft.org/v36/i07/paper\n: Journal of Statistical Software, August 2010, Vol. 36, Issue 7\n: depmixS4: An R Package for Hidden Markov Models\n\n* http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf\n: depmixS4: An R Package for Hidden Markov Models\n\n\n* http://www.cs.columbia.edu/4761/notes07/chapter4.3-HMM.pdf\n: Chapter 4: Hidden Markov Models (Columbia University)\n\n\n* http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n: Chapter 11 Markov Chains (Dartmouth College)\n\n\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n* http://www.tjdb.org/123456789/7390/1/IJRSP%2039(1)%2039-44.pdf\n: Higher order Markov chain models for monsoon rainfall over West Bengal, India','utf-8'),(61,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R packages for HMM ==\n\n=== R의 HMM 기능들 ===\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n\n* http://www.jstatsoft.org/v36/i07/paper\n: Journal of Statistical Software, August 2010, Vol. 36, Issue 7\n: depmixS4: An R Package for Hidden Markov Models\n\n* http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf\n: depmixS4: An R Package for Hidden Markov Models\n\n\n* http://www.cs.columbia.edu/4761/notes07/chapter4.3-HMM.pdf\n: Chapter 4: Hidden Markov Models (Columbia University)\n\n\n* http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n: Chapter 11 Markov Chains (Dartmouth College)\n\n\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n* http://www.tjdb.org/123456789/7390/1/IJRSP%2039(1)%2039-44.pdf\n: Higher order Markov chain models for monsoon rainfall over West Bengal, India','utf-8'),(62,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R packages for HMM ==\n\n=== R의 HMM 기능들 ===\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n\n* http://www.jstatsoft.org/v36/i07/paper\n: Journal of Statistical Software, August 2010, Vol. 36, Issue 7\n: depmixS4: An R Package for Hidden Markov Models\n\n* http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf\n: depmixS4: An R Package for Hidden Markov Models\n\n\n* http://www.cs.columbia.edu/4761/notes07/chapter4.3-HMM.pdf\n: Chapter 4: Hidden Markov Models (Columbia University)\n\n\n* http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n: Chapter 11 Markov Chains (Dartmouth College)\n\n\n* http://www.princeton.edu/~rvan/orf557/hmm080728.pdf\n: Hidden Markov Models - Lecture Notes 2008-07-28 (Princeton University)\n\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n* http://www.tjdb.org/123456789/7390/1/IJRSP%2039(1)%2039-44.pdf\n: Higher order Markov chain models for monsoon rainfall over West Bengal, India','utf-8'),(63,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R packages for HMM ==\n\n=== R의 HMM 기능들 ===\n\n:- backward\n:: Computes the backward probabilities \n\n:- baumWelch\n:: Inferring the parameters of a Hidden Markov Model via the Baum-Welch algorithm (initial HMM과 given sequence of observations에 대해서, Baum-Welch algorithm은 HMM에 대한 optimal parameter들을 추론한다)\n\n:- dishonestCasino: \n:- forward\n:- HMM\n:- initHMM\n:- posterior\n:- simHMM\n:- viterbi\n:- viterbiTraining\n\n\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n\n* http://www.jstatsoft.org/v36/i07/paper\n: Journal of Statistical Software, August 2010, Vol. 36, Issue 7\n: depmixS4: An R Package for Hidden Markov Models\n\n* http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf\n: depmixS4: An R Package for Hidden Markov Models\n\n\n* http://www.cs.columbia.edu/4761/notes07/chapter4.3-HMM.pdf\n: Chapter 4: Hidden Markov Models (Columbia University)\n\n\n* http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n: Chapter 11 Markov Chains (Dartmouth College)\n\n\n* http://www.princeton.edu/~rvan/orf557/hmm080728.pdf\n: Hidden Markov Models - Lecture Notes 2008-07-28 (Princeton University)\n\n\n\n* http://www.stat.ucla.edu/~twng/storage/Hidden%20Markov%20Models%20for%20Time%20Series%20An%20Introduction%20Using%20R.pdf\n: Hidden Markov Models for Time Series - An Introduction Using R (UCLA)\n\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n* http://www.tjdb.org/123456789/7390/1/IJRSP%2039(1)%2039-44.pdf\n: Higher order Markov chain models for monsoon rainfall over West Bengal, India','utf-8');
INSERT INTO `radiohead_text` VALUES (64,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n[[Bnote patidea 2013-003]]\n\n\n== PATENT-BRIAN-2013-004 ==\n\n=== Event-based I/O Prediction ===\n\n=== 요약 ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n=== 기술 상세 ===\n\n* I/O 발생 메커니즘의 모델링\n: I/O prediction을 위해, type-I event (indirect I/O trigger)와 type-II event (direct I/O trigger)를 관찰 및 분석함. Type-I event의 예로서, user의 log-in, 기 log-in되어 있던 user session의 상태 변경 (idle-to-busy), system service에 대한 invocation from local or remote node 등이 포함될 수 있으며, type-II event의 예로서, user application 의 실행, system service의 실행 등이 포함될 수 있음. Type-I event는 type-II event의 발생에 영향을 미치고, type-II event는 I/O request event (data access)의 발생에 영향을 미치므로, \'type-I event\' -> \'type-II event\' -> \'I/O request event\' 까지 이어지는 인과사슬 (cause-effect chain) 기반으로 I/O 발생 메커니즘을 모델링하였음.\n\n\n* Event 정보 수집: type-I event 정보 및 type-II event 정보 정의 및 수집 방법.\n\n\n* I/O 패턴 학습 (Offline Learning): 수집된 Event 정보를 기반으로 I/O 패턴을 learning하는 방법\n\n\n* Run-time I/O 예측 (Online Matching): 시스템 모니터링에서 얻어지는 최근 및 현재의 event들과 learning으로 얻어진 I/O 패턴 DB에 기반하여 Run-time으로 미래 I/O 예측\n\n\n* 시스템 구조: 정보 수집 및 패턴 학습 모듈의 시스템 내 위치 (I/O 패턴 DB 등의 위치 및 실시간 예측 모듈과의 관계)\n\n\n* 순서도: 수집부터 학습까지 전체적인 task의 흐름\n\n\n* 예상 효과: 성능적 측면에서 사용자가 체감하는 lower latency, higher throughput 효과, 이미 발생해버린 congestion time을 피해 미리 적절한 곳에 data를 배치시킴으로써 얻어지는 bottleneck prevention 효과.\n\n\n=== Data Collection ===\n\n* 역할: I/O 패턴 학습/발굴에 필요한 정보 수집\n\n* 수집 정보 종류\n: 전조 기반 예측에 필요한 간접 I/O 유발자, 직접 I/O 유발자, I/O 발생 이벤트 등 I/O 예측에 도움이 될 수 있는 다양한 정보를 수집. 다음 정보들이 포함될 수 있음.\n:- indirect I/O trigger trace: {(user-id | user-type), status-type, before-status, after-status, event-time}\n:- direct I/O trigger trace: {(program-id | program-type), status-type, before-status, after-status, process-owner, event-time [, process-current-working-directory, files-opened]}\n:- I/O trace: {access-time, address-accessed, read/write-flag, io-size, file-path, process, co-accessed-addresses}\n\n\n* 수집 모듈의 위치\n: storage-server가 분리되어 있는 구조인 SAN/NAS 타입의 architecture인 경우에는 application이 구동되는 server-side에 정보 수집 모듈이 위치한다. storage-server가 결합되어 있는 converged-architecture 기반의 system인 경우에는 해당 system 내에 정보 수집 모듈이 위치한다.\n\n\n* 정보 수집 방식\n: 운영체체가 제공하는 시스템 상태 정보 제공 interface를 이용하거나, blktrace, lsof, debug file system 등 이용 (e.g., /sys/kernel/debug/ 등) 혹은 /proc file system 등을 이용 가능.\n\n== PATENT-BRIAN-2013-005 ==\n\n=== Coaccess-based I/O Prediction ===\n\n\n=== 요약 ===\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n\n=== 기술 상세 ===\n\n* Data의 coaccess 특성에 기반한 proactive data placement 기법\n: 소정의 time window 내에 같이 access되는 확률이 높은 data 그룹이 있을 때, 그 그룹 내의 data가 access되는 것을 알게 되면, 그 그룹 내의 나머지 data들도 미리 fast media에 가져다 놓음으로써 성능 향상을 얻을 수 있게 하는 방법임.\n\n\n\n\n* 시뮬레이션 기반의 성능 향상 효과 비교\n:- Fast media의 4KB random read latency는 96.47 usec (SSD 830 128GB 기준), slow media의 4KB random read latency는 1,884.03 usec (HDD 7,200rpm 2TB 기준)으로 설정 (iozone에 의한 실측값 사용)\n:- 실험 시작 전에는 4GB 크기의 target data가 slow media (HDD)에 저장되어 있음.\n:- 여기에 1,000,000 개의 4KB random read request로 구성되는 workload를 인가. (각 read request들은 서로 중복되는 data를 가리키지 않으며, 4GB 크기의 target data를 한 번씩 다 request하게 된다고 가정)\n:- Workload 패턴 학습 결과, 4GB의 전체 data 중, \'가\' type의 coaccess 특성을 가지는 10MB 크기의 data 그룹 (data set 당 2,500개의 4KB data 포함)이 200개가 있으며, \'나\' type의 coaccess 특성을 가지는 1MB 크기의 data 그룹 (data set 당 250개의 4KB data 포함)이 1,000개 있는 것을 발견한 것으로 가정.\n:- \'가\' type의 coaccess 패턴은 해당 data 그룹 내 100개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 80%인 것으로 정의.\n:- \'나\' type의 coaccess 패턴을 해당 data 그룹 내 50개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 100%인 것으로 정의.\n\n\n\n\n: 에 대해 본 기법을 적용한 경우와 그렇지 않은 경우의 성능 차이를 IOPS로 비교해보면 다음과 같다. 기법을 적용하기 전인 baseline의 경우에는 \n\n\n* coaccessness 분석에 필요한 parameter로서, \n\n\n\n\n\n\n\n: 예를 들어 설명하면 다음과 같다.\n: data 1, data 2, ..., data 10000으로 구성된 data set이 있으며, 각 data들의 크기는 4KB이다. 학습을 통해서, (1) 이 중 어느 한 data가 access되었을 때, 1분 이내에 나머지 data들도 같이 access되게 될 확률은 55%, (2) 이 중 2개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 80%, (3) 이 중 3개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 95%라는 것을 알고 있다고 가정한다.\n: fast media의 크기는 4GB 이며, 기존에 \n\n\n   이라는 것을 알고 있다면, data 1, 2, 3이 access되었을 때 data 4~128까지도 미리 proactive하게 fast tier에 배치시킴으로써, 그로 인한 성능 이득을 얻을 수 있게 된다. (!!!실제 실험 사례?!!!)\n\n\n\n\n\n\n\n\n\n== PATENT-BRIAN-2013-006 ==\n\n\n=== Data Lifetime (Valid-to-invalid) Interval 분석에 기반한 Data Placement (Tiering) ===\n\n\n* Data 별로, 혹은 LBA 별로, 혹은 Erase Block 별로? Data의 Valid to Invalid Time (Duration) 패턴을 분석할 수 있을까?\n: 이를 SSD 내에서 Low Overhead로 수행할 수 있나?\n: 혹은, 굳이 SSD 내에서 수행할 필요가 있나?\n: Host side에서 LBA 별로 분석 될 수 있다면, 이 정보를 기반으로 Tiering 시에 활용 할 수 있을 것임\n: 특정 Address에 대해서 W이 되고 나서, 다시 W이 되기까지의 시간을 측정하여 ARM 하면 될 듯. 그 두 W 간에 얼마나 많은 R들이 오는지에 대해서도 고려하여 placement 시 고려할 수 있을 것임.\n\nMeasurement Window 내에서 \'W 간의 Interval이 길다\'는 식으로 인식하는 것과, \'W의 횟수가 잦다\'고 인식하는 것 간에 어떤 차이가 있을 수 있을까?\n\n <pre>\n    01234567890123456789\na1: W-R-W-R-W-R-W-W-R--W : T20W06R04 : Short-Lifetime, Many-Reads : SLC\na2: W---W-R-W---W-W----W : T20W06R01 : Short-Lifetime, Few-Reads : SLC, DRAM?\na3: W-RRRR-W---RR-W-R-RW : T20W04R08 : Mid-Lifetime, Many-Reads : MLC\na4: W------W--R---W----W : T20W04R01 : Mid-Lifetime, Few-Reads : MLC\na5: W-R-R-R-RR-RRRR----R : T20W01R10 : Long-Lifetime, Many-Reads : MLC\na6: W------------R----R- : T20W01R02 : Long-Lifetime, Few-Reads : TLC\n</pre>\n\n=== Idle-time Prediction Based Free Space Management in SSD ===\n\n* Idle-time의 도래 시기와 지속 시간 패턴 분석에 기반한 SSD 내의 Free Space 확보 방법\n:- SSD 내에서 Low overhead (in computation, memory space)로 Idle-time 도래 시기 및 지속 시간 분석이 가능한가?\n:- 가능하다면, SSD 내에서 Garbage Collection 등으로 Free Space를 확보하는 데 있어서 어떤 장점을 가질 수 있는가?\n\n\n\n: 이를 기반으로 \n\n\n\n=== 요약 ===\n* \n\n\n=== 기술 상세 ===\n\n*\n\n== Memo for {PATENT-BRIAN-2013-004, PATENT-BRIAN-2013-005} ==\n\n\n=== Think outside the box // Questions ===\n\n* 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n\n* Tiering을 좀 더 다양한 각도에서 바라보자\n:- 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 것이 기존의 tiering 개념임.\n:- SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n:- SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n:- access pattern-aware optimal placement (APOP) - 이런 것은 어떨까?\n:- Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n\n* write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반의 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지?\n\n\n* 만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? SSD-internal memory buffer를 이용해서 SSD를 NVRAM처럼 쓰도록 하는 것이 실제로 말이 되는 이야기일까?\n\n\n\n\n\n=== 기존 기술의 한계점 (Intelligent I/O Prediction 기술의 필요성) ===\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache만으로는 성능 향상을 기대하기 어렵게 됨.\n\n <pre>\n실제 tracelog 사례를 보여주고 hot area가 변화하고 있음을 보일 것.\n</pre>\n\n* Data Placement (Tiering) 관점에서 본 EMC의 FAST, FAST-VP 기술의 한계점\n:- Coarse-grained Data Migration (768KB 까지 작게 나눌 수 있다고는 하지만, 실제로 그렇게 해서 성능이 얼마나 잘 나오는지는 확인 필요. 귀중한 자원인 DRAM을 관리를 위한 metadata로 가득 채우게 될 위험도 있음)\n:- Flexible하지 않은 Tiering 주기\n:- Busy-hour 지난 후 Idle-time에 Tiering 실시\n:- 낮은 지능의 미래 I/O 예측 능력\n:: data access count를 기반으로 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두도록 하는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n\n=== 본 발명의 차별화 포인트 ===\n\n* 본 기술의 특징 1: I/O를 triggering하는 전조 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Indicator는 Run-time에 저비용으로 감지될 수 있는 형태의 정보들로 구성\n:- 미래에 access될 data를 예측케 하는 Indicator간의 mapping은 Bayesian Inference 이용\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n=== 특허 검색/분석 결과 {storage I/O workload prediction} ===\n\n* 검색식: storage I/O workload prediction\n\n* http://portal.uspto.gov/pair/PublicPair 에서 특허 상태 확인 가능.\n\n* I/O prediction을 기반으로 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n:- 기존엔 proactive tiering이 없었나? -> IBM 특허가 나름 유사함. (US7930505, US7512766)\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n==== 1. 비교가 될만한 특허들 ====\n\n예측에 대한 언급이 되어 있는 스토리지 시스템 관련 특허들. 크게 두 가지로서, 그 중의 하나는 가상 volume에 대해 peak I/O가 발생하는 주기를 분석하여, peak I/O 발생시점이 닥치기 전에 미리 가상 volume의 data를 migration 시키는 방법에 대한 것이나, 어느 위치의 data가 언제 access될지에 대한 예측은 아님 (IBM 특허). 다른 하나는 I/O 예측을 기반으로 미래에 자주 access될만한 data를 미리 여러 노드에 분산시키는 방법에 대한 것이나, 예측 방법 자체에 대한 내용은 다루고 있지 않음 (HP 특허)\n\n\n; US7930505, US7512766 (IBM)\n\n* US7930505 (Application Number: US 12/053582), \"Controlling preemptive work balancing in data storage,\" IBM, 특허일 2011-04-19\n: storage network의 I/O 관찰을 통해 peak I/O activity의 repeating instance에 대한 signature를 VLU (virtual logical unit) 단위로 생성하고, 이에 기반하여 언제 heavy I/O가 도래할 것인지를 예측하여, heavy I/O가 오기 전에 미래 data를 faster media로 migration하도록 하는 tiering 기술에 대한 특허. (이미 I/O peak time이 온 후에 hot data를 옮기는 것은 오히려 시스템의 IO성능을 악화시킬 수도 있기 때문에 미리 migration을 하고자 하는 것임)\n: VLU 단위로, peak I/O activity가 어느 주기로 발생할지에 대한 예측임. 지능 수준과 Fine-granularity 측면에서 초보적인 수준임.\n\n* US7512766 (Application Number: US 11/213494), \"Controlling preemptive work balancing in data storage\", IBM, 특허일 2009-03-31\n: 위의 US7930505 특허와 동일 건으로 보임\n\n\n; US6912635 (HP)\n\n* US6912635 (Application Number: US 10/140968), \"Distributing workload evenly across storage media in a storage array\", HP, 특허일 2005-06-28\n: 미래에 자주 access될만한 data를 여러 arrayed storage device에 분산시키겠다는 기술. 그러나 I/O에 대한 예측 방법은 다루어지고 있지 않음.\n\n==== 2. 검색은 되었으나 관련은 없는 특허들 ====\n\n무언가를 예측한다는 측면, Storage System에 대한 기술이라는 측면에서는 관련이 있으나, 미래에 Access될 Data를 예측한다는 측면에서 제안 기술과 관련 없는 특허들\n\n\n; US7640231 (IBM)\n* US7640231 (Application Number: 11/280012), \"Approach based on self-evolving models for performance guarantees in a shared storage system\", IBM, 특허일 2009-12-29\n: network 기반 storage system에서, average latency 혹은 I/O request rate 등에 대한 주기적인 performance sampling을 기반으로, 주어진 objective function의 값을 최대화할 수 있도록 system resource를 제어할 수 있게 하는 기술.\n\n\n; US7774491, US7519725 (IBM)\n\n* US7774491 (Application Number: 12/359377), \"Utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2010-08-10\n: 복수의 클라이언트들을 대상으로 통계적인 성능 QoS를 보장하기 위한 informed throttling 방법\n: A system for utilizing informed throttling to guarantee quality of service to a plurality of clients\n\n* US7519725 (Application Number: US 10/444712), \"System and method for utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2009-04-14\n: resolve the problem of providing statistical performance guarantees for applications generating streams of read/write accesses (I/Os) on a shared, potentially distributed storage system of finite resources, by initiating throttling whenever an I/O stream is receiving insufficient resources\n\n\n; US7827283 (IBM)\n* US7827283 (Application Number: 10/367444), \"System for managing and controlling storage access requirements\", IBM, 2010-11-02\n: SAN 등의 환경에서, storage bandwidth를 관리하고 reserve하는 기술\n: A Resource Management and Reservation System (RMRS) for managing and reserving storage bandwidth, is a platform independent middleware layer that provides an interface to applications, their database management systems, or some other higher level data management systems like ADRS which does data management on behalf of the applications\n\n\n; US8433554, US8140319 (IBM)\n\n* US8433554, \"Predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2013-04-30\n: An approach for predicting performance and capacity of an information technology (IT) system before the IT system is built is described, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n* US8140319, \"Method and system for predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2012-03-20\n: A method and system for predicting performance and capacity of an information technology (IT) system before the IT system is built, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n\n\n; US8255915 (HP)\n* US8255915, \"Workload management for computer system with container hierarchy and workload-group policies\" HP, 특허일 2012-08-28\n\n\n; US7050956 (HP)\n* US7050956 (Application Number: US 09/843930), \"Method and apparatus for morphological modeling of complex systems to predict performance\", HP, 특허일 2006-05-23\n: I/O 성능 예측을 위한 complex storage system에서의 data flow 모델링 방법\n\n\n; US6718434 (HP)\n* US6718434 (Application Number: US 09/871043), \"Method and apparatus for assigning raid levels\", HP, 특허일 2004-04-06\n: Data를 disk array에 load하기 전에, workload 명세 및 device 명세에 기반하여, 적합한 RAID level을 판단하여 disk array에 할당 (RAID build)하는 방법\n\n\n; US8112586 (EMC)\n* US8112586 (Application Number: US 12/190071), \"Predicting and optimizing I/O performance characteristics in a multi-level caching system\", EMC, 특허일 2012-02-07\n: multi-level caching 시스템에서, cost대비 performance를 최적화할 수 있도록, workload에 따른 resource allocation design에 대한 recommendation 기술 (multi-level cache들 간의 workload propagation을 예측하는 것이 핵심)\n\n\n; US6629266 (IBM)\n* US6629266 (Application Number: US 09/442001), \"Method and system for transparent symptom-based selective software rejuvenation\", IBM, 특허일 2003-09-30\n: software가 언제 동작이 멈추는지를 예측함으로써, software의 dependability를 향상할 수 있게 하는 방법\n\n\n; US5088058 (Unisys)\n* US5088058 (Application Number: 07/237120), \"Apparatus and method for evaluating and predicting computer I/O performance using I/O workload snapshots for model input\", Unisys Corporation, 특허일 1992-02-11\n: I/O delay 등에 대한 확률적 모형을 기반으로 disk 시스템의 성능을 검증하고 예측하는 시뮬레이션 기반의 방법\n: A method of evaluating and predicting the performance of an I/O disk system comprised of one or more disk controllers and cooperating disk units using a simulation model containing a stact constructed such that delays are formulated as sets of stochastic processes\n\n=== Comparison of I/O Prediction Approaches ===\n\n미래에 access될 data의 위치와 시기를 예측하기 위한 방법으로 event-based I/O prediction, coaccess-based I/O prediction, periodicity-based I/O prediction 방식이 제안되며, 각각의 방식은 예측에 필요한 연산량 및 input data의 richness, 그리고 workload 변화에 대한 적응성 측면에서 장단점을 가지고 있음. 따라서, 상황에 따라서 적절한 방식을 이용하면 됨.\n\n{| style=\"border-collapse: collapse; border: 1px solid #000\" border=1\n| style=\"width: 25%\" | 비교 항목\n| style=\"width: 25%\" | periodicity-based I/O prediction\n| style=\"width: 25%\" | coaccess-based I/O prediction\n| style=\"width: 25%\" | event-based I/O prediction \n|-\n| context richness required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| computation overhead required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| adaptability to workload changes (lower is better)\n| high\n| high\n| high\n|-\n|}\n\n\n* 전조 기반 예측 vs. 주기성 기반 예측\n{| border=1\n| 비교 항목\n| 전조 기반 예측\n| 주기성 기반 예측\n|-\n| 기술 요약\n| 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식\n| address 별 access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식\n|-\n| 분석 오버헤드\n| High (학습/마이닝에 드는 비용임)\n| Low (주기성 파악 비용임)\n|-\n| Run-time 오버헤드\n| Mid (실시간 전조 모니터링 및 매칭 비용)\n| Low (주기성 계산)\n|-\n| Workload 변화 적응성\n| High\n| High (Workload 변경 사실 알림 hint 필요)\n|-\n| ...\n| ...\n| ...\n|-\n| Incrementality of Mining/Learning\n| ...\n| ...\n|}\n\n* 전조 기반 예측은, 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식임.  주기성 기반 예측은, data access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식임.\n\n\n=== Intelligent Data Placement ===\n\n* Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case\n: 여기에 HML이 사용될 수 있는 것이라면, 어떤 방식으로 어떤 이유로 그런 것일까?\n\n\n* I/O 예측 방식으로서 다음 방식이 제안되며, 각 방식에 따라서 prediction model, 수집되어야 하는 context data, I/O 패턴 DB에 저장되는 정보의 종류 및 형태가 달라질 수 있음. 예측에 이용할 수 있는 context 정보의 richness에 따라 적절한 방식을 사용할 수 있다. (서로 다른 발명이므로, 별도의 특허로 낸다)\n:- event-based I/O prediction\n:- coaccess-based I/O prediction\n:: 여기서, periodicity-based I/O prediction은 기존 특허에서 언급되었으므로, 본 발명에서는 크게 언급하지 않기로 한다.\n\n\n* I/O prediction 결과에 따라 적절한 위치에 data를 배치하게 되는데, 여기서 적절한 위치라 함은 SSD와 같은 fast tier가 될 수도 있겠으나, sequential read/write 같은 경우 striping된 HDD array가 될 수도 있고, 혹은 SAS HDD가 될 수도 있겠음. 또한 SSD 중에서도 SLC, MLC, TLC 로 구분되는 fine-grained fast tier로 구분될 수도 있음\n\n\n* 미래에 오게 될 I/O의 type도 알 수 있을까? 즉, {random/sequential}x{read/write} type 중 어느 것에 해당되는지를 안다면, 그리고 I/O의 intensiveness도 예상할 수 있다면 그에 맞춰서 placement 시 적절한 tier 혹은 적절한 node로 찾아가게 할 수 있을 것임.\n\n\n* context 정보는, 수집되는 위치에 따라, server-side에서 수집되는 context data와 storage-side에서 수집되는 context data로 분류될 수 있음. Event context data는 server-side에서 수집 가능하며, coaccess context data 및 periodicity context data는 storage-side에서 수집될 수 있음.\n\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n: 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n\n=== Design/Implementation Issues ===\n\n* workload type/class tagging 필요\n\n* cache와 tiering을 같이 쓸 때, DRAM cache가 아니라 Flash Cache의 경우 쓸데없이 모든 data가 Flash Cache에 다 들어가게 두기 보다는, tiering에서 cover하지 못하는 것들 위주로 caching하도록 하면 Flash Cache의 공간 낭비를 줄임으로써, 유효 cache 공간을 늘일 수 있게 되고, 그만큼의 성능 향상으로 연결될 수 있음.\n\n* DRAM 기반의 write cache를 적극 이용하여 마치 log-structured file system에서 제공하는 I/O merging and sequentialization 효과를 얻을 수 있음.\n\n* Tiering되는 data는 caching되는 data보다 상대적으로 coarse-grained하게 grouping하여 다룸으로써, 한꺼번에 많은 data를 move할 수 있도록 하고, meta data 관리에 필요한 overhead를 줄일 수 있게 한다. 당장 access되지 않더라도 일정 시간 내에 같이 access될 수 있는 data들은 함께 faster tier로 옮겨진다. 이때 필요한 정보가 coaccess 정보임.\n\n* Cache - Tiering 기법의 연동 방법: 예측이 실패했거나, target tier media의 가용 공간이 부족하여 tiering에서 놓치는 missing data가 있을 수 있음. 이러한 missing data들이 temporal locality가 있는 경우에는 당장은 faster tier에서 serve하지 못하더라도 cache에서 serve될 수 있는데, temporal locality가 적은 경우에는 cache만으로 cover되지 않는다. \n\n* Management table size optimization issue - 만약 모든 LBA 별로 event-based I/O prediction을 해야 한다면 현실적으로 사용 가능한 아키텍쳐가 아닐 수 있음. 만약 spatial locality가 있어서 LBA 1,000개 단위로 묶어서 대응할 수 있다면, 최소 1/1000 의 management table size 감소 효과를 얻을 수 있음. pattern mining 혹은 machine learning 실행 시에는 record 수가 감소함으로써 얻는 이득은 1,000배 이상이 될 수 있음. 관건은 얼마나 LBA 묶음을 크게 만들어서 예측에 사용할 수 있는지임.\n\n\n=== I/O Insight ===\n\n* ML 기반의 I/O Insight 발굴 (Data Placement를 위한 Macroscopic Guideline)\n: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:-> 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n\n* I/O Insight의 요건:\n:# indication, as simple as possible\n:# indication, as specific as possible\n:# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n:# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n\n* I/O Insight의 형태\n: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n=== X-Y Modeling ===\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n=== 예측 실패 대비 (별도 특허화 가능) ===\n\n* I/O 예측이 실패할 수 있으며, 이로 인한 성능 penalty를 최소화하기 위한 방법 필요.\n:- fast tier에 여유공간이 있다면 1타겟과 2타겟을 모두 올려놓아도 되지 않을까? 1타겟과 2타겟을 placement함으로써 예측 정확도를 95%까지 올릴 수 있다면, 꽤 쓸만할 수 있겠음. 그리고 1타겟과 2타겟의 모든 데이터를 다 fast tier에 올려두지 말고 일부만 fast tier에 올려두었다가, 둘 중에 하나의 타겟이 hit되었을 때, hit된 타겟의 나머지 data를 재빨리 fast tier에 올리도록 하는 incremental 방법을 사용할 수 있겠음.\n:- 그러나 이것 또한 예측 실패에 대한 완벽한 prevention이 될 수는 없다. 따라서 예측 실패가 발생하더라도 재빨리 그 성능 penalty를 mitigation할 수 있는 방법이 필요. \n:- cache를 이용하여 mitigation할 수 있을까?\n\n=== Data Read/Write 관련 Issues ===\n\n* read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n\n* 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n\n* write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간에, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면?\n\n* 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있음.\n\n* 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간이 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 scatter했던 I/O들을 나중에 다시 읽어들이려 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 원하는 고성능을 낼 수 있게 된다.\n\n* replica-based dynamic clustering\n: 위와 같은 write-and-then-read-again 상황에서, 해당 data를 저장했었던 HDD의 I/O busy/idle 여부에 상관 없이, 최고의 효과를 낼 수 있게 하기 위해, replica management를 적극적으로 활용하는 것은 어떨까? HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까?\n\n* data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 논의가 아님. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 논의임.\n\n* 그러나, 조심해야 할 것은 위의 논의는 상당히 case-dependency를 상당히 많이 탈 수도 있다는 것임. 책상 위의 아이디어로 끝날 수도 있기 때문에 조심스럽게 접근하는 것이 필요함.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-008 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n\n\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n----\n=== (SmartSSD API) SSD-internal I/O pattern logging interface and mechanism ===\n\n\n==== Questions ====\n\n* SmartSSD에서 제공할 수 있는 logging 서비스로서 어떤 것들이 있을 수 있나?\n:- 어느? 정도의 free space를 필요로하는 critical (heavy) I/O가 어떤? 주기로 도래할 지 알 수 있을까?\n:- I/O prediction에 도움될 수 있는 정보를 SmartSSD가 기록했다가 필요한 때 (안전한 방법으로?) 줄 수 있을까?\n:- I/O prediction까지는 아니더라도 해당 SSD의 I/O wellness(어떻게 정의?)를 측정했다가 bottleneck을 회피하는 데 유용한 정보를 제공할 수 있을까?\n:- 어떤 SSD가 현재 주어지고 있는 I/O workload에 대해 list performance spec을 만족하는데 어려움을 겪고 있다는 것을 알려줄 수 있는 정보가 어떤 것이 있을까? (SSD 내 write buffer의 fullness 혹은 I/O queue의 유동성? amount of free space in over-provisioning area?) (-> I/O wellness 라는 metric을 이것과 연관시켜 정의할 수도 있으며, 결국 SLA 혹은 Performance QoS와 연관시킬 수 있음)\n:- 지금까지의 S.M.A.R.T.와 비교하였을 때, 어떤 차별점이 있는가? 기존의 S.M.A.R.T.가 제공하지 못하던 타입의 정보를 제공하고, 이로 인해 기존에는 불가능했던 새로운 알고리즘 구현 혹은 새로운 서비스 제공이 가능해짐을 보이면 좋겠음.\n\n\n\n:- workload의 history?\n:- 각 erase block이 erase되어야만 했던 주기?\n:- free space의 추이 (얼마나 많이 남아 돌던지)?\n:- I/O의 heaviness 패턴?\n:- SSD내 write buffer (혹은 write cache)의 utilization정보? (즉, write buffer가 넘칠 정도가 되어서 I/O wait을 해야만 했던 상황이 얼마나 자주 발생했는지, write buffer의 가득찬 정도를 백분율로 표현한다고 했을 때, 평균/표준편차 등으로 대표할 수 있는 normal distribution을 따르는지?\n:- 전체 sequential read/write \n\n\n==== 배경 ====\n\n* 각 SSD 모델별로, 또 동일한 모델의 SSD라 하더라도 어떤 workload에 노출되어왔는지에 따라, 현재 낼 수 있는 I/O performance가 동일하지 않을 수 있다 (!check! 관련 실험 결과 - SSD 830을 이용하여 같은 시간 동안 하나의 host에서 생성되는 workload들을 나누어 받았다 하더라도 내부적인 free block의 갯수 및 random write에 의한 block 내 파편화등의 상태가 상이하여 성능에 차이가 나는 것을 보여줄 것. uFLIP 혹은 filebench 활용 가능). 한 예로, SSD가 어느 호스트의 어느 file system에 매핑되었는지에 따라 그 SSD가 받는 workload의 특성이 다를 수 있다. file system mount point가 달라지거나, 기존과 다른 새로운 응용이 설치/서비스되는 상황이 되면 \n\n\n----\n=== I/O Prediction-based Proactive Data Placement ===\n\n* 미래에 access될 data를 예측하여 적소에 미리 가져다 놓는 기술로서, (1) I/O prediction을 위한 정보 수집 모듈, (2) 주어진 정보들에 기반하여 다가올 I/O를 예측하는 모듈, (3) data placement를 수행하는 모듈로 구성됨.\n\n\n* 본 기술의 특징 1: I/O를 triggering하는 indicator (전조) 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Run-time에 저비용으로 감지될 수 있는 Indicator 정보와, 이에 의해 access될 data 간의 mapping은 Bayesian Inference 이용\n\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n\n----\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n----\n\n=== Tiering Method Avoiding I/O Bottleneck ===\n\n* I/O Prediction에 의해 예측된 미래의 Hot Data를 Proactive하게 Placement한다고 할 때, 예상되는 access pattern으로 인해 발생 가능한 I/O Bottleneck을 최소화할 수 있도록 배치하는 방법\n\n* 예를 들어 미래의 hot data block 1 (200MB), hot data block 2 (100MB), hot data block 3 (400MB)이 1분 후에 access 될 것으로 예측되었다고 가정. data block 1에 대해서는 4KB 단위로 random read가 dominant한 access pattern이 예상되고, data block 2에 대해서는 1024KB 단위의 sequential read가 dominant한 access pattern이 예상되고, data block 3에 대해서는 4KB 단위의 random read와 512KB 단위의 sequential write이 각각 80:20의 비율로 mix된 형태의 access pattern이 예상된다고 했을 때, 각 data block을 어디에 어떤 형태로 placement시킬 것인가?\n\n: machine learning 혹은 pattern mining에 기반한 base workload pattern data를 기반으로 incremental하게 update될 수 있는 data access의 spatial locality와 periodicity\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n\n* 기존 기술과의 대비 (naive I/O prediction vs. full-fledged I/O prediction)\n: 기존 기술은 누적 hit count 정보에 의지하는 naive I/O prediction으로 볼 수 있음. 이러한 방식은 workload의 변화를 제대로 반영하지 못하기 때문에, static한 workload에 대해서는 잘 동작하지만, moving target처럼 dynamic한 workload에 대해서는 오히려 과거의 누적 hit count metric이 현재의 workload 패턴을 왜곡하는 문제점을 가지고 있음. (temporal locality와 spatial locality가 깨지는 순간이 여러번 존재할 수록 기존의 naive I/O prediction 방식으로 성능 향상은 어려워짐)\n\n\n\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(65,'== 20130530_134618 ==\n\n=== Frequent patterns/items mining examples with R::arules ===\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n* References\n:# [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))]\n:# [http://www.analysis-of-patterns.net/files/bgoethals.pdf Frequent Pattern Mining Slides - Universiteit Antwerpen]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning - Wikipedia]\n:# [http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf Frequent Pattern Mining in Web Log Data]\n:# [http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010 Temporal Pattern Mining Tutorial KDD 2010]\n:# [http://cran.r-project.org/web/packages/arules/index.html arules: Mining Association Rules and Frequent Itemsets]\n:# [http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf Introduction to arules - A computational environment for mining association rules and frequent item sets -- http://cran.r-project.org/]\n:# [http://lyle.smu.edu/IDA/arules/ Intelligent Data Analysis Lab @ SMU]\n:# [http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))]\n:# [http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/ Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning]\n:# [http://en.wikipedia.org/wiki/Sequence_mining Sequence mining]\n:# [http://en.wikipedia.org/wiki/GSP_Algorithm GSP Algorithm]\n:# [http://en.wikipedia.org/wiki/Apriori_algorithm Apriori algorithm]\n:# [http://en.wikipedia.org/wiki/Market_basket_analysis Market basket analysis]\n:# [http://en.wikipedia.org/wiki/Time_series Time series]\n\n\n----','utf-8'),(66,'== ## bNote-2013-03-29 ==\n\n=== DailyTask ===\n\n* IOWA\n\n\n=== MBO 2013 (목표 최종 확정) ===\n\n* 정명준 MBO\n <pre>\n\n30%, ~10/31\n- I/O Workload Analysis 기술 연구\n  : Dominant I/O Pattern Mining 및 Machine Learning 기반의\n    I/O 패턴 모델링 및 예측\n  : I/O Pattern 분석/예측 모델 수립\n  : I/O Pattern Mining/Learning 엔진 구현 (Python, R, Shell-script)\n\n30%, ~10/31\n- Data Placement 기술 연구\n  : Workload Analysis 결과로 얻어진 I/O Insight/Prediction을\n    활용하여 Data를 적소에 미리 배치\n  : Linux Kernel Module 형태로 Tiering 기술 형태로 구현\n  : Proactive Data Placement를 통해 분산 스토리지의 I/O 성능\n    80% 이상 개선 검증 (시뮬레이션, 혹은 Real 시스템 기반)\n  \n20%, ~10/31\n- A급 특허 3건 작성 및 심의 통과\n\n20%, ~10/31\n- 논문 1편 (To be accepted)\n\n</pre>\n\n\n* 과제 MBO (이전문님)\n <pre>\n* 분산 플랫폼 관련 특허 15편 이상 특허심의 통과 (전략출원 2편 이상 심의 통과) (30%)\n* 분산 플랫폼 관련 논문 2편 이상 accept (20%)\n* I/O coordination과 Proactive Placement를 통해 분산 스토리지의 I/O 성능 80% 이상 개선 검증\n  (HW RAID 대비) (15%)\n* 분산 Deduplication 기술을 통해 분산 스토리지에서 데이터 제거효율 3배, Coverage 4 node 달성 (3x@4node) (15%)\n* 분산 I/O Coordination 관련 기술이전 1건 (20%)\n</pre>\n\n=== 과제 변경 ===\n\n <pre>\n\n과제명: Intelligent Large-scale Data Management\n (구과제명) Real-Time Big Data Platform\n\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n\n</pre>\n\n== ## bNote-2013-03-28 ==\n\n=== SW설계기술리더양성 교육 지원 ===\n* SW Architect 사전 교육\n\n==== 현업 프로젝트 기획서 ====\n\n* 지원과정\n: SW 설계 리더 양성과정\n\n* 과제명\n: Data-intensive Storage\n\n* 프로젝트 참여자\n <pre>\n이주평	전문 연구원	Project Leader\n정명준	전문 연구원	시스템 설계, 요소기술 연구, 기능모듈 구현\n유개원	전문 연구원	요소기술 연구, 기능모듈 구현\n이형주	SDS 차장	기능모듈 구현, 기능/성능 검증\n</pre>\n\n* 과제 담당 임원\n: 심은수 상무\n\n* 과제 개요\n <pre>\n[배경 및 현안]\n□ 데이터 폭증으로 데이터센터/기업의 클라우드 스토리지 니즈 증대\n□ 클라우드 스토리지의 핵심 경쟁력은 성능 및 용량 향상 기술에 있음\n□ H/W 수준을 높이거나 S/W 최적화 기반으로 시스템의 성능을 개선\n   하는 기존 접근 방식으로는 H/W 한계를 넘어서는 성능 향상은 어려움\n□ 데이터 I/O 속도와 데이터 저장 효율을 획기적으로 개선할 수 있게 하는\n   지능적 Data Management 기술은 클라우드 스토리지 시스템의 경쟁력을\n   혁신하는 핵심 S/W 기술임\n\n[목적]\n□ 본 Sub Task에서는 지능적 Data Management 기술 중,\n   데이터 I/O 속도 향상 기술을 연구/개발한다\n   * I/O Workload Analysis에 기반한 Proactive Data Placement 기술 확보\n     - Real trace data에 대한 I/O Workload Analysis를 통해\n       dominant workload 패턴 발굴 및 I/O 예측 모델 학습\n     - I/O 예측 모델에 기반한 multi-tier (horizontal - vertical) 간\n       proactive data 배치 수행\n</pre>\n\n* 목표\n <pre>\n[기능/성능/품질]\n□ I/O Workload Analysis에 기반한 Proactive Data Placement\n  - I/O Workload Analysis 모듈\n    - Real trace data 수집 기능\n    - Trace data parsing 및 transform 기능 (analysis를 위한 전처리)\n    - Dominant workload pattern 추출 및 I/O model 학습\n  - Proactive Data Placement 모듈\n    - Tier management 및 data move 기능\n	- I/O monitoring 및 hot/cold 판단 기능\n\n[중간 산출물]\n□ Hot/Cold Data Placement 모듈\n  - 핵심적인 automated tiering 기능 구현\n    : Data access 패턴 관찰을 통해, hot data는 고속의 storage tier에,\n      cold data는 상대적으로 느린 속도의 storage tier에, 주기적 배치\n  - 동일 수준의 시스템에서, 동일 workload에 대해,\n    경쟁사 (E사) 수준의 성능 달성 (metric: average IOPS)\n\n[최종 결과물]\n□ Proactive Data placement 모듈\n  - I/O 예측 모델에 기반한 proactive data placement\n    : Dominant workload 패턴 분석 및 I/O 예측 모델에 기반한\n	  multi-tier 간 선제적 data 배치를 통해 I/O 성능 향상\n  - 동일 수준의 시스템에서, 동일 workload에 대해,\n    경쟁사 (E사) 대비 100% 이상의 성능 향상 달성 (metric: average IOPS)\n</pre>\n\n* 기대 효과\n <pre>\n□ 지능적 Data Management 기술은 Big Data를 다루는\n   클라우드 스토리지 서비스의 핵심 기술로 활용 가능\n   - Big Data 시장에서는 특히 스토리지 분야가 연간 61.4%의 성장율로\n     전체 시장 성장을 주도\n</pre>\n\n* 과제 구성\n <pre>\n[전체 Architecture]\n□ Proactive Data Placement 시스템은\n   Workload Analysis 모듈과 Data Placement 모듈로 구성됨\n  * Workload Analysis 모듈은 Trace Log 데이터에 대한\n    Off-line I/O Analysis를 수행하여 I/O 패턴에 대한 Insight을 확보함\n    (e.g., 어느 위치의 Data가 언제쯤 Access될 것인지를 예측)\n  * Data Placement 모듈은 I/O 패턴에 대한 Insight 정보와, 실시간으로\n    모니터링되는 시스템 상태 정보를 이용하여, Data를 미리 적소에 배치함\n\n[과제 적용부 기술항목]\n□ Workload Analysis 모듈: I/O Prediction Model Optimization 이슈\n  - 응용 및 시스템 특성에 따라 Workload 특성이 다를 수 있음\n    Workload 별로 Prediction Model을 구성하는 주요 X\'s 의 최적화 필요\n□ Data Placement 모듈: Overhead 최소화 및 Tiering 구현 최적화 이슈 \n  - Real-time Monitoring으로 인해 시스템에 가해지는 Overhead 최소화 필요\n  - Tiering 기능 구현 시 I/O 특성 및 시스템 구조를 반영한 최적화 필요\n</pre>\n\n\n==== 입과 추천서 ====\n\n[본인 업무 이력]\n\n* 2004.08 ~ 2007.09 : Security & Trusted Computing 기술 연구/개발\n:- 휴대폰 Content/Right Protection 기술인 OMA DRM S/W 개발, 무선사에 기술 이전\n:- Secure MMC를 위한 Crypto Engine 개발 참여 및 MMC IOP T/F 활동, 메모리사에 기여\n:- System의 무결성 보장 기술인 Trusted Computing 기술 연구 주도, Mandatory Access Control 기술을 무선사에 이전, LiMo (Linux Mobile) Security 표준에 반영 (SubPL)\n:- A급 특허 6건 출원, 논문 2건 (ACM SACMAT \'08 등)\n\n* 2007.10 ~ 2008.05 : 전사 6시그마 MBB (Master Black Belt) 양성 과정\n:- 제 16기 6시그마 MBB 과정에 입과하여 6시그마 이론 연구 및 실습 과제를 진행하고 BB 교육 과정 강의를 진행하였음. MBB 인증 시험 통과\n\n* 2008.06 ~ 2010.10 : Virtualization 및 Operating System 기술 연구/개발\n:- H/W가상화 기술인 Xen Hypervisor의 Security 연구 참여\n:- OS가상화 기술 기반의 State Migration S/W 개발, 스토리지사업부로 기술이전(SubPL)\n:- Russia연구소와 협력, Android 부팅속도를 향상시키는 FastBoot 기술 연구 (SubPL)\n:- 본사 사업지원팀 Vision 2020 T/F에 핵심 멤버로 참여, 15개 미래 기술 테마 발굴\n:- A급 특허 6건 출원 (전략 출원 2건), 논문 1건 (MobiCom \'09)\n\n* 2010.11 ~ 2013.현재 : Data-intensive Storage 기술 연구/개발\n:- I/O Workload Analysis에 기반한 Proactive Data Placement 기술 연구 주도\n::- Workload Analysis에서 획득한 I/O에 대한 근본적인 이해를 바탕으로 Data Management 알고리즘을 혁신, 스토리지 시스템 성능을 향상시키는 기술임\n:- 본 과제는 메모리사의 사업영역 확장 및 \'클라우드 스토리지 서비스\'를 위한 스토리지 시스템 기술 확보에 기여하고 있음\n:- A급 특허 6건 출원, 논문 3건 (ICCE 등)\n\n[소속부서장 추천 사유]\n\n* (양성 후 활용계획)\n:- 스토리지 시스템 설계/구현 시 S/W Architect로 활용\n* (인물평 및 추천사유)\n:- 정명준 전문은 시스템 분야에 대한 깊은 기술적 이해와 원만한 커뮤니케이션 능력을 바탕으로한 성공적인 프로젝트 발굴/주도 경험을 가지고 있습니다.\n:- 향후 Architect로서, 해당 과제의 S/W 설계 리딩을 통해 스토리지 시스템의 차별화된 기술 경쟁력을 만들어 내는 데에 기여할 수 있을 것으로 판단되어, 금번 S/W 설계 리더 과정에 추천합니다.\n\n== ## bNote-2013-03-26 ==\n\n=== DailyTask ===\n\n* IOWA Proactive Data Placement Formulation\n* Data Representation (as a pre-processing for association rules mining)\n\n* Patentization\n:- Distributed Multi-level Caching\n:- IO Pattern-optimal Data Placement for Tiering\n:- Virtualization-aware Caching/Tiering/Placement\n\n* Study\n:- Btier\n:- Bcache\n:- Fusion IO Caching Technology (directCache, ioTurbine)\n:- EMC FAST (Fully Automated Storage Tiering)\n:- OpenStack\n:- Xen\n:- VASA, VAAI (VMware의 storage virtualization 기술들)\n:- PCIe fabric switching\n:- Software Defined Storage\n:- Virstore? (VMware가 인수?)\n\n* 심상무님께 주간보고 내용\n:- Tiering Test SW Platform 구축 건 (Open source 활용, SDS 이형주 차장님과 함께)\n:- Real Trace Log Data 확보 진행 건 (수퍼컴센터의 Analytics Workload Trace, VDI Trace)\n\n=== Patentization ===\n\n* Access Pattern Aware Tiering\n\n\n=== Memo ===\n\n* Turbine: <기계> 높은 압력의 유체를 날개바퀴의 날개에 부딪치게 함으로써 회전하는 힘을 얻는 원동기. 사용하는 유체의 종류에 따라 수력 터빈, 증기 터빈, 가스 터빈 따위가 있다.\n\n== ## bNote-2013-03-25 ==\n\n=== DailyTask ===\n\n* 업무 File 정리\n* IOWA Proactive Data Placement Formulation\n\n=== Patidea ===\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n:- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [http://www-03.ibm.com/systems/software/gpfs/][http://public.dhe.ibm.com/common/ssi/ecm/en/pos03096usen/POS03096USEN.PDF]\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n* advanced tiering: access pattern-aware optimal placement (APOP)\n:- Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n:: 예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n:- 이에 필요한 data access pattern 모니터링/분석 방법\n:: 데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\n::: NIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n:- 이를 위해 필요한 system architecture 구조\n:: 기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n=== Formulation: IOWA based Proactive Data Placement ===\n\n* Formulating [[IOWA]] or [[I/O Workload Analysis]]\n:- to clarify the total amount of work\n:- to clarify the sub-tasks (can be modularized)\n:- to clarify the area to get focused\n\n\n=== Information: S사, K사 ===\n\n* Kaminario, Solidfire\n: From 서정민 전문\n: SSIC 미팅노트로부터 FACT를 각색한 정보를 공유합니다.  제 의견은 반영하지 않았습니다.\n\n* SolidFire (SSIC 초기미팅 결과)\n:* 1. Company Overview\n::- 3 years, 82 people\n::- $37M in funding (현재 Series B 단계로, 2013년 Series C 가능성 있음)\n::- 12 customers, 4 announced: 2 private cloud enterprise customers. \n::- Multi-tenancy가 기반인 Cloud 시장을 타겟으로 제품 제작 (OpenStack, CloudStack 연동)\n:* 2. Technology: QoS, Scalability, Inline deduplication/Compression\n:: (a) QoS\n::: OS 내에서 QoS를 Volume 단위로 관리 \n::: IOPS/latency QoS support (No R/W separate QoS) \n:: (b) Scalability\n::: Full data distribution across all the nodes \n::: All the nodes contributes to rebuilds\n:* 3. Current Arch./Tech. (GA)\n::- System configuration: 5~100 nodes (they have 40 nodes in test)\n::- H/W Configuration\n:: (a) CPU: Dual 2.5GHz Sandy Bridge with 6 cores each. \n::: 10 Cores는 mostly compute intensive work including dedup and compression. \n::: 2 Core는 handles IO to SSDs\n:: (b) SSD: Viking for boot/metadata, Intel SATA SSDs (relies on supercap in SSD)\n:: (c) Network: iSCSI (FC/NFS in the future, NFS just for small filer)\n::- Performance\n::: Latency Avg is .5ms to 2ms. Worst is 20 to 30 ms. \n:* 4. 금년도 추가 개발계획 (일부)\n::- Remote replication, sync and async, coming in Q3\n::- Encryption is also on the roadmap. \n\n* Kaminario (SSIC 초기 미팅 결과)\n:* 1. Company Overview\n::- Found in 2008.3, Sequoia(VC) funded\n::- 30 patents (the engineers have 76 from the previous jobs)\n::- Target: general-purpose storage system (OLTP, OLAP, VDI)\n::: focusing Latency, Throughput, IOPS all\n::- Shipping scale-out systems for the last 2 and 1/2 years\n::- Competitors: XtremIO, SolidFire\n::- 엔터프라이즈 기능 포커스: resiliency, self-healing, automation 중심\n:* 2. Technology\n::- Core 기술에 대한 파악 결과 없음\n:* 3. Previous Arch.\n::- Dell Blade 서버 방식으로 Fusion-IO 탑재\n:* 4. Current Arch. (개발 중)\n::- 1U rack server 기반 SMART or STEC SAS SSDs 사용\n::: low cost SSDs, low end Xeon, 32GB memory 등 Cost를 줄이는 방식 채용\n::: \"They use LSI SAS controller but don’t use dual port functionality.\"\n::: No SATA SSD (SATA SSD는 신뢰성 문제 야기하는 것으로 판단)\n::: -> SSIC 전문가는 SAS Dual port 기술 개발을 실패하지 않았는가 하는 의문 제기\n::- Performance is about 100,000 IOPS/node.\n::- No Dedup/compression \n::- \"Their SPC-1 result has 20x better price performance than previous SPC results. \"\n::- Currently focus on reducing long tail numbers.  (already has good IOPS)\n::: Performance degradation: < 25% at loss of data node\n\n=== References ===\n\n* [http://www.kaseya.com/download/en-us/white_papers/KaseyaBuyersGuidePaper.pdf IT Systems Management Buyers’ Guide // Kaseya]\n* [http://www.sata-io.org/technology/6Gbdetails.asp SATA-IO Revision 3.1 Specification // Queued Trim Command]\n\n----\n\n== ## bNote-2013-03-22 ==\n\n <pre>\n(EMC (Forum OR World) VNX) ((performance OR \"iops\") AND (\"per dollar\" OR \"dollar per\" OR \"per $\" OR \"/$\" OR \"$/\")) \"vs\" (filetype:pdf OR filetype:ppt OR filetype:pptx)\n</pre>\n\n=== DailyTask ===\n\n----\n==== Books of Machine Learning / Data Mining ====\n* \"Machine Learning\" // Tom Mitchell, McGraw Hill, 1997 ((B.GOOD))\n:- [http://www.cs.cmu.edu/~tom/mlbook.html Book]\n:- [http://www.cs.cmu.edu/~tom/mlbook-chapter-slides.html Slides]\n\n* \"Mining of Massive Datasets\" // Anand Rajaraman, Jeffrey David Ullman ((B.GOOD))\n:- [http://i.stanford.edu/~ullman/mmds.html Book - Online Version]\n:- [http://i.stanford.edu/~ullman/mmds/book.pdf Download the latest book (PDF, 415 pages, approximately 2.5MB)]\n\n----\n\n==== Machine Learning / Data Mining ====\n\n* [http://en.wikipedia.org/wiki/Gradient_descent Gradient Descent]\n* [http://ko.wikipedia.org/wiki/%EC%9D%8C%ED%95%A8%EC%88%98]\n* [http://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95%EC%82%AC%EC%83%81]\n* [http://ko.wikipedia.org/wiki/%EB%AF%B8%EC%A0%81%EB%B6%84%ED%95%99]\n* [http://ko.wikipedia.org/wiki/%ED%8E%B8%EB%AF%B8%EB%B6%84]\n* [http://ko.wikipedia.org/wiki/%ED%8F%89%EA%B7%A0%EA%B0%92_%EC%A0%95%EB%A6%AC]\n* [http://www.iiswc.org/iiswc2008/Papers/012.pdf] Characterization of Storage Workload Traces from Production Windows Servers // Swaroop Kavalanekar, Bruce Worthington, Qi Zhang, Vishal Sharda\n\n----\n\n== ## bNote-2013-03-21 ==\n\n=== Official Death of ... ===\n* What to do? why?\n\n=== DailyTask ===\n\n* list of candidate tasks\n\n:- (~) IOWA DM/ML Planning\n::- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n::- Hadoop case 고려 (in-local-node vertical placement between DRAM-SSD-HDD 뿐만 아니라 horizontal placement (among distributed nodes in Hadoop cluster)도 고려)\n\n:- IOWA ML Study\n::- IOWA:: Bayesian Network study\n::- IOWA:: Neural Network study\n::- IOWA:: HMM study\n::- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n::- (~) IOWA:: Association Rule Mining, Frequent Item Sets Mining (Market Basket Problem)\n\n:- page cache to be revisited\n\n:- Real IO trace 확보 작업\n::- 김혁호 책임이 보내준 업무요청 사례 기반, 업무요청 문서 작성할 것\n::- (V) NetApp (김주영 과장)에게 연락 -> 스토리지 모델 정보 요청 -> 추가 문의 (Management Console에서 보다 자세한 정보를 볼 수는 없는지? NetApp OnCommand System Manager 같은 Tool을 사용해서라도..) -> 김주영 과장과 통화 (오전 10:20 ~ 46), 이번 주 중에 찾아뵙고 말씀 나누는 걸로 ...\n::- EMC 연락\n::- (~) Dell (지근영 대리)에게 연락\n:::- Dell EqualLogic PS6100XV iSCSI arrays [http://www.dell.com/us/enterprise/p/equallogic-ps6100xv/pd] [http://www.equallogic.com/products/default.aspx?id=10661]\n\n:- (~) Distributed Computing Problem Study\n::- CAP theorem (CAP: Consistency, Availability, Partition tolerance) [http://en.wikipedia.org/wiki/CAP_theorem]\n::- Paxos [http://en.wikipedia.org/wiki/Paxos_(computer_science)]\n::- State Machine Replication [http://en.wikipedia.org/wiki/State_machine_replication]\n\n----\n=== Formulation: IOWA Proactive Data Placement (moved to ## bNote-2013-03-25) ===\n----\n\n== ## bNote-2013-03-20 ==\n\n=== DailyTask ===\n\n* list of candidate tasks\n\n:- (~) IOWA DM/ML Planning\n::- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n::- Hadoop case 고려 (in-local-node vertical placement between DRAM-SSD-HDD 뿐만 아니라 horizontal placement (among distributed nodes in Hadoop cluster)도 고려)\n\n:- IOWA ML Study\n::- IOWA:: Bayesian Network study\n::- IOWA:: Neural Network study\n::- IOWA:: HMM study\n::- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n::- (~) IOWA:: Association Rule Mining, Frequent Item Sets Mining (Market Basket Problem)\n\n:- page cache to be revisited\n\n:- Real IO trace 확보 작업\n::- 김혁호 책임이 보내준 업무요청 사례 기반, 업무요청 문서 작성할 것\n::- (V) NetApp (김주영 과장)에게 연락 -> 스토리지 모델 정보 요청 -> 추가 문의 (Management Console에서 보다 자세한 정보를 볼 수는 없는지? NetApp OnCommand System Manager 같은 Tool을 사용해서라도..) -> 김주영 과장과 통화 (오전 10:20 ~ 46), 이번 주 중에 찾아뵙고 말씀 나누는 걸로 ...\n::- EMC 연락\n::- (~) Dell (지근영 대리)에게 연락\n:::- Dell EqualLogic PS6100XV iSCSI arrays [http://www.dell.com/us/enterprise/p/equallogic-ps6100xv/pd] [http://www.equallogic.com/products/default.aspx?id=10661]\n\n:- (V) 2013년 MBO 작성\n::- IOWA PDP (I/O Workload Analysis based Proactive Data Placement) 와 IOBA (I/O Bottleneck Analysis) 두 아이템으로 작성\n\n:- (~) Distributed Computing Problem Study\n::- CAP theorem (CAP: Consistency, Availability, Partition tolerance) [http://en.wikipedia.org/wiki/CAP_theorem]\n::- Paxos [http://en.wikipedia.org/wiki/Paxos_(computer_science)]\n::- State Machine Replication [http://en.wikipedia.org/wiki/State_machine_replication]\n\n----\n\n=== Formulation: IOWA Proactive Data Placement (moved to #bNote-2013-03-21) ===\n\n* [[http://kandinsky/wikini/index.php/Bnote_2013#Formulation:_IOWA_Proactive_Data_Placement]]\n\n=== Supercom Usage Statistics ===\n\n <pre>\nblusjune@jimi-hendrix:[~] $ ssh a1mjjung@supercom\na1mjjung@supercom\'s password:\nLast login: Tue Mar 19 19:28:27 2013 from 75.2.93.158\n----------------------------------------------------------\n| During : 20130311 ~ 20130317                            |\n| Username : a1mjjung , Application(Total jobs) : unix(3)\n----------------------------------------------------------\nTotal RUN time : 2 min 36 secs\nAverage RUN time : 52 secs\nMaximum RUN time : 1 min 14 secs\nAverage Wait time 1 secs\nMaximum Wait time 1 secs\n ---------------------------------------------------------\n</pre>\n\n----\n== ## bNote-2013-03-19 ==\n\n\n\n=== The Market-Basket Model ===\n\n\n=== IOWA::Outlook (MSN FileServer IO Trace // msnfs) ===\n\n* # of IOs (Read/Write/All)\n<pre>\na1mjjung@secm:[microsoft_msn_filesrvr_6h] $ wc -l tracelog.msn_filesrvr.[ARW]\n\n  29345085 tracelog.msn_filesrvr.A\n  19729611 tracelog.msn_filesrvr.R\n   9615474 tracelog.msn_filesrvr.W\n</pre>\n\n* Reads Outlook\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130313_154605.sigio_25.iowsz_2000.t1_2000] $ cat __simout.sigio_25.iowsz_2000.t1_2000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  17\n__valu__sig__ _n_o_sigaddrs :  173401\n__valu__sig__ _sigioc_acc :  5005517\n__valu__sig__ _sigaddrs_efficiency :  28.8667135714\n__valu__sig__ _n_o_addr_total :  5053016\n__valu__sig__ _ioc_total :  19729611\n</pre>\n\n* Writes Outlook\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130313_155325.sigio_25.iowsz_2000.t1_2000] $ cat __simout.sigio_25.iowsz_2000.t1_2000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  8\n__valu__sig__ _n_o_sigaddrs :  211100\n__valu__sig__ _sigioc_acc :  2989803\n__valu__sig__ _sigaddrs_efficiency :  14.1629701563\n__valu__sig__ _n_o_addr_total :  4506823\n__valu__sig__ _ioc_total :  9615474\n</pre>\n\n* Microsoft Production Workload Trace - Related Articles\n\n:- \"Characterization of Storage Workload Traces from Production Windows Servers\", IISWC 2008, Swaroop Kavalanekar, Bruce Worthington, Qi Zhang, Vishal Sharda, Microsoft Corporation [http://www.iiswc.org/iiswc2008/sildes/4_3.pdf Slides], [http://www.iiswc.org/iiswc2008/Papers/012.pdf Papers]\n\n:- \"Write Off-Loading: Practical Power Management for Enterprise Storage\" [http://static.usenix.org/event/fast08/tech/full_papers/narayanan/narayanan.pdf FAST 2008]\n\n=== R Tutorial (Data Frame, Preview) ===\n\n----\n==== Data Frame ====\nA data frame is used for storing data tables. It is a list of vectors of equal length. For example, the following variable df is a data frame containing three vectors n, s, b.\n\n <pre>\n> n = c(2, 3, 5) \n> s = c(\"aa\", \"bb\", \"cc\") \n> b = c(TRUE, FALSE, TRUE) \n> df = data.frame(n, s, b)       # df is a data frame\n</pre>\n\n----\n==== Built-in Data Frame ====\nWe use built-in data frames in R for our tutorials. For example, here is a built-in data frame in R, called mtcars.\n\n <pre>\n> mtcars \n               mpg cyl disp  hp drat   wt ... \nMazda RX4     21.0   6  160 110 3.90 2.62 ... \nMazda RX4 Wag 21.0   6  160 110 3.90 2.88 ... \nDatsun 710    22.8   4  108  93 3.85 2.32 ... \n               ............\n</pre>\n\nThe top line of the table, called the header, contains the column names. Each horizontal line afterward denotes a data row, which begins with the name of the row, and then followed by the actual data. Each data member of a row is called a cell.\n\nTo retrieve data in a cell, we would enter its row and column coordinates in the single square bracket \"[]\" operator. The two coordinates are separated by a comma. In other words, the coordinates begins with row position, then followed by a comma, and ends with the column position. The order is important.\n\nHere is the cell value from the first row, second column of mtcars.\n\n <pre>\n> mtcars[1, 2] \n[1] 6\n</pre>\n\nMoreover, we can use the row and column names instead of the numeric coordinates.\n\n <pre>\n> mtcars[\"Mazda RX4\", \"cyl\"] \n[1] 6\n</pre>\n\nLastly, the number of data rows in the data frame is given by the nrow function.\n\n <pre>\n> nrow(mtcars)    # number of data rows \n[1] 32\n</pre>\n\nAnd the number of columns of a data frame is given by the ncol function.\n\n <pre>\n> ncol(mtcars)    # number of columns \n[1] 11\n</pre>\n\nFurther details of the mtcars data set is available in the R documentation.\n\n <pre>\n> help(mtcars)\n</pre>\n\n----\n\n==== Preview ====\n\nPreview\nInstead of printing out the entire data frame, it is often desirable to preview it with the head function beforehand.\n\n <pre>\n> head(mtcars) \n               mpg cyl disp  hp drat   wt ... \nMazda RX4     21.0   6  160 110 3.90 2.62 ... \n               ............\n</pre>\n\n----\n\n==== Data Import ====\n\n\n\n\n\n\n\n\nIt is necessary to import the sample textbook data into R before you start working on your homework.\n\n* Excel File\n: Quite often, the sample data is in Excel format, and needs to be imported into R prior to use. For this, we use the read.xls function from the gdata package. It reads from an Excel spreadsheet and returns a data frame. The following shows how to load an Excel spreadsheet named \"mydata.xls\". As the package is not in the core R library, it has to be installed and loaded into the R workspace.\n\n <pre>\n> library(gdata)                   # load the gdata package \n> help(read.xls)                   # documentation \n> mydata = read.xls(\"mydata.xls\")  # read from first sheet\n</pre>\n\n* Minitab File\n: If the data file is in Minitab Portable Worksheet format, it can be opened with the read.mtp function from the foreign package. It returns a list of components in the Minitab worksheet.\n\n <pre>\n> library(foreign)                 # load the foreign package \n> help(read.mtp)                   # documentation \n> mydata = read.mtp(\"mydata.mtp\")  # read from .mtp file\n</pre>\n\n* SPSS File\n: For the data files in SPSS format, it can be opened with the read.spss function from the foreign package. There is a \"to.data.frame\" option for choosing whether a data frame is to be returned.\n\n <pre>\n> library(foreign)                 # load the foreign package \n> help(read.spss)                  # documentation \n> mydata = read.spss(\"myfile\", to.data.frame=TRUE)\n</pre>\n\n* Table File\n: A data table can resides in a text file. The cells inside the table are separated by blank characters. Here is an example of a table with 4 rows and 3 columns.\n\n <pre>\n100   a1   b1 \n200   a2   b2 \n300   a3   b3 \n400   a4   b4\n</pre>\n\nNow copy and paste the table above in a file named \"mydata.txt\" with a text editor. Then load the data into the workspace with the read.table function.\n\n <pre>\n> mydata = read.table(\"mydata.txt\")  # read text file \n> mydata                             # print data frame \n   V1 V2 V3 \n1 100 a1 b1 \n2 200 a2 b2 \n3 300 a3 b3 \n4 400 a4 b4\n</pre>\n\nFor further detail of the read.table function, please consult the R documentation.\n\n <pre>\n> help(read.table)\n</pre>\n\n* CSV File\nThe sample data can also be in comma separated values (CSV) format. Each cell inside such data file is separated by a special character, which usually is a comma, although other characters can be used as well.\n\nThe first row of the data file should contain the column names instead of the actual data. Here is a sample of the expected format.\n\n <pre>\nCol1,Col2,Col3 \n100,a1,b1 \n200,a2,b2 \n300,a3,b3\n</pre>\n\nAfter we copy and paste the data above in a file named \"mydata.csv\" with a text editor, we can read the data with the read.csv function.\n\n <pre>\n> mydata = read.csv(\"mydata.csv\")  # read csv file \n> mydata                           # print data frame \n  Col1 Col2 Col3 \n1  100   a1   b1 \n2  200   a2   b2 \n3  300   a3   b3\n</pre>\n\nIn various European locales, as the comma character serves as decimal point, the read.csv2 function should be used instead. For further detail of the read.csv and read.csv2 functions, please consult the R documentation.\n\n <pre>\n> help(read.csv)\n</pre>\n\n----\n\n== ## bNote-2013-03-18 ==\n\n=== DailyPlan ===\n\n* list of candidate tasks\n\n:- Real IO trace 확보 작업\n::- 김혁호 책임이 보내준 업무요청 사례 기반, 업무요청 문서 작성할 것\n::- [V://j] NetApp (김주영 과장)에게 연락 -> 스토리지 모델 정보 요청 -> 추가 문의 (Management Console에서 보다 자세한 정보를 볼 수는 없는지? NetApp OnCommand System Manager 같은 Tool을 사용해서라도..) -> 김주영 과장과 통화 (오전 10:20 ~ 46), 이번 주 중에 찾아뵙고 말씀 나누는 걸로 ...\n::- EMC 연락\n::- [~] Dell (지근영 대리)에게 연락\n:::- Dell EqualLogic PS6100XV iSCSI arrays [http://www.dell.com/us/enterprise/p/equallogic-ps6100xv/pd] [http://www.equallogic.com/products/default.aspx?id=10661]\n\n:- IOWA ML Planning\n::- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n::- Hadoop case 고려 (in-local-node vertical placement between DRAM-SSD-HDD 뿐만 아니라 horizontal placement (among distributed nodes in Hadoop cluster)도 고려)\n\n:- IOWA ML Study\n::- IOWA:: Bayesian Network study\n::- IOWA:: Neural Network study\n::- IOWA:: HMM study\n::- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n\n:- Netflix의 Cloud Computing Challenge 내용 파악\n(심상무님 지시: 거기가 우리보다 앞서 있으니, 어떤 기술들이 필요한지, 이슈가 무엇인지에 대한 힌트를 얻을 수 있을 것임)\n\n:- page cache to be revisited\n\n\n----\n\n=== Linux File Systems: Ext2 vs. Ext3 vs. Ext4 ===\n\n* [http://www.thegeekstuff.com/2011/05/ext2-ext3-ext4/]\n\next2, ext3 and ext4 are all filesystems created for Linux. This article explains the following:\n\n:- High level difference between these filesystems.\n:- How to create these filesystems.\n:- How to convert from one filesystem type to another.\n\n==== Ext2 ====\n\n* Ext2 stands for second extended file system.\n* It was introduced in 1993. Developed by Remy Card.\n* This was developed to overcome the limitation of the original ext file system.\n* Ext2 does not have journaling feature.\n* On flash drives, usb drives, ext2 is recommended, as it doesn’t need to do the over head of journaling.\n* Maximum individual file size can be from 16 GB to 2 TB\n* Overall ext2 file system size can be from 2 TB to 32 TB\n\n\n==== Ext3 ====\n\n* Ext3 stands for third extended file system.\n* It was introduced in 2001. Developed by Stephen Tweedie.\n* Starting from Linux Kernel 2.4.15 ext3 was available.\n* The main benefit of ext3 is that it allows journaling.\n* Journaling has a dedicated area in the file system, where all the changes are tracked. When the system crashes, the possibility of file system corruption is less because of journaling.\n* Maximum individual file size can be from 16 GB to 2 TB\n* Overall ext3 file system size can be from 2 TB to 32 TB\n* There are three types of journaling available in ext3 file system.\n:- Journal ? Metadata and content are saved in the journal.\n:- Ordered ? Only metadata is saved in the journal. Metadata are journaled only after writing the content to disk. This is the default.\n:- Writeback ? Only metadata is saved in the journal. Metadata might be journaled either before or after the content is written to the disk.\n* You can convert a ext2 file system to ext3 file system directly (without backup/restore).\n\n\n==== Ext4 ====\n\n* Ext4 stands for fourth extended file system.\n* It was introduced in 2008.\n* Starting from Linux Kernel 2.6.19 ext4 was available.\n* Supports huge individual file size and overall file system size.\n* Maximum individual file size can be from 16 GB to 16 TB\n* Overall maximum ext4 file system size is 1 EB (exabyte). 1 EB = 1024 PB (petabyte). 1 PB = 1024 TB (terabyte).\n* Directory can contain a maximum of 64,000 subdirectories (as opposed to 32,000 in ext3)\n* You can also mount an existing ext3 fs as ext4 fs (without having to upgrade it).\n* Several other new features are introduced in ext4: multiblock allocation, delayed allocation, journal checksum. fast fsck, etc. All you need to know is that these new features have improved the performance and reliability of the filesystem when compared to ext3.\n* In ext4, you also have the option of turning the journaling feature “off”.\n\n\n----\n\n=== Frequent patterns/items mining examples with R::arules ===\n\n[[Association rule mining]]\n\n=== R ===\n\n* png file output work-around against \'plot() error\' in R\n:- [http://www.mail-archive.com/r-help@r-project.org/msg40658.html]\n <pre>\nThe png() device does not need an X server to connect to. I think it\nused to in versions gone by, but not any more. Here I\'ve disabled X so\nthat X11() doesn\'t work, but png() still does:\n\n > x11()\n Error in X11(d$display, d$width, d$height, d$pointsize, d$gamma,\nd$colortype,  :\n   unable to start device X11cairo\n In addition: Warning message:\n In x11() : unable to open connection to X11 display \'\'\n > png(file=\"foo2.png\")\n > plot(1:10)\n > dev.off()\n null device\n          1\n\n I suspect your R was compiled without png support. What does the\n\'capabilities()\' function in R tell you?\n\n > capabilities()\n    jpeg      png     tiff    tcltk      X11     aqua http/ftp  sockets\n    TRUE     TRUE     TRUE     TRUE    FALSE    FALSE     TRUE     TRUE\n  libxml     fifo   cledit    iconv      NLS  profmem    cairo\n    TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE\n</pre>\n\n\n\n==== An Introduction to R ====\n\n* [http://cran.r-project.org/doc/manuals/R-intro.html#The-read_002etable_0028_0029-function An Introduction to R - Table of Contents]\n\n\n==== R Tutorial - (http://www.r-tutor.com/) ====\n\n* [http://www.r-tutor.com/gpu-computing/gaussian-process/rvbm Bayesian Classification with Gaussian Process]\n* [http://www.r-tutor.com/content/r-tutorial-ebook R Tutorial with Bayesian Statistics Using OpenBUGS]\n* [http://www.r-tutor.com/bayesian-statistics/openbugs Bayesian Inference Using OpenBUGS]\n* [http://www.r-tutor.com/gpu-computing/svm/rpusvm-2 Support Vector Machine with GPU, Part II]\n\n==== R: Input and output: scripts, saving and loading data ((B.GOOD)) ====\n\n* [http://egret.psychol.cam.ac.uk/statistics/R/savingloading.html Cambridge University]\n\n\n* General file-handling commands\n <pre>\nsetwd(\"c:/myfiles\") # use / or \\\\ to separate directories under Windows (\\\\ becomes \\ once processed through the escape character mechanism)\ndir() # list the contents of the current directory\n</pre>\n\n\n* Running scripts\n <pre>\nsource(\"myfile.R\") # load and execute a script of R commands\n</pre>\n\n* For a startup script\n: edit \".Rprofile\" in your home directory (for details see ?Startup). Here\'s an example\n <pre>\n# RNC ~/.Rprofile\n\n# auto width adjustment\n.adjustWidth <- function(...){\n       options(width=Sys.getenv(\"COLUMNS\"))\n       TRUE\n}\n.adjustWidthCallBack <- addTaskCallback(.adjustWidth)\n\n.First <- function() cat(\"\\n   Script ~/.Rprofile executed.\\n\\n\")\n.Last <- function()  cat(\"\\n   Goodbye!\\n\\n\")\n</pre>\n\n\n* Redirecting output\n <pre>\nsink(\"myfile.txt\") # redirect console output to a file\nsink() # restore output to the screen\n\npdf(\"mygraph.pdf\") # subsequent graphical output will go to a PDF\npng(\"mygraph.png\") # subsequent graphical output will go to a PNG\njpeg(\"mygraph.jpeg\") # subsequent graphical output will go to a JPEG\nbmp(\"mygraph.bmp\") # subsequent graphical output will go to a BMP\npostscript(\"mygraph.ps\") # subsequent graphical output will go to a PostScript file\ndev.off() # back to the screen\n</pre>\n\n\n* Text files\n <pre>\nmy.data = read.csv(filename)\nmy.data = read.csv(file.choose())\n# Note: (1) = and <- are synonymous, and are the assignment operator (while == tests for equality)\n#       (2) file.choose() pops up a live filename picker\n#       (3) The default is to assume a header row with variable names (header=TRUE),\n#           and no row names, but you can change all these defaults (e.g. row.names=1 reads\n#           row names from the first column).\n\nattach(my.data) # you might then want to attach the new data to the path, though this is optional\n\nwrite.csv(my.data, filename2) # Write the data to a new file. There are several options available; see the help (use ?write.csv)\nwrite.csv(my.data, file=\"d:/temp/newfile.csv\", row.names=FALSE) # Here\'s one: turn off row names to avoid creating a spurious additional column.\n\nread.table(...)  # } A more generic way to read/write tabular data from/to disk\nwrite.table(...) # } (read.csv and write.csv are specialized versions of read.table and write.table)\n</pre>\n\n\n* Microsoft Excel spreadsheets\n <pre>\nlibrary(RODBC)\nchannel <- odbcConnectExcel(\"Osteomalacia_data.xls\") # specify the filename\npatientdata <- sqlFetch(channel, \"Vitamin_D_levels\") # specify a sheet within the spreadsheet\nindexcasedata <- sqlFetch(channel, \"Sheet2\") # by default Excel names individual sheets Sheet1, Sheet2, ..., though you may have renamed them something more informative\nodbcClose(channel)\n</pre>\n\n\n* SPSS data\n <pre>\nlibrary(foreign)\nmydata <- data.frame(read.spss(\"filename.sav\"))\n# Remember you can also use file.choose() in place of the filename, as above.\n</pre>\n\n\n* ODBC data sources (databases)\n <pre>\n# 1. Connect\nlibrary(RODBC)\nchannel <- odbcConnect(\"my_DSN\") # specify your DSN here\n# if you need to specify a username/password, use:\n#  channel <-odbcConnect(\"mydsn\", uid=\"username\", pwd=\"password\")\n\n# 2. List all tables\nsqlTables(channel)\n\n# 3. Fetch a whole table into a data frame\nmydataframe <- sqlFetch(channel, \"my_table_name\") # fetch a table from the database in its entirety\nclose(channel)\n\n# 4. Fetch the results of a query into a data frame. Example:\nmydf2 <- sqlQuery(channel, \"SELECT * FROM MonkeyCantab_LOOKUP_TaskTypes WHERE TaskType < 6\")\n</pre>\n\nIf you\'re using MySQL, you can talk to the database directly:\n <pre>\nlibrary(RMySQL) # use install.packages(\"RMySQL\") if this produces an error\n# if the install.packages() command produces an error, under Ubuntu:\n# use \"sudo apt-get install libmysql++-dev\" (in addition to MySQL itself, i.e. the\n# \"mysql-server mysql-client mysql-navigator mysql-admin\" packages)\ncon <- dbConnect(MySQL(), host=\"localhost\", port=3306, dbname=\"mydatabase\", user=\"myuser\", password=\"mypassword\")\ndbListTables(con)\ndbListFields(con, \"table_name\")\nd <- dbReadTable(con, \"table_name\")\ne <- dbGetQuery(con, \"SELECT COUNT(*) FROM table_name\")\n# and much more possible\n</pre>\n\n\n* R native format\n <pre>\nsave(myobject1, myobject2, ..., file=\"D:/temp/mydata.rda\")\nload(file=\"D:/temp/mydata.rda\")\n# note that the load command recreates the \"mydata\" object without prompting\n# you can also use save.image() to save a whole workspace\n</pre>\n\n\n* Other data-moving techniques\nTo export the definition of an R object (which you can then re-import using \"object = THISTHING\"):\n <pre>\ndput(object, \"\")\n</pre>\n\nTo read a tabular object with a header row from the clipboard\n <pre>\nobject = read.table(\"clipboard\", header=T)\n</pre>\n\n----\n\n=== Samsung SSD 840 Series Information ===\n\n* [http://thessdreview.com/our-reviews/samsung-840-series-240gb-ssd-review-the-worlds-first-tlc-ssd-takes-the-stage/4/ Samsung 840 Series 250GB SSD Review ? The Worlds First TLC SSD Takes Center Stage]\n\n* [http://www.techspot.com/review/578-samsung-840-pro-ssd/ Samsung 840 Pro SSD Review]\n\n----\n\n=== Gnuplot Tips ===\n\n\n* How to unset key [http://people.duke.edu/~hpgavin/gnuplot.html]\n <pre>\n      Create a title:                  > set title \"Force-Deflection Data\" \n      Put a label on the x-axis:       > set xlabel \"Deflection (meters)\"\n      Put a label on the y-axis:       > set ylabel \"Force (kN)\"\n      Change the x-axis range:         > set xrange [0.001:0.005]\n      Change the y-axis range:         > set yrange [20:500]\n      Have Gnuplot determine ranges:   > set autoscale\n      Move the key:                    > set key 0.01,100\n      Delete the key:                  > unset key\n      Put a label on the plot:         > set label \"yield point\" at 0.003, 260 \n      Remove all labels:               > unset label\n      Plot using log-axes:             > set logscale\n      Plot using log-axes on y-axis:   > unset logscale; set logscale y \n      Change the tic-marks:            > set xtics (0.002,0.004,0.006,0.008)\n      Return to the default tics:      > unset xtics; set xtics auto\n</pre>\n\n* Mouse and hotkey support in interactive terminals\n\n: Interaction with the current plot via mouse and hotkeys is supported for the X11, OS/2 Presentation Manager, ggi and Windows terminals. See `mouse input` for more information on mousing. See help for bind for information on hotkeys. Also see the documentation for individual mousing terminals `ggi`, `pm`, `windows` and `x11`.\n\n: Here are briefly some useful hotkeys. Hit \'h\' in the interactive interval for help. Hit \'m\' to switch mousing on/off. Hit \'g\' for grid, \'l\' for log and \'e\' for replot. Hit \'r\' for ruler to measure peak distances (linear scale) or peak ratios (log scale), and \'5\' for polar coordinates inside a map. Zoom by mouse (MB3), and move in the zoom history by \'p\', \'u\', \'n\'; hit \'a\' for autoscale. Use other mouse buttons to put current mouse coordinates to clipboard (double click of MB1), add temporarily or permanently labels to the plot (middle mouse button MB2). Rotate a 3D surface by mouse. Hit spacebar to switch to the gnuplot command window.\n\n: Sample script: mousevariables.dem\n\n* [http://www.gnuplot.info/docs_4.0/gnuplot.html#Mouse_and_hotkey_support_in_interactive_terminals Mouse and hotkey support in interactive terminals -- Gnuplot info]\n\n=== NetApp Storage System Management Software ===\n\n* NetApp OnCommand System Manager [http://www.netapp.com/us/products/management-software/system-manager.aspx]\n:\n\n== ## bNote-2013-03-15 ==\n\n=== DailyPlanning 2013-03-15 ===\n\n* list of candidate tasks\n\n:- [V] HML basic concept study, 오늘 AP 주제에 대해 lightreading\n\n:- [V] 엄교수님께 특허 일정 전달\n\n:- Real IO trace 확보 작업\n::- [V] 김혁호 책임과 미팅 > 13:30 미팅 수행 (업무요청하기로 함)\n::- NetApp, Dell, EMC 측과 연락\n::- 지근영 대리에게 연락\n\n:- IOWA:: Bayesian Network study\n:- IOWA:: Neural Network study\n:- IOWA:: HMM study\n:- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n\n:- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n\n:- page cache to be revisited\n\n:- Data Placement 이슈: Media Difference (RAM,SSD,HDD) 외에 어떤 이슈가 있는가? Hadoop 같은 경우 노드 간 수평적 이동 이슈 있음. 매우 중요.\n\n:- 계획 외 업무들\n::- 이주평전문님의 본사팀과의 Conference Call 위해, 상무님 회의 대신 참석 (차주 화요일: 소장님께보고, 목요일: 부원장님께보고), 액션아이템 이전문님과 팀원께 전달.\n::- AP 세미나 참석 (최희열 전문)\n::- 팀 미팅: 소장님보고 자료 대응 방안 논의 -> IO Prediction 기반의 time 차원 제어로 공간적인 IO 속도 제약 극복 (마치 SS랩의 cooperative caching case처럼)\n\n\n=== HML Study:: \"Reducing the Dimensionality of Data with Neural Networks\" ===\n\n* Gradient descent [http://en.wikipedia.org/wiki/Gradient_descent]\n\n:- Gradient descent is a first-order optimization algorithm [http://en.wikipedia.org/wiki/First-order_approximation]\n\n:- Gradient descent to find the local minimum, gradient ascent to find the local maximum\n:: Gradient descent를 이용하여 function의 local \'\'\'minimum\'\'\'을 찾아내기 위해서는, 현재 지점에서의 function의 \'\'\'negative\'\'\' of the gradient (or of the approximate gradient)에 비례하는 taking steps를 한다. 만약 \'\'\'positive\'\'\' of the gradient에 비례하여 taking step한다면 그 function의 local \'\'\'maximum\'\'\'에 다가가게 된다. 이러한 절차는 gradient ascent라고 한다.\n\n\n* Gradient [http://en.wikipedia.org/wiki/Gradient]\n: Vector calculus에서, scalar field의 gradient는 다음 조건을 만족하는 vector field이다.\n:: direction은 scalar field의 증가분 (rate of increase)이 가장 최대가 되는 방향이다\n:: magnitude는 그 증가분이 된다 {{ In vector calculus, the gradient of a scalar field is a vector field that points in the direction of the greatest rate of increase of the scalar field, and whose magnitude is that rate of increase. }}\n\n\n* Orders of approximation [http://en.wikipedia.org/wiki/First-order_approximation]\n: terms for how precise an approximation is.\n: to indicate progressively more refined approximations: in increasing order of precision, a zeroth order approximation, a first order approximation, a second order approximation, and so forth\n: (Formally) an nth order of approximation\n:: one where the order of magnitude of the error is at most x^n, 혹은 big O notation으로 나타낸다면, error는 O(x^n) 이다.\n: detailed explanation with examples\n::- Zeroth-order (constant; a flat line with no slope; a polynomial of degree 0)\n::: x = [0, 1, 2]\n::: y = [3, 3, 5]\n::: y ~ f(x) = 3.67\n::- First-order (a linear approximation; straight line with a slope; a polynomial of degree 1)\n::: x = [0, 1, 2]\n::: y = [3, 3, 5]\n::: y ~ f(x) = x + 2.67\n::- Second-order (a quadratic polynomial; geometrically, a parabola; a polynomial of degree 2)\n::: x = [0, 1, 2]\n::: y = [3, 3, 5]\n::: y ~ f(x) = x^2 - x + 3\n\n=== Memo ===\n\n* 엄교수님과 연락 내용\n\n:- 엄교수님께 특허 일정 전달 (2013-03-15, 10\n:: 교수님, 안녕하세요? 기술원의 정명준전문입니다. 특허일정을 알아본 결과 3월 29일까지 직무발명서 시스템 등록을 하면 된다고 합니다. 앞으로 2주 정도 여유가 있네요 ^^ 그동안 실험결과를 정리하고 핵심아이디어 및 청구항을 잘 정리하면 될 것 같습니다. 차주 목요일 쯤에 한 번 조박사와 통화하여 기술상세/청구항/기존특허비교/침해적발등을 같이 논의해보면 어떨까합니다만 교수님 보시기에는 어떠신지요? 오늘도 멋진 하루 보내시구요, 항상 감사합니다. 정명준 드림.\n\n=== 연락처 (자주 사용하는) ===\n\n* 기술원 이주평 전문 : 01025984182, 010-2598-4182, #9956 : jupyung.lee@samsung.com\n* 기술원 신현정 전문 : 0173249294, 017-324-9294, #9747 : pharoah@samsung.com\n* 기술원 서정민 전문 : 01025441231, 010-2544-1231, #9817 : tony.seo@samsung.com\n* 기술원 구본철 전문 : 01091905907, 010-9190-5907, #9704 : bc.gu@samsung.com \n* 기술원 유개원 전문 : : gaewon.you@samsung.com\n* 기술원 최희열 전문 : 01096236578, 010-9623-6578, #9692 : heeyoul.choi@samsung.com\n* 기술원 문민영 전문 : , , #9716 :\n* 기술원 최영상 전문 : , , #9951 :\n* 기술원 박상도 전문 : , , #9586 :\n* 기술원 전바롬 전문 : , , #9547 :\n* 기술원 송인철 전문 : , , #9962 :\n* 기술원 박정현 연구원 : , , #9238 :\n\n* 기술원 심은수 상무 : 01020518077, 010-2051-8077, #9950 : eunsoo.shim@samsung.com\n* 기술원 서영완 전문 : 01030020208, 010-3002-0208, #9843 : sywpro@samsung.com\n* 기술원 유연아 사원 : 01090338452, 010-9033-8452, #9858 : yeonah78.yu@samsung.com\n\n* 삼성 SDS ESDM 인프라그룹 이형주 차장님: _ : hj001.lee@partner.samsung.com\n\n* 기술원 에어컨 안나올 때 (기술원 통합 방재 센터, 과장, 지원팀 > 환경안전그룹): #9120 :\n* VDI (SBC) 문제 있을 때 (VDI HelpDesk): #8272 : \n* 네트워크 안될 때 (방화벽 등) 어디로?: :\n* CLMS 시스템 문의 - 한지연 선임 / 서초 인사 CI 그룹: :\n\n* 서울대 컴퓨터공학부 엄현상 교수님 : 0162324667, 016-232-4667, 02-880-6755 : hseom@cse.snu.ac.kr\n* 서울대 컴퓨터공학부 조인순 박사 : 01051317886, 010-5131-7886, 02-880-9330 : insoonjo@gmail.com\n* 서울대 컴퓨터공학부 성민영 석사과정 : 01047245304, 010-4724-5304 : mysung@dcslab.snu.ac.kr\n\n=== HML (Hierarchical Machine Learning) AP (Advanced Program) ===\n\n* 세미나 일정\n\n{| border=\"1\"\n| 이름\n| 논문제목\n| 날짜\n|-\n| 최희열\n| Reducing the dimensionality of data with neural networks [http://www.cs.toronto.edu/~hinton/science.pdf]\n| 03월 15일 \n|-\n| 민윤홍	\n| A fast learning algorithm for deep belief nets [http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf]\n| 03월 22일 \n|-\n| 성재모	\n| Graphical Models \n| 03월 29일 \n|-\n| 정명준	\n| Hierarchical Temporal Memory including HTM Cortical Learning Algorithms \n| 04월 05일 \n|-\n| 박상도 	\n| How to Grow a Mind: Statistics, Structure, and Abstraction	\n| 04월 12일\n|-\n| 전바롬	\n| Learning Hierarchical Models of Scenes, Objects, and Parts\n| 04월 19일\n|-\n| 이호섭\n| Building high-level features using large scale unsupervised learning\n| 4월 26일\n|-\n| 박정현\n| High-Performance Neural Networks for Visual Object Classification\n| 05월 03일\n|-\n| 이호식\n| Deep Neural Networks for Acoustic Modeling in Speech Recognition\n| 5월 10일\n|-\n| 이예하\n| Unsupervised feature learning for audio classification using convolutioinal deep belief networks\n| 05월 24일\n|-\n| 송인철\n| Multimodal Deep Learning\n| 05월 31일\n|-\n|}\n\n== ## bNote-2013-03-14 ==\n\n\n=== Hidden Markov model (HMM) ===\n\n[[Hidden Markov model (HMM)]]\n\n== ## bNote-2013-03-13 ==\n\n\n=== Supercom Usage Statistics ===\n\n <pre>\n----------------------------------------------------------\n| During : 20130304 ~ 20130310                            |\n| Username : a1mjjung , Application(Total jobs) : matlab(1)\n----------------------------------------------------------\nTotal RUN time : 2 min 16 secs\nAverage RUN time : 2 min 16 secs\nMaximum RUN time : 2 min 16 secs\nAverage Wait time 1 secs\nMaximum Wait time 1 secs\n ---------------------------------------------------------\n|                       Application(Total jobs) : unix(14)\n----------------------------------------------------------\nTotal RUN time : 13 min 32 secs\nAverage RUN time : 58 secs\nMaximum RUN time : 3 min 23 secs\nAverage Wait time 1 secs\nMaximum Wait time 2 secs\n ---------------------------------------------------------\n\n</pre>\n\n=== ACM Transactions on Storage ===\n\n삼성 SDS 강석우 상무님 요청으로 우리 팀이 Review하게 됨.\n\n* [http://mc.manuscriptcentral.com/tos Welcome to the ACM Transactions on Storage manuscript submission site]\n\n== ## bNote-2013-03-12 ==\n\n\n=== SNIA Real IO Traces ===\n\n\n----\n==== Microsoft Production MSNStorageFileServer ( msnfs ) ====\n\n* Summary (Reads/Writes - All)\n: 2008-03-10 01:00 + 6 hours\n: Total # of IOs\n:: = 29,345,085 (total)\n:: = 19,729,611 (reads) + 9,615,474 (writes)\n:: = 29345085 = 19729611 + 9615474\n: Average IOPS\n:: = 1358.56 (= 29345085 / (6 * 3600))\n: Average interval time between IOs\n:: = 736 micro-seconds (= (6 * 3600 * 10^6 ) / 29345085)\n\n\n* Summary (Reads)\n <pre>\na1mjjung@secm:[R] $ pwd\n/home/X0101/a1mjjung/x/iowa.sidewinder/sidewinder/microsoft_msn_filesrvr_6h/MSNStorageFileServer/tdir/iowa-mw30m/R\n\na1mjjung@secm:[R] $ grep __valu__sig__ f030.infile_R.iowa.anal_s0010 \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  17\n__valu__sig__ _n_o_sigaddrs :  173401\n__valu__sig__ _sigioc_acc :  5005517\n__valu__sig__ _sigaddrs_efficiency :  28.8667135714\n__valu__sig__ _n_o_addr_total :  5053016\n__valu__sig__ _ioc_total :  19729611\n</pre>\n\n\n* Trace Log Field Information\n <pre>\n\n       1,         2,                 3,        4,      5,          6,      7,           8,       9,       10,          11,      12,       13,         14,       15\nDiskRead, TimeStamp, ProcessName (PID), ThreadID, IrpPtr, ByteOffset, IOSize, ElapsedTime, DiskNum, IrpFlags, DiskSvcTime, I/O Pri,  VolSnap, FileObject, FileName\n\nDiskWrite, TimeStamp, ProcessName (PID), ThreadID, IrpPtr, ByteOffset, IOSize, ElapsedTime, DiskNum, IrpFlags, DiskSvcTime, I/O Pri, VolSnap, FileObject, FileName\n</pre>\n\n\n* [[Trace Data Preprocessing Screenshot - Microsoft Production Trace - MSN FileServer]]\n\n\n----\n\n==== MSR Cambridge IO Traces ====\n\n\n* Summary\n: 22.4GB Trace Data from Data center servers\n:: \'\'\'13 servers, 36 volumes, 179 disks, 1 week\'\'\'\n\n\n* Backgrounds\n: Many enterprise servers are less I/O intensive than TPC benchmarks, which are specifically designed to stress the system under test. Enterprise workloads also show significant variation in usage over time, for example due to diurnal patterns.\n: In order to understand better the I/O patterns generated by standard data center servers, we instrumented the core servers in our building\'s data center to generate per volume block-level traces for one week.\n\n\n* References\n:* \"Write Off-Loading: Practical Power Management for Enterprise Storage\" - FAST 2008 [http://www.usenix.org/event/fast08/tech/narayanan.html]\n::- Messages from this paper\n::: The traditional view is that enterprise workloads make spinning disks down ineffective because idle periods are too short. We analyzed block-level traces from 36 volumes in an enterprise data center for one week and concluded that significant idle periods exist, and that they can be further increased by modifying the read/write patterns using write off-loading. Write off-loading allows write requests on spun-down disks to be temporarily redirected to persistent storage elsewhere in the data center.\n:* \"Self-organizing Storage (SOS) Project - Software\" \n::- Tools: nfsdump/nfsscan [http://www.eecs.harvard.edu/sos/software/index.html]\n::- SOS Project Traces [http://www.eecs.harvard.edu/sos/traces.html]\n\n\n* Trace Log Field Names\n: Timestamp, Hostname, DiskNumber, Type(Read/Write), Offset, Size, ResponseTime\n:- Timestamp: the time the I/O was issued in \"Windows filetime\"\n:- Hostname: the hostname (should be the same as that in the trace file name)\n:- DiskNumber: the disknumber (should be the same as in the trace file name)\n:- Type: \"Read\" or \"Write\"\n:- Offset: starting offset of the I/O in bytes (from the start of the logical disk)\n:- Size: transfer size of the I/O request in bytes\n:- ResponseTime: time taken by the I/O to complete, in \"Windows filetime\"\n\n\n* Trace Log Example\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ head -100 rsrch_0.csv \n\n128166372003061629,rsrch,0,Read,7014609920,24576,41286\n128166372016382155,rsrch,0,Write,1317441536,8192,1963\n128166372026382245,rsrch,0,Write,2436440064,4096,1835\n128166372036348580,rsrch,0,Write,3196526592,57344,35436\n128166372036379390,rsrch,0,Write,3154132992,4096,4626\n128166372036382264,rsrch,0,Write,3154124800,4096,1752\n128166372053100669,rsrch,0,Write,7609925632,10240,2053\n128166372053101032,rsrch,0,Write,15282630656,16384,1691\n128166372053101054,rsrch,0,Write,7612473344,16384,1668\n</pre>\n\n\n* IO Trace Nodes\n{| border=\"1\"\n| Node\n| Description\n| # of volumes\n| # of IOs\n|-\n| usr\n| User home directories\n| 3\n|-\n| proj\n| Project directories\n| 5\n|-\n| prn\n| Print server\n| 2\n|-\n| hm\n| Hardware monitoring\n| 2\n|-\n| rsrch\n| Research projects\n| 3\n|-\n| prxy\n| Firewall/WebProxy\n| 2\n|-\n| src1\n| Source control\n| 3\n|-\n| src2\n| Source control\n| 3\n|-\n| stg\n| Web staging\n| 2\n|-\n| ts\n| Terminal server\n| 1\n|-\n| web\n| Web/SQL server\n| 4\n|-\n| mds\n| Media server\n| 2\n|-\n| wdev\n| Test web server\n| 4\n|-\n|}\n\n\n* # of IOs\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ wc -l *.csv\n    3993316 hm_0.csv\n     609311 hm_1.csv\n    1211034 mds_0.csv\n    1637711 mds_1.csv\n    5585886 prn_0.csv\n   11233411 prn_1.csv\n    4224524 proj_0.csv\n   23639742 proj_1.csv\n   29266482 proj_2.csv\n    2244644 proj_3.csv\n    6465639 proj_4.csv\n   12518968 prxy_0.csv\n  168638964 prxy_1.csv\n    1433655 rsrch_0.csv\n      13780 rsrch_1.csv\n     207587 rsrch_2.csv\n   37415613 src1_0.csv\n   45746222 src1_1.csv\n    1907773 src1_2.csv\n    1557814 src2_0.csv\n     657774 src2_1.csv\n    1156885 src2_2.csv\n    2030915 stg_0.csv\n    2196861 stg_1.csv\n    1801734 ts_0.csv\n    2237889 usr_0.csv\n   45283980 usr_1.csv\n   10570046 usr_2.csv\n    1143261 wdev_0.csv\n       1055 wdev_1.csv\n     181266 wdev_2.csv\n        682 wdev_3.csv\n    2029945 web_0.csv\n     160891 web_1.csv\n    5175368 web_2.csv\n      31380 web_3.csv\n  434212008 total\n</pre>\n\n\n* List of traces\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ l\ntotal 22956880\ndrwxrwxr-x 2 a1mjjung X0101       4096 Mar 12 16:07 ./\ndrwxrwxr-x 3 a1mjjung X0101       4096 Mar 12 16:04 ../\n-r--r--r-- 1 a1mjjung X0101       1262 Oct 31  2008 DISCLAIMER.txt\n-r--r--r-- 1 a1mjjung X0101       1712 Oct 31  2008 MD5.txt\n-r--r--r-- 1 a1mjjung X0101       1815 Oct 31  2008 README.txt\n-r--r--r-- 1 a1mjjung X0101  202270441 Oct 30  2008 hm_0.csv\n-r--r--r-- 1 a1mjjung X0101   29765989 Oct 30  2008 hm_1.csv\n-r--r--r-- 1 a1mjjung X0101   63955502 Oct 30  2008 mds_0.csv\n-r--r--r-- 1 a1mjjung X0101   88337038 Oct 30  2008 mds_1.csv\n-r--r--r-- 1 a1mjjung X0101  291595716 Oct 30  2008 prn_0.csv\n-r--r--r-- 1 a1mjjung X0101  597136927 Oct 30  2008 prn_1.csv\n-r--r--r-- 1 a1mjjung X0101  233038754 Oct 30  2008 proj_0.csv\n-r--r--r-- 1 a1mjjung X0101 1305533029 Oct 30  2008 proj_1.csv\n-r--r--r-- 1 a1mjjung X0101 1614727432 Oct 30  2008 proj_2.csv\n-r--r--r-- 1 a1mjjung X0101  119913539 Oct 30  2008 proj_3.csv\n-r--r--r-- 1 a1mjjung X0101  350117046 Oct 30  2008 proj_4.csv\n-r--r--r-- 1 a1mjjung X0101  658840568 Oct 30  2008 prxy_0.csv\n-r--r--r-- 1 a1mjjung X0101 9043988744 Oct 30  2008 prxy_1.csv\n-r--r--r-- 1 a1mjjung X0101   77717781 Oct 31  2008 rsrch_0.csv\n-r--r--r-- 1 a1mjjung X0101     755814 Oct 31  2008 rsrch_1.csv\n-r--r--r-- 1 a1mjjung X0101   11154823 Oct 31  2008 rsrch_2.csv\n-r--r--r-- 1 a1mjjung X0101 2077380082 Oct 31  2008 src1_0.csv\n-r--r--r-- 1 a1mjjung X0101 2536095762 Oct 31  2008 src1_1.csv\n-r--r--r-- 1 a1mjjung X0101  101236500 Oct 31  2008 src1_2.csv\n-r--r--r-- 1 a1mjjung X0101   82511780 Oct 31  2008 src2_0.csv\n-r--r--r-- 1 a1mjjung X0101   35607343 Oct 31  2008 src2_1.csv\n-r--r--r-- 1 a1mjjung X0101   63026546 Oct 31  2008 src2_2.csv\n-r--r--r-- 1 a1mjjung X0101  105682669 Oct 31  2008 stg_0.csv\n-r--r--r-- 1 a1mjjung X0101  116358242 Oct 31  2008 stg_1.csv\n-r--r--r-- 1 a1mjjung X0101   93309044 Oct 31  2008 ts_0.csv\n-r--r--r-- 1 a1mjjung X0101  118478959 Oct 31  2008 usr_0.csv\n-r--r--r-- 1 a1mjjung X0101 2451360295 Oct 31  2008 usr_1.csv\n-r--r--r-- 1 a1mjjung X0101  574047026 Oct 31  2008 usr_2.csv\n-r--r--r-- 1 a1mjjung X0101   60262085 Oct 31  2008 wdev_0.csv\n-r--r--r-- 1 a1mjjung X0101      56014 Oct 31  2008 wdev_1.csv\n-r--r--r-- 1 a1mjjung X0101    9593588 Oct 31  2008 wdev_2.csv\n-r--r--r-- 1 a1mjjung X0101      35650 Oct 31  2008 wdev_3.csv\n-r--r--r-- 1 a1mjjung X0101  107571350 Oct 31  2008 web_0.csv\n-r--r--r-- 1 a1mjjung X0101    8507311 Oct 31  2008 web_1.csv\n-r--r--r-- 1 a1mjjung X0101  276121116 Oct 31  2008 web_2.csv\n-r--r--r-- 1 a1mjjung X0101    1649607 Oct 31  2008 web_3.csv\n</pre>\n\n\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ wc -l *.csv\n    3993316 hm_0.csv\n     609311 hm_1.csv\n    1211034 mds_0.csv\n    1637711 mds_1.csv\n    5585886 prn_0.csv\n   11233411 prn_1.csv\n    4224524 proj_0.csv\n   23639742 proj_1.csv\n   29266482 proj_2.csv\n    2244644 proj_3.csv\n    6465639 proj_4.csv\n   12518968 prxy_0.csv\n  168638964 prxy_1.csv\n    1433655 rsrch_0.csv\n      13780 rsrch_1.csv\n     207587 rsrch_2.csv\n   37415613 src1_0.csv\n   45746222 src1_1.csv\n    1907773 src1_2.csv\n    1557814 src2_0.csv\n     657774 src2_1.csv\n    1156885 src2_2.csv\n    2030915 stg_0.csv\n    2196861 stg_1.csv\n    1801734 ts_0.csv\n    2237889 usr_0.csv\n   45283980 usr_1.csv\n   10570046 usr_2.csv\n    1143261 wdev_0.csv\n       1055 wdev_1.csv\n     181266 wdev_2.csv\n        682 wdev_3.csv\n    2029945 web_0.csv\n     160891 web_1.csv\n    5175368 web_2.csv\n      31380 web_3.csv\n  434212008 total\n</pre>\n\n=== Real IO Trace 수집 (삼성 SDS 강석우 상무) ===\n\n* 삼성SDS 강석우 상무 (클라우드 플랫폼 팀장)\n\n* 삼성 SDS 박성록 수석보 (클라우드 플랫폼 운영그룹)\n\n* 진행 현황\n\n:* EMC\n::- 박정원 과장에게 연락함. 담당자인 이임호 부장 소개해줌.\n::- 이임호 부장은 아직 연락 못함\n\n:* Dell\n::- 지근영 대리와 통화/메일 (TraceLog 요청사항을 Dell에게 전달하겠다고 함)\n:::- Trace Log 데이터 예제 전달\n\n:* NetApp\n::- 김주영 과장과 통화/메일\n:::- Trace Log 데이터 예제 전달\n\n:* Supercom 센터\n\n\n* 스토리지 상주 지원 인력\n:* EMC\n::- 노영변부장 : 010-5384-6426 (EMC KOREA 삼성 PMO)\n::- 박정원과장 : 010-9052-7805 (EMC KOREA 센터 상주지원)(연락하였음)\n:::- Phone call, IO trace 수집에 대해 설명 -> 담당자 연결 시켜줌 (이임호 부장)\n::- 이임호 부장: 010-3203-7823 (EMC 삼성전담, 프리세일즈 기술컨설턴트)\n:::- Not yet connected (another phone call)\n:* Dell\n::- 이정민차장 : 010-2908-0759 \n::- 지근영대리 : 010-9530-2819 (한빛마이크로 센터상주지원)\n:::- DELL 기술지원 업무(수원ICT센터) 상주 : (6층 전산실 DELL EQL스토리지 기술지원)\n:::- 연락처: <dell.korea@samsung.com> (443-803  경기?수원시?영통구?매탄3동 410-1 삼성SDS 수원ICT S/W연구소 4층)\n:::- 3/12 연락하였음. (전화/메신저/메일로 상황 설명 하였으며, 현재 Dell에 요청 전달된 상태임)\n:* NetApp(상주지원 없음)\n::- 최병석이사 : 010-8998-7138(NetApp Korea)\n::- 김주영과장 : 010-9577-4272 (아리라)\n:::- Email sent, Phone call\n\n <pre>\n\n\n------- Original Message -------\n\nSender : 강석우<sukwkang@samsung.com> 상무/팀장/클라우드플랫폼팀/삼성SDS\n\nDate : 2013-03-08 14:06 (GMT+09:00)\n\nTitle : Re: Re: Fwd: 스토리지 벤더 현황입니다.\n\n \n\n아마 스토리지 벤더 별로 자체 테스트 시스템이 있기 때문에 테스트 시스템에서 로그수집이 가능할 겁니다. 아니면 본사에서 이미 가지고 있을수도 있구요.\n\n \n\n강석우 드림\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-03-08 14:02 (GMT+09:00)\n\nTitle : Re: Fwd: 스토리지 벤더 현황입니다.\n\n \n\n대단히 감사합니다, 강 상무님.\n\n \n\n심은수 드림\n\n \n\n------- Original Message -------\n\nSender : 강석우<sukwkang@samsung.com> 상무/팀장/클라우드플랫폼팀/삼성SDS\n\nDate : 2013-03-08 13:58 (GMT+09:00)\n\nTitle : Fwd: 스토리지 벤더 현황입니다.\n\n \n\n심상무님,\n\n \n\n아래의 스토리지 벤더에 연락해서 요청을 하시면 됩니다. SDS의 클라우드 팀 강석우 상무 소개로 연락했다고 말씀하시구요. 만약 협조를 잘 안하면 저에게 다시 연락주세요. 제가 협조하도록 만들겠습니다. :-)\n\n \n\n강석우 드림\n\n \n\n------- Original Message -------\n\nSender : 박성록<rocky@samsung.com> 수석보/클라우드플랫폼운영그룹/삼성SDS\n\nDate : 2013-03-08 13:48 (GMT+09:00)\n\nTitle : 스토리지 벤더 현황입니다.\n\n \n\n \n\n안녕하십니까?  클라우드플랫폼운영그룹 박성록수석보입니다.\n\n \n\n스토리지 상주 지원 인력입니다.\n\n1. EMC \n\n  - 노영변부장 : 010-5384-6426 (EMC KOREA 삼성 PMO)\n\n  - 박정원과장 : 010-9052-7805  (EMC KOREA 센터 상주지원)\n\n \n\n2. Dell\n\n  - 이정민차장 : 010-2908-0759 \n\n  - 지근영대리 : 010-9530-2819 (한빛마이크로 센터상주지원)\n\n \n\n3. NetApp(상주지원 없음)\n\n - 최병석이사 : 010-8998-7138(NetApp Korea)\n\n - 김주영과장 : 010-9577-4272 (아리라)\n  \n</pre>\n\n=== Multimedia Streaming vs. IOWA-based PDP ===\n\n* eMBMS: Evolved Multimedia Broadcast Multicast Service\n: Expway\'s eMBMS [http://blog.expway.com/ Expway\'s eMBMS Solution Allows Mobile Operators to off-Load Mobile Traffic by 20%]\n:- Key Technical Features\n::- FLUTE (File Delivery over Unidirectional Transport) protocol\n::- Forward Error Correction\n::- File Repair\n::- Service Announcement\n::- DASH Video Protocol\n:- Mobile Traffic Prediction (in 2016)\n::- 70% of mobile traffic will be video\n::- 10% of all TV viewing will be on tablets\n::- This equauls to 25 million DVDs sent every single hour\n:- Expway Company\n::- 7 years of experience focused on mobile broadcast software\n:::- Robust and Mature Products\n:::- Optimized Bandwidth Usage\n:::- Low Footprint Terminal Stack\n\n\n\n* DASH: Dynamic Adaptive Streaming over HTTP (a.k.a MPEG-DASH)\n: Internet 상으로 media content에 대한 고품질 streaming을 가능하게 하는 기술 (기존 HTTP 웹서버들로부터 deliver됨)\n: 컨텐츠를 small HTTP-based file segement들로 쪼개어 다룬다는 점에서 Apple의 HTTP Live Streaming (HLS)와 유사하다고 볼 수 있음. (컨텐츠 예: movie, sports event의 live broadcast 등)\n\n\n* HTTP Live Streaming (HLS): HTTP-based media streaming communications protocol (QuickTime과 iOS software의 일부로 Apple이 구현함)\n:- 동작원리\n:: overall stream을 작은 HTTP-based file downloads로 쪼개어 다룬다 (각각의 download는 overall potentially unbounded transport stream의 하나의 short chunk를 담당). stream 세션 시작 시에는, available한 variouis sub-stream들에 대한 metadata를 포함하고 있는 extended M2U (m3u8) playlist를 download한다.\n:- 장점\n:: 표준화된 HTTP transaction만을 사용하기 때문에, HLS는 일반 HTTP traffic을 허용하는 firewall, proxy server들은 모두 통과 가능 (RTP와 같은 UDP 기반 프로토콜은 그렇지 못함)하며, 널리 사용 가능한 CDN 인프라를 통해서 쉽게 deliver될 수 있음.\n:- 특징\n:: AES와 같은 암호화 메커니즘 및 HTTPS 기반의 secure key distribution 방법, simple DRM 시스템을 제공함\n:: HLS의 이후 버전에서는 [[trick mode]][http://en.wikipedia.org/wiki/Trick_mode] 기반의 fast-forward/rewind 및 subtitle의 통합도 지원할 예정임 (2013-03-12 현재)\n:- 표준화\n:: Apple에서는 HLS (HTTP Live Streaming)를 Internet Draft로 작성하였음. (first stage in the process of submitting it to the IETF, as an Informational Request For Comments)\n\n\n\n=== Technical Articles ===\n\n* 상무님께서 보내주신 \"Future of Cloud Storage\" 메일에 대한 신전문님 정리\n\n아래 글들을 읽은 소감 or 요약입니다.\n글들이 이것저것 다양하네요.\n\n* [http://gigaom.com/2011/10/29/the-future-beyond-the-cloud-is-in-our-hands/ The future beyond the cloud is in our hands]\n\n: \"Cloud is all\"의 관점은 mobile device가 ubiquitous, unlimited, low-cost conntection 인데요.. 이런 생각은 현재의 mobile network의 한계와 mobile device의 빠른 발전까지 고려치 않은 비전이라는 비판입니다.\n\n:# 첫째, LTE 같은 mobile network이 향후 cloud를 모두 책임지지는 못한다는 것이고요. (WAN에서의 Bandwidth란 이미 wireless network을 사용하고 있는 user가 쓰는 용량으로 계산된 것이므로)\n:# 둘째, 향후 mobile은 n-core의 multi-gigaherts procesor와 1TB 이상의 local storage 이므로 이를 cloud에서 활용하자는 얘기입니다.\n\n: 예전의 장수석님의 Mobile Cloud 과제가 생각납니다. ^^; Mobile의 능력과 wireless network를 활용하자는 겁니다.\n::- what if those devices can talk to one another in a peer-to-peer or mesh network? \n::- What’s the aggregate power and capability of billions of these things, especially if there will be ways for them to work with and talk to one another both alone and in conjunction with cloud-based services?\n\n: 예로 든 것이 Amazon Silk과 Google의 Offline Mail입니다.\n::- Silk는 클라우드를 이용해 acceleration하는 브라우저입니다. 클라우드에서 사용자의 웹 패턴을 분석하여 미리 preloading하고 mobile에 최적화하여 속도를 높이는 건데요.역으로 mobile에 맞게 웹페이지를 작게 축소해서 mobile에 가져오기 때문에 클라우드가 동작하지 않더라도 속도를 유지시킬 수 있다고 합니다. 클라우드와 mobile간에 서로 보완하는 거지요.\n::- offline Mail은 말그대로 메일서버가 되지 않아도 전송외에 모든 메일 관리가 가능하도록 mobile에 데이터를 미리 다 가져다 놓는 겁니다.\n \n\n* [http://blog.cloudhq.net/post/36559564253/the-future-of-cloud-storage The Future of Cloud Storage]\n\n: 정확히는 이런 문제점이 나올 거다라는 cloud issue에 관한 prediction이라고 볼 수 있습니다.\n\n:# End of Files and Folders : 클라우드 서비스들이 전통적인 File이나 folder 개념을 사용하지 않고 자기만의 interface, 데이터 분류 방식을 쓰므로 나중에 cloud 간의 호환성 문제가 발생함\n:# End of Free Storage: cloud storage 시장이 mature된다면 결국 free storage는 없어질 거다. 지금은 홍보용인 거다.\n:# Data ownership Troubles : 클라우드에 데이터가 올라간 순간 사용자는 data에 대한 control를 잃어버린다.\n:# Encryption will become necessary : encryption이 필수..\n\n\n* [http://www.cs.berkeley.edu/~kubitron/talks/printable/CISCO-cloudstore.pdf Lessons from the OceanStore Project - UC Berkeley]\n\n: 첫번째 글에서 좀 더 나아간 형태..Cloud 시대를 맞이한 P2P의 재조명 되겠습니다. 과거 P2P가 공짜 미디어를 훔치는 수단이었다면 새로운 P2P는 extreme scale를 제공할 수 잇는 system의 새 디자인으로 사용할 수 있다는 얘기입니다. 여러 Client뿐 아니라 여러 Cloue Storage Provider들도 같이 참여하면 서로 윈윈할 수 있다는 얘기입니다.\n\n \n* [http://www.cloudsigma.com/blog/13-the-future-of-cloud-storage The Future of Cloud Storage (and what is wrong with the present)]\n: SAN도 local Storage도 이제 끝났다.  Distributed Replicated Block Device(DRBD) 라고 얘기하고 있습니다만, converged server+storage 형태로 Server 노드에 Storage까지 합체한 형태로 죽 붙이고 replication을 다른 node에 함으로써 latency, fail over 등의 Converged architecture장점을 얘기하고 있네요. Open Solution으로는 sheepdog이 있고 상용화버전으로는 Amplidata가 있다고 합니다.\n::>> open source로서 Linux Kernel (2.6.33 version 부터) 에 구현되어 있는 DRBD도 있음. (아래 그림 참고) HA (High Availability) 제공에 초점이 맞춰져 있고, replication mode도 fully-synchronous와 asynchronous mode, 그리고, 그 사이에, protection level과 performance 간의 tradeoff를 고려한, semi-synchronous mode (memory synchronous mode 라고도 합니다)가 지원됨.  이 Linux의 DRBD는, 우리 RACS 1 의 기술이 networked storage로 확장될 때 매우 유용한 기술적 base가 될 수 있을 것 같습니다. (마치 Local Disk Array에 대한 RACS 1이 Linux MD를 활용하여 구현되고 있는 것처럼, Networked Replicated Disk Array 기술 구현 시에 Linux DRBD를 잘 활용할 수도 있을 것임) [http://www.ibm.com/developerworks/linux/library/l-drbd/index.html High availability with the Distributed Replicated Block Device]\n\n <pre>\n\n------- Original Message -------\n\nDate : 2013-03-11 10:44 (GMT+09:00)\nTitle : Fwd: future of cloud storage\n\nFYI, \n\n저희 궁극의 시나리오가 \'무한대의 local 저장용량을 제공하는\' pervasive storage 쪽으로 잡히면서,\n상무님이 cloud storage의 핵심기술에 대한 깊은 이해를 요구하고 계십니다.\n아래 메일도 참고하시고 시간을 내어 관련 기술에 대한 study를 진행하도록 하겠습니다.\n\n\n------- Original Message -------\nDate : 2013-03-08 20:55 (GMT+09:00)\nTitle : future of cloud storage\n\n이 전문,\n\n\n아마 이 전문도 구글 검색하면 금방 찾을 글들일텐데, 하여간 내가 본 것들입니다.\n바로 아래 것은 많은 시사점을 주는 것 같습니다.\n\nhttp://gigaom.com/2011/10/29/the-future-beyond-the-cloud-is-in-our-hands/\n\n \n그 외의 글들.\n\nhttp://blog.cloudhq.net/post/36559564253/the-future-of-cloud-storage\n\nhttp://www.cs.berkeley.edu/~kubitron/talks/printable/CISCO-cloudstore.pdf\n\nhttp://www.cloudsigma.com/blog/13-the-future-of-cloud-storage\n\n \n우리가 pervasive storage로 서비스 시나리오를 잡은 만큼, 그 분야의 서비스/기술 발전 전망을 할 수 있어야겠습니다.\n</pre>\n\n== ## bNote-2013-03-11 ==\n\n\n=== Akamai - CDN Acceleration ===\n\n* 관련 기사들\n:# [[아카마이, \"쌩쌩 웹사이트 만들려면\"]] [http://www.bloter.net/archives/141513 bloter.net, 2013-01-24]\n:# [[아카마이, \'블레이즈\' 인수 - 인텔리전트 플랫폼 강화]] [http://www.bloter.net/archives/95561 bloter.net, 2012-02-09]\n:# [[아카마이, \"CDN 넘어 하이퍼커넥티드로\"]] [http://www.bloter.net/archives/92711 bloter.net, 2012-01-19]\n:# [[아카마이, CDN 장악 가속화 ... 코텐도 인수설]] [http://www.bloter.net/archives/85622 bloter.net, 2011-11-28]\n:# [[아카마이, \"스트리밍/보안 서비스를 빠르고 안전하게\"]] [http://www.bloter.net/archives/67854 bloter.net, 2011-07-13]\n:# [[네트워크 업계 \"기다렸다, 런던올림픽\"]] [http://www.bloter.net/archives/92823 bloter.net, 2012-01-22]\n\n\n=== 과제 목표 설정 ===\n\n* IOWA-based PDP 과제 목표 metric\n: 경쟁사, 기술원 현수준, 기술원 목표수준, 접근방식 등\n:- 필수 고려 사항\n:: 어째서 그러한 목표 수준을 잡았는지?\n:- 점검 사항\n:: EMC FAST 등 Automatic Tiering 기술의 현수준 파악 필요\n:- 접근 방식의 독창성/진보성\n::- ProactiveDP가 MWC 2013에 언급된 eMBMS, DASH 등과 어떤 차별점을 갖는가?\n:::- eMBMS (Evolved Multimedia Broadcast Multicast Service): LTE를 이용해 수많은 사용자에게 방송 컨텐츠를 동시에 효과적으로 배포하는 기술임. 스트리밍 전송 및 비 피크타임에 전송해 단말에 저장된 형태로 있다가 사용자가 원할 때 시청. <span style=\"color:blue\">level of intelligence</span>가 중요한 비교점이 될 수 있음.\n:::- DASH (Dynamic Adaptive Streaming over HTTP)\n::- CDN (Content Delivery Network)에서 Akamai와는 어떻게 차별되나?\n\n* 기술 진화 고민\n: evolution of technology as a driving force from old-age to the pervasive storage\n\n=== IOWA: bpo_a.20130305_104633.real_whole_trace.log ===\n\n* Trace information\n:- Machine under IOTracing: radiohead (Linux 3.2.0-34)\n:- Tracing time: 72 hours\n:- Trace log file: /x/var/iowa/sidewinder/iowa/preproc/tdir/myrealtrace/bpo_a.20130305_104633.real_whole_trace.log\n\n==== further Write pattern analysis (LBA-to-name processing, for top 18 addresses) ====\n\n* IO statistical summary\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  1115\n__valu__sig__ _n_o_sigaddrs :  18\n__valu__sig__ _sigioc_acc :  40010\n__valu__sig__ _sigaddrs_efficiency :  2222.77777778\n__valu__sig__ _n_o_addr_total :  40979\n__valu__sig__ _ioc_total :  157632\n</pre>\n\n\n* top 18 addresses\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__list__hitnum_and_addrs__.log.v1 | sort -n  | tail -20\n1093 : [1266683272]\n1105 : [1267153912]\n1113 : [1661223296]\n\n---- top 18 addr starts below ----\n1115 : [12856320, 12857344]\n1237 : [2480]\n1686 : [1266681984]\n1693 : [1266683032]\n1730 : [1266683160]\n1732 : [2352]\n1941 : [1661223328]\n2077 : [2048]\n2110 : [1267095912]\n2188 : [327569400]\n2265 : [1266683048]\n2309 : [327568408]\n2342 : [327568936]\n2364 : [1661223320]\n2395 : [1661223136]\n2526 : [1401209744]\n7185 : [1661223128]\n</pre>\n\n\n* Bar Graph for Hits_per_Addr (MyRealTrace, Radiohead, 72h)\n<!-- [[File:Myrealtrace on radiohead 72h cdst hits per addr impulses.v01.png | 500px]] -->\n[[File:Myrealtrace on radiohead 72h cdst hits per addr impulses.v01.png]]\n\n\n===== analysis table =====\n\n{| border=\"1\"\n| address\n| # of hits\n| device node\n| process accessed\n| periodicity (1000 IOs)\n| corresponding file/dir\n| notes\n|-\n| 1661223128\n| 7185\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.964271213967\n| /home/hendrix/.config/google-chrome/Default/Cookies-journal\n| inode (39585621)\n|-\n| 1401209744\n| 2526\n| (8,1) /dev/sda1\n| BrowserBlocking\n| 0.926207876573\n| /home/hendrix/.config/google-chrome/Default/Local Storage/https_plus.google.com_0.localstorage-journal\n| inode (39714913)\n|-\n| 1661223136\n| 2395\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.957927324401\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 1661223320\n| 2364\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.957927324401\n| /home/hendrix/.config/google-chrome/Default/Cookies-journal\n| inode (39585621)\n|-\n| 327568936\n| 2342\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.938895655704\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 327568408\n| 2309\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.951583434836\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 1266683048\n| 2265\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.957927324401\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 327569400\n| 2188\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.894488428745\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 1267095912\n| 2110\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.951583434836\n| /home/hendrix/.config/google-chrome/Default/Cookies-journal\n| inode (39585621)\n|-\n| 2048\n| 2077\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.94523954527\n| INVALID BLOCK\n| inode (N/A)\n|-\n| 1661223328\n| 1941\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.957927324401\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 2352\n| 1732\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.957927324401\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 1266683160\n| 1730\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.938895655704\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 1266683032\n| 1693\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.964271213967\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 1266681984\n| 1686\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.957927324401\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 2480\n| 1237\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.951583434836\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 12856320\n| 1115\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.94523954527\n| /home/hendrix/.cache/google-chrome/Default/Cache/data_1\n| inode (39585855)\n|-\n| 12857344\n| 1115\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.94523954527\n| /home/hendrix/.cache/google-chrome/Default/Cache/index\n| inode (39585853)\n|-\n|}\n\n===== analysis result (processing output) =====\n\n <pre>\nblusjune@radiohead:[top_hot_18] $ l\ntotal 16\ndrwxrwxr-x 2 blusjune blusjune 4096 Mar 11 18:44 ./\ndrwxrwxr-x 3 blusjune blusjune 4096 Mar 11 14:49 ../\n-rwxr-xr-x 1 blusjune blusjune 2566 Mar 11 18:40 .bdx.0100.y.lba_to_name.sh*\n-rw-rw-r-- 1 blusjune blusjune 1768 Mar 11 18:44 .conf.lba_to_name.sh\n\nblusjune@radiohead:[top_hot_18] $ _BDX \nBDX[ /x/var/iowa/tdir/s05/w_ptrn_analysis/top_hot_18 ]# 0100 : lba_to_name\n\n\'_conf__target_lba_list\' is from:\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ tail -29 __simout.sigio_25.iowsz_100.t1_1000.__list__hitnum_and_addrs__.log.v2\n708 : [1795164168]\n840 : [12856336]\n841 : [1270876464]\n842 : [1661223968]\n913 : [1266682992]\n929 : [1270876456]\n943 : [1266681864]\n949 : [1266751536]\n956 : [1266751544]\n1093 : [1266683272]\n1105 : [1267153912]\n1113 : [1661223296]\n1115 : [12856320, 12857344]\n1237 : [2480]\n1686 : [1266681984]\n1693 : [1266683032]\n1730 : [1266683160]\n1732 : [2352]\n1941 : [1661223328]\n2077 : [2048]\n2110 : [1267095912]\n2188 : [327569400]\n2265 : [1266683048]\n2309 : [327568408]\n2342 : [327568936]\n2364 : [1661223320]\n2395 : [1661223136]\n2526 : [1401209744]\n7185 : [1661223128]\n\n#>> configuration started\n#<< _conf__target_dev (e.g., /dev/sda) : /dev/sda\n#<< _conf__target_dev_part (e.g., /dev/sda1) : /dev/sda1\n----\n/dev/sda1   *        2048  3907026943  1953512448   83  Linux\nBlock size:               4096\n----\n#<< _conf__lba_fs_start: 2048\n#<< _conf__fblk_size: 4096\n#<< _conf__sector_size [512]: \n#>> configuration completed\n\n#>> START Processing\n/dev/sda1: 1795164168 -> _EXCEPTION_ # inode for 224395265 is NOT FOUND -- Skip processing\n/dev/sda1: 12856336 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_1 # DEV:LBA:FBLK:INODE( /dev/sda1 : 12856336 : 1606786 : 39585855 )\n/dev/sda1: 1270876464 -> _EXCEPTION_ # inode for 158859302 is NOT FOUND -- Skip processing\n/dev/sda1: 1661223968 -> /home/blusjune/.config/google-chrome/Default/Local Storage/https_plus.google.com_0.localstorage # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223968 : 207652740 : 39714912 )\n/dev/sda1: 1266682992 -> _EXCEPTION_ # inode for 158335118 is NOT FOUND -- Skip processing\n/dev/sda1: 1270876456 -> _EXCEPTION_ # inode for 158859301 is NOT FOUND -- Skip processing\n/dev/sda1: 1266681864 -> _EXCEPTION_ # inode for 158334977 is NOT FOUND -- Skip processing\n/dev/sda1: 1266751536 -> /home/blusjune/.config/google-chrome # DEV:LBA:FBLK:INODE( /dev/sda1 : 1266751536 : 158343686 : 39585586 )\n/dev/sda1: 1266751544 -> /home/blusjune/.config/google-chrome/Default # DEV:LBA:FBLK:INODE( /dev/sda1 : 1266751544 : 158343687 : 39585590 )\n/dev/sda1: 1266683272 -> _EXCEPTION_ # inode for 158335153 is NOT FOUND -- Skip processing\n/dev/sda1: 1267153912 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_0 # DEV:LBA:FBLK:INODE( /dev/sda1 : 1267153912 : 158393983 : 39585854 )\n/dev/sda1: 1661223296 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_0 # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223296 : 207652656 : 39585854 )\n/dev/sda1: 12857344 -> /home/blusjune/.cache/google-chrome/Default/Cache/index # DEV:LBA:FBLK:INODE( /dev/sda1 : 12857344 : 1606912 : 39585853 )\n/dev/sda1: 12856320 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_1 # DEV:LBA:FBLK:INODE( /dev/sda1 : 12856320 : 1606784 : 39585855 )\n/dev/sda1: 2480 -> _EXCEPTION_ # inode for 54 is NOT FOUND -- Skip processing\n/dev/sda1: 1266681984 -> _EXCEPTION_ # inode for 158334992 is NOT FOUND -- Skip processing\n/dev/sda1: 1266683032 -> _EXCEPTION_ # inode for 158335123 is NOT FOUND -- Skip processing\n/dev/sda1: 1266683160 -> _EXCEPTION_ # inode for 158335139 is NOT FOUND -- Skip processing\n/dev/sda1: 2352 -> _EXCEPTION_ # inode for 38 is NOT FOUND -- Skip processing\n/dev/sda1: 1661223328 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223328 : 207652660 : 39585620 )\n/dev/sda1: 2048 -> _EXCEPTION_ # fsblock 0 seems INVALID BLOCK -- Skip processing\n/dev/sda1: 1267095912 -> /home/blusjune/.config/google-chrome/Default/Cookies-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1267095912 : 158386733 : 39585621 )\n/dev/sda1: 327569400 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 327569400 : 40945919 : 39585620 )\n/dev/sda1: 1266683048 -> _EXCEPTION_ # inode for 158335125 is NOT FOUND -- Skip processing\n/dev/sda1: 327568408 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 327568408 : 40945795 : 39585620 )\n/dev/sda1: 327568936 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 327568936 : 40945861 : 39585620 )\n/dev/sda1: 1661223320 -> /home/blusjune/.config/google-chrome/Default/Cookies-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223320 : 207652659 : 39585621 )\n/dev/sda1: 1661223136 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223136 : 207652636 : 39585620 )\n/dev/sda1: 1401209744 -> /home/blusjune/.config/google-chrome/Default/Local Storage/https_plus.google.com_0.localstorage-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1401209744 : 175150962 : 39714913 )\n/dev/sda1: 1661223128 -> /home/blusjune/.config/google-chrome/Default/Cookies-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223128 : 207652635 : 39585621 )\n\nblusjune@radiohead:[top_hot_18] $ \n\n\n</pre>\n\n\n=== LBA-to-name processing ===\n\n* .bd/x/exphist info.\n: bd/x/exphist/sidewinder/sidewinder-20130311_190748/tools/lba_to_name/\n\n <pre>\nblusjune@jimi-hendrix:[lba_to_name] $ pwd\n/home/blusjune/bd/x/exphist/sidewinder/sidewinder-20130311_190748/tools/lba_to_name\nblusjune@jimi-hendrix:[lba_to_name] $ l\ntotal 16\ndrwxrwxr-x 2 blusjune blusjune 4096 Mar 11 19:03 ./\ndrwxrwxr-x 4 blusjune blusjune 4096 Mar 11 19:07 ../\n-rwxr-xr-x 1 blusjune blusjune 3869 Mar 11 19:03 .bdx.0100.y.lba_to_name.sh*\n-rw-rw-r-- 1 blusjune blusjune  300 Mar 11 19:03 .conf.lba_to_name.sh\n</pre>\n\n\n* References\n: [http://www.expertslogin.com/general/how-to-fix-repair-bad-blocks-in-linux/ How To Fix / Repair Bad Blocks In Linux] (B.GOOD)\n\n\n* How to find the corresponding location (file/dir) for some LBA (e.g., 1661223128)?\n\n: step 1: We need to find the block size of the filesystem\n <pre>\nblusjune@radiohead:[google] $ sudo tune2fs -l /dev/sda1  | grep Block\nBlock count:              488378112\nBlock size:               4096\nBlocks per group:         32768\n</pre>\n\n: step 2: check the starting sector of the partition (file system)\n <pre>\nblusjune@radiohead:[google] $ sudo fdisk -lu /dev/sda\n\nDisk /dev/sda: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x00001eb9\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *        2048  3907026943  1953512448   83  Linux\n\n</pre>\n\n: step 3: File the filesystem block that contains this target LBA\n <pre>\nb = (int) ((L-S) * 512 / B)\n\n, where\nb = File system block number (we need this answer)\nB = File system block size in bytes (4096 - we got this in step 1)\nL = target LBA (1661223128 - given)\nS = Starting sector of the partition as shown by fdisk -lu (2048 - we got this in step 2)\n\n, so the answer b is 207652635 (207,652,635)\n\n</pre>\n\n: step 4: Check the file/dir name with given b (207652635)\n <pre>\nblusjune@radiohead:[s05] $ sudo debugfs /dev/sda1 \ndebugfs 1.42 (29-Nov-2011)\n\ndebugfs:  testb 207652635\nBlock 207652635 marked in use\n\ndebugfs:  icheck 207652635\nBlock	Inode number\n207652635	39585621\n\ndebugfs:  ncheck 39585621\nInode	Pathname\n39585621	/home/blusjune/.config/google-chrome/Default/Cookies-journal\n\ndebugfs:\n</pre>\n\n== ## bNote-2013-03-08 ==\n\n=== lsof command guide/examples ===\n\n* [http://www.ibm.com/developerworks/aix/library/au-lsof.html Finding open files with lsof]\n\n:- Sean A. Walberg, Senior Network Engineer\n:- Summary:  Learn more about your system by seeing which files are open. Knowing which files an application has open, or which application has a particular file open, enables you to make better decisions as a system administrator. For instance, you shouldn\'t unmount a file system while files on it are open. Using lsof, you can check for open files and stopped processes before unmounting, as needed. Likewise, if you find an unknown file, you can find the application holding it open.\n\n\n=== debugfs command guide/examples ===\n\n* [http://www.cs.montana.edu/courses/309/topics/4-disks/debugfs_example.html debugfs examples - original article from montana.edu]\n\n <pre>\ndebugfs Command Examples\n\n\n# Use debufs to prowl around a file system.\n\n> debugfs /dev/hda6\ndebugfs 1.19, 13-Jul-2000 for EXT2 FS 0.5b, 95/08/09\n\n# list files\n\ndebugfs:  ls\n2790777 (12) .   32641 (12) ..   2790778 (12) dir1   2790781 (16) file1\n2790782 (4044) file2\n\n#  List the files with a long listing\n\n#  Format is:\n# Field 1:  Inode number.\n# Field 2:  First one or two digits is the type of node:\n#    2 = Character device\n#    4 = Directory\n#    6 = Block device\n#    10 = Regular file\n#    12 = Symbolic link\n#  \n#    The Last four digits are the Linux permissions\n# 3. Owner uid\n# 4. Group gid\n# 5. Size in bytes.\n# 6. Date \n# 7. Time of last creation.\n# 8. Filename.\n\ndebugfs:  ls -l\n2790777  40700   2605   2601    4096  5-Nov-2001 15:30 .\n 32641   40755   2605   2601    4096  5-Nov-2001 14:25 ..\n2790778  40700   2605   2601    4096  5-Nov-2001 12:43 dir1\n2790781 100600   2605   2601      14  5-Nov-2001 15:29 file1\n2790782 100600   2605   2601      14  5-Nov-2001 15:30 file2\n\n# dump the contents of file1\n\ndebugfs: cat file1\nThis is file1 \n\n# dump an inode to a file (same as cat, but to a file) and using\n#  instead of the file name.\n\ndebugfs: dump <2790782> file1-debugfs\n\n# dump the contents of an inode\n\ndebugfs: stat file1 \nInode: 2790782   Type: regular    Mode:  0600   Flags: 0x0   Generation: 46520506\nUser:  2605   Group:  2601   Size: 14\nFile ACL: 0    Directory ACL: 0\nLinks: 1   Blockcount: 8\nFragment:  Address: 0    Number: 0    Size: 0\nctime: 0x3be712ea -- Mon Nov  5 15:30:02 2001\natime: 0x3be712ea -- Mon Nov  5 15:30:02 2001\nmtime: 0x3be712ea -- Mon Nov  5 15:30:02 2001\nBLOCKS:\n5603924\nTOTAL: 1\n\n# Dump an directory inode and look at it.\n\ndebugfs: dump dir1 dir1-debugfs\n\n# Leave debugfs or use another xterm to look at the contents\n# using od or xxd.  The format of a directory (ext2 version 2.0) is:\n\n# Field 1. Four byte inode number.\n# Field 2. Two byte directory entry length.\n# Field 3. Two byte file name length. \n# Field 5. Filename (1-255 characters).\n# Pad.     The filename is padded to be a multiple of 4 bytes long.\n\n\n# use -c to see the file names and single byte values\n# You can see the file names and identify the locatin of\n# the other fields.  Of importance, the length of the \n# entries (octal); . (4-5), .. (20-21), file3 (34-35),\n# file4 (54-55), .file4.swp (74-75),  ...\n\n> od -c dir1-dump  \n0000000   z 225   *  \\0  \\f  \\0 001 002   .  \\0  \\0  \\0   y 225   *  \\0\n0000020  \\f  \\0 002 002   .   .  \\0  \\0 202 225   *  \\0 020  \\0 005 001\n0000040   f   i   l   e   3  \\0  \\0  \\0 201 225   *  \\0   ? 017 005 001\n0000060   f   i   l   e   4  \\0  \\0  \\0 177 225   *  \\0   ? 017  \\n 001\n0000100   .   f   i   l   e   4   .   s   w   p  \\0  \\0 200 225   *  \\0\n0000120   ? 017 006 001   f   i   l   e   4   ~   .   s   w   p   x  \\0\n0000140  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0\n*\n0010000\n\n# use -d to see the two byte values\n\n> od -d dir1-dump  \n0000000 38266    42    12   513    46     0 38265    42\n0000020    12   514 11822     0 38274    42    16   261\n0000040 26982 25964    51     0 38273    42  4056   261\n0000060 26982 25964    52     0 38271    42  4040   266\n0000100 26158 27753 13413 29486 28791     0 38272    42\n0000120  4020   262 26982 25964 32308 29486 28791   120\n0000140     0     0     0     0     0     0     0     0\n*\n0010000\n\n# You can see that the lengths of the entries are:\n#    . = 12, .. = 12, file3 = 16, file4 = 4096\n# Whoa! what happened there.  The file .file4.swp\n# and any other files in the directory have been deleted,\n# so the length of the entry goes to the end of the block\n#\n# use -l to see the four byte values.  We can see the inode\n# values of the files.\n\n> od -l dir1-dump  \n0000000     2790778    33619980          46     2790777\n0000020    33685516       11822     2790786    17104912\n0000040  1701603686          51     2790785    17108952\n0000060  1701603686          52     2790783    17436616\n0000100  1818846766  1932407909       28791     2790784\n0000120    17174452  1701603686  1932426804     7893111\n0000140           0           0           0           0\n*\n0010000\n\n\n-------------------------------------------------------------------<\n\n\n\n#\n# You inadvertently delete a file you want back.  The file was named\n# /home/harkin/test/file2.  Immediately do the following.\n#\n\n\n> umount /home\n\n# so that you don\'t create a new file that overwrites the inode\n# or use one of the file blocks.\n\n# Execute df to find out what partition /home is on\n\n>df \nFilesystem           1k-blocks      Used Available Use% Mounted on\n/dev/hda1              1011928    507860    452664  53% /\n/dev/hda8             27364092   1890176  24083896   8% /home\n/dev/hda5              8064272   3492760   4161860  46% /usr\n/dev/hda7              1011928     87956    872568  10% /var\nclowns:/db/boze       17783240  10494056   7183568  60% /home/bozo/db\n\n# Get the data on the /home filesystem\n\ntune2fs -l /dev/hda8 | grep \"Block size\"\n\n   Block size:               4096\n\n# So the block size is 4096 bytes.\n\n# Create a file system to duplicate the /home file system in case\n# you screw up royally.  This disk should be exactly the same size\n# as the file system you are backing up.  Fortunately there is an\n# unused disk /dev/hdb.\n\n> fdisk /dev/hdb\nCommand (m for help): n\nCommand action\n   l   logical (5 or over)\n   p   primary partition (1-4)\np\n\n+27364092K\nw\n\n# copy /home to the backup location\n\ndd if=/dev/hda8 of=/dev/hdb1 bs=4096\n\n# Now use debugfs to try to fix things.  We need to try to\n# find the inode of the deleted file.  Use lsdel to \n# list all of the deleted inodes on the file system.\n\ndebugfs -w            # to allow writing\ndebugfs:  lsdel\n3061 deleted inodes found.\n Inode  Owner  Mode    Size    Blocks    Time deleted\n                     .\n                     .\n3296723   2605 100600    652    1/   1 Fri Nov  2 07:30:33 2001\n3296724   2605 100600   1545    1/   1 Fri Nov  2 07:30:33 2001\n3296725   2605 100600    355    1/   1 Fri Nov  2 07:30:33 2001\n3296731   2605 100600    440    1/   1 Fri Nov  2 07:30:33 2001\n3296732   2605 100600   3536    1/   1 Fri Nov  2 07:30:33 2001\n3296733   2605 100600   2365    1/   1 Fri Nov  2 07:30:33 2001\n3296734   2605 100600    443    1/   1 Fri Nov  2 07:30:33 2001\n3296850   2605 100600   2046    1/   1 Fri Nov  2 07:30:33 2001\n3296851   2605 100600    729    1/   1 Fri Nov  2 07:30:33 2001\n3296852   2605 100600    850    1/   1 Fri Nov  2 07:30:33 2001\n3296853   2605 100600   3251    1/   1 Fri Nov  2 07:30:33 2001\n3296854   2605 100600   3733    1/   1 Fri Nov  2 07:30:33 2001\n3296855   2605 100600   3109    1/   1 Fri Nov  2 07:30:33 2001\n3296856   2605 100600   3211    1/   1 Fri Nov  2 07:30:33 2001\n652818   2605 100600 171791   43/  43 Fri Nov  2 16:07:33 2001\n897613   2605 100600   2096    1/   1 Mon Nov  5 07:49:28 2001\n979218   2605 100600   3797    1/   1 Mon Nov  5 07:49:29 2001\n979219   2605 100600   4096    1/   1 Mon Nov  5 07:49:29 2001\n179573   2605 100600   9113    3/   3 Mon Nov  5 12:41:16 2001\n636513   2605 100600   1327    1/   1 Mon Nov  5 12:41:16 2001\n636520   2605 100600     20    1/   1 Mon Nov  5 12:41:16 2001\n1338319   2605 100600   6998    2/   2 Mon Nov  5 12:48:55 2001\n \n# Based on the time and date, the inode to restore is 179573, 636513 \n# or 636520.  Try to figure out which one.\n\ndebugfs:cat <179573> \n   .\n   .\n\ndebugfs:cat <636513>\n   .\n\n# This is rather inconvenient.  If the directory where the files were\n# deleted from still exists, use the cd command to get there and then\n# use ls -d  which lists the files in the directory only, including\n# those with the deleted flag set. \n\n1566721  (12) .    32641  (12) ..    1566788  (60) 530\n1566790 (48) file1   1566791 (24) file2\n<1566747>  (20) file3\n\nThe inode numbers in brackets are deleted files.  A better looking display\ncomes with ls -ld.\n\n\n# So now you know which inode you need to restore.\n# To restore the file, you need to modify the inode, not the \n# directory entry.  This can be done with the modify_inode (mi)\n# command.  Specifically, change the deletion time to zero\n# and the link count to 1.\n\ndebugfs: mi <636513>\ndebugfs:  mi <148003>\n                              Mode    [0100644] \n                           User ID    [510] \n                          Group ID    [510] \n                              Size    [8123] \n                     Creation time    [904216575] \n                 Modification time    [904234782] \n                       Access time    [904234782] \n                     Deletion time    [904236721] 0\n                        Link count    [0] 1\n                       Block count    [16] \n                        File flags    [0x0] \n                         Reserved1    [0] \n                          File acl    [0] \n                     Directory acl    [0] \n                  Fragment address    [0] \n                   Fragment number    [0] \n                     Fragment size    [0] \n                   Direct Block #0    [100321] \n                   Direct Block #1    [100322] \n                   Direct Block #2    [100323] \n                   Direct Block #3    [100324] \n                   Direct Block #4    [200456] \n                   Direct Block #5    [200457] \n                   Direct Block #6    [200675] \n                   Direct Block #7    [200675] \n                   Direct Block #8    [304568] \n                   Direct Block #9    [0] \n                  Direct Block #10    [0] \n                  Direct Block #11    [0] \n                    Indirect Block    [0] \n             Double Indirect Block    [0] \n             Triple Indirect Block    [0] \n\n# It has been recovered.\n\n# This won\'t work for files with indirect blocks and you might find that\n# one or more blocks have been reused already.  If so, you can\n# recover as much data as possible by dumping the blocks to a file.\n\ndebugfs: dump <100321> /tmp > file1.000\ndebugfs: dump <100322> /tmp >> file1.000\n\n# and so on. For files that are longer than 12 blocks, you have to \n# trace the indirect, double-indirect and triple-indirect blocks.\n\n</pre>\n\n=== blktrace advanced ===\n\n* legacy \'blktrace\' data output\n <pre>\nDev_ID CPU_ID   SN   Timestamp      PID   Phz Act Address   Offset ProcessName\n------------------------------------------------------------------------------------  \n  8,16   1      929  2200.865379372 26328  A   R 3188196112 + 8 <- (8,17) 3188194064\n  8,17   1      930  2200.865379890 26328  Q   R 3188196112 + 8 [mysqld]\n  8,17   1      931  2200.865380598 26328  G   R 3188196112 + 8 [mysqld]\n  8,17   1      932  2200.865381014 26328  P   N [mysqld]\n  8,17   1      933  2200.865381784 26328  I   R 3188196112 + 8 [mysqld]\n  8,17   1      934  2200.865382107 26328  U   N [mysqld] 1\n  8,17   1      935  2200.865382605 26328  D   R 3188196112 + 8 [mysqld]\n  8,17   1      936  2200.871162161     0  C   R 3188196112 + 8 [0]\n  8,16   1      937  2200.871189524 26328  A   R 3188589744 + 8 <- (8,17) 3188587696\n  8,17   1      938  2200.871190517 26328  Q   R 3188589744 + 8 [mysqld]\n  8,17   1      939  2200.871192023 26328  G   R 3188589744 + 8 [mysqld]\n  8,17   1      940  2200.871192992 26328  P   N [mysqld]\n  8,17   1      941  2200.871194468 26328  I   R 3188589744 + 8 [mysqld]\n  8,17   1      942  2200.871195233 26328  U   N [mysqld] 1\n</pre>\n\n* legacy \'lsof\' data output\n <pre>\nCOMMAND     PID            USER   FD      TYPE             DEVICE SIZE/OFF       NODE NAME\n-------------------------------------------------------------------------------------------------\ninit          1            root  cwd       DIR                8,1     4096          2 /\ninit          1            root  rtd       DIR                8,1     4096          2 /\ninit          1            root  txt       REG                8,1   163096   57147454 /sbin/init\ninit          1            root  mem       REG                8,1    52120   96997047 /lib/x86_64-linux-gnu/libnss_files-2.15.so\ninit          1            root  mem       REG                8,1    47680   96993415 /lib/x86_64-linux-gnu/libnss_nis-2.15.so\ninit          1            root  mem       REG                8,1    97248   96997056 /lib/x86_64-linux-gnu/libnsl-2.15.so\ninit          1            root  mem       REG                8,1    35680   96997048 /lib/x86_64-linux-gnu/libnss_compat-2.15.so\ninit          1            root  mem       REG                8,1  1811128   96993409 /lib/x86_64-linux-gnu/libc-2.15.so\ninit          1            root  mem       REG                8,1    31752   96993413 /lib/x86_64-linux-gnu/librt-2.15.so\ninit          1            root  mem       REG                8,1   135366   96993411 /lib/x86_64-linux-gnu/libpthread-2.15.so\ninit          1            root  mem       REG                8,1   276392   96996820 /lib/x86_64-linux-gnu/libdbus-1.so.3.5.8\ninit          1            root  mem       REG                8,1    38888   96996879 /lib/x86_64-linux-gnu/libnih-dbus.so.1.0.0\ninit          1            root  mem       REG                8,1    96240   96996881 /lib/x86_64-linux-gnu/libnih.so.1.0.0\ninit          1            root  mem       REG                8,1   149280   96997051 /lib/x86_64-linux-gnu/ld-2.15.so\ninit          1            root    0u      CHR                1,3      0t0       1029 /dev/null\ninit          1            root    1u      CHR                1,3      0t0       1029 /dev/null\ninit          1            root    2u      CHR                1,3      0t0       1029 /dev/null\ninit          1            root    3r     FIFO                0,8      0t0       3001 pipe\ninit          1            root    4w     FIFO                0,8      0t0       3001 pipe\ninit          1            root    5r     0000                0,9        0       6797 anon_inode\ninit          1            root    6r     0000                0,9        0       6797 anon_inode\ninit          1            root    7u     unix 0xffff88020e5cc680      0t0       7152 socket\ninit          1            root    8u     unix 0xffff8802127caa40      0t0      10936 socket\ninit          1            root    9u     unix 0xffff8802116623c0      0t0       1999 socket\ninit          1            root   10u     unix 0xffff880211663400      0t0      10692 socket\ninit          1            root   12w      REG               8,18     2664   12845346 /var/log/upstart/mysql.log.1 (deleted)\ninit          1            root   14u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   16u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   17u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   18u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   20w      REG               8,18     1342   12845172 /var/log/upstart/modemmanager.log.1 (deleted)\ninit          1            root   21u      CHR                5,2      0t0       7184 /dev/ptmx\nkthreadd      2            root  cwd       DIR                8,1     4096          2 /\nkthreadd      2            root  rtd       DIR                8,1     4096          2 /\nkthreadd      2            root  txt   unknown                                        /proc/2/exe\nksoftirqd     3            root  cwd       DIR                8,1     4096          2 /\nksoftirqd     3            root  rtd       DIR                8,1     4096          2 /\nksoftirqd     3            root  txt   unknown                                        /proc/3/exe\n...\napache2    2241            root  mem       REG                8,1  1852792   96996819 /lib/x86_64-linux-gnu/libcrypto.so.1.0.0\napache2    2241            root  mem       REG                8,1   374608   96996818 /lib/x86_64-linux-gnu/libssl.so.1.0.0\napache2    2241            root  mem       REG                8,1  1030512   96997045 /lib/x86_64-linux-gnu/libm-2.15.so\napache2    2241            root  mem       REG                8,1    66784   96996836 /lib/x86_64-linux-gnu/libbz2.so.1.0.4\napache2    2241            root  mem       REG                8,1  1518928   64756408 /usr/lib/x86_64-linux-gnu/libdb-5.1.so\napache2    2241            root  mem       REG                8,1   105288   96993414 /lib/x86_64-linux-gnu/libresolv-2.15.so\napache2    2241            root  mem       REG                8,1  8644728   65276931 /usr/lib/apache2/modules/libphp5.so\napache2    2241            root  mem       REG                8,1    34824   65276243 /usr/lib/apache2/modules/mod_negotiation.so\napache2    2241            root  mem       REG                8,1    18432   65276567 /usr/lib/apache2/modules/mod_mime.so\napache2    2241            root  mem       REG                8,1    10240   65276556 /usr/lib/apache2/modules/mod_env.so\napache2    2241            root  mem       REG                8,1    10240   65276178 /usr/lib/apache2/modules/mod_dir.so\napache2    2241            root  mem       REG                8,1    92720   96996948 /lib/x86_64-linux-gnu/libz.so.1.2.3.4\napache2    2241            root  mem       REG                8,1    22528   65276571 /usr/lib/apache2/modules/mod_deflate.so\napache2    2241            root  mem       REG                8,1    26624   65275788 /usr/lib/apache2/modules/mod_cgi.so\napache2    2241            root  mem       REG                8,1    34824   65276563 /usr/lib/apache2/modules/mod_autoindex.so\napache2    2241            root  mem       REG                8,1    10248   65275257 /usr/lib/apache2/modules/mod_authz_user.so\napache2    2241            root  mem       REG                8,1    10248   65276546 /usr/lib/apache2/modules/mod_authz_host.so\napache2    2241            root  mem       REG                8,1    10248   65275256 /usr/lib/apache2/modules/mod_authz_groupfile.so\napache2    2241            root  mem       REG                8,1     6152   65275193 /usr/lib/apache2/modules/mod_authz_default.so\napache2    2241            root  mem       REG                8,1    10248   65276547 /usr/lib/apache2/modules/mod_authn_file.so\napache2    2241            root  mem       REG                8,1    10248   65276545 /usr/lib/apache2/modules/mod_auth_basic.so\napache2    2241            root  mem       REG                8,1    14336   65275224 /usr/lib/apache2/modules/mod_alias.so\napache2    2241            root  mem       REG                8,1    14768   96993408 /lib/x86_64-linux-gnu/libdl-2.15.so\napache2    2241            root  mem       REG                8,1    18896   96996944 /lib/x86_64-linux-gnu/libuuid.so.1.3.0\napache2    2241            root  mem       REG                8,1   170024   96996871 /lib/x86_64-linux-gnu/libexpat.so.1.5.2\napache2    2241            root  mem       REG                8,1    43288   96997046 /lib/x86_64-linux-gnu/libcrypt-2.15.so\napache2    2241            root  mem       REG                8,1  1811128   96993409 /lib/x86_64-linux-gnu/libc-2.15.so\napache2    2241            root  mem       REG                8,1   135366   96993411 /lib/x86_64-linux-gnu/libpthread-2.15.so\napache2    2241            root  mem       REG                8,1   234720   64752153 /usr/lib/libapr-1.so.0.4.6\napache2    2241            root  mem       REG                8,1   142840   64752206 /usr/lib/libaprutil-1.so.0.3.12\napache2    2241            root  mem       REG                8,1   247896   96996910 /lib/x86_64-linux-gnu/libpcre.so.3.12.1\napache2    2241            root  mem       REG                8,1   149280   96997051 /lib/x86_64-linux-gnu/ld-2.15.so\napache2    2241            root  DEL       REG                0,4               11842 /dev/zero\napache2    2241            root    0r      CHR                1,3      0t0       1029 /dev/null\napache2    2241            root    1w      CHR                1,3      0t0       1029 /dev/null\napache2    2241            root    2w      REG               8,18    12640   12845390 /var/log/apache2/error.log\napache2    2241            root    3u     IPv4              14201      0t0        TCP *:http (LISTEN)\napache2    2241            root    4r     FIFO                0,8      0t0    7004026 pipe\napache2    2241            root    5w     FIFO                0,8      0t0    7004026 pipe\napache2    2241            root    6w      REG               8,18        0   12845132 /var/log/apache2/other_vhosts_access.log\napache2    2241            root    7w      REG               8,18   127987   12845338 /var/log/apache2/access.log\n\n</pre>\n\n:* [Q] what does \'node\' information mean in \'lsof\' output?\n\n\n* converged iotrace\n: join the information from the legacy blktrace data and lsof, inotify data\n\n\n\n=== SNIA IO Trace Data Files ===\n\n\n[http://iotta.snia.org/traces SNIA I/O Trace Data Files]\n\nThe categories (or types) of I/O traces include:\n\n* Application Traces [This category is currently empty]\n: Application Traces record calls made by a specific application.\n* Block I/O Traces\n: Block I/O Traces typically include block level (e.g., at the logical volume manager, disk driver, etc. level) and block protocol (e.g., SCSI, ATA, Fibre Channel) traces.\n* Historical Traces [This category is currently empty]\n: Historical Traces include all traces that 10 or more years of age.\n* NFS Traces\n: Network File System Traces are typically those for NFS and CIFS and which reflect the protocol used by such network file systems.\n* Parallel Traces\nParallel traces, generally taken from supercomputers, record the system calls made by multiple computers running in parallel.\nSSSI WIOCP Metrics\nSSSI WIOCP, the SNIA Solid State Storage Initiative (SSSI) Workload I/O Capture Program (WIOCP), collects already-summarized empirical metrics separately for both monitored devices and processes/applications.\nStatic Snapshots\nStatic Snapshots are traces taken statically of a file system rather than of system calls.\nSystem Call Traces\nSystem Call I/O Traces typically reflect operating system calls to the file system.\nTools\nHere you can find the tools used for reading the various trace files.\n\n\n==== MSR Cambridge Traces ====\n\n* [http://iotta.snia.org/traces/388 List of Traces]\n\n* MSR Cambridge Traces 1\n: 1-week block I/O Traces of enterprise servers at MSR Cambridge.\n: The citation for the MSRC traces can be found [http://static.usenix.org/event/fast08/tech/narayanan.html FAST 2008, \"Write Off-loading: Practical Power Management for Enterprise Storage\", Dushyanth Narayanan, Austin Donnelly, and Antony Rowstron, Microsoft Research Ltd.]\n\n\n\n==== Microsoft Enterprise Traces ====\n\nTraces collected at Microsoft using the event tracing for Windows framework.\n\n* [http://iotta.snia.org/traces/130 List of Traces]\n\n* TPCC Traces 1\n: TPC-C benchmark traces collected at Microsoft using the event tracing for Windows framework\n\n* TPCC Traces 2\n: TPC-C benchmark traces collected at Microsoft using the event tracing for Windows framework\n: These six 6-minute long traces were collected at various points during a TPC-C run, all of which were during periods of steady-state activity.\n\n* TPCE Traces\n: TPC-E benchmark traces collected at Microsoft using the event tracing for Windows framework\n: These 6 traces were collected during a ~ 84-minute TPC-E run which included a ~ 20-minute warm-up time.\n\n* Exchange Server Traces\n: Production traces collected at Microsoft using the event tracing for Windows framework\n: Collected for Exchange Server for a duration of 24 hours. The single tarball includes 96 trace files, each with a duration of 15 minutes.\n\n\n\n==== Microsoft Production Server Traces ====\n\n* [http://iotta.snia.org/traces/158 List of Traces]\n\n* BuildServer00 ~ BuildServer07\n: Traces of the 25 hours activity on the Microsoft Build Server\n* Development Tools Release\n: Collected for Developers Tools Release Server for a duration of 24 hours\n* Display Ads Data Server, Display Ads Payload Server\n: Collected over a period of 24 hours for Display Ads Data/Platform payload server\n* Live Maps Back End\n: Collected for LiveMaps back-end server for a duration of 24 hours\n* MSN Storage CFS\n: Collected for MSN Storage Metadata Server for a duration of 6 hours\n* MSN Storage File Server\n: Collected for MSN Storage file server for a duration of 6 hours\n* Radius Authentication\n: Collected for RADIUS authentication server\n* Radius Back End SQL Server\n: Collected for RADIUS back-end server\n\n=== 서울대 장병탁 교수님 세미나 ===\n\n* Hypernetwork ML/AI 기술\n:- [http://bi.snu.ac.kr/Courses/g-ai06_2/book-ch4-hypernetmemory-part3.pdf The Hypernetwork Model of Memory)]\n:- [http://bi.snu.ac.kr/Publications/Theses/BS12f_ChunHS.pdf 하이퍼네트워크 연상메모리 기반의 이미지-텍스트 교차검색 (Image-Text Crossmodal Retrieval via Hypernetwork Memory]\n:: 2nd wrong answer: (하이퍼네트워크 메모리 기반의 이미지-텍스트 교차검색)\n:: 1st wrong answer: (하이퍼네트워크 기반의 이미지-텍스트 연상 교차 검색)\n\n=== 상무님께서 보내주신 Storage 미래 관련 글들 ===\n\n\n* 기타\n\n: [http://wwpi.com/index.php?option=com_content&view=article&id=8158]\n\n: [http://lib.stanford.edu/files/pasig-jan2012/11B7%20Francis%20PASIG_2011_Francis_final.pdf]\n\n: [http://www.datarecoverygroup.com/articles/data-storage-history-and-future Data Storage History and Future]\n\n\n\n\n* 스토리지 미디어의 발전 전망\n: 우리가 스토리지 미디어를 개발하지는 않지만, 미디어의 발전 전망을 고려해서 소프트웨어의 미래를 전망해야겠지요.\n\n: [http://www-03.ibm.com/ibm/history/ibm100/us/en/icons/racetrack/ IBM100] \n\n: [http://static.usenix.org/events/fast02/coufal.pdf FAST 2002]\n\n\n\n\n* DNA를 데이터 스토리지로 이용하는 것에 관한 또 다른 글입니다. 장점/비용 이슈 언급됨.\n\n: [http://www.lifehacker.com.au/2013/01/is-dna-the-future-of-data-storage/ DNA를 data storage로 이용하기]\n\n \n* Storage의 미래\n\n: [http://blogs.computerworld.com/data-storage/20865/future-data-storage-revealed Future data storage revealed]\n\n: [http://blogs.computerworld.com/data-storage/21537/top-10-storage-predictions-back-future Top 10 storage predictions]\n\n: [http://blogs.computerworld.com/data-storage/21360/will-private-cloud-kill-storage-area-network Will Private Cloud Kill SAN?]\n\n== ## bNote-2013-03-07 ==\n\n\n=== Find X\'s ===\n\n==== System Event (esp., file system change) Tracing/Monitoring/Collecting ====\n\n* LTTng\n* DTrace\n* FTrace\n* Strace\n* SystemTap\n* inotify ***\n* FAM (File Alteration Monitor) [http://oss.sgi.com/projects/fam/]\n* Gamin (File and directory monitoring system defined to be a subset of the FAM system [http://people.gnome.org/~veillard/gamin/overview.html]\n\n----\n\n==== inotify ====\n\n* inotify - monitoring file system events\n: The inotify API provides a mechanism for monitoring file system events. Inotify can be used to monitor individual files, or to monitor directories. When a directory is monitored, inotify will return events for the directory itself, and for files inside the directory.\n: The following system calls are used with this API: inotify_init(2) (or inotify_init1(2)), inotify_add_watch(2), inotify_rm_watch(2), read(2), and close(2).\n\n* /proc interfaces\n: /proc/sys/fs/inotify/max_queued_events\n: /proc/sys/fs/inotify/max_user_instances\n: /proc/sys/fs/inotify/max_user_watches\n\n* Versions\n: Inotify was merged into the 2.6.13 Linux kernel. The required library interfaces were added to glibc in version 2.4. (IN_DONT_FOLLOW, IN_MASK_ADD, and IN_ONLYDIR were only added in version 2.5.)\n\n* Check whether inotify is enabled or not\n $ grep INOTIFY_USER /boot/config-$(uname -r)\n CONFIG_INOTIFY_USER=y\n\n* Installation on Ubuntu by apt-get\n: aptitude install inotify-tools python-inotifyx libinotifytools0-dev\n\n* inotify는 다음 event들에 대해서만 detection 가능함\n:* access\n:* modify\n:* attrib\n:: watched file에 대한 메타데이터가 변경되거나, watched directory 내의 file이 변경된 경우, \'attrib\' event가 발생\n:* close_write\n:* close_nowrite\n:* close\n:* open\n:* moved_to\n:* moved_from\n:* move\n:* move_self\n:: 이 event 이후에는 file or directory는 no longer being watched된다... 는데, delete event의 경우와 무엇이 다른가?\n:* create\n:* delete\n:* delete_self\n:* unmount\n\n\n\n\n* References\n# [http://www.infoq.com/articles/inotify-linux-file-system-event-monitoring InfoQ -- Inotify: Efficient, Real-time Linux File System Event Monitoring]\n# [http://www.ibm.com/developerworks/linux/library/l-ubuntu-inotify/index.html Monitor file system activity with inotify (B.GOOD)]\n# [http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=90586523eb4b349806887c62ee70685a49415124 git.kernel.org -- fsnotify: unified filesystem notification backend, 2009-05-21~2009-06-11]\n# [http://stackoverflow.com/questions/9614184/how-to-trace-per-file-io-operations-in-linux How to trace per-file IO operations in Linux? -- /proc/PID/fd/, systemtap, strace, fanotify]\n# [http://stackoverflow.com/questions/1835947/how-do-i-program-for-linuxs-new-fanotify-file-system-monitoring-feature Stackoverflow -- How do I program for Linux\'s new \'fanotify\' file system monitoring feature?]\n# [http://stackoverflow.com/questions/8381566/best-way-to-monitor-file-system-changes-in-linux Stackoverflow -- Best way to monitor file system changes in Linux]\n# [http://ubuntuforums.org/showthread.php?t=663950 python inotify example -- Ubuntu Forums]\n# [http://pyinotify.sourceforge.net/ Pyinotify: monitor filesystem events with Python under Linux - Brief Tutorial]\n# [http://github.com/seb-m/pyinotify pyinotify github]\n\n=== directory-file-addr spatial locality ===\n\n* Directory Hierarchy\n\n <pre>\nblusjune@jimi-hendrix:[dir_file_addr_spatial_locality] $ find r0\nr0\nr0/d1\nr0/d1/d13\nr0/d1/d11\nr0/d1/d12\nr0/d2\nr0/d2/d21\nr0/d2/d21/d212\nr0/d2/d21/d211\nr0/d2/d21/d213\nr0/d2/d22\nr0/d2/d22/d221\nr0/d2/d22/d222\n</pre>\n\n== ## bNote-2013-03-06 ==\n\n=== LBA-to-name processing (DELETEME) ===\n\nDone actually 2013-03-11 14:35. [http://www.expertslogin.com/general/how-to-fix-repair-bad-blocks-in-linux/ How To Fix / Repair Bad Blocks In Linux B.GOOD]\n\n\n* How to find the corresponding location (file/dir) for some LBA (e.g., 1661223128)?\n\n: step 1: We need to find the block size of the filesystem\n <pre>\nblusjune@radiohead:[google] $ sudo tune2fs -l /dev/sda1  | grep Block\nBlock count:              488378112\nBlock size:               4096\nBlocks per group:         32768\n</pre>\n\n: step 2: check the starting sector of the partition (file system)\n <pre>\nblusjune@radiohead:[google] $ sudo fdisk -lu /dev/sda\n\nDisk /dev/sda: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x00001eb9\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *        2048  3907026943  1953512448   83  Linux\n\n</pre>\n\n: step 3: File the filesystem block that contains this target LBA\n <pre>\nb = (int) ((L-S) * 512 / B)\n\n, where\nb = File system block number (we need this answer)\nB = File system block size in bytes (4096 - we got this in step 1)\nL = target LBA (1661223128 - given)\nS = Starting sector of the partition as shown by fdisk -lu (2048 - we got this in step 2)\n\n, so the answer b is 207652635 (207,652,635)\n\n</pre>\n\n: step 4: Check the file/dir name with given b (207652635)\n <pre>\nblusjune@radiohead:[s05] $ sudo debugfs /dev/sda1 \ndebugfs 1.42 (29-Nov-2011)\n\ndebugfs:  testb 207652635\nBlock 207652635 marked in use\n\ndebugfs:  icheck 207652635\nBlock	Inode number\n207652635	39585621\n\ndebugfs:  ncheck 39585621\nInode	Pathname\n39585621	/home/blusjune/.config/google-chrome/Default/Cookies-journal\n\ndebugfs:\n</pre>\n\n=== IOWA.sim.iox (myreal_72h) ===\n\n* base trace log on radiohead\n:- name: bpo_a.20130305_104633.real_whole_trace.log\n:- size: 197,372,058 bytes (197372058)\n\n <pre>\nblusjune@radiohead:[s05] $ pwd\n/x/var/iowa/sidewinder/iowa/iowa.sim.iox/tdir/s05\n\nblusjune@radiohead:[s05] $ l\ntotal 210572\ndrwxrwxr-x  2 blusjune blusjune      4096 Mar  6 11:07 ./\ndrwxrwxr-x 10 blusjune blusjune      4096 Mar  6 19:51 ../\nlrwxrwxrwx  1 blusjune blusjune        38 Mar  5 10:43 .bdx.0100.y.proc_after_trace_s10.sh -> ../.bdx.0100.y.proc_after_trace_s10.sh\n-rw-rw-r--  1 root     root     197372058 Mar  5 13:34 bpo_a.20130305_104633.real_whole_trace.log\n-rw-rw-r--  1 blusjune blusjune   4945483 Mar  6 11:06 tracelog.myrealtrace.log.A.addr\n-rw-rw-r--  1 blusjune blusjune   1081666 Mar  6 11:06 tracelog.myrealtrace.log.R.addr\n-rw-rw-r--  1 blusjune blusjune   3863817 Mar  6 11:06 tracelog.myrealtrace.log.W.addr\n-rw-rw-r--  1 blusjune blusjune   8339772 Mar  6 11:06 tracelog.myrealtrace.log.p1.out\n</pre>\n\n\n* R.addr analysis\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_200532.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  1\n__valu__sig__ _n_o_sigaddrs :  43683\n__valu__sig__ _sigioc_acc :  43683\n__valu__sig__ _sigaddrs_efficiency :  1.0\n__valu__sig__ _n_o_addr_total :  43683\n__valu__sig__ _ioc_total :  43683\n</pre>\n\n\n* W.addr analysis\n:- Used as weekly report item (2013-03-06), and lead to a patent\n::- just 18 addresses cover 25% of IO (40,010 IOs out of 157,632 IOs)\n::- x 2222.8 caching efficiency\n\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  1115\n__valu__sig__ _n_o_sigaddrs :  18\n__valu__sig__ _sigioc_acc :  40010\n__valu__sig__ _sigaddrs_efficiency :  2222.77777778\n__valu__sig__ _n_o_addr_total :  40979\n__valu__sig__ _ioc_total :  157632\n</pre>\n\n\n* Processes Contributed to the IO Workload\n\n <pre>\na1mjjung@secm:[myrealtrace.radiohead.v01] $ cat bpo_a.20130305_104633.real_whole_trace.log  | grep \'\\[\' | sed -e \'s/.*\\(\\[.*\\]\\)/\\1/g\' | sort -u\n[(null)]\n[(null)] 1\n[(null)] 3\n[(null)] 4\n[(null)] 6\n[(null)] 7\n[0]\n[BrowserBlocking]\n[BrowserBlocking] 1\n[Chrome_CacheThr]\n[Chrome_CacheThr] 1\n[Chrome_CacheThr] 10\n[Chrome_CacheThr] 2\n[Chrome_CacheThr] 3\n[Chrome_CacheThr] 6\n[Chrome_CacheThr] 9\n[Chrome_ChildIOT]\n[Chrome_DBThread]\n[Chrome_DBThread] 1\n[Chrome_DBThread] 2\n[Chrome_DBThread] 3\n[Chrome_DBThread] 4\n[Chrome_DBThread] 5\n[Chrome_DBThread] 6\n[Chrome_FileThre]\n[Chrome_FileThre] 1\n[Chrome_FileThre] 2\n[Chrome_FileThre] 3\n[Chrome_FileThre] 4\n[Chrome_HistoryT]\n[Chrome_HistoryT] 1a1mjjung@secm:[myrealtrace.radiohead.v01] $ cat bpo_a.20130305_104633.real_whole_trace.log  | grep \'\\[\' | sed -e \'s/.*\\(\\[.*\\]\\)/\\1/g\' | sort -u\n[(null)]\n[(null)] 1\n[(null)] 3\n[(null)] 4\n[(null)] 6\n[(null)] 7\n[0]\n[BrowserBlocking]\n[BrowserBlocking] 1\n[Chrome_CacheThr]\n[Chrome_CacheThr] 1\n[Chrome_CacheThr] 10\n[Chrome_CacheThr] 2\n[Chrome_CacheThr] 3\n[Chrome_CacheThr] 6\n[Chrome_CacheThr] 9\n[Chrome_ChildIOT]\n[Chrome_DBThread]\n[Chrome_DBThread] 1\n[Chrome_DBThread] 2\n[Chrome_DBThread] 3\n[Chrome_DBThread] 4\n[Chrome_DBThread] 5\n[Chrome_DBThread] 6\n[Chrome_FileThre]\n[Chrome_FileThre] 1\n[Chrome_FileThre] 2\n[Chrome_FileThre] 3\n[Chrome_FileThre] 4\n[Chrome_HistoryT]\n[Chrome_HistoryT] 1\n[Chrome_HistoryT] 10\n[Chrome_HistoryT] 11\n[Chrome_HistoryT] 12\n[Chrome_HistoryT] 14\n[Chrome_HistoryT] 15\n[Chrome_HistoryT] 16\n[Chrome_HistoryT] 2\n[Chrome_HistoryT] 3\n[Chrome_HistoryT] 4\n[Chrome_HistoryT] 5\n[Chrome_HistoryT] 6\n[Chrome_HistoryT] 7\n[Chrome_HistoryT] 8\n[Chrome_HistoryT] 9\n[Chrome_IOThread]\n[Chrome_IOThread] 1\n[Chrome_ProcessL]\n[Chrome_SafeBrow]\n[Chrome_SafeBrow] 1\n[Chrome_SafeBrow] 4\n[Chrome_SafeBrow] 5\n[Chrome_SyncThre]\n[Chrome_SyncThre] 1\n[Chrome_SyncThre] 10\n[Chrome_SyncThre] 11\n[Chrome_SyncThre] 12\n[Chrome_SyncThre] 13\n[Chrome_SyncThre] 14\n[Chrome_SyncThre] 15\n[Chrome_SyncThre] 16\n[Chrome_SyncThre] 2\n[Chrome_SyncThre] 3\n[Chrome_SyncThre] 4\n[Chrome_SyncThre] 5\n[Chrome_SyncThre] 6\n[Chrome_SyncThre] 7\n[Chrome_SyncThre] 8\n[Chrome_SyncThre] 9\n[Chrome_WebKitTh]\n[Chrome_WebKitTh] 1\n[MediaStreamDevi]\n[MediaStreamDevi] 1\n[NetworkChangeNo]\n[NetworkChangeNo] 1\n[WorkerPool/2786]\n[WorkerPool/2786] 1\n[WorkerPool/2787]\n[WorkerPool/2787] 1\n[WorkerPool/2788]\n[WorkerPool/2788] 1\n[WorkerPool/2839]\n[WorkerPool/2839] 1\n[WorkerPool/2840]\n[WorkerPool/2840] 1\n[WorkerPool/2840] 2\n[WorkerPool/2981]\n[WorkerPool/2981] 1\n[WorkerPool/2982]\n[WorkerPool/2982] 1\n[_BDX]\n[_BDX] 1\n[apache2]\n[apache2] 1\n[apache2ctl]\n[apache2ctl] 1\n[bash]\n[bash] 1\n[bash] 2\n[bash] 3\n[bash] 4\n[bash] 5\n[blktrace]\n[cat]\n[cat] 1\n[checkarray]\n[checkarray] 1\n[chrome]\n[chrome] 1\n[chrome] 2\n[chrome] 3\n[chrome] 4\n[console-kit-dae]\n[console-kit-dae] 1\n[console-kit-dae] 4\n[cp]\n[cp] 1\n[cron]\n[dbus-launch]\n[dbus-launch] 1\n[env]\n[env] 1\n[expr]\n[expr] 2\n[flush-8:0]\n[flush-8:0] 1\n[flush-8:0] 10\n[flush-8:0] 11\n[flush-8:0] 12\n[flush-8:0] 13\n[flush-8:0] 14\n[flush-8:0] 15\n[flush-8:0] 16\n[flush-8:0] 2\n[flush-8:0] 3\n[flush-8:0] 4\n[flush-8:0] 5\n[flush-8:0] 6\n[flush-8:0] 7\n[flush-8:0] 8\n[flush-8:0] 9\n[git-pull]\n[git-pull] 1\n[git]\n[git] 1\n[gnome-pty-helpe]\n[gnome-pty-helpe] 1\n[gnome-terminal]\n[gnome-terminal] 1\n[gnome-terminal] 2\n[gnome-terminal] 3\n[gnome-terminal] 4\n[gnome-terminal] 5\n[gnome-terminal] 7\n[google-chrome]\n[google-chrome] 1\n[inotify_reader]\n[inotify_reader] 1\n[jbd2/sda1-8]\n[jbd2/sda1-8] 1\n[jbd2/sda1-8] 16\n[jbd2/sda1-8] 2\n[jbd2/sda1-8] 3\n[jbd2/sda1-8] 4\n[jbd2/sda1-8] 5\n[kworker/0:1]\n[kworker/0:2]\n[kworker/2:1]\n[kworker/2:2]\n[kworker/3:0]\n[kworker/4:0]\n[kworker/4:1]\n[kworker/4:2]\n[kworker/5:0]\n[kworker/5:1]\n[kworker/5:2]\n[kworker/7:2]\n[libvirtd]\n[libvirtd] 1\n[libvirtd] 2\n[ls]\n[ls] 1\n[mysqld]\n[nmbd]\n[nmbd] 2\n[nmbd] 4\n[nmbd] 5\n[nmbd] 6\n[python]\n[python] 1\n[python] 2\n[python] 3\n[python] 4\n[python] 5\n[rserver]\n[rserver] 1\n[rserver] 2\n[rserver] 3\n[rserver] 6\n[rserver] 8\n[rsession]\n[rsession] 1\n[run-parts]\n[run-parts] 1\n[sh]\n[sh] 1\n[smbd]\n[smbd] 1\n[smbd] 2\n[smbd] 3\n[sshd]\n[sshd] 1\n[sshd] 4\n[sshd] 5\n[swapper/0]\n[udevd]\n[udevd] 1\n[update-motd-fsc]\n[update-motd-fsc] 1\n[update-motd-upd]\n[update-motd-upd] 1\n[upstart-udev-br]\n[upstart-udev-br] 1\n[xdg-settings]\n[xdg-settings] 1\n[Chrome_HistoryT] 10\n[Chrome_HistoryT] 11\n[Chrome_HistoryT] 12\n[Chrome_HistoryT] 14\n[Chrome_HistoryT] 15\n[Chrome_HistoryT] 16\n[Chrome_HistoryT] 2\n[Chrome_HistoryT] 3\n[Chrome_HistoryT] 4\n[Chrome_HistoryT] 5\n[Chrome_HistoryT] 6\n[Chrome_HistoryT] 7\n[Chrome_HistoryT] 8\n[Chrome_HistoryT] 9\n[Chrome_IOThread]\n[Chrome_IOThread] 1\n[Chrome_ProcessL]\n[Chrome_SafeBrow]\n[Chrome_SafeBrow] 1\n[Chrome_SafeBrow] 4\n[Chrome_SafeBrow] 5\n[Chrome_SyncThre]\n[Chrome_SyncThre] 1\n[Chrome_SyncThre] 10\n[Chrome_SyncThre] 11\n[Chrome_SyncThre] 12\n[Chrome_SyncThre] 13\n[Chrome_SyncThre] 14\n[Chrome_SyncThre] 15\n[Chrome_SyncThre] 16\n[Chrome_SyncThre] 2\n[Chrome_SyncThre] 3\n[Chrome_SyncThre] 4\n[Chrome_SyncThre] 5\n[Chrome_SyncThre] 6\n[Chrome_SyncThre] 7\n[Chrome_SyncThre] 8\n[Chrome_SyncThre] 9\n[Chrome_WebKitTh]\n[Chrome_WebKitTh] 1\n[MediaStreamDevi]\n[MediaStreamDevi] 1\n[NetworkChangeNo]\n[NetworkChangeNo] 1\n[WorkerPool/2786]\n[WorkerPool/2786] 1\n[WorkerPool/2787]\n[WorkerPool/2787] 1\n[WorkerPool/2788]\n[WorkerPool/2788] 1\n[WorkerPool/2839]\n[WorkerPool/2839] 1\n[WorkerPool/2840]\n[WorkerPool/2840] 1\n[WorkerPool/2840] 2\n[WorkerPool/2981]\n[WorkerPool/2981] 1\n[WorkerPool/2982]\n[WorkerPool/2982] 1\n[_BDX]\n[_BDX] 1\n[apache2]\n[apache2] 1\n[apache2ctl]\n[apache2ctl] 1\n[bash]\n[bash] 1\n[bash] 2\n[bash] 3\n[bash] 4\n[bash] 5\n[blktrace]\n[cat]\n[cat] 1\n[checkarray]\n[checkarray] 1\n[chrome]\n[chrome] 1\n[chrome] 2\n[chrome] 3\n[chrome] 4\n[console-kit-dae]\n[console-kit-dae] 1\n[console-kit-dae] 4\n[cp]\n[cp] 1\n[cron]\n[dbus-launch]\n[dbus-launch] 1\n[env]\n[env] 1\n[expr]\n[expr] 2\n[flush-8:0]\n[flush-8:0] 1\n[flush-8:0] 10\n[flush-8:0] 11\n[flush-8:0] 12\n[flush-8:0] 13\n[flush-8:0] 14\n[flush-8:0] 15\n[flush-8:0] 16\n[flush-8:0] 2\n[flush-8:0] 3\n[flush-8:0] 4\n[flush-8:0] 5\n[flush-8:0] 6\n[flush-8:0] 7\n[flush-8:0] 8\n[flush-8:0] 9\n[git-pull]\n[git-pull] 1\n[git]\n[git] 1\n[gnome-pty-helpe]\n[gnome-pty-helpe] 1\n[gnome-terminal]\n[gnome-terminal] 1\n[gnome-terminal] 2\n[gnome-terminal] 3\n[gnome-terminal] 4\n[gnome-terminal] 5\n[gnome-terminal] 7\n[google-chrome]\n[google-chrome] 1\n[inotify_reader]\n[inotify_reader] 1\n[jbd2/sda1-8]\n[jbd2/sda1-8] 1\n[jbd2/sda1-8] 16\n[jbd2/sda1-8] 2\n[jbd2/sda1-8] 3\n[jbd2/sda1-8] 4\n[jbd2/sda1-8] 5\n[kworker/0:1]\n[kworker/0:2]\n[kworker/2:1]\n[kworker/2:2]\n[kworker/3:0]\n[kworker/4:0]\n[kworker/4:1]\n[kworker/4:2]\n[kworker/5:0]\n[kworker/5:1]\n[kworker/5:2]\n[kworker/7:2]\n[libvirtd]\n[libvirtd] 1\n[libvirtd] 2\n[ls]\n[ls] 1\n[mysqld]\n[nmbd]\n[nmbd] 2\n[nmbd] 4\n[nmbd] 5\n[nmbd] 6\n[python]\n[python] 1\n[python] 2\n[python] 3\n[python] 4\n[python] 5\n[rserver]\n[rserver] 1\n[rserver] 2\n[rserver] 3\n[rserver] 6\n[rserver] 8\n[rsession]\n[rsession] 1\n[run-parts]\n[run-parts] 1\n[sh]\n[sh] 1\n[smbd]\n[smbd] 1\n[smbd] 2\n[smbd] 3\n[sshd]\n[sshd] 1\n[sshd] 4\n[sshd] 5\n[swapper/0]\n[udevd]\n[udevd] 1\n[update-motd-fsc]\n[update-motd-fsc] 1\n[update-motd-upd]\n[update-motd-upd] 1\n[upstart-udev-br]\n[upstart-udev-br] 1\n[xdg-settings]\n[xdg-settings] 1\n\n</pre>\n\n\n=== IOWA.sim.iox (tpcc_250gb_48h) ===\n\n* R.addr analysis\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_182112.sigio_25.iowsz_100.t1_10000] $ cat __simout.sigio_25.iowsz_100.t1_10000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  7\n__valu__sig__ _n_o_sigaddrs :  126606\n__valu__sig__ _sigioc_acc :  1169938\n__valu__sig__ _sigaddrs_efficiency :  9.24077847811\n__valu__sig__ _n_o_addr_total :  1691608\n__valu__sig__ _ioc_total :  4113312\n</pre>\n\n\n* W.addr analysis\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_190100.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  601\n__valu__sig__ _n_o_sigaddrs :  46304\n__valu__sig__ _sigioc_acc :  47940036\n__valu__sig__ _sigaddrs_efficiency :  1035.33249827\n__valu__sig__ _n_o_addr_total :  4910080\n__valu__sig__ _ioc_total :  191622453\n</pre>\n\n x39.02 = ( total_#_of_IOs / total_#_of_addrs_hit_actually )\n x1035.33 = ( 25%_sig_IOs / 25%_sig_addrs )\n\n=== R \'e1071\' package install (command line) ===\n\n <pre>\na1mjjung@secm:[e1071] $ R CMD INSTALL e1071\nJob <13412785> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura007>>\nWarning: invalid package \'e1071\'\nError: ERROR: no packages specified\na1mjjung@secm:[e1071] $ cd ..\nla1mjjung@secm:[e1071_build] $ l\ntotal 280\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:45 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\na1mjjung@secm:[e1071_build] $ R CMD INSTALL e1071\nJob <13412786> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n* installing to library \'/home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15\'\n* installing *source* package \'e1071\' ...\n** package \'e1071\' successfully unpacked and MD5 sums checked\nchecking for C++ compiler default output file name... a.out\nchecking whether the C++ compiler works... yes\nchecking whether we are cross compiling... no\nchecking for suffix of executables... \nchecking for suffix of object files... o\nchecking whether we are using the GNU C++ compiler... yes\nchecking whether g++ accepts -g... yes\n** libs\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c Rsvm.c -o Rsvm.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cmeans.c -o cmeans.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cshell.c -o cshell.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c floyd.c -o floyd.o\ng++ -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c svm.cpp -o svm.o\ng++ -shared -L/usr/local/lib64 -o e1071.so Rsvm.o cmeans.o cshell.o floyd.o svm.o\ninstalling to /home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15/e1071/libs\n** R\n** inst\n** preparing package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n   \'svmdoc.Rnw\' \n** testing if installed package can be loaded\n\n* DONE (e1071)\na1mjjung@secm:[e1071_build] $ R\nJob <13412787> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n\nR version 2.15.2 (2012-10-26) -- \"Trick or Treat\"\nCopyright (C) 2012 The R Foundation for Statistical Computing\nISBN 3-900051-07-0\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType \'license()\' or \'licence()\' for distribution details.\n\nR is a collaborative project with many contributors.\nType \'contributors()\' for more information and\n\'citation()\' on how to cite R or R packages in publications.\n\nType \'demo()\' for some demos, \'help()\' for on-line help, or\n\'help.start()\' for an HTML browser interface to help.\nType \'q()\' to quit R.\n\n> ?svm\nNo documentation for \'svm\' in specified packages and libraries:\nyou could try \'??svm\'\n> library(e1071) \nLoading required package: class\n> ?svm\n> \nSave workspace image? [y/n/c]: y\na1mjjung@secm:[e1071_build] $ pwd\n/home/X0101/a1mjjung/x/R/e1071_build\na1mjjung@secm:[e1071_build] $ ls -alF\ntotal 284\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:47 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\n-rw------- 1 a1mjjung X0101     25 Mar  6 18:47 .Rhistory\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:46 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\n\n</pre>\n\n\n\n\n=== 3rd meeting with Dr. CHOI ===\n\n\n\n=== 수퍼컴 (supercom) ===\n\n\n* account\n: ID: a1mjjung\n: PW: wjdaudwns\n: IP address: 202.20.183.10 (ssh)\n\n* password change\n: 한지연 사원 (jiyoun92.han@partner.samsung.com) (031-280-8147)\n\n* python 2.7.x from 2.6.x\n: [a1mjjung@login03 ~]$ /apps/Python/Python-2.7.3/bin/python\n\n== ## bNote-2013-03-05 ==\n\n=== 최희열 전문과 2차 미팅 ===\n\n* IO trace (궤적)의 변화 예측\n: IO 궤적의 변화를 신속 정확하게 예측하는 것의 중요성\n:: 언제 hot data가 fast-tier에 올라가야 할지,\n:: 현재의 hot data 말고 새로운 hot data는 언제 올라가야할지?\n:: 현재 fast-tier에 올라가 있는 hot data가 언제 unload되어야 할지?\n\ncached/tiered hot data가 계속 hot storage area에 \n: IO hot zone 궤적 변화의 전조로 사용될 수 있을만한 시스템 정보/상태 변화\n: IO hot zone 궤적이 정말로 변화하는 것일지 아니면 기존 궤적에서의 fluctuation인지?\n\n* Challenging Problems\n\n: 단위 시간 당 hot zone의 갯수가 항상 유지되는 것은 아님. 즉, hot zone의 갯수는 1개로 유지된 채, hot zone의 위치만 이동하는 형태는 아닐 수 있음. hot zone의 갯수가 1개였다가 4개로, 7개로, 줄어서 5개로, 다시 줄어서 2개로, 이런 식으로\n\n== ## bNote-2013-03-05 ==\n\n=== IOWA to ML Formulation ===\n\n* IO trace (궤적)의 변화 예측\n: IO 궤적의 변화를 신속 정확하게 예측하는 것의 중요성\n:: 언제 hot data가 fast-tier에 올라가야 할지,\n:: 현재의 hot data 말고 새로운 hot data는 언제 올라가야할지?\n:: 현재 fast-tier에 올라가 있는 hot data가 언제 unload되어야 할지?\n\ncached/tiered hot data가 계속 hot storage area에 \n: IO hot zone 궤적 변화의 전조로 사용될 수 있을만한 시스템 정보/상태 변화\n: IO hot zone 궤적이 정말로 변화하는 것일지 아니면 기존 궤적에서의 fluctuation인지?\n\n* Challenging Problems\n\n: 단위 시간 당 hot zone의 갯수가 항상 유지되는 것은 아님. 즉, hot zone의 갯수는 1개로 유지된 채, hot zone의 위치만 이동하는 형태는 아닐 수 있음. hot zone의 갯수가 1개였다가 4개로, 7개로, 줄어서 5개로, 다시 줄어서 2개로, 이런 식으로','utf-8'),(67,'== 20130530_134618 ==\n=== Introduction to Association Rule Mining ===\n\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n\n\n\n=== Frequent patterns/items mining examples with R::arules ===\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n* References\n:# [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))]\n:# [http://www.analysis-of-patterns.net/files/bgoethals.pdf Frequent Pattern Mining Slides - Universiteit Antwerpen]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning - Wikipedia]\n:# [http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf Frequent Pattern Mining in Web Log Data]\n:# [http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010 Temporal Pattern Mining Tutorial KDD 2010]\n:# [http://cran.r-project.org/web/packages/arules/index.html arules: Mining Association Rules and Frequent Itemsets]\n:# [http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf Introduction to arules - A computational environment for mining association rules and frequent item sets -- http://cran.r-project.org/]\n:# [http://lyle.smu.edu/IDA/arules/ Intelligent Data Analysis Lab @ SMU]\n:# [http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))]\n:# [http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/ Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning]\n:# [http://en.wikipedia.org/wiki/Sequence_mining Sequence mining]\n:# [http://en.wikipedia.org/wiki/GSP_Algorithm GSP Algorithm]\n:# [http://en.wikipedia.org/wiki/Apriori_algorithm Apriori algorithm]\n:# [http://en.wikipedia.org/wiki/Market_basket_analysis Market basket analysis]\n:# [http://en.wikipedia.org/wiki/Time_series Time series]\n\n\n----','utf-8'),(68,'== Introduction to Association Rule Mining ==\n\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n\n\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* [http://www.analysis-of-patterns.net/files/bgoethals.pdf Frequent Pattern Mining Slides - Universiteit Antwerpen]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning - Wikipedia]\n:# [http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf Frequent Pattern Mining in Web Log Data]\n:# [http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010 Temporal Pattern Mining Tutorial KDD 2010]\n:# [http://cran.r-project.org/web/packages/arules/index.html arules: Mining Association Rules and Frequent Itemsets]\n:# [http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf Introduction to arules - A computational environment for mining association rules and frequent item sets -- http://cran.r-project.org/]\n:# [http://lyle.smu.edu/IDA/arules/ Intelligent Data Analysis Lab @ SMU]\n:# [http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))]\n:# [http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/ Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning]\n:# [http://en.wikipedia.org/wiki/Sequence_mining Sequence mining]\n:# [http://en.wikipedia.org/wiki/GSP_Algorithm GSP Algorithm]\n:# [http://en.wikipedia.org/wiki/Apriori_algorithm Apriori algorithm]\n:# [http://en.wikipedia.org/wiki/Market_basket_analysis Market basket analysis]\n:# [http://en.wikipedia.org/wiki/Time_series Time series]\n\n\n----','utf-8'),(69,'== Introduction to Association Rule Mining ==\n\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n\n\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(70,'== Introduction to Association Rule Mining ==\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(71,'\n== 20130530_144254 ==\n\n=== Introductory Articles to R Statistical Computing Software ===\n\n* http://www.cran.r-project.org/doc/manuals/R-intro.pdf\n: An Introduction to R -- Notes on R: A Programming Environment for Data Analysis and Graphics (Version 3.0.1 (2013-05-16))\n\n\n\n\n== 20130528_213910 ==\n\n=== suppress the command output in R ===\n\n* use sink() function, please!\n <pre>\nsink(file=\"arm_inspect.650k-support_0.012-225x488.log\") # enabling sink operation \n# (to redirect all the stdout to the file specified)\n\ninspect(f_650k_0012)\n\nsink() # disabling sink operation\n# (after this command, you can see the output message to stdout)\n</pre>\n\n\n* References\n\n:* https://stat.ethz.ch/pipermail/r-help/2007-August/138070.html\n:: R sink behavior (stat.ethz.ch)\n\n:* http://stat.ethz.ch/R-manual/R-patched/library/base/html/sink.html\n:: Send R Output to a File\n\n\n\n\n=== defining function in R (user-defined function in R) ===\n\n* use allocation operator \'<-\' to define my custom function\n <pre>\niowa_arm_f <- function (opt_dsrc, opt_support) {\n	print(\">> IOWA ARM: started\");\n	sink(file=\"/dev/null\");\n	t <- as(opt_dsrc, \"transactions\");\n	f <- eclat(opt_dsrc, parameter=list(support=opt_support, tidLists=T));\n	sink();\n	print(\">> IOWA ARM: finished\");\n	dim(tidLists(f));\n	return(f);\n}\n</pre>\n\n* use the function as usual\n <pre>\nf <- iowa_arm_f(ciop_d010, 0.012);\ndim(tidLists(f));\n</pre>\n\n* References\n\n:* http://www.statmethods.net/management/userfunctions.html\n:: User-written Functions -- Quick-R ((B.GOOD))\n\n\n\n== 20130524_103604 ==\n\n\n=== source() ===\n\n <pre>\nsource(\'data_set.R\');\n</pre>\n\n=== arules eclat() algorithm for association rules mining ===\n\n <pre>\nlibrary(arules);\ndata <- list(\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_1187\", \"_addr_23813\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\n			 c(\"_addr_23813\", \"_addr_51276758\"),\n			 c(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\n			 c(\"_addr_51276758\", \"_addr_319275\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\n			 c(\"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\n			 c(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\n			 c(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\n			 c(\"_addr_319275\", \"_addr_51276758\")\n			 );\n\nt <- as(data, \"transactions\");\nf <- eclat(data, parameter=list(tidLists=T, support=0.25))\ndim(tidLists(f))\nas(tidLists(f), \"list\")\nimage(tidLists(f))\ninspect(f)\n</pre>\n\n\n\n\n=== read.table(), write.table(), unlist() ===\n\n <pre>\nblusjune@jimi-hendrix:[basics] $ cat filein.L20 \n\n248425 , 100276969472\n248772 , 78847959040\n249197 , 139913195520\n251828 , 75536334848\n253182 , 76534030336\n254083 , 143595069440\n254961 , 143598755840\n255185 , 150857949184\n255393 , 118433374208\n255755 , 100324941824\n256025 , 85407301632\n258666 , 95264866304\n259078 , 142196629504\n261133 , 88597774336\n263287 , 97312505856\n264135 , 112585678848\n267323 , 96259047424\n267351 , 140665122816\n267634 , 139540049920\n268982 , 117314224128\n</pre>\n\n <pre>\n> datain;\n\n   timestamp      address\n   1     248425 100276969472\n   2     248772  78847959040\n   3     249197 139913195520\n   4     251828  75536334848\n   5     253182  76534030336\n   6     254083 143595069440\n   7     254961 143598755840\n   8     255185 150857949184\n   9     255393 118433374208\n   10    255755 100324941824\n   11    256025  85407301632\n   12    258666  95264866304\n   13    259078 142196629504\n   14    261133  88597774336\n   15    263287  97312505856\n   16    264135 112585678848\n   17    267323  96259047424\n   18    267351 140665122816\n   19    267634 139540049920\n   20    268982 117314224128\n</pre>\n\n <pre>\ndatain <- read.table(\'filein.L20\', col.names=c(\"timestamp\", \"address\"), sep=\",\", header=F);\ndataout <- datain %% 10;\ndataout_ul <- unlist(dataout, use.names=F)\nwrite.table(dataout, file=\"fileout.L20\", append=F, quote=T, sep=\" , \", row.names=F, col.names=T);\n</pre>\n\n\n\n\n=== scan() ===\n\n <pre>\ndatain_double <- scan(file=\"d010\", what=double(), sep=\"\", strip.white=T);\ndatain_char <- scan(file=\"d010\", what=\"character\", sep=\"\\n\", strip.white=T);\ndatain_char <- scan(file=\"d010\", what=\"char\", sep=\";\", strip.white=T);\n</pre>\n\n\n\n\n* Case 1: \'d010\' - data file of wrong format\n\n <pre>\nblusjune@jimi-hendrix:[basics] $ cat d010\n8 4 0 5 7;\n11 5 8;\n8 6 3 5 1;\n2 11 1;\n4 6 3 12 4;\n6 7 0 10 4 7;\n7 6 3;\n3 10 7 6 7 6;\n7 10;\n11 10\n</pre>\n\n </pre>\n> scan(file=\'d010\', what=\"numeric\", sep=\";\\n\", strip.white=T)\nError in scan(file = \"d010\", what = \"numeric\", sep = \";\\n\", strip.white = T) : \n  invalid \'sep\' value: must be one byte\n\n> scan(file=\'d010\', what=\"numeric\", sep=\"\\n\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7;\"    \"11 5 8;\"       \"8 6 3 5 1;\"    \"2 11 1;\"      \n [5] \"4 6 3 12 4;\"   \"6 7 0 10 4 7;\" \"7 6 3;\"        \"3 10 7 6 7 6;\"\n [9] \"7 10;\"         \"11 10\"        \n\n> scan(file=\'d010\', what=\"numeric\", sep=\";\", strip.white=T)\nRead 19 items\n [1] \"8 4 0 5 7\"    \"\"             \"11 5 8\"       \"\"             \"8 6 3 5 1\"   \n [6] \"\"             \"2 11 1\"       \"\"             \"4 6 3 12 4\"   \"\"            \n[11] \"6 7 0 10 4 7\" \"\"             \"7 6 3\"        \"\"             \"3 10 7 6 7 6\"\n[16] \"\"             \"7 10\"         \"\"             \"11 10\"       \n\n> scan(file=\'d010\', what=\"numeric\", sep=\"\", strip.white=T)\nRead 40 items\n [1] \"8\"   \"4\"   \"0\"   \"5\"   \"7;\"  \"11\"  \"5\"   \"8;\"  \"8\"   \"6\"   \"3\"   \"5\"  \n[13] \"1;\"  \"2\"   \"11\"  \"1;\"  \"4\"   \"6\"   \"3\"   \"12\"  \"4;\"  \"6\"   \"7\"   \"0\"  \n[25] \"10\"  \"4\"   \"7;\"  \"7\"   \"6\"   \"3;\"  \"3\"   \"10\"  \"7\"   \"6\"   \"7\"   \"6;\" \n[37] \"7\"   \"10;\" \"11\"  \"10\" \n\n</pre>\n\n\n\n\n* Case 2: \'d020\' - data file of good format\n\n <pre>\nblusjune@jimi-hendrix:[basics] $ cat d020\n8 4 0 5 7; 11 5 8; 8 6 3 5 1; 2 11 1; 4 6 3 12 4; 6 7 0 10 4 7; 7 6 3; 3 10 7 6 7 6; 7 10; 11 10\n</pre>\n\n <pre>\n> scan(file=\'d020\', what=\"numeric\", sep=\"\", strip.white=T)\nRead 40 items\n [1] \"8\"   \"4\"   \"0\"   \"5\"   \"7;\"  \"11\"  \"5\"   \"8;\"  \"8\"   \"6\"   \"3\"   \"5\"  \n[13] \"1;\"  \"2\"   \"11\"  \"1;\"  \"4\"   \"6\"   \"3\"   \"12\"  \"4;\"  \"6\"   \"7\"   \"0\"  \n[25] \"10\"  \"4\"   \"7;\"  \"7\"   \"6\"   \"3;\"  \"3\"   \"10\"  \"7\"   \"6\"   \"7\"   \"6;\" \n[37] \"7\"   \"10;\" \"11\"  \"10\" \n\n> scan(file=\'d020\', what=\"numeric\", sep=\";\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7\"    \"11 5 8\"       \"8 6 3 5 1\"    \"2 11 1\"       \"4 6 3 12 4\"  \n [6] \"6 7 0 10 4 7\" \"7 6 3\"        \"3 10 7 6 7 6\" \"7 10\"         \"11 10\"     \n</pre>\n\n\n\n\n* Case 3: \'d030\' - data file of good format\n\n <pre>\n8 4 0 5 7\n11 5 8\n8 6 3 5 1\n2 11 1\n4 6 3 12 4\n6 7 0 10 4 7\n7 6 3\n3 10 7 6 7 6\n7 10\n11 10\n</pre>\n\n <pre>\n> scan(file=\'d030\', what=\'numeric\', sep=\"\", strip.white=T)\nRead 40 items\n [1] \"8\"  \"4\"  \"0\"  \"5\"  \"7\"  \"11\" \"5\"  \"8\"  \"8\"  \"6\"  \"3\"  \"5\"  \"1\"  \"2\"  \"11\"\n[16] \"1\"  \"4\"  \"6\"  \"3\"  \"12\" \"4\"  \"6\"  \"7\"  \"0\"  \"10\" \"4\"  \"7\"  \"7\"  \"6\"  \"3\" \n[31] \"3\"  \"10\" \"7\"  \"6\"  \"7\"  \"6\"  \"7\"  \"10\" \"11\" \"10\"\n\n> scan(file=\'d030\', what=\'numeric\', sep=\";\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7\"    \"11 5 8\"       \"8 6 3 5 1\"    \"2 11 1\"       \"4 6 3 12 4\"  \n [6] \"6 7 0 10 4 7\" \"7 6 3\"        \"3 10 7 6 7 6\" \"7 10\"         \"11 10\"       \n\n> scan(file=\'d030\', what=\'numeric\', sep=\"\\n\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7\"    \"11 5 8\"       \"8 6 3 5 1\"    \"2 11 1\"       \"4 6 3 12 4\"  \n [6] \"6 7 0 10 4 7\" \"7 6 3\"        \"3 10 7 6 7 6\" \"7 10\"         \"11 10\"  \n</pre>\n\n\n\n\n== 20130502_032415 ==\n\n=== R script ===\n\n <pre>\na1mjjung@secm:[r_stat] $ cat > iowa_anal_1010.R << EOF\n#iowa_anal_1010.R\n#20130430_140506\n\n\nlibrary(e1071);\nlibrary(arules);\nlibrary(scatterplot3d);\n\n\nrm(list=ls());\n\n\nmyd <- read.table(\'r_stat_infile\', col.names=c(\"_hitcnt_\", \"_addr_\", \"_mwid_\"));\nmyd_mwid <- unlist(myd[3]);\nmyd_addr <- unlist(myd[2]);\nmyd_hitcnt <- unlist(myd[1]);\n\n\nmymat <- cbind(myd_mwid, myd_addr, myd_hitcnt);\ncolnames(mymat) <- c(\"_mwid_\", \"_addr_\", \"_hcnt_\");\nnocl <- 22; itmax <- 100; (cl1 <- kmeans(mymat, nocl, iter.max=itmax));\n\n\npng(\'output_col_by_hitcnt.png\', width=1024, height=2048);\nplot(myd_mwid, myd_addr, col=myd_hitcnt);\ndev.off();\npng(\'output_col_by_cluster.png\', width=1024, height=2048);\nplot(myd_mwid, myd_addr, col=cl1$cluster);\ndev.off();\nEOF\n\n</pre>\n\n\n\n== 20130428_233708 ==\n\n\n\n=== R (r_stat) references ===\n\n\n* manual/introduction to R\n:- [http://cran.r-project.org/doc/manuals/R-intro.pdf R introduction]\n:- [http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf arules (Association Rules Mining)]\n:- [http://www.cl.cam.ac.uk/~av308/vlachos_msc_thesis.pdf using SVM]\n\n\n* \'%in%\' and example() function\n\n <pre>\nOn 8 May 2011 21:18, Berwin A Turlach <[hidden email]> wrote:\n> G\'day Dan,\n>\n> On Sun, 8 May 2011 05:06:27 -0400\n> Dan Abner <[hidden email]> wrote:\n>\n>> Hello everyone,\n>>\n>> I am attempting to use the %in% operator with the ! to produce a NOT\n>> IN type of operation. Why does this not work? Suggestions?\n\nAlternatively,\n\nexample(`%in%`)\n\nor\n\n`%ni%` = Negate(`%in%`)\n\nHTH,\n\nbaptiste\n</pre>\n\n\n\n\n=== kmeans-svm #2 :: applying the kmeans result to svm (as a guideline for supervising) ((B.GOOD)) ===\n\n <pre>\n\n# combining the columns xy (time_t1, file_id) and cl4 (cluster_id)\nxy_cl4 <- cbind(xy, cl4$cluster);\n\n# assign the column names\ncolnames(xy_cl4) <- c(\"time_t1\", \"file_id\", \"cluster_id\");\n\n# prepare the x\'s and y for svm\nsmv_x <- subset(xy_cl4, select= -cluster_id);\nsmv_y <- subset(xy_cl4, select= cluster_id);\n\n# load the library \'e1071\'\nlibrary(e1071);\n\n# execute svm analysis to get model \'m\'\nm <- svm(svm_x, svm_y);\n\n# predict a new value with the model \'m\' and x\'s data\npred <- predict(m, svm_x, decision.values = T);\n\n# check the prediction result by comparing the predicted value \'pred\' with original value \'svm_y\'\ntable(pred, svm_y);\n\n# plot the result\nplot(cmdscale(dist(xy_cl4[, -3])), col=pred, pch=c(\"o\", \"+\")[1:120 %in% m$index + 1]);\n\n# write the resultant xy_cl4 table to the file(.csv)\nwrite.csv(xy_cl4, file=\"xy_cl4.csv\", row.names=T, col.names=T);\n\n</pre>\n\n\n\n\n=== kmeans-svm #1 :: kmeans example ((B.GOOD)) ===\n\n <pre>\n\n# clear all the data in memory\nrm(list=ls());\n\n# read the data from file (.csv format)\nmyd <- read.csv(\'traces/iowa_v3.csv\', header=T);\n\n# unlist() to convert \'list\' type data to \'vector\' type data for further numerical calculation\nmyd_c1 <- unlist(myd[1]); # field: time_t0\nmyd_c2 <- unlist(myd[2]); # field: time_t1\nmyd_c3 <- unlist(myd[3]); # field: prog\nmyd_c4 <- unlist(myd[4]); # field: file\n\n# create 2-D matrix (x-y) for 2-D kmeans analysis\nxy <- cbind(myd_c2, myd_c4); # combining columns: \'time_t1\' and \'file\'\n\n# assign the column names for x-label and y-label for better look of plot()\ncolnames(xy) <- c(\"time_t1\", \"file_id\");\n\n# load the library \'e1071\'\nlibrary(e1071);\n\n# execute kmeans analysis until we get satisfactory clustering performance value (value = between_SS / total_SS)\n# number of clusters is set to \'6\' huristically after plotting data xy to see the outlook\n# number of maximum iterations is set to \'100\' huristically after executing kmeans analysis several times\ncl1 <- kmeans(xy, 6, iter.max=100); # initial trial, the clustering performance value is ... (cannot evaluate)\ncl2 <- kmeans(xy, 6, iter.max=100); # second trial, the clustering performance value is higher than the first trial\ncl3 <- kmeans(xy, 6, iter.max=100); # third trial, the clustering performance value is higher than the second trial\n(cl4 <- kmeans(xy, 6, iter.max=100)); # fourth trial, the clustering performance value is not going higher than the last trial\n\n# 2-D plotting\nplot(xy, col=cl4$cluster);\n\n</pre>\n\n\n\n\n* most outer parentheses are used to see the result of execution\n\n <pre>\n\n> (cl4 <- kmeans(xy, 6, iter.max=100))\nK-means clustering with 6 clusters of sizes 19, 22, 19, 15, 20, 25\n\nCluster means:\n   time_t1    file_id\n1 10.52632 32.2105263\n2 14.40909 43.3181818\n3 18.57895 33.5263158\n4 22.00000  0.2666667\n5  6.50000 11.7000000\n6  2.00000  1.0000000\n\nClustering vector:\n  time_t11   time_t12   time_t13   time_t14   time_t15   time_t16   time_t17 \n         6          6          6          6          5          5          5 \n  time_t18   time_t19  time_t110  time_t111  time_t112  time_t113  time_t114 \n         5          1          2          1          1          2          2 \n time_t115  time_t116  time_t117  time_t118  time_t119  time_t120  time_t121 \n         2          2          3          3          3          3          4 \n time_t122  time_t123  time_t124  time_t125  time_t126  time_t127  time_t128 \n         4          4          6          6          6          6          6 \n time_t129  time_t130  time_t131  time_t132  time_t133  time_t134  time_t135 \n         5          5          5          5          1          1          1 \n time_t136  time_t137  time_t138  time_t139  time_t140  time_t141  time_t142 \n         1          2          2          2          2          2          3 \n time_t143  time_t144  time_t145  time_t146  time_t147  time_t148  time_t149 \n         3          3          4          4          4          6          6 \n time_t150  time_t151  time_t152  time_t153  time_t154  time_t155  time_t156 \n         6          6          6          5          5          5          5 \n time_t157  time_t158  time_t159  time_t160  time_t161  time_t162  time_t163 \n         1          1          1          1          2          2          2 \n time_t164  time_t165  time_t166  time_t167  time_t168  time_t169  time_t170 \n         2          3          3          3          3          4          4 \n time_t171  time_t172  time_t173  time_t174  time_t175  time_t176  time_t177 \n         4          6          6          6          6          6          5 \n time_t178  time_t179  time_t180  time_t181  time_t182  time_t183  time_t184 \n         5          5          5          1          1          1          1 \n time_t185  time_t186  time_t187  time_t188  time_t189  time_t190  time_t191 \n         2          2          2          2          3          3          3 \n time_t192  time_t193  time_t194  time_t195  time_t196  time_t197  time_t198 \n         3          4          4          4          6          6          6 \n time_t199 time_t1100 time_t1101 time_t1102 time_t1103 time_t1104 time_t1105 \n         6          6          5          5          5          5          1 \ntime_t1106 time_t1107 time_t1108 time_t1109 time_t1110 time_t1111 time_t1112 \n         1          1          1          2          2          2          2 \ntime_t1113 time_t1114 time_t1115 time_t1116 time_t1117 time_t1118 time_t1119 \n         3          3          3          3          4          4          4 \ntime_t1120 \n         6 \n\nWithin cluster sum of squares by cluster:\n[1]  51.89474 212.09091 117.36842  14.93333  53.20000 122.00000\n (between_SS / total_SS =  98.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"        \n> \n\n</pre>\n\n\n\n\n=== example() function in R ((B.GOOD)) ===\n\nexample() function is very very very useful to grasp the quick overview of some \'concept\' or \'function\' in R\n\n <pre>\n\nexample(svm)\nexample(kmeans)\nexample(\'%in%\')\nexample(rm)\nexample(plot)\n\n</pre>\n\n\n\n\n=== convert list to vector ===\n\nNote: the first line of \'iowa_v3.csv\' file should be the header information line (not the empty line containing just \',\')\n\n <pre>\nx_as_list_type <- read.csv(\'traces/iowa_v3.csv\', header=T)\nx_as_int_vector <- unlist(x_as_list_type, use.name=F)\n</pre>\n\n\n\n\n== 20130329_111903 ==\n\n=== R script file execution from command line ===\n\n$ cat > ex1.R << EOF\n <pre>\nlibrary(\"arules\")\ndata(\"Epub\")\nEpub\nsummary(Epub)\nquit(save=\"no\")\n\n## NOTE:\n# the last line \'quit(save=\"no\")\' is very important\n# to avoid the hang-like situation of R\nEOF\n</pre>\n\n$ R < ex1.R\n <pre>\nJob <13833365> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura009>>\n\nR version 2.15.2 (2012-10-26) -- \"Trick or Treat\"\nCopyright (C) 2012 The R Foundation for Statistical Computing\nISBN 3-900051-07-0\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType \'license()\' or \'licence()\' for distribution details.\n\nR is a collaborative project with many contributors.\nType \'contributors()\' for more information and\n\'citation()\' on how to cite R or R packages in publications.\n\nType \'demo()\' for some demos, \'help()\' for on-line help, or\n\'help.start()\' for an HTML browser interface to help.\nType \'q()\' to quit R.\n\n[Previously saved workspace restored]\n\n> Loading required package: Matrix\nLoading required package: lattice\n\nAttaching package: \'arules\'\n\nThe following object(s) are masked from \'package:base\':\n\n    %in%, write\n\n> > transactions in sparse format with\n 15729 transactions (rows) and\n 936 items (columns)\n> transactions as itemMatrix in sparse format with\n 15729 rows (elements/itemsets/transactions) and\n 936 columns (items) and a density of 0.001758755 \n\nmost frequent items:\ndoc_11d doc_813 doc_4c6 doc_955 doc_698 (Other) \n    356     329     288     282     245   24393 \n\nelement (itemset/transaction) length distribution:\nsizes\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n11615  2189   854   409   198   121    93    50    42    34    26    12    10 \n   14    15    16    17    18    19    20    21    22    23    24    25    26 \n   10     6     8     6     5     8     2     2     3     2     3     4     5 \n   27    28    30    34    36    38    41    43    52    58 \n    1     1     1     2     1     2     1     1     1     1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.646   2.000  58.000 \n\nincludes extended item information - examples:\n   labels\n1 doc_11d\n2 doc_13d\n3 doc_14c\n\nincludes extended transaction information - examples:\n      transactionID           TimeStamp\n10792  session_4795 2003-01-02 10:59:00\n10793  session_4797 2003-01-02 21:46:01\n10794  session_479a 2003-01-03 00:50:38\n> \n</pre>\n\n\n=== paste() example ===\n\n* example 1\n <pre>\n> 1:6\n[1] 1 2 3 4 5 6\n> paste(1:6)\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n> paste(\"A\", 1:6)\n[1] \"A 1\" \"A 2\" \"A 3\" \"A 4\" \"A 5\" \"A 6\"\n> paste(\"A\", 1:6, sep=\"\")\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\" \"A6\"\n> 2:7\n[1] 2 3 4 5 6 7\n> seq(8,3,by=-1)\n[1] 8 7 6 5 4 3\n> paste(\"A\", 1:6, 2:7, seq(8,3,by=-1), sep=\"\")\n[1] \"A128\" \"A237\" \"A346\" \"A455\" \"A564\" \"A673\"\n> paste(\"A\", 1:6, 2:7, seq(8,3,by=-1), sep=\"-\")\n[1] \"A-1-2-8\" \"A-2-3-7\" \"A-3-4-6\" \"A-4-5-5\" \"A-5-6-4\" \"A-6-7-3\"\n> paste(\"A\", 1:6, 2:7, seq(8,3,by=-1), sep=\"/\")\n[1] \"A/1/2/8\" \"A/2/3/7\" \"A/3/4/6\" \"A/4/5/5\" \"A/5/6/4\" \"A/6/7/3\"\n</pre>\n\n* example 2\n <pre>\n> stopifnot(identical(str1 <- paste(\"A\", 1:6, sep=\"\"), str2 <- paste0(\"A\", 1:6)))\n> str1\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\" \"A6\"\n> str2\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\" \"A6\"\n\n> paste(\"Today is\", date())\n[1] \"Today is Fri Mar 29 11:22:15 2013\"\n</pre>\n\n\n\n\n== 20130327_183541 ==\n\n=== list() examples ===\n\n <pre>\n> myl = list(apple=1, banana=2, cherry=3, durian=4, elderberry=5)\n> myl\n$apple\n[1] 1\n\n$banana\n[1] 2\n\n$cherry\n[1] 3\n\n$durian\n[1] 4\n\n$elderberry\n[1] 5\n\n> myl$apple\n[1] 1\n> myl$banana\n[1] 2\n> myl$cherry\n[1] 3\n> myl$durian\n[1] 4\n> myl$elderberry\n[1] 5\n> \n</pre>\n\n\n=== read-from/save-to a file ===\n\n <pre>\n# create a formatted transaction data\n> myd <- paste(\"apple,banana\", \"apple\", \"banana,cherry\", \"banana,cherry,durian\", sep=\"\\n\")> cat(myd)\napple,banana\napple\nbanana,cherry\n\n# write the transaction data to the file\n> write(myd, file = \"myd_basket_2\") \n\n# read the transaction data from the file, and save it to a variable\n> myt <- read.transactions(\"myd_basket_2\", format = \"basket\", sep=\",\")\n\n# inspect the transaction variable\n> inspect(myt)\n  items   \n1 {apple, \n   banana}\n2 {apple} \n3 {banana,\n   cherry}\n4 {banana,\n   cherry,\n   durian}\n</pre>\n\n\n\n\n=== clear workspace (delete data) ===\n\n* References\n* [https://stat.ethz.ch/pipermail/r-help/2007-August/137694.html Clear Workspace in R]\n* [http://stackoverflow.com/questions/11761992/remove-data-from-workspace Advanced method to clear data in R]\n\n* simply, remove three data \'data_1\', \'data_2\', \'data_3\'\n <pre>\nrm(data_1, data_2, data_3)\n</pre>\n\n* remove all the data searchable by ls()\n <pre>\nrm(list = ls())\n# \'list\' is a name of parameter to be passed into \'rm()\' function,\n# so it cannot be changed, it should be \"list\" literally.\n</pre>\n\n* remove all objects whose name begins with the string \"tmp\"\n <pre>\nrm(list = ls()[grep(\"^tmp\", ls())])\nrm(list = ls(pattern=\"^tmp\"))	# making use of the \'pattern\' argument\n</pre>\n\n== 20130313_172639 ==\n\n\n=== SVM example ===\n\n <pre>\n     data(iris)\n     attach(iris)\n     \n     ## classification mode\n     # default with factor response:\n     model <- svm(Species ~ ., data = iris)\n     \n     # alternatively the traditional interface:\n     x <- subset(iris, select = -Species)\n     y <- Species\n     model <- svm(x, y) \n     \n     print(model)\n     summary(model)\n     \n     # test with train data\n     pred <- predict(model, x)\n     # (same as:)\n     pred <- fitted(model)\n     \n     # Check accuracy:\n     table(pred, y)\n     \n     # compute decision values and probabilities:\n     pred <- predict(model, x, decision.values = TRUE)\n     attr(pred, \"decision.values\")[1:4,]\n     \n     # visualize (classes by color, SV by crosses):\n     plot(cmdscale(dist(iris[,-5])),\n          col = as.integer(iris[,5]),\n          pch = c(\"o\",\"+\")[1:150 %in% model$index + 1])\n</pre>\n\n\n=== cmdscale (Classical MultiDimensional Scaling) ===\n\n* Description\n: Classical multidimensional scaling of a data matrix. (a.k.a. principal coordinates analysis (Gower, 1966)\n\n* Usage\n: cmdscale(d, k=2, eig=FALSE, add=FALSE, x.ret=FALSE)\n\n* Arguments\n: \'\'\'d [mandatory]\'\'\': a distance structure such as that returned by \'dist\' or a full symmetric matrix containing the dissimilarities\n: k [optional]: the maximum dimension of the space which the data are to be represented in; must be in {1, 2, ..., n-1}\n: eig [optional]: indicates whether eigenvalues should be returned\n: add [optional]: logical indicating if an additive constant c* should be computed, and added to the non-diagonal dissimilarities such that the modified dissimilarities are Euclidean\n: x.ret [optional]: indicates whether the doubly centered symmetric distance matrix should be returned\n\n* Details\n: Multidimensional scaling takes a set of dissimilarities and returns a set of points such that the distances between the points are approximately equal to the dissimilarities\n\n\n=== dist (Distance matrix computation) ===\n\nThis computes and returns the distance matrix computed by using the specified distance measure to compute the distances between the rows of a data matrix\n\n* Usage\n: dist(x, method = \"euclidean\", diag = FALSE, upper = FALSE, p = 2)\n\n* Arguments\n: x\n:: a numeric matrix, data frame or \'dist\' object\n: method\n:: the distance measure to be used. this must be one of \"euclidean\", \"maximum\", \"manhattan\", \"canberra\", \"binary\", or \"minkowski\" (any unambiguous substring can be given)\n: diag\n\n* Examples (by blusjune)\n\n <pre>\n> mat_a\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    1    1\n[2,]    2    2    2    2\n[3,]    3    3    3    3\n[4,]    0    0    0    0\n> dist(mat_a)\n  1 2 3\n2 2    \n3 4 2  \n4 2 4 6\n> cmdscale(dist(mat_a))\n     [,1]          [,2]\n[1,]    1  5.809542e-08\n[2,]   -1  3.057654e-09\n[3,]   -3  9.172961e-09\n[4,]    3 -9.172961e-09\n> dist(cmdscale(dist(mat_a)))\n  1 2 3\n2 2    \n3 4 2  \n4 2 4 6\n</pre>\n\n <pre>\n> mat_b\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    2    2    2\n[3,]    0    0    0\n> dist(mat_b)\n         1        2\n2 1.732051         \n3 1.732051 3.464102\n> cmdscale(dist(mat_b))\n              [,1]         [,2]\n[1,]  7.412908e-33 1.564993e-08\n[2,] -1.732051e+00 2.477578e-09\n[3,]  1.732051e+00 2.477578e-09\n> dist(cmdscale(dist(mat_b)))\n         1        2\n2 1.732051         \n3 1.732051 3.464102\n\n> ((1-2)**2 + (1-2)**2 + (1-2)**2) ** 0.5\n[1] 1.732051\n> ((2-0)**2 + (2-0)**2 + (2-0)**2) ** 0.5\n[1] 3.464102\n</pre>\n\n\n== 20130306_185022 ==\n\n=== R package (\'e1071\') installation from command line ===\n\n: Assumes that already downloaded and unpacked properly the \'e1071_1.6-1.tar.gz\' file\n\n <pre>\n\n1mjjung@secm:[e1071] $ R CMD INSTALL e1071\nJob <13412785> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura007>>\nWarning: invalid package \'e1071\'\nError: ERROR: no packages specified\n\n\n\n\na1mjjung@secm:[e1071] $ cd ..\nla1mjjung@secm:[e1071_build] $ l\ntotal 280\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:45 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\n\n\n\n\na1mjjung@secm:[e1071_build] $ R CMD INSTALL e1071\nJob <13412786> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n* installing to library \'/home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15\'\n* installing *source* package \'e1071\' ...\n** package \'e1071\' successfully unpacked and MD5 sums checked\nchecking for C++ compiler default output file name... a.out\nchecking whether the C++ compiler works... yes\nchecking whether we are cross compiling... no\nchecking for suffix of executables... \nchecking for suffix of object files... o\nchecking whether we are using the GNU C++ compiler... yes\nchecking whether g++ accepts -g... yes\n** libs\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c Rsvm.c -o Rsvm.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cmeans.c -o cmeans.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cshell.c -o cshell.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c floyd.c -o floyd.o\ng++ -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c svm.cpp -o svm.o\ng++ -shared -L/usr/local/lib64 -o e1071.so Rsvm.o cmeans.o cshell.o floyd.o svm.o\ninstalling to /home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15/e1071/libs\n** R\n** inst\n** preparing package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n   \'svmdoc.Rnw\' \n** testing if installed package can be loaded\n\n* DONE (e1071)\n\n\n\n\na1mjjung@secm:[e1071_build] $ R\nJob <13412787> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n\nR version 2.15.2 (2012-10-26) -- \"Trick or Treat\"\nCopyright (C) 2012 The R Foundation for Statistical Computing\nISBN 3-900051-07-0\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType \'license()\' or \'licence()\' for distribution details.\n\nR is a collaborative project with many contributors.\nType \'contributors()\' for more information and\n\'citation()\' on how to cite R or R packages in publications.\n\nType \'demo()\' for some demos, \'help()\' for on-line help, or\n\'help.start()\' for an HTML browser interface to help.\nType \'q()\' to quit R.\n\n> ?svm\nNo documentation for \'svm\' in specified packages and libraries:\nyou could try \'??svm\'\n> library(e1071) \nLoading required package: class\n> ?svm\n> \nSave workspace image? [y/n/c]: y\n\n\n\n\na1mjjung@secm:[e1071_build] $ pwd\n/home/X0101/a1mjjung/x/R/e1071_build\n\na1mjjung@secm:[e1071_build] $ ls -alF\ntotal 284\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:47 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\n-rw------- 1 a1mjjung X0101     25 Mar  6 18:47 .Rhistory\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:46 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\n\n</pre>\n\n\n\n== 20130304_190007 ==\n\n\n\n=== test type of some object ===\n\n <pre>\n> x <- scan(\"/tmp/scan.txt\", what=list(NULL, name=character()))\n> x <- x[sapply(x, length) > 0]\n> is.vector(x)\n\n\n> x = mat.or.vec(100,1)\n> if (is.integer(x) == TRUE) { print (\"YES\") } else { print (\"NO\") }\n[1] \"NO\"\n> if (is.vector(x) == TRUE) { print (\"YES, vector\") } else { print (\"NO, NOT vector\") }\n[1] \"YES, vector\"\n</pre>\n\n\n\n\n=== Data import (load data from a file) ===\n\n* scan()\n <pre>\n > x1 <- scan(\"/etc/hosts\", what=character())\n\n > x1     \n [1] \"127.0.0.1\"       \"localhost\"       \"#127.0.1.1\"      \"stones\"         \n [5] \"#\"               \"The\"             \"following\"       \"lines\"          \n [9] \"are\"             \"desirable\"       \"for\"             \"IPv6\"           \n[13] \"capable\"         \"hosts\"           \"::1\"             \"ip6-localhost\"  \n[17] \"ip6-loopback\"    \"fe00::0\"         \"ip6-localnet\"    \"ff00::0\"        \n[21] \"ip6-mcastprefix\" \"ff02::1\"         \"ip6-allnodes\"    \"ff02::2\"        \n[25] \"ip6-allrouters\"  \"10.0.2.15\"       \"stones-eth0\"     \"#192.168.1.109\" \n[29] \"stones\"          \"hd-master-01\"    \"#192.168.1.110\"  \"pavement\"       \n[33] \"hd-slave-0001\"   \"192.168.1.112\"   \"stones\"          \"hd-master-01\"   \n[37] \"hd-slave-0001\"   \"kandinsky\"       \"192.168.1.110\"   \"pavement\"       \n[41] \"hd-slave-0002\"  \n</pre>\n\n* read.table()\n <pre>\n> iot_r <- read.table(\'tracelog.msn_filesrvr.R\')  \n</pre>\n\n\n\n=== function defintion, for loop in R ===\n\n <pre>\n> avg_smoothing <- function(src, srcl, sf) {\n    tgtl = srcl + 1 - sf\n    tgt <- mat.or.vec(tgtl, 1)\n    for (i in 1:tgtl) {\n        tgt[i] = mean(src[i:(i+sf-1)])\n    }\n    return (tgt)\n}\n\n> vec1 <- rnorm(100, mean=10, sd=1)\n> vec1_sf2 <- avg_smoothing(vec1, 100, 2)\n> vec1_sf4 <- avg_smoothing(vec1, 100, 4)\n> vec1_sf8 <- avg_smoothing(vec1, 100, 8)\n\n> plot(vec1, col=\"gray\", type=\"l\")\n> points(vec1_sf2, col=\"red\", type=\"l\")\n> points(vec1_sf4, col=\"blue\", type=\"l\")\n> points(vec1_sf8, col=\"green\", type=\"l\")\n</pre>\n\n\n\n\n== 20130127_225539 ==\n\n=== R Installation ===\n\n* to install R statistical computing software\n** me@matrix$ sudo apt-get install r-base\n\n* to start R from command line, just type \'R\' in your terminal\n** me@matrix$ R\n\n* RStudio download [http://www.rstudio.com/ide/download/]\n** RStudio Desktop [http://www.rstudio.com/ide/download/desktop]\n**: If you run R on your desktop\n** RStudio Server [http://www.rstudio.com/ide/download/server]\n**: If you run R on a Linux server and want to enable users to remotely access RStudio using a web browser\n*** RStudio Server Getting Started [http://www.rstudio.com/ide/docs/server/getting_started]\n\n <pre>\nsudo apt-get install r-base\nwget http://download2.rstudio.org/rstudio-server-0.97.312-amd64.deb\nsudo gdebi rstudio-server-0.97.312-amd64.deb \nbsc.adm.create_me\ngoogle-chrome http://kandinsky:8787 # type ID/PW (me?!)\n</pre>\n\n\n* Debian Packages of R Software [http://cran.r-project.org/bin/linux/debian/README.html]\n\n=== R Guide/Tutorial/Example ===\n\n* R Tutorial [http://www.r-tutor.com/]\n** R Tutorial:: Data Import [http://www.r-tutor.com/r-introduction/data-frame/data-import]\n\n\n* Example R scripts [http://people.eng.unimelb.edu.au/aturpin/R/index.html]]\n\n\n* R by example [http://www.mayin.org/ajayshah/KB/R/index.html]\n\n\n* R example:: Kmeans [http://stat.ethz.ch/R-manual/R-patched/library/stats/html/kmeans.html]\n\n\n* R package install howto\n; e1071\n: Misc Functions of the Department of Statistics (e1071), TU Wien\n:* package-installation and loading\n install.packages(\"e1071\") # installing the package \'e1071\'\n library(\"e1071\") # loading the installed package \'e1071\'\n\n\n* Importing SAS, SPSS and Stata files into R [http://staff.washington.edu/glynn/r_import.pdf]\n\n\n=== R Troubleshooting ===\n\n* Problems importing csv file/converting from integer to double in R [http://stackoverflow.com/questions/8381839/problems-importing-csv-file-converting-from-integer-to-double-in-r]\n\n\n=== Misc. ===\n\n* WEKA Tutorial [http://www.cs.utexas.edu/users/ml/tutorials/Weka-tut/]\n* weka is a metric prefix for 10^30','utf-8'),(72,'== Introduction to Association Rule Mining ==\n\n\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(73,'== Introduction to Association Rule Mining ==\n\n\n\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(74,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(75,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n\n\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(76,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n\n\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ((B.GOOD)) ===\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(77,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n\n\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series','utf-8'),(78,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(79,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n: FIM/ARM (Frequent Itemset Mining / Association Rule Mining)\n\n\n== FIM/ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM/ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(80,': FIM/ARM (Frequent Itemset Mining / Association Rule Mining)\n\n== FIM/ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM/ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(81,'FIM/ARM (Frequent Itemset Mining / Association Rule Mining)\n\n== FIM/ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM/ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(82,'FIM/ARM (Frequent Itemset Mining / Association Rule Mining)\n\n\n\n== FIM/ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM/ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(83,'Frequent Itemset Mining (FIM) / Association Rule Mining (ARM)\n\n\n\n\n== FIM/ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM/ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(84,'#REDIRECT [[Frequent Itemset Mining (FIM) / Association Rule Mining (ARM)]]','utf-8'),(85,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(86,'#REDIRECT [[PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]','utf-8'),(87,'#REDIRECT [[Bnote Frequent Itemset Mining (FIM) / Association Rule Mining (ARM)]]','utf-8'),(88,'== Lessons learned ==\n\n* \"An Analysis of Traces from a Production MapReduce Cluster,\" CCGRID 2010\n\n* CCGRID 2010에 소개되었던 CMU의 연구인 [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5493490 An Analysis of Traces from a Production MapReduce Cluster // Kavulya, S. CMU // CCGRID 2010]에는 Yahoo!의 M45 Supercomputing Cluster에서의 10달 간의 MapReduce tracelog에 대한 분석 결과가 있다. 이를 보면, task type과 failure 와의 연관성이 있음을 알 수 있다.\n\n* Failure와 straggler 간의 관계는 다음과 같다. straggler는 아직 명확한 exception이 일어나서 task가 명시적인 failure로 mark가 된 상태는 아니지만, 통상적인 경우보다 꽤 느리게 task가 수행되고 있는 상태를 의미한다.\n\n\n== Excerpts from paper ==\n\n=== Data Summary ===\n\n{| class=\"wikitable\" border=\"1\" \n|+ Summary M45 dataset\n! Key\n! Value\n|-\n| Log Period\n|\n: Apr 25 - Nov 12, 2008\n: Jan 19 - Apr 24, 2009\n|-\n| Hadoop versions\n|\n: 0.16: Apr 2008 -\n: 0.17: Jun 2008 -\n: 0.18: Jan 2009 -\n|-\n| Number of active users\n| 31\n|-\n| Number of jobs\n| 171079\n|-\n| Successful jobs\n| 165948 (97%)\n|-\n| Failed jobs\n| 4100 (2.4%)\n|-\n| Cancelled jobs\n| 1031 (0.6%)\n|-\n| Average maps per job\n| 154 +/- 558 sigma\n|-\n| Average reduces per job\n| 19 +/- 145 sigma\n|-\n| Average nodes per job\n| 27 +/- 22 sigma\n|-\n| Maximum nodes per job\n| 299\n|-\n| Average job duration\n| 1214 +/- 13875 seconds\n|-\n| Maximum job duration\n| 6.84 days\n|-\n| Node days used\n| 132624\n|}\n\n\n=== Memo ===\n\n* There were 4100 failed jobs and 1031 incomplete jobs in our dataset. These failures accounted for 3% of the total jobs run.\n\n* We observed that:\n# <span style=\"color:red\">\'\'\'Most jobs fail within 150 seconds after the first aborted task\'\'\'</span>. Figure 3(c) shows that 90% of jobs exhibited an error latency of less than 150 seconds from the first aborted task to the last retry of that task (the default number of retries for aborted tasks was 4). We observed a maximum error latency of 4.3 days due to a copy failure in a single reduce task. Better diagnosis and recovery approaches are needed to reduce error latency in long-running tasks.\n# Most failures occurred during the map phase. Failures due to task exceptions in the <span style=\"color:red\">\'\'\'map phase\'\'\'</span> were the most prevalent - 36% of these failures were due to <span style=\"color:red\">\'\'\'array indexing errors\'\'\'</span> (see Figure 4). <span style=\"color:red\">\'\'\'IO exceptions\'\'\'</span> were common in the <span style=\"color:red\">\'\'\'reduce phase\'\'\'</span> accounting for 23% of failures. Configuration problems, such as missing files, led to failures during job initialization.\n# Application hangs and node failures were more prevalent in cancelled jobs. Task timeouts, which occur when a task hangs and fails to report progress for more than 10 minutes, and lost TaskTracker daemons due to node or process failures were more prevalent in cancelled jobs than in failed jobs.\n# Spurious exceptions exist in the logs. The logs contained spurious exceptions due to debugging statements that were turned on by default in certain versions of Hadoop. For example, a debug exception to troubleshoot a problem in the DFS client attributed to data buffering, and an exception due to a disabled feature that ignored the nonzero exit codes in Hadoop streaming, accounted for 90% of exceptions from January 21 to February 18, 2009. This motivates the need for error-log analysis tools that highlight important exceptions to users','utf-8'),(89,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(90,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n=== Pretty Good As An Introduction to Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(91,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(92,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(93,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(94,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(95,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(96,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Examples and Resources on Association Rule Mining with R (R-bloggers.com)\n===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n* http://www.rdatamining.com/resources/onlinedocs\n: A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD))\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(97,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Examples and Resources on Association Rule Mining with R (R-bloggers.com)\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== Association Rule Mining with R ===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n* http://www.rdatamining.com/resources/onlinedocs\n: A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD))\n\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(98,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Examples and Resources on Association Rule Mining with R (R-bloggers.com)\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== Examples of Association Rule Mining with R ===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(99,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n: A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Examples and Resources on Association Rule Mining with R (R-bloggers.com)\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== Examples of Association Rule Mining with R ===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(100,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n: A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== Examples of Association Rule Mining with R ===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(101,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n: A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== Examples of Association Rule Mining with R ===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique. (R-bloggers.com)\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8');
/*!40000 ALTER TABLE `radiohead_text` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `radiohead_transcache`
--

DROP TABLE IF EXISTS `radiohead_transcache`;
