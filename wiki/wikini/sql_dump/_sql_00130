INSERT INTO `radiohead_text` VALUES (64,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n[[Bnote patidea 2013-003]]\n\n\n== PATENT-BRIAN-2013-004 ==\n\n=== Event-based I/O Prediction ===\n\n=== 요약 ===\n\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n=== 기술 상세 ===\n\n* I/O 발생 메커니즘의 모델링\n: I/O prediction을 위해, type-I event (indirect I/O trigger)와 type-II event (direct I/O trigger)를 관찰 및 분석함. Type-I event의 예로서, user의 log-in, 기 log-in되어 있던 user session의 상태 변경 (idle-to-busy), system service에 대한 invocation from local or remote node 등이 포함될 수 있으며, type-II event의 예로서, user application 의 실행, system service의 실행 등이 포함될 수 있음. Type-I event는 type-II event의 발생에 영향을 미치고, type-II event는 I/O request event (data access)의 발생에 영향을 미치므로, \'type-I event\' -> \'type-II event\' -> \'I/O request event\' 까지 이어지는 인과사슬 (cause-effect chain) 기반으로 I/O 발생 메커니즘을 모델링하였음.\n\n\n* Event 정보 수집: type-I event 정보 및 type-II event 정보 정의 및 수집 방법.\n\n\n* I/O 패턴 학습 (Offline Learning): 수집된 Event 정보를 기반으로 I/O 패턴을 learning하는 방법\n\n\n* Run-time I/O 예측 (Online Matching): 시스템 모니터링에서 얻어지는 최근 및 현재의 event들과 learning으로 얻어진 I/O 패턴 DB에 기반하여 Run-time으로 미래 I/O 예측\n\n\n* 시스템 구조: 정보 수집 및 패턴 학습 모듈의 시스템 내 위치 (I/O 패턴 DB 등의 위치 및 실시간 예측 모듈과의 관계)\n\n\n* 순서도: 수집부터 학습까지 전체적인 task의 흐름\n\n\n* 예상 효과: 성능적 측면에서 사용자가 체감하는 lower latency, higher throughput 효과, 이미 발생해버린 congestion time을 피해 미리 적절한 곳에 data를 배치시킴으로써 얻어지는 bottleneck prevention 효과.\n\n\n=== Data Collection ===\n\n* 역할: I/O 패턴 학습/발굴에 필요한 정보 수집\n\n* 수집 정보 종류\n: 전조 기반 예측에 필요한 간접 I/O 유발자, 직접 I/O 유발자, I/O 발생 이벤트 등 I/O 예측에 도움이 될 수 있는 다양한 정보를 수집. 다음 정보들이 포함될 수 있음.\n:- indirect I/O trigger trace: {(user-id | user-type), status-type, before-status, after-status, event-time}\n:- direct I/O trigger trace: {(program-id | program-type), status-type, before-status, after-status, process-owner, event-time [, process-current-working-directory, files-opened]}\n:- I/O trace: {access-time, address-accessed, read/write-flag, io-size, file-path, process, co-accessed-addresses}\n\n\n* 수집 모듈의 위치\n: storage-server가 분리되어 있는 구조인 SAN/NAS 타입의 architecture인 경우에는 application이 구동되는 server-side에 정보 수집 모듈이 위치한다. storage-server가 결합되어 있는 converged-architecture 기반의 system인 경우에는 해당 system 내에 정보 수집 모듈이 위치한다.\n\n\n* 정보 수집 방식\n: 운영체체가 제공하는 시스템 상태 정보 제공 interface를 이용하거나, blktrace, lsof, debug file system 등 이용 (e.g., /sys/kernel/debug/ 등) 혹은 /proc file system 등을 이용 가능.\n\n== PATENT-BRIAN-2013-005 ==\n\n=== Coaccess-based I/O Prediction ===\n\n\n=== 요약 ===\n* 미래에 access될 data를 예측하여 적절한 위치에 미리 가져다 놓기 위한 event-based I/O prediction 방법 및 구조에 대한 발명으로서,\n: (1) I/O prediction을 위해 수집되는 정보들의 종류 및 형태;\n: (2) I/O 예측에 필요한 주요 I/O 패턴을 발견/학습하고 I/O 패턴 DB를 구성하는 방법;\n: (3) I/O 패턴 DB의 정보들과 Run-time 모니터링을 기반으로 미래 I/O를 예측하는 방법;\n: (4) 예측된 I/O 정보를 기반으로 Proactive Data Placement를 수행하는 방법;\n: 이상의 것들을 가능케 하는 시스템 구조; 로 구성됨.\n\n* I/O 예측을 기반으로 실제 I/O request event가 발생하기 전에 proactive하게 data를 placement할 수 있으며, 이로 인한 성능 향상을 얻을 수 있음.\n\n\n=== 기술 상세 ===\n\n* Data의 coaccess 특성에 기반한 proactive data placement 기법\n: 소정의 time window 내에 같이 access되는 확률이 높은 data 그룹이 있을 때, 그 그룹 내의 data가 access되는 것을 알게 되면, 그 그룹 내의 나머지 data들도 미리 fast media에 가져다 놓음으로써 성능 향상을 얻을 수 있게 하는 방법임.\n\n\n\n\n* 시뮬레이션 기반의 성능 향상 효과 비교\n:- Fast media의 4KB random read latency는 96.47 usec (SSD 830 128GB 기준), slow media의 4KB random read latency는 1,884.03 usec (HDD 7,200rpm 2TB 기준)으로 설정 (iozone에 의한 실측값 사용)\n:- 실험 시작 전에는 4GB 크기의 target data가 slow media (HDD)에 저장되어 있음.\n:- 여기에 1,000,000 개의 4KB random read request로 구성되는 workload를 인가. (각 read request들은 서로 중복되는 data를 가리키지 않으며, 4GB 크기의 target data를 한 번씩 다 request하게 된다고 가정)\n:- Workload 패턴 학습 결과, 4GB의 전체 data 중, \'가\' type의 coaccess 특성을 가지는 10MB 크기의 data 그룹 (data set 당 2,500개의 4KB data 포함)이 200개가 있으며, \'나\' type의 coaccess 특성을 가지는 1MB 크기의 data 그룹 (data set 당 250개의 4KB data 포함)이 1,000개 있는 것을 발견한 것으로 가정.\n:- \'가\' type의 coaccess 패턴은 해당 data 그룹 내 100개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 80%인 것으로 정의.\n:- \'나\' type의 coaccess 패턴을 해당 data 그룹 내 50개의 data가 access되면, 1분 이내에 나머지 data들이 같이 access될 확률이 100%인 것으로 정의.\n\n\n\n\n: 에 대해 본 기법을 적용한 경우와 그렇지 않은 경우의 성능 차이를 IOPS로 비교해보면 다음과 같다. 기법을 적용하기 전인 baseline의 경우에는 \n\n\n* coaccessness 분석에 필요한 parameter로서, \n\n\n\n\n\n\n\n: 예를 들어 설명하면 다음과 같다.\n: data 1, data 2, ..., data 10000으로 구성된 data set이 있으며, 각 data들의 크기는 4KB이다. 학습을 통해서, (1) 이 중 어느 한 data가 access되었을 때, 1분 이내에 나머지 data들도 같이 access되게 될 확률은 55%, (2) 이 중 2개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 80%, (3) 이 중 3개의 data가 access되었을 때 1분 이내에 나머지 data들도 같이 access될 확률은 95%라는 것을 알고 있다고 가정한다.\n: fast media의 크기는 4GB 이며, 기존에 \n\n\n   이라는 것을 알고 있다면, data 1, 2, 3이 access되었을 때 data 4~128까지도 미리 proactive하게 fast tier에 배치시킴으로써, 그로 인한 성능 이득을 얻을 수 있게 된다. (!!!실제 실험 사례?!!!)\n\n\n\n\n\n\n\n\n\n== PATENT-BRIAN-2013-006 ==\n\n\n=== Data Lifetime (Valid-to-invalid) Interval 분석에 기반한 Data Placement (Tiering) ===\n\n\n* Data 별로, 혹은 LBA 별로, 혹은 Erase Block 별로? Data의 Valid to Invalid Time (Duration) 패턴을 분석할 수 있을까?\n: 이를 SSD 내에서 Low Overhead로 수행할 수 있나?\n: 혹은, 굳이 SSD 내에서 수행할 필요가 있나?\n: Host side에서 LBA 별로 분석 될 수 있다면, 이 정보를 기반으로 Tiering 시에 활용 할 수 있을 것임\n: 특정 Address에 대해서 W이 되고 나서, 다시 W이 되기까지의 시간을 측정하여 ARM 하면 될 듯. 그 두 W 간에 얼마나 많은 R들이 오는지에 대해서도 고려하여 placement 시 고려할 수 있을 것임.\n\nMeasurement Window 내에서 \'W 간의 Interval이 길다\'는 식으로 인식하는 것과, \'W의 횟수가 잦다\'고 인식하는 것 간에 어떤 차이가 있을 수 있을까?\n\n <pre>\n    01234567890123456789\na1: W-R-W-R-W-R-W-W-R--W : T20W06R04 : Short-Lifetime, Many-Reads : SLC\na2: W---W-R-W---W-W----W : T20W06R01 : Short-Lifetime, Few-Reads : SLC, DRAM?\na3: W-RRRR-W---RR-W-R-RW : T20W04R08 : Mid-Lifetime, Many-Reads : MLC\na4: W------W--R---W----W : T20W04R01 : Mid-Lifetime, Few-Reads : MLC\na5: W-R-R-R-RR-RRRR----R : T20W01R10 : Long-Lifetime, Many-Reads : MLC\na6: W------------R----R- : T20W01R02 : Long-Lifetime, Few-Reads : TLC\n</pre>\n\n=== Idle-time Prediction Based Free Space Management in SSD ===\n\n* Idle-time의 도래 시기와 지속 시간 패턴 분석에 기반한 SSD 내의 Free Space 확보 방법\n:- SSD 내에서 Low overhead (in computation, memory space)로 Idle-time 도래 시기 및 지속 시간 분석이 가능한가?\n:- 가능하다면, SSD 내에서 Garbage Collection 등으로 Free Space를 확보하는 데 있어서 어떤 장점을 가질 수 있는가?\n\n\n\n: 이를 기반으로 \n\n\n\n=== 요약 ===\n* \n\n\n=== 기술 상세 ===\n\n*\n\n== Memo for {PATENT-BRIAN-2013-004, PATENT-BRIAN-2013-005} ==\n\n\n=== Think outside the box // Questions ===\n\n* 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n\n* Tiering을 좀 더 다양한 각도에서 바라보자\n:- 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 것이 기존의 tiering 개념임.\n:- SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n:- SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n:- access pattern-aware optimal placement (APOP) - 이런 것은 어떨까?\n:- Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n\n* write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반의 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지?\n\n\n* 만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? SSD-internal memory buffer를 이용해서 SSD를 NVRAM처럼 쓰도록 하는 것이 실제로 말이 되는 이야기일까?\n\n\n\n\n\n=== 기존 기술의 한계점 (Intelligent I/O Prediction 기술의 필요성) ===\n\n* Naive prediction\n: 기존 방식에서는 \'과거에 hot했던 데이터가 앞으로도 hot할 것\'이라는 가정을 전제로 hot/cold data placement를 실시하였음. (이를 위해 관찰했던 정보는 누적 access count 혹은 recency 정보로서, 과거에 어떤 data가 hot했고 cold했는지를 판단하기 위한 목적으로 사용되었으며, 이를 기반으로 미래의 I/O를 예상한 것임. 이러한 것을 prediction이라고 본다면, naive I/O prediction이라고 볼 수 있겠음.) data access 패턴의 변화 없이 한 번 hot 했던 data가 계속 hot하게 유지되는 경우에는 기존 방식이 잘 동작하였으나, data access 패턴에 변화가 생기면 기존 방식으로는 성능 향상 효과를 보기 어렵다는 한계가 있음.\n\n* Naive I/O prediction 기반의 부정확한 예측이 SSD에 미치는 부정적인 영향\n: 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다.\n\n* Caching-only 접근의 한계\n: Data access 패턴의 skewness가 높아서 일부 data에 대부분의 access가 집중되는 경우에는, cache 적용만으로도 높은 성능 향상 효과를 볼 수 있었으나, source data의 크기가 매우 크거나, data access 패턴의 skewness가 그다지 높지 않은 경우에는 cache만으로는 성능 향상을 기대하기 어렵게 됨.\n\n <pre>\n실제 tracelog 사례를 보여주고 hot area가 변화하고 있음을 보일 것.\n</pre>\n\n* Data Placement (Tiering) 관점에서 본 EMC의 FAST, FAST-VP 기술의 한계점\n:- Coarse-grained Data Migration (768KB 까지 작게 나눌 수 있다고는 하지만, 실제로 그렇게 해서 성능이 얼마나 잘 나오는지는 확인 필요. 귀중한 자원인 DRAM을 관리를 위한 metadata로 가득 채우게 될 위험도 있음)\n:- Flexible하지 않은 Tiering 주기\n:- Busy-hour 지난 후 Idle-time에 Tiering 실시\n:- 낮은 지능의 미래 I/O 예측 능력\n:: data access count를 기반으로 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두도록 하는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n\n=== 본 발명의 차별화 포인트 ===\n\n* 본 기술의 특징 1: I/O를 triggering하는 전조 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Indicator는 Run-time에 저비용으로 감지될 수 있는 형태의 정보들로 구성\n:- 미래에 access될 data를 예측케 하는 Indicator간의 mapping은 Bayesian Inference 이용\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n=== 특허 검색/분석 결과 {storage I/O workload prediction} ===\n\n* 검색식: storage I/O workload prediction\n\n* http://portal.uspto.gov/pair/PublicPair 에서 특허 상태 확인 가능.\n\n* I/O prediction을 기반으로 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n:- 기존엔 proactive tiering이 없었나? -> IBM 특허가 나름 유사함. (US7930505, US7512766)\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n==== 1. 비교가 될만한 특허들 ====\n\n예측에 대한 언급이 되어 있는 스토리지 시스템 관련 특허들. 크게 두 가지로서, 그 중의 하나는 가상 volume에 대해 peak I/O가 발생하는 주기를 분석하여, peak I/O 발생시점이 닥치기 전에 미리 가상 volume의 data를 migration 시키는 방법에 대한 것이나, 어느 위치의 data가 언제 access될지에 대한 예측은 아님 (IBM 특허). 다른 하나는 I/O 예측을 기반으로 미래에 자주 access될만한 data를 미리 여러 노드에 분산시키는 방법에 대한 것이나, 예측 방법 자체에 대한 내용은 다루고 있지 않음 (HP 특허)\n\n\n; US7930505, US7512766 (IBM)\n\n* US7930505 (Application Number: US 12/053582), \"Controlling preemptive work balancing in data storage,\" IBM, 특허일 2011-04-19\n: storage network의 I/O 관찰을 통해 peak I/O activity의 repeating instance에 대한 signature를 VLU (virtual logical unit) 단위로 생성하고, 이에 기반하여 언제 heavy I/O가 도래할 것인지를 예측하여, heavy I/O가 오기 전에 미래 data를 faster media로 migration하도록 하는 tiering 기술에 대한 특허. (이미 I/O peak time이 온 후에 hot data를 옮기는 것은 오히려 시스템의 IO성능을 악화시킬 수도 있기 때문에 미리 migration을 하고자 하는 것임)\n: VLU 단위로, peak I/O activity가 어느 주기로 발생할지에 대한 예측임. 지능 수준과 Fine-granularity 측면에서 초보적인 수준임.\n\n* US7512766 (Application Number: US 11/213494), \"Controlling preemptive work balancing in data storage\", IBM, 특허일 2009-03-31\n: 위의 US7930505 특허와 동일 건으로 보임\n\n\n; US6912635 (HP)\n\n* US6912635 (Application Number: US 10/140968), \"Distributing workload evenly across storage media in a storage array\", HP, 특허일 2005-06-28\n: 미래에 자주 access될만한 data를 여러 arrayed storage device에 분산시키겠다는 기술. 그러나 I/O에 대한 예측 방법은 다루어지고 있지 않음.\n\n==== 2. 검색은 되었으나 관련은 없는 특허들 ====\n\n무언가를 예측한다는 측면, Storage System에 대한 기술이라는 측면에서는 관련이 있으나, 미래에 Access될 Data를 예측한다는 측면에서 제안 기술과 관련 없는 특허들\n\n\n; US7640231 (IBM)\n* US7640231 (Application Number: 11/280012), \"Approach based on self-evolving models for performance guarantees in a shared storage system\", IBM, 특허일 2009-12-29\n: network 기반 storage system에서, average latency 혹은 I/O request rate 등에 대한 주기적인 performance sampling을 기반으로, 주어진 objective function의 값을 최대화할 수 있도록 system resource를 제어할 수 있게 하는 기술.\n\n\n; US7774491, US7519725 (IBM)\n\n* US7774491 (Application Number: 12/359377), \"Utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2010-08-10\n: 복수의 클라이언트들을 대상으로 통계적인 성능 QoS를 보장하기 위한 informed throttling 방법\n: A system for utilizing informed throttling to guarantee quality of service to a plurality of clients\n\n* US7519725 (Application Number: US 10/444712), \"System and method for utilizing informed throttling to guarantee quality of service to I/O streams\", IBM, 특허일 2009-04-14\n: resolve the problem of providing statistical performance guarantees for applications generating streams of read/write accesses (I/Os) on a shared, potentially distributed storage system of finite resources, by initiating throttling whenever an I/O stream is receiving insufficient resources\n\n\n; US7827283 (IBM)\n* US7827283 (Application Number: 10/367444), \"System for managing and controlling storage access requirements\", IBM, 2010-11-02\n: SAN 등의 환경에서, storage bandwidth를 관리하고 reserve하는 기술\n: A Resource Management and Reservation System (RMRS) for managing and reserving storage bandwidth, is a platform independent middleware layer that provides an interface to applications, their database management systems, or some other higher level data management systems like ADRS which does data management on behalf of the applications\n\n\n; US8433554, US8140319 (IBM)\n\n* US8433554, \"Predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2013-04-30\n: An approach for predicting performance and capacity of an information technology (IT) system before the IT system is built is described, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n* US8140319, \"Method and system for predicting system performance and capacity using software module performance statistics\", IBM, 특허일 2012-03-20\n: A method and system for predicting performance and capacity of an information technology (IT) system before the IT system is built, where the predicting uses a database of performance statistics measured for reusable software modules.\n\n\n\n; US8255915 (HP)\n* US8255915, \"Workload management for computer system with container hierarchy and workload-group policies\" HP, 특허일 2012-08-28\n\n\n; US7050956 (HP)\n* US7050956 (Application Number: US 09/843930), \"Method and apparatus for morphological modeling of complex systems to predict performance\", HP, 특허일 2006-05-23\n: I/O 성능 예측을 위한 complex storage system에서의 data flow 모델링 방법\n\n\n; US6718434 (HP)\n* US6718434 (Application Number: US 09/871043), \"Method and apparatus for assigning raid levels\", HP, 특허일 2004-04-06\n: Data를 disk array에 load하기 전에, workload 명세 및 device 명세에 기반하여, 적합한 RAID level을 판단하여 disk array에 할당 (RAID build)하는 방법\n\n\n; US8112586 (EMC)\n* US8112586 (Application Number: US 12/190071), \"Predicting and optimizing I/O performance characteristics in a multi-level caching system\", EMC, 특허일 2012-02-07\n: multi-level caching 시스템에서, cost대비 performance를 최적화할 수 있도록, workload에 따른 resource allocation design에 대한 recommendation 기술 (multi-level cache들 간의 workload propagation을 예측하는 것이 핵심)\n\n\n; US6629266 (IBM)\n* US6629266 (Application Number: US 09/442001), \"Method and system for transparent symptom-based selective software rejuvenation\", IBM, 특허일 2003-09-30\n: software가 언제 동작이 멈추는지를 예측함으로써, software의 dependability를 향상할 수 있게 하는 방법\n\n\n; US5088058 (Unisys)\n* US5088058 (Application Number: 07/237120), \"Apparatus and method for evaluating and predicting computer I/O performance using I/O workload snapshots for model input\", Unisys Corporation, 특허일 1992-02-11\n: I/O delay 등에 대한 확률적 모형을 기반으로 disk 시스템의 성능을 검증하고 예측하는 시뮬레이션 기반의 방법\n: A method of evaluating and predicting the performance of an I/O disk system comprised of one or more disk controllers and cooperating disk units using a simulation model containing a stact constructed such that delays are formulated as sets of stochastic processes\n\n=== Comparison of I/O Prediction Approaches ===\n\n미래에 access될 data의 위치와 시기를 예측하기 위한 방법으로 event-based I/O prediction, coaccess-based I/O prediction, periodicity-based I/O prediction 방식이 제안되며, 각각의 방식은 예측에 필요한 연산량 및 input data의 richness, 그리고 workload 변화에 대한 적응성 측면에서 장단점을 가지고 있음. 따라서, 상황에 따라서 적절한 방식을 이용하면 됨.\n\n{| style=\"border-collapse: collapse; border: 1px solid #000\" border=1\n| style=\"width: 25%\" | 비교 항목\n| style=\"width: 25%\" | periodicity-based I/O prediction\n| style=\"width: 25%\" | coaccess-based I/O prediction\n| style=\"width: 25%\" | event-based I/O prediction \n|-\n| context richness required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| computation overhead required for learning (lower is better)\n| low\n| mid\n| high\n|-\n| adaptability to workload changes (lower is better)\n| high\n| high\n| high\n|-\n|}\n\n\n* 전조 기반 예측 vs. 주기성 기반 예측\n{| border=1\n| 비교 항목\n| 전조 기반 예측\n| 주기성 기반 예측\n|-\n| 기술 요약\n| 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식\n| address 별 access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식\n|-\n| 분석 오버헤드\n| High (학습/마이닝에 드는 비용임)\n| Low (주기성 파악 비용임)\n|-\n| Run-time 오버헤드\n| Mid (실시간 전조 모니터링 및 매칭 비용)\n| Low (주기성 계산)\n|-\n| Workload 변화 적응성\n| High\n| High (Workload 변경 사실 알림 hint 필요)\n|-\n| ...\n| ...\n| ...\n|-\n| Incrementality of Mining/Learning\n| ...\n| ...\n|}\n\n* 전조 기반 예측은, 주어진 시스템에서 어떤 이벤트가 발생했을 때 확률적으로 어느 위치의 data가 언제 access되는지에 대한 학습을 통해 미래의 I/O를 예측하는 방식임.  주기성 기반 예측은, data access 패턴에 내포된 주기성을 파악하여, 이를 기반으로 다음 번 해당 data가 access될 시기를 예측하는 방식임.\n\n\n=== Intelligent Data Placement ===\n\n* Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case\n: 여기에 HML이 사용될 수 있는 것이라면, 어떤 방식으로 어떤 이유로 그런 것일까?\n\n\n* I/O 예측 방식으로서 다음 방식이 제안되며, 각 방식에 따라서 prediction model, 수집되어야 하는 context data, I/O 패턴 DB에 저장되는 정보의 종류 및 형태가 달라질 수 있음. 예측에 이용할 수 있는 context 정보의 richness에 따라 적절한 방식을 사용할 수 있다. (서로 다른 발명이므로, 별도의 특허로 낸다)\n:- event-based I/O prediction\n:- coaccess-based I/O prediction\n:: 여기서, periodicity-based I/O prediction은 기존 특허에서 언급되었으므로, 본 발명에서는 크게 언급하지 않기로 한다.\n\n\n* I/O prediction 결과에 따라 적절한 위치에 data를 배치하게 되는데, 여기서 적절한 위치라 함은 SSD와 같은 fast tier가 될 수도 있겠으나, sequential read/write 같은 경우 striping된 HDD array가 될 수도 있고, 혹은 SAS HDD가 될 수도 있겠음. 또한 SSD 중에서도 SLC, MLC, TLC 로 구분되는 fine-grained fast tier로 구분될 수도 있음\n\n\n* 미래에 오게 될 I/O의 type도 알 수 있을까? 즉, {random/sequential}x{read/write} type 중 어느 것에 해당되는지를 안다면, 그리고 I/O의 intensiveness도 예상할 수 있다면 그에 맞춰서 placement 시 적절한 tier 혹은 적절한 node로 찾아가게 할 수 있을 것임.\n\n\n* context 정보는, 수집되는 위치에 따라, server-side에서 수집되는 context data와 storage-side에서 수집되는 context data로 분류될 수 있음. Event context data는 server-side에서 수집 가능하며, coaccess context data 및 periodicity context data는 storage-side에서 수집될 수 있음.\n\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n: 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n\n=== Design/Implementation Issues ===\n\n* workload type/class tagging 필요\n\n* cache와 tiering을 같이 쓸 때, DRAM cache가 아니라 Flash Cache의 경우 쓸데없이 모든 data가 Flash Cache에 다 들어가게 두기 보다는, tiering에서 cover하지 못하는 것들 위주로 caching하도록 하면 Flash Cache의 공간 낭비를 줄임으로써, 유효 cache 공간을 늘일 수 있게 되고, 그만큼의 성능 향상으로 연결될 수 있음.\n\n* DRAM 기반의 write cache를 적극 이용하여 마치 log-structured file system에서 제공하는 I/O merging and sequentialization 효과를 얻을 수 있음.\n\n* Tiering되는 data는 caching되는 data보다 상대적으로 coarse-grained하게 grouping하여 다룸으로써, 한꺼번에 많은 data를 move할 수 있도록 하고, meta data 관리에 필요한 overhead를 줄일 수 있게 한다. 당장 access되지 않더라도 일정 시간 내에 같이 access될 수 있는 data들은 함께 faster tier로 옮겨진다. 이때 필요한 정보가 coaccess 정보임.\n\n* Cache - Tiering 기법의 연동 방법: 예측이 실패했거나, target tier media의 가용 공간이 부족하여 tiering에서 놓치는 missing data가 있을 수 있음. 이러한 missing data들이 temporal locality가 있는 경우에는 당장은 faster tier에서 serve하지 못하더라도 cache에서 serve될 수 있는데, temporal locality가 적은 경우에는 cache만으로 cover되지 않는다. \n\n* Management table size optimization issue - 만약 모든 LBA 별로 event-based I/O prediction을 해야 한다면 현실적으로 사용 가능한 아키텍쳐가 아닐 수 있음. 만약 spatial locality가 있어서 LBA 1,000개 단위로 묶어서 대응할 수 있다면, 최소 1/1000 의 management table size 감소 효과를 얻을 수 있음. pattern mining 혹은 machine learning 실행 시에는 record 수가 감소함으로써 얻는 이득은 1,000배 이상이 될 수 있음. 관건은 얼마나 LBA 묶음을 크게 만들어서 예측에 사용할 수 있는지임.\n\n\n=== I/O Insight ===\n\n* ML 기반의 I/O Insight 발굴 (Data Placement를 위한 Macroscopic Guideline)\n: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:-> 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n\n* I/O Insight의 요건:\n:# indication, as simple as possible\n:# indication, as specific as possible\n:# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n:# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n\n* I/O Insight의 형태\n: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n=== X-Y Modeling ===\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n=== 예측 실패 대비 (별도 특허화 가능) ===\n\n* I/O 예측이 실패할 수 있으며, 이로 인한 성능 penalty를 최소화하기 위한 방법 필요.\n:- fast tier에 여유공간이 있다면 1타겟과 2타겟을 모두 올려놓아도 되지 않을까? 1타겟과 2타겟을 placement함으로써 예측 정확도를 95%까지 올릴 수 있다면, 꽤 쓸만할 수 있겠음. 그리고 1타겟과 2타겟의 모든 데이터를 다 fast tier에 올려두지 말고 일부만 fast tier에 올려두었다가, 둘 중에 하나의 타겟이 hit되었을 때, hit된 타겟의 나머지 data를 재빨리 fast tier에 올리도록 하는 incremental 방법을 사용할 수 있겠음.\n:- 그러나 이것 또한 예측 실패에 대한 완벽한 prevention이 될 수는 없다. 따라서 예측 실패가 발생하더라도 재빨리 그 성능 penalty를 mitigation할 수 있는 방법이 필요. \n:- cache를 이용하여 mitigation할 수 있을까?\n\n=== Data Read/Write 관련 Issues ===\n\n* read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n\n* 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n\n* write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간에, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면?\n\n* 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있음.\n\n* 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간이 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 scatter했던 I/O들을 나중에 다시 읽어들이려 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 원하는 고성능을 낼 수 있게 된다.\n\n* replica-based dynamic clustering\n: 위와 같은 write-and-then-read-again 상황에서, 해당 data를 저장했었던 HDD의 I/O busy/idle 여부에 상관 없이, 최고의 효과를 낼 수 있게 하기 위해, replica management를 적극적으로 활용하는 것은 어떨까? HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까?\n\n* data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 논의가 아님. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 논의임.\n\n* 그러나, 조심해야 할 것은 위의 논의는 상당히 case-dependency를 상당히 많이 탈 수도 있다는 것임. 책상 위의 아이디어로 끝날 수도 있기 때문에 조심스럽게 접근하는 것이 필요함.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-008 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n\n\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n----\n=== (SmartSSD API) SSD-internal I/O pattern logging interface and mechanism ===\n\n\n==== Questions ====\n\n* SmartSSD에서 제공할 수 있는 logging 서비스로서 어떤 것들이 있을 수 있나?\n:- 어느? 정도의 free space를 필요로하는 critical (heavy) I/O가 어떤? 주기로 도래할 지 알 수 있을까?\n:- I/O prediction에 도움될 수 있는 정보를 SmartSSD가 기록했다가 필요한 때 (안전한 방법으로?) 줄 수 있을까?\n:- I/O prediction까지는 아니더라도 해당 SSD의 I/O wellness(어떻게 정의?)를 측정했다가 bottleneck을 회피하는 데 유용한 정보를 제공할 수 있을까?\n:- 어떤 SSD가 현재 주어지고 있는 I/O workload에 대해 list performance spec을 만족하는데 어려움을 겪고 있다는 것을 알려줄 수 있는 정보가 어떤 것이 있을까? (SSD 내 write buffer의 fullness 혹은 I/O queue의 유동성? amount of free space in over-provisioning area?) (-> I/O wellness 라는 metric을 이것과 연관시켜 정의할 수도 있으며, 결국 SLA 혹은 Performance QoS와 연관시킬 수 있음)\n:- 지금까지의 S.M.A.R.T.와 비교하였을 때, 어떤 차별점이 있는가? 기존의 S.M.A.R.T.가 제공하지 못하던 타입의 정보를 제공하고, 이로 인해 기존에는 불가능했던 새로운 알고리즘 구현 혹은 새로운 서비스 제공이 가능해짐을 보이면 좋겠음.\n\n\n\n:- workload의 history?\n:- 각 erase block이 erase되어야만 했던 주기?\n:- free space의 추이 (얼마나 많이 남아 돌던지)?\n:- I/O의 heaviness 패턴?\n:- SSD내 write buffer (혹은 write cache)의 utilization정보? (즉, write buffer가 넘칠 정도가 되어서 I/O wait을 해야만 했던 상황이 얼마나 자주 발생했는지, write buffer의 가득찬 정도를 백분율로 표현한다고 했을 때, 평균/표준편차 등으로 대표할 수 있는 normal distribution을 따르는지?\n:- 전체 sequential read/write \n\n\n==== 배경 ====\n\n* 각 SSD 모델별로, 또 동일한 모델의 SSD라 하더라도 어떤 workload에 노출되어왔는지에 따라, 현재 낼 수 있는 I/O performance가 동일하지 않을 수 있다 (!check! 관련 실험 결과 - SSD 830을 이용하여 같은 시간 동안 하나의 host에서 생성되는 workload들을 나누어 받았다 하더라도 내부적인 free block의 갯수 및 random write에 의한 block 내 파편화등의 상태가 상이하여 성능에 차이가 나는 것을 보여줄 것. uFLIP 혹은 filebench 활용 가능). 한 예로, SSD가 어느 호스트의 어느 file system에 매핑되었는지에 따라 그 SSD가 받는 workload의 특성이 다를 수 있다. file system mount point가 달라지거나, 기존과 다른 새로운 응용이 설치/서비스되는 상황이 되면 \n\n\n----\n=== I/O Prediction-based Proactive Data Placement ===\n\n* 미래에 access될 data를 예측하여 적소에 미리 가져다 놓는 기술로서, (1) I/O prediction을 위한 정보 수집 모듈, (2) 주어진 정보들에 기반하여 다가올 I/O를 예측하는 모듈, (3) data placement를 수행하는 모듈로 구성됨.\n\n\n* 본 기술의 특징 1: I/O를 triggering하는 indicator (전조) 정보가 감지되면 해당되는 data block을 미리 필요한 장소에 proactive하게 placement시키는 방식 (기존의 accumulated access frequency 기반 naive prediction과 차별화됨)\n:- I/O prediction에 의거하여 \'어떤\' data에 대한 tiering이 \'언제\' 일어나야 할 지를 결정 (elastic tiering period 특징을 가짐 - 기존 방식은 static tiering period)\n:- Run-time에 저비용으로 감지될 수 있는 Indicator 정보와, 이에 의해 access될 data 간의 mapping은 Bayesian Inference 이용\n\n\n* 본 기술의 특징 2: data access의 skewness를 고려한 선택적 caching/tiering\n: Address별 data access skewness 및 workload의 dynamicity를 관찰하여 constant, highly skewed workload에 대해서는 저비용의 reactive method (e.g., caching)를 적용하고, hot data의 무게중심이 dynamic하게 변화하는 workload에 대해서는 I/O prediction을 이용하여 proactive한 tiering을 실시\n\n\n\n\n----\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n\n----\n\n=== Tiering Method Avoiding I/O Bottleneck ===\n\n* I/O Prediction에 의해 예측된 미래의 Hot Data를 Proactive하게 Placement한다고 할 때, 예상되는 access pattern으로 인해 발생 가능한 I/O Bottleneck을 최소화할 수 있도록 배치하는 방법\n\n* 예를 들어 미래의 hot data block 1 (200MB), hot data block 2 (100MB), hot data block 3 (400MB)이 1분 후에 access 될 것으로 예측되었다고 가정. data block 1에 대해서는 4KB 단위로 random read가 dominant한 access pattern이 예상되고, data block 2에 대해서는 1024KB 단위의 sequential read가 dominant한 access pattern이 예상되고, data block 3에 대해서는 4KB 단위의 random read와 512KB 단위의 sequential write이 각각 80:20의 비율로 mix된 형태의 access pattern이 예상된다고 했을 때, 각 data block을 어디에 어떤 형태로 placement시킬 것인가?\n\n: machine learning 혹은 pattern mining에 기반한 base workload pattern data를 기반으로 incremental하게 update될 수 있는 data access의 spatial locality와 periodicity\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n\n* 기존 기술과의 대비 (naive I/O prediction vs. full-fledged I/O prediction)\n: 기존 기술은 누적 hit count 정보에 의지하는 naive I/O prediction으로 볼 수 있음. 이러한 방식은 workload의 변화를 제대로 반영하지 못하기 때문에, static한 workload에 대해서는 잘 동작하지만, moving target처럼 dynamic한 workload에 대해서는 오히려 과거의 누적 hit count metric이 현재의 workload 패턴을 왜곡하는 문제점을 가지고 있음. (temporal locality와 spatial locality가 깨지는 순간이 여러번 존재할 수록 기존의 naive I/O prediction 방식으로 성능 향상은 어려워짐)\n\n\n\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(65,'== 20130530_134618 ==\n\n=== Frequent patterns/items mining examples with R::arules ===\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n* References\n:# [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))]\n:# [http://www.analysis-of-patterns.net/files/bgoethals.pdf Frequent Pattern Mining Slides - Universiteit Antwerpen]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning - Wikipedia]\n:# [http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf Frequent Pattern Mining in Web Log Data]\n:# [http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010 Temporal Pattern Mining Tutorial KDD 2010]\n:# [http://cran.r-project.org/web/packages/arules/index.html arules: Mining Association Rules and Frequent Itemsets]\n:# [http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf Introduction to arules - A computational environment for mining association rules and frequent item sets -- http://cran.r-project.org/]\n:# [http://lyle.smu.edu/IDA/arules/ Intelligent Data Analysis Lab @ SMU]\n:# [http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))]\n:# [http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/ Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning]\n:# [http://en.wikipedia.org/wiki/Sequence_mining Sequence mining]\n:# [http://en.wikipedia.org/wiki/GSP_Algorithm GSP Algorithm]\n:# [http://en.wikipedia.org/wiki/Apriori_algorithm Apriori algorithm]\n:# [http://en.wikipedia.org/wiki/Market_basket_analysis Market basket analysis]\n:# [http://en.wikipedia.org/wiki/Time_series Time series]\n\n\n----','utf-8'),(66,'== ## bNote-2013-03-29 ==\n\n=== DailyTask ===\n\n* IOWA\n\n\n=== MBO 2013 (목표 최종 확정) ===\n\n* 정명준 MBO\n <pre>\n\n30%, ~10/31\n- I/O Workload Analysis 기술 연구\n  : Dominant I/O Pattern Mining 및 Machine Learning 기반의\n    I/O 패턴 모델링 및 예측\n  : I/O Pattern 분석/예측 모델 수립\n  : I/O Pattern Mining/Learning 엔진 구현 (Python, R, Shell-script)\n\n30%, ~10/31\n- Data Placement 기술 연구\n  : Workload Analysis 결과로 얻어진 I/O Insight/Prediction을\n    활용하여 Data를 적소에 미리 배치\n  : Linux Kernel Module 형태로 Tiering 기술 형태로 구현\n  : Proactive Data Placement를 통해 분산 스토리지의 I/O 성능\n    80% 이상 개선 검증 (시뮬레이션, 혹은 Real 시스템 기반)\n  \n20%, ~10/31\n- A급 특허 3건 작성 및 심의 통과\n\n20%, ~10/31\n- 논문 1편 (To be accepted)\n\n</pre>\n\n\n* 과제 MBO (이전문님)\n <pre>\n* 분산 플랫폼 관련 특허 15편 이상 특허심의 통과 (전략출원 2편 이상 심의 통과) (30%)\n* 분산 플랫폼 관련 논문 2편 이상 accept (20%)\n* I/O coordination과 Proactive Placement를 통해 분산 스토리지의 I/O 성능 80% 이상 개선 검증\n  (HW RAID 대비) (15%)\n* 분산 Deduplication 기술을 통해 분산 스토리지에서 데이터 제거효율 3배, Coverage 4 node 달성 (3x@4node) (15%)\n* 분산 I/O Coordination 관련 기술이전 1건 (20%)\n</pre>\n\n=== 과제 변경 ===\n\n <pre>\n\n과제명: Intelligent Large-scale Data Management\n (구과제명) Real-Time Big Data Platform\n\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n\n</pre>\n\n== ## bNote-2013-03-28 ==\n\n=== SW설계기술리더양성 교육 지원 ===\n* SW Architect 사전 교육\n\n==== 현업 프로젝트 기획서 ====\n\n* 지원과정\n: SW 설계 리더 양성과정\n\n* 과제명\n: Data-intensive Storage\n\n* 프로젝트 참여자\n <pre>\n이주평	전문 연구원	Project Leader\n정명준	전문 연구원	시스템 설계, 요소기술 연구, 기능모듈 구현\n유개원	전문 연구원	요소기술 연구, 기능모듈 구현\n이형주	SDS 차장	기능모듈 구현, 기능/성능 검증\n</pre>\n\n* 과제 담당 임원\n: 심은수 상무\n\n* 과제 개요\n <pre>\n[배경 및 현안]\n□ 데이터 폭증으로 데이터센터/기업의 클라우드 스토리지 니즈 증대\n□ 클라우드 스토리지의 핵심 경쟁력은 성능 및 용량 향상 기술에 있음\n□ H/W 수준을 높이거나 S/W 최적화 기반으로 시스템의 성능을 개선\n   하는 기존 접근 방식으로는 H/W 한계를 넘어서는 성능 향상은 어려움\n□ 데이터 I/O 속도와 데이터 저장 효율을 획기적으로 개선할 수 있게 하는\n   지능적 Data Management 기술은 클라우드 스토리지 시스템의 경쟁력을\n   혁신하는 핵심 S/W 기술임\n\n[목적]\n□ 본 Sub Task에서는 지능적 Data Management 기술 중,\n   데이터 I/O 속도 향상 기술을 연구/개발한다\n   * I/O Workload Analysis에 기반한 Proactive Data Placement 기술 확보\n     - Real trace data에 대한 I/O Workload Analysis를 통해\n       dominant workload 패턴 발굴 및 I/O 예측 모델 학습\n     - I/O 예측 모델에 기반한 multi-tier (horizontal - vertical) 간\n       proactive data 배치 수행\n</pre>\n\n* 목표\n <pre>\n[기능/성능/품질]\n□ I/O Workload Analysis에 기반한 Proactive Data Placement\n  - I/O Workload Analysis 모듈\n    - Real trace data 수집 기능\n    - Trace data parsing 및 transform 기능 (analysis를 위한 전처리)\n    - Dominant workload pattern 추출 및 I/O model 학습\n  - Proactive Data Placement 모듈\n    - Tier management 및 data move 기능\n	- I/O monitoring 및 hot/cold 판단 기능\n\n[중간 산출물]\n□ Hot/Cold Data Placement 모듈\n  - 핵심적인 automated tiering 기능 구현\n    : Data access 패턴 관찰을 통해, hot data는 고속의 storage tier에,\n      cold data는 상대적으로 느린 속도의 storage tier에, 주기적 배치\n  - 동일 수준의 시스템에서, 동일 workload에 대해,\n    경쟁사 (E사) 수준의 성능 달성 (metric: average IOPS)\n\n[최종 결과물]\n□ Proactive Data placement 모듈\n  - I/O 예측 모델에 기반한 proactive data placement\n    : Dominant workload 패턴 분석 및 I/O 예측 모델에 기반한\n	  multi-tier 간 선제적 data 배치를 통해 I/O 성능 향상\n  - 동일 수준의 시스템에서, 동일 workload에 대해,\n    경쟁사 (E사) 대비 100% 이상의 성능 향상 달성 (metric: average IOPS)\n</pre>\n\n* 기대 효과\n <pre>\n□ 지능적 Data Management 기술은 Big Data를 다루는\n   클라우드 스토리지 서비스의 핵심 기술로 활용 가능\n   - Big Data 시장에서는 특히 스토리지 분야가 연간 61.4%의 성장율로\n     전체 시장 성장을 주도\n</pre>\n\n* 과제 구성\n <pre>\n[전체 Architecture]\n□ Proactive Data Placement 시스템은\n   Workload Analysis 모듈과 Data Placement 모듈로 구성됨\n  * Workload Analysis 모듈은 Trace Log 데이터에 대한\n    Off-line I/O Analysis를 수행하여 I/O 패턴에 대한 Insight을 확보함\n    (e.g., 어느 위치의 Data가 언제쯤 Access될 것인지를 예측)\n  * Data Placement 모듈은 I/O 패턴에 대한 Insight 정보와, 실시간으로\n    모니터링되는 시스템 상태 정보를 이용하여, Data를 미리 적소에 배치함\n\n[과제 적용부 기술항목]\n□ Workload Analysis 모듈: I/O Prediction Model Optimization 이슈\n  - 응용 및 시스템 특성에 따라 Workload 특성이 다를 수 있음\n    Workload 별로 Prediction Model을 구성하는 주요 X\'s 의 최적화 필요\n□ Data Placement 모듈: Overhead 최소화 및 Tiering 구현 최적화 이슈 \n  - Real-time Monitoring으로 인해 시스템에 가해지는 Overhead 최소화 필요\n  - Tiering 기능 구현 시 I/O 특성 및 시스템 구조를 반영한 최적화 필요\n</pre>\n\n\n==== 입과 추천서 ====\n\n[본인 업무 이력]\n\n* 2004.08 ~ 2007.09 : Security & Trusted Computing 기술 연구/개발\n:- 휴대폰 Content/Right Protection 기술인 OMA DRM S/W 개발, 무선사에 기술 이전\n:- Secure MMC를 위한 Crypto Engine 개발 참여 및 MMC IOP T/F 활동, 메모리사에 기여\n:- System의 무결성 보장 기술인 Trusted Computing 기술 연구 주도, Mandatory Access Control 기술을 무선사에 이전, LiMo (Linux Mobile) Security 표준에 반영 (SubPL)\n:- A급 특허 6건 출원, 논문 2건 (ACM SACMAT \'08 등)\n\n* 2007.10 ~ 2008.05 : 전사 6시그마 MBB (Master Black Belt) 양성 과정\n:- 제 16기 6시그마 MBB 과정에 입과하여 6시그마 이론 연구 및 실습 과제를 진행하고 BB 교육 과정 강의를 진행하였음. MBB 인증 시험 통과\n\n* 2008.06 ~ 2010.10 : Virtualization 및 Operating System 기술 연구/개발\n:- H/W가상화 기술인 Xen Hypervisor의 Security 연구 참여\n:- OS가상화 기술 기반의 State Migration S/W 개발, 스토리지사업부로 기술이전(SubPL)\n:- Russia연구소와 협력, Android 부팅속도를 향상시키는 FastBoot 기술 연구 (SubPL)\n:- 본사 사업지원팀 Vision 2020 T/F에 핵심 멤버로 참여, 15개 미래 기술 테마 발굴\n:- A급 특허 6건 출원 (전략 출원 2건), 논문 1건 (MobiCom \'09)\n\n* 2010.11 ~ 2013.현재 : Data-intensive Storage 기술 연구/개발\n:- I/O Workload Analysis에 기반한 Proactive Data Placement 기술 연구 주도\n::- Workload Analysis에서 획득한 I/O에 대한 근본적인 이해를 바탕으로 Data Management 알고리즘을 혁신, 스토리지 시스템 성능을 향상시키는 기술임\n:- 본 과제는 메모리사의 사업영역 확장 및 \'클라우드 스토리지 서비스\'를 위한 스토리지 시스템 기술 확보에 기여하고 있음\n:- A급 특허 6건 출원, 논문 3건 (ICCE 등)\n\n[소속부서장 추천 사유]\n\n* (양성 후 활용계획)\n:- 스토리지 시스템 설계/구현 시 S/W Architect로 활용\n* (인물평 및 추천사유)\n:- 정명준 전문은 시스템 분야에 대한 깊은 기술적 이해와 원만한 커뮤니케이션 능력을 바탕으로한 성공적인 프로젝트 발굴/주도 경험을 가지고 있습니다.\n:- 향후 Architect로서, 해당 과제의 S/W 설계 리딩을 통해 스토리지 시스템의 차별화된 기술 경쟁력을 만들어 내는 데에 기여할 수 있을 것으로 판단되어, 금번 S/W 설계 리더 과정에 추천합니다.\n\n== ## bNote-2013-03-26 ==\n\n=== DailyTask ===\n\n* IOWA Proactive Data Placement Formulation\n* Data Representation (as a pre-processing for association rules mining)\n\n* Patentization\n:- Distributed Multi-level Caching\n:- IO Pattern-optimal Data Placement for Tiering\n:- Virtualization-aware Caching/Tiering/Placement\n\n* Study\n:- Btier\n:- Bcache\n:- Fusion IO Caching Technology (directCache, ioTurbine)\n:- EMC FAST (Fully Automated Storage Tiering)\n:- OpenStack\n:- Xen\n:- VASA, VAAI (VMware의 storage virtualization 기술들)\n:- PCIe fabric switching\n:- Software Defined Storage\n:- Virstore? (VMware가 인수?)\n\n* 심상무님께 주간보고 내용\n:- Tiering Test SW Platform 구축 건 (Open source 활용, SDS 이형주 차장님과 함께)\n:- Real Trace Log Data 확보 진행 건 (수퍼컴센터의 Analytics Workload Trace, VDI Trace)\n\n=== Patentization ===\n\n* Access Pattern Aware Tiering\n\n\n=== Memo ===\n\n* Turbine: <기계> 높은 압력의 유체를 날개바퀴의 날개에 부딪치게 함으로써 회전하는 힘을 얻는 원동기. 사용하는 유체의 종류에 따라 수력 터빈, 증기 터빈, 가스 터빈 따위가 있다.\n\n== ## bNote-2013-03-25 ==\n\n=== DailyTask ===\n\n* 업무 File 정리\n* IOWA Proactive Data Placement Formulation\n\n=== Patidea ===\n\n* proactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n:- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [http://www-03.ibm.com/systems/software/gpfs/][http://public.dhe.ibm.com/common/ssi/ecm/en/pos03096usen/POS03096USEN.PDF]\n:- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\n\n* Read cache, write cache colocation을 하면 어떨까?\n\n* advanced tiering: access pattern-aware optimal placement (APOP)\n:- Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n:: 예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n:- 이에 필요한 data access pattern 모니터링/분석 방법\n:: 데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\n::: NIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n:- 이를 위해 필요한 system architecture 구조\n:: 기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n=== Formulation: IOWA based Proactive Data Placement ===\n\n* Formulating [[IOWA]] or [[I/O Workload Analysis]]\n:- to clarify the total amount of work\n:- to clarify the sub-tasks (can be modularized)\n:- to clarify the area to get focused\n\n\n=== Information: S사, K사 ===\n\n* Kaminario, Solidfire\n: From 서정민 전문\n: SSIC 미팅노트로부터 FACT를 각색한 정보를 공유합니다.  제 의견은 반영하지 않았습니다.\n\n* SolidFire (SSIC 초기미팅 결과)\n:* 1. Company Overview\n::- 3 years, 82 people\n::- $37M in funding (현재 Series B 단계로, 2013년 Series C 가능성 있음)\n::- 12 customers, 4 announced: 2 private cloud enterprise customers. \n::- Multi-tenancy가 기반인 Cloud 시장을 타겟으로 제품 제작 (OpenStack, CloudStack 연동)\n:* 2. Technology: QoS, Scalability, Inline deduplication/Compression\n:: (a) QoS\n::: OS 내에서 QoS를 Volume 단위로 관리 \n::: IOPS/latency QoS support (No R/W separate QoS) \n:: (b) Scalability\n::: Full data distribution across all the nodes \n::: All the nodes contributes to rebuilds\n:* 3. Current Arch./Tech. (GA)\n::- System configuration: 5~100 nodes (they have 40 nodes in test)\n::- H/W Configuration\n:: (a) CPU: Dual 2.5GHz Sandy Bridge with 6 cores each. \n::: 10 Cores는 mostly compute intensive work including dedup and compression. \n::: 2 Core는 handles IO to SSDs\n:: (b) SSD: Viking for boot/metadata, Intel SATA SSDs (relies on supercap in SSD)\n:: (c) Network: iSCSI (FC/NFS in the future, NFS just for small filer)\n::- Performance\n::: Latency Avg is .5ms to 2ms. Worst is 20 to 30 ms. \n:* 4. 금년도 추가 개발계획 (일부)\n::- Remote replication, sync and async, coming in Q3\n::- Encryption is also on the roadmap. \n\n* Kaminario (SSIC 초기 미팅 결과)\n:* 1. Company Overview\n::- Found in 2008.3, Sequoia(VC) funded\n::- 30 patents (the engineers have 76 from the previous jobs)\n::- Target: general-purpose storage system (OLTP, OLAP, VDI)\n::: focusing Latency, Throughput, IOPS all\n::- Shipping scale-out systems for the last 2 and 1/2 years\n::- Competitors: XtremIO, SolidFire\n::- 엔터프라이즈 기능 포커스: resiliency, self-healing, automation 중심\n:* 2. Technology\n::- Core 기술에 대한 파악 결과 없음\n:* 3. Previous Arch.\n::- Dell Blade 서버 방식으로 Fusion-IO 탑재\n:* 4. Current Arch. (개발 중)\n::- 1U rack server 기반 SMART or STEC SAS SSDs 사용\n::: low cost SSDs, low end Xeon, 32GB memory 등 Cost를 줄이는 방식 채용\n::: \"They use LSI SAS controller but don’t use dual port functionality.\"\n::: No SATA SSD (SATA SSD는 신뢰성 문제 야기하는 것으로 판단)\n::: -> SSIC 전문가는 SAS Dual port 기술 개발을 실패하지 않았는가 하는 의문 제기\n::- Performance is about 100,000 IOPS/node.\n::- No Dedup/compression \n::- \"Their SPC-1 result has 20x better price performance than previous SPC results. \"\n::- Currently focus on reducing long tail numbers.  (already has good IOPS)\n::: Performance degradation: < 25% at loss of data node\n\n=== References ===\n\n* [http://www.kaseya.com/download/en-us/white_papers/KaseyaBuyersGuidePaper.pdf IT Systems Management Buyers’ Guide // Kaseya]\n* [http://www.sata-io.org/technology/6Gbdetails.asp SATA-IO Revision 3.1 Specification // Queued Trim Command]\n\n----\n\n== ## bNote-2013-03-22 ==\n\n <pre>\n(EMC (Forum OR World) VNX) ((performance OR \"iops\") AND (\"per dollar\" OR \"dollar per\" OR \"per $\" OR \"/$\" OR \"$/\")) \"vs\" (filetype:pdf OR filetype:ppt OR filetype:pptx)\n</pre>\n\n=== DailyTask ===\n\n----\n==== Books of Machine Learning / Data Mining ====\n* \"Machine Learning\" // Tom Mitchell, McGraw Hill, 1997 ((B.GOOD))\n:- [http://www.cs.cmu.edu/~tom/mlbook.html Book]\n:- [http://www.cs.cmu.edu/~tom/mlbook-chapter-slides.html Slides]\n\n* \"Mining of Massive Datasets\" // Anand Rajaraman, Jeffrey David Ullman ((B.GOOD))\n:- [http://i.stanford.edu/~ullman/mmds.html Book - Online Version]\n:- [http://i.stanford.edu/~ullman/mmds/book.pdf Download the latest book (PDF, 415 pages, approximately 2.5MB)]\n\n----\n\n==== Machine Learning / Data Mining ====\n\n* [http://en.wikipedia.org/wiki/Gradient_descent Gradient Descent]\n* [http://ko.wikipedia.org/wiki/%EC%9D%8C%ED%95%A8%EC%88%98]\n* [http://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95%EC%82%AC%EC%83%81]\n* [http://ko.wikipedia.org/wiki/%EB%AF%B8%EC%A0%81%EB%B6%84%ED%95%99]\n* [http://ko.wikipedia.org/wiki/%ED%8E%B8%EB%AF%B8%EB%B6%84]\n* [http://ko.wikipedia.org/wiki/%ED%8F%89%EA%B7%A0%EA%B0%92_%EC%A0%95%EB%A6%AC]\n* [http://www.iiswc.org/iiswc2008/Papers/012.pdf] Characterization of Storage Workload Traces from Production Windows Servers // Swaroop Kavalanekar, Bruce Worthington, Qi Zhang, Vishal Sharda\n\n----\n\n== ## bNote-2013-03-21 ==\n\n=== Official Death of ... ===\n* What to do? why?\n\n=== DailyTask ===\n\n* list of candidate tasks\n\n:- (~) IOWA DM/ML Planning\n::- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n::- Hadoop case 고려 (in-local-node vertical placement between DRAM-SSD-HDD 뿐만 아니라 horizontal placement (among distributed nodes in Hadoop cluster)도 고려)\n\n:- IOWA ML Study\n::- IOWA:: Bayesian Network study\n::- IOWA:: Neural Network study\n::- IOWA:: HMM study\n::- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n::- (~) IOWA:: Association Rule Mining, Frequent Item Sets Mining (Market Basket Problem)\n\n:- page cache to be revisited\n\n:- Real IO trace 확보 작업\n::- 김혁호 책임이 보내준 업무요청 사례 기반, 업무요청 문서 작성할 것\n::- (V) NetApp (김주영 과장)에게 연락 -> 스토리지 모델 정보 요청 -> 추가 문의 (Management Console에서 보다 자세한 정보를 볼 수는 없는지? NetApp OnCommand System Manager 같은 Tool을 사용해서라도..) -> 김주영 과장과 통화 (오전 10:20 ~ 46), 이번 주 중에 찾아뵙고 말씀 나누는 걸로 ...\n::- EMC 연락\n::- (~) Dell (지근영 대리)에게 연락\n:::- Dell EqualLogic PS6100XV iSCSI arrays [http://www.dell.com/us/enterprise/p/equallogic-ps6100xv/pd] [http://www.equallogic.com/products/default.aspx?id=10661]\n\n:- (~) Distributed Computing Problem Study\n::- CAP theorem (CAP: Consistency, Availability, Partition tolerance) [http://en.wikipedia.org/wiki/CAP_theorem]\n::- Paxos [http://en.wikipedia.org/wiki/Paxos_(computer_science)]\n::- State Machine Replication [http://en.wikipedia.org/wiki/State_machine_replication]\n\n----\n=== Formulation: IOWA Proactive Data Placement (moved to ## bNote-2013-03-25) ===\n----\n\n== ## bNote-2013-03-20 ==\n\n=== DailyTask ===\n\n* list of candidate tasks\n\n:- (~) IOWA DM/ML Planning\n::- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n::- Hadoop case 고려 (in-local-node vertical placement between DRAM-SSD-HDD 뿐만 아니라 horizontal placement (among distributed nodes in Hadoop cluster)도 고려)\n\n:- IOWA ML Study\n::- IOWA:: Bayesian Network study\n::- IOWA:: Neural Network study\n::- IOWA:: HMM study\n::- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n::- (~) IOWA:: Association Rule Mining, Frequent Item Sets Mining (Market Basket Problem)\n\n:- page cache to be revisited\n\n:- Real IO trace 확보 작업\n::- 김혁호 책임이 보내준 업무요청 사례 기반, 업무요청 문서 작성할 것\n::- (V) NetApp (김주영 과장)에게 연락 -> 스토리지 모델 정보 요청 -> 추가 문의 (Management Console에서 보다 자세한 정보를 볼 수는 없는지? NetApp OnCommand System Manager 같은 Tool을 사용해서라도..) -> 김주영 과장과 통화 (오전 10:20 ~ 46), 이번 주 중에 찾아뵙고 말씀 나누는 걸로 ...\n::- EMC 연락\n::- (~) Dell (지근영 대리)에게 연락\n:::- Dell EqualLogic PS6100XV iSCSI arrays [http://www.dell.com/us/enterprise/p/equallogic-ps6100xv/pd] [http://www.equallogic.com/products/default.aspx?id=10661]\n\n:- (V) 2013년 MBO 작성\n::- IOWA PDP (I/O Workload Analysis based Proactive Data Placement) 와 IOBA (I/O Bottleneck Analysis) 두 아이템으로 작성\n\n:- (~) Distributed Computing Problem Study\n::- CAP theorem (CAP: Consistency, Availability, Partition tolerance) [http://en.wikipedia.org/wiki/CAP_theorem]\n::- Paxos [http://en.wikipedia.org/wiki/Paxos_(computer_science)]\n::- State Machine Replication [http://en.wikipedia.org/wiki/State_machine_replication]\n\n----\n\n=== Formulation: IOWA Proactive Data Placement (moved to #bNote-2013-03-21) ===\n\n* [[http://kandinsky/wikini/index.php/Bnote_2013#Formulation:_IOWA_Proactive_Data_Placement]]\n\n=== Supercom Usage Statistics ===\n\n <pre>\nblusjune@jimi-hendrix:[~] $ ssh a1mjjung@supercom\na1mjjung@supercom\'s password:\nLast login: Tue Mar 19 19:28:27 2013 from 75.2.93.158\n----------------------------------------------------------\n| During : 20130311 ~ 20130317                            |\n| Username : a1mjjung , Application(Total jobs) : unix(3)\n----------------------------------------------------------\nTotal RUN time : 2 min 36 secs\nAverage RUN time : 52 secs\nMaximum RUN time : 1 min 14 secs\nAverage Wait time 1 secs\nMaximum Wait time 1 secs\n ---------------------------------------------------------\n</pre>\n\n----\n== ## bNote-2013-03-19 ==\n\n\n\n=== The Market-Basket Model ===\n\n\n=== IOWA::Outlook (MSN FileServer IO Trace // msnfs) ===\n\n* # of IOs (Read/Write/All)\n<pre>\na1mjjung@secm:[microsoft_msn_filesrvr_6h] $ wc -l tracelog.msn_filesrvr.[ARW]\n\n  29345085 tracelog.msn_filesrvr.A\n  19729611 tracelog.msn_filesrvr.R\n   9615474 tracelog.msn_filesrvr.W\n</pre>\n\n* Reads Outlook\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130313_154605.sigio_25.iowsz_2000.t1_2000] $ cat __simout.sigio_25.iowsz_2000.t1_2000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  17\n__valu__sig__ _n_o_sigaddrs :  173401\n__valu__sig__ _sigioc_acc :  5005517\n__valu__sig__ _sigaddrs_efficiency :  28.8667135714\n__valu__sig__ _n_o_addr_total :  5053016\n__valu__sig__ _ioc_total :  19729611\n</pre>\n\n* Writes Outlook\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130313_155325.sigio_25.iowsz_2000.t1_2000] $ cat __simout.sigio_25.iowsz_2000.t1_2000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  8\n__valu__sig__ _n_o_sigaddrs :  211100\n__valu__sig__ _sigioc_acc :  2989803\n__valu__sig__ _sigaddrs_efficiency :  14.1629701563\n__valu__sig__ _n_o_addr_total :  4506823\n__valu__sig__ _ioc_total :  9615474\n</pre>\n\n* Microsoft Production Workload Trace - Related Articles\n\n:- \"Characterization of Storage Workload Traces from Production Windows Servers\", IISWC 2008, Swaroop Kavalanekar, Bruce Worthington, Qi Zhang, Vishal Sharda, Microsoft Corporation [http://www.iiswc.org/iiswc2008/sildes/4_3.pdf Slides], [http://www.iiswc.org/iiswc2008/Papers/012.pdf Papers]\n\n:- \"Write Off-Loading: Practical Power Management for Enterprise Storage\" [http://static.usenix.org/event/fast08/tech/full_papers/narayanan/narayanan.pdf FAST 2008]\n\n=== R Tutorial (Data Frame, Preview) ===\n\n----\n==== Data Frame ====\nA data frame is used for storing data tables. It is a list of vectors of equal length. For example, the following variable df is a data frame containing three vectors n, s, b.\n\n <pre>\n> n = c(2, 3, 5) \n> s = c(\"aa\", \"bb\", \"cc\") \n> b = c(TRUE, FALSE, TRUE) \n> df = data.frame(n, s, b)       # df is a data frame\n</pre>\n\n----\n==== Built-in Data Frame ====\nWe use built-in data frames in R for our tutorials. For example, here is a built-in data frame in R, called mtcars.\n\n <pre>\n> mtcars \n               mpg cyl disp  hp drat   wt ... \nMazda RX4     21.0   6  160 110 3.90 2.62 ... \nMazda RX4 Wag 21.0   6  160 110 3.90 2.88 ... \nDatsun 710    22.8   4  108  93 3.85 2.32 ... \n               ............\n</pre>\n\nThe top line of the table, called the header, contains the column names. Each horizontal line afterward denotes a data row, which begins with the name of the row, and then followed by the actual data. Each data member of a row is called a cell.\n\nTo retrieve data in a cell, we would enter its row and column coordinates in the single square bracket \"[]\" operator. The two coordinates are separated by a comma. In other words, the coordinates begins with row position, then followed by a comma, and ends with the column position. The order is important.\n\nHere is the cell value from the first row, second column of mtcars.\n\n <pre>\n> mtcars[1, 2] \n[1] 6\n</pre>\n\nMoreover, we can use the row and column names instead of the numeric coordinates.\n\n <pre>\n> mtcars[\"Mazda RX4\", \"cyl\"] \n[1] 6\n</pre>\n\nLastly, the number of data rows in the data frame is given by the nrow function.\n\n <pre>\n> nrow(mtcars)    # number of data rows \n[1] 32\n</pre>\n\nAnd the number of columns of a data frame is given by the ncol function.\n\n <pre>\n> ncol(mtcars)    # number of columns \n[1] 11\n</pre>\n\nFurther details of the mtcars data set is available in the R documentation.\n\n <pre>\n> help(mtcars)\n</pre>\n\n----\n\n==== Preview ====\n\nPreview\nInstead of printing out the entire data frame, it is often desirable to preview it with the head function beforehand.\n\n <pre>\n> head(mtcars) \n               mpg cyl disp  hp drat   wt ... \nMazda RX4     21.0   6  160 110 3.90 2.62 ... \n               ............\n</pre>\n\n----\n\n==== Data Import ====\n\n\n\n\n\n\n\n\nIt is necessary to import the sample textbook data into R before you start working on your homework.\n\n* Excel File\n: Quite often, the sample data is in Excel format, and needs to be imported into R prior to use. For this, we use the read.xls function from the gdata package. It reads from an Excel spreadsheet and returns a data frame. The following shows how to load an Excel spreadsheet named \"mydata.xls\". As the package is not in the core R library, it has to be installed and loaded into the R workspace.\n\n <pre>\n> library(gdata)                   # load the gdata package \n> help(read.xls)                   # documentation \n> mydata = read.xls(\"mydata.xls\")  # read from first sheet\n</pre>\n\n* Minitab File\n: If the data file is in Minitab Portable Worksheet format, it can be opened with the read.mtp function from the foreign package. It returns a list of components in the Minitab worksheet.\n\n <pre>\n> library(foreign)                 # load the foreign package \n> help(read.mtp)                   # documentation \n> mydata = read.mtp(\"mydata.mtp\")  # read from .mtp file\n</pre>\n\n* SPSS File\n: For the data files in SPSS format, it can be opened with the read.spss function from the foreign package. There is a \"to.data.frame\" option for choosing whether a data frame is to be returned.\n\n <pre>\n> library(foreign)                 # load the foreign package \n> help(read.spss)                  # documentation \n> mydata = read.spss(\"myfile\", to.data.frame=TRUE)\n</pre>\n\n* Table File\n: A data table can resides in a text file. The cells inside the table are separated by blank characters. Here is an example of a table with 4 rows and 3 columns.\n\n <pre>\n100   a1   b1 \n200   a2   b2 \n300   a3   b3 \n400   a4   b4\n</pre>\n\nNow copy and paste the table above in a file named \"mydata.txt\" with a text editor. Then load the data into the workspace with the read.table function.\n\n <pre>\n> mydata = read.table(\"mydata.txt\")  # read text file \n> mydata                             # print data frame \n   V1 V2 V3 \n1 100 a1 b1 \n2 200 a2 b2 \n3 300 a3 b3 \n4 400 a4 b4\n</pre>\n\nFor further detail of the read.table function, please consult the R documentation.\n\n <pre>\n> help(read.table)\n</pre>\n\n* CSV File\nThe sample data can also be in comma separated values (CSV) format. Each cell inside such data file is separated by a special character, which usually is a comma, although other characters can be used as well.\n\nThe first row of the data file should contain the column names instead of the actual data. Here is a sample of the expected format.\n\n <pre>\nCol1,Col2,Col3 \n100,a1,b1 \n200,a2,b2 \n300,a3,b3\n</pre>\n\nAfter we copy and paste the data above in a file named \"mydata.csv\" with a text editor, we can read the data with the read.csv function.\n\n <pre>\n> mydata = read.csv(\"mydata.csv\")  # read csv file \n> mydata                           # print data frame \n  Col1 Col2 Col3 \n1  100   a1   b1 \n2  200   a2   b2 \n3  300   a3   b3\n</pre>\n\nIn various European locales, as the comma character serves as decimal point, the read.csv2 function should be used instead. For further detail of the read.csv and read.csv2 functions, please consult the R documentation.\n\n <pre>\n> help(read.csv)\n</pre>\n\n----\n\n== ## bNote-2013-03-18 ==\n\n=== DailyPlan ===\n\n* list of candidate tasks\n\n:- Real IO trace 확보 작업\n::- 김혁호 책임이 보내준 업무요청 사례 기반, 업무요청 문서 작성할 것\n::- [V://j] NetApp (김주영 과장)에게 연락 -> 스토리지 모델 정보 요청 -> 추가 문의 (Management Console에서 보다 자세한 정보를 볼 수는 없는지? NetApp OnCommand System Manager 같은 Tool을 사용해서라도..) -> 김주영 과장과 통화 (오전 10:20 ~ 46), 이번 주 중에 찾아뵙고 말씀 나누는 걸로 ...\n::- EMC 연락\n::- [~] Dell (지근영 대리)에게 연락\n:::- Dell EqualLogic PS6100XV iSCSI arrays [http://www.dell.com/us/enterprise/p/equallogic-ps6100xv/pd] [http://www.equallogic.com/products/default.aspx?id=10661]\n\n:- IOWA ML Planning\n::- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n::- Hadoop case 고려 (in-local-node vertical placement between DRAM-SSD-HDD 뿐만 아니라 horizontal placement (among distributed nodes in Hadoop cluster)도 고려)\n\n:- IOWA ML Study\n::- IOWA:: Bayesian Network study\n::- IOWA:: Neural Network study\n::- IOWA:: HMM study\n::- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n\n:- Netflix의 Cloud Computing Challenge 내용 파악\n(심상무님 지시: 거기가 우리보다 앞서 있으니, 어떤 기술들이 필요한지, 이슈가 무엇인지에 대한 힌트를 얻을 수 있을 것임)\n\n:- page cache to be revisited\n\n\n----\n\n=== Linux File Systems: Ext2 vs. Ext3 vs. Ext4 ===\n\n* [http://www.thegeekstuff.com/2011/05/ext2-ext3-ext4/]\n\next2, ext3 and ext4 are all filesystems created for Linux. This article explains the following:\n\n:- High level difference between these filesystems.\n:- How to create these filesystems.\n:- How to convert from one filesystem type to another.\n\n==== Ext2 ====\n\n* Ext2 stands for second extended file system.\n* It was introduced in 1993. Developed by Remy Card.\n* This was developed to overcome the limitation of the original ext file system.\n* Ext2 does not have journaling feature.\n* On flash drives, usb drives, ext2 is recommended, as it doesn’t need to do the over head of journaling.\n* Maximum individual file size can be from 16 GB to 2 TB\n* Overall ext2 file system size can be from 2 TB to 32 TB\n\n\n==== Ext3 ====\n\n* Ext3 stands for third extended file system.\n* It was introduced in 2001. Developed by Stephen Tweedie.\n* Starting from Linux Kernel 2.4.15 ext3 was available.\n* The main benefit of ext3 is that it allows journaling.\n* Journaling has a dedicated area in the file system, where all the changes are tracked. When the system crashes, the possibility of file system corruption is less because of journaling.\n* Maximum individual file size can be from 16 GB to 2 TB\n* Overall ext3 file system size can be from 2 TB to 32 TB\n* There are three types of journaling available in ext3 file system.\n:- Journal ? Metadata and content are saved in the journal.\n:- Ordered ? Only metadata is saved in the journal. Metadata are journaled only after writing the content to disk. This is the default.\n:- Writeback ? Only metadata is saved in the journal. Metadata might be journaled either before or after the content is written to the disk.\n* You can convert a ext2 file system to ext3 file system directly (without backup/restore).\n\n\n==== Ext4 ====\n\n* Ext4 stands for fourth extended file system.\n* It was introduced in 2008.\n* Starting from Linux Kernel 2.6.19 ext4 was available.\n* Supports huge individual file size and overall file system size.\n* Maximum individual file size can be from 16 GB to 16 TB\n* Overall maximum ext4 file system size is 1 EB (exabyte). 1 EB = 1024 PB (petabyte). 1 PB = 1024 TB (terabyte).\n* Directory can contain a maximum of 64,000 subdirectories (as opposed to 32,000 in ext3)\n* You can also mount an existing ext3 fs as ext4 fs (without having to upgrade it).\n* Several other new features are introduced in ext4: multiblock allocation, delayed allocation, journal checksum. fast fsck, etc. All you need to know is that these new features have improved the performance and reliability of the filesystem when compared to ext3.\n* In ext4, you also have the option of turning the journaling feature “off”.\n\n\n----\n\n=== Frequent patterns/items mining examples with R::arules ===\n\n[[Association rule mining]]\n\n=== R ===\n\n* png file output work-around against \'plot() error\' in R\n:- [http://www.mail-archive.com/r-help@r-project.org/msg40658.html]\n <pre>\nThe png() device does not need an X server to connect to. I think it\nused to in versions gone by, but not any more. Here I\'ve disabled X so\nthat X11() doesn\'t work, but png() still does:\n\n > x11()\n Error in X11(d$display, d$width, d$height, d$pointsize, d$gamma,\nd$colortype,  :\n   unable to start device X11cairo\n In addition: Warning message:\n In x11() : unable to open connection to X11 display \'\'\n > png(file=\"foo2.png\")\n > plot(1:10)\n > dev.off()\n null device\n          1\n\n I suspect your R was compiled without png support. What does the\n\'capabilities()\' function in R tell you?\n\n > capabilities()\n    jpeg      png     tiff    tcltk      X11     aqua http/ftp  sockets\n    TRUE     TRUE     TRUE     TRUE    FALSE    FALSE     TRUE     TRUE\n  libxml     fifo   cledit    iconv      NLS  profmem    cairo\n    TRUE     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE\n</pre>\n\n\n\n==== An Introduction to R ====\n\n* [http://cran.r-project.org/doc/manuals/R-intro.html#The-read_002etable_0028_0029-function An Introduction to R - Table of Contents]\n\n\n==== R Tutorial - (http://www.r-tutor.com/) ====\n\n* [http://www.r-tutor.com/gpu-computing/gaussian-process/rvbm Bayesian Classification with Gaussian Process]\n* [http://www.r-tutor.com/content/r-tutorial-ebook R Tutorial with Bayesian Statistics Using OpenBUGS]\n* [http://www.r-tutor.com/bayesian-statistics/openbugs Bayesian Inference Using OpenBUGS]\n* [http://www.r-tutor.com/gpu-computing/svm/rpusvm-2 Support Vector Machine with GPU, Part II]\n\n==== R: Input and output: scripts, saving and loading data ((B.GOOD)) ====\n\n* [http://egret.psychol.cam.ac.uk/statistics/R/savingloading.html Cambridge University]\n\n\n* General file-handling commands\n <pre>\nsetwd(\"c:/myfiles\") # use / or \\\\ to separate directories under Windows (\\\\ becomes \\ once processed through the escape character mechanism)\ndir() # list the contents of the current directory\n</pre>\n\n\n* Running scripts\n <pre>\nsource(\"myfile.R\") # load and execute a script of R commands\n</pre>\n\n* For a startup script\n: edit \".Rprofile\" in your home directory (for details see ?Startup). Here\'s an example\n <pre>\n# RNC ~/.Rprofile\n\n# auto width adjustment\n.adjustWidth <- function(...){\n       options(width=Sys.getenv(\"COLUMNS\"))\n       TRUE\n}\n.adjustWidthCallBack <- addTaskCallback(.adjustWidth)\n\n.First <- function() cat(\"\\n   Script ~/.Rprofile executed.\\n\\n\")\n.Last <- function()  cat(\"\\n   Goodbye!\\n\\n\")\n</pre>\n\n\n* Redirecting output\n <pre>\nsink(\"myfile.txt\") # redirect console output to a file\nsink() # restore output to the screen\n\npdf(\"mygraph.pdf\") # subsequent graphical output will go to a PDF\npng(\"mygraph.png\") # subsequent graphical output will go to a PNG\njpeg(\"mygraph.jpeg\") # subsequent graphical output will go to a JPEG\nbmp(\"mygraph.bmp\") # subsequent graphical output will go to a BMP\npostscript(\"mygraph.ps\") # subsequent graphical output will go to a PostScript file\ndev.off() # back to the screen\n</pre>\n\n\n* Text files\n <pre>\nmy.data = read.csv(filename)\nmy.data = read.csv(file.choose())\n# Note: (1) = and <- are synonymous, and are the assignment operator (while == tests for equality)\n#       (2) file.choose() pops up a live filename picker\n#       (3) The default is to assume a header row with variable names (header=TRUE),\n#           and no row names, but you can change all these defaults (e.g. row.names=1 reads\n#           row names from the first column).\n\nattach(my.data) # you might then want to attach the new data to the path, though this is optional\n\nwrite.csv(my.data, filename2) # Write the data to a new file. There are several options available; see the help (use ?write.csv)\nwrite.csv(my.data, file=\"d:/temp/newfile.csv\", row.names=FALSE) # Here\'s one: turn off row names to avoid creating a spurious additional column.\n\nread.table(...)  # } A more generic way to read/write tabular data from/to disk\nwrite.table(...) # } (read.csv and write.csv are specialized versions of read.table and write.table)\n</pre>\n\n\n* Microsoft Excel spreadsheets\n <pre>\nlibrary(RODBC)\nchannel <- odbcConnectExcel(\"Osteomalacia_data.xls\") # specify the filename\npatientdata <- sqlFetch(channel, \"Vitamin_D_levels\") # specify a sheet within the spreadsheet\nindexcasedata <- sqlFetch(channel, \"Sheet2\") # by default Excel names individual sheets Sheet1, Sheet2, ..., though you may have renamed them something more informative\nodbcClose(channel)\n</pre>\n\n\n* SPSS data\n <pre>\nlibrary(foreign)\nmydata <- data.frame(read.spss(\"filename.sav\"))\n# Remember you can also use file.choose() in place of the filename, as above.\n</pre>\n\n\n* ODBC data sources (databases)\n <pre>\n# 1. Connect\nlibrary(RODBC)\nchannel <- odbcConnect(\"my_DSN\") # specify your DSN here\n# if you need to specify a username/password, use:\n#  channel <-odbcConnect(\"mydsn\", uid=\"username\", pwd=\"password\")\n\n# 2. List all tables\nsqlTables(channel)\n\n# 3. Fetch a whole table into a data frame\nmydataframe <- sqlFetch(channel, \"my_table_name\") # fetch a table from the database in its entirety\nclose(channel)\n\n# 4. Fetch the results of a query into a data frame. Example:\nmydf2 <- sqlQuery(channel, \"SELECT * FROM MonkeyCantab_LOOKUP_TaskTypes WHERE TaskType < 6\")\n</pre>\n\nIf you\'re using MySQL, you can talk to the database directly:\n <pre>\nlibrary(RMySQL) # use install.packages(\"RMySQL\") if this produces an error\n# if the install.packages() command produces an error, under Ubuntu:\n# use \"sudo apt-get install libmysql++-dev\" (in addition to MySQL itself, i.e. the\n# \"mysql-server mysql-client mysql-navigator mysql-admin\" packages)\ncon <- dbConnect(MySQL(), host=\"localhost\", port=3306, dbname=\"mydatabase\", user=\"myuser\", password=\"mypassword\")\ndbListTables(con)\ndbListFields(con, \"table_name\")\nd <- dbReadTable(con, \"table_name\")\ne <- dbGetQuery(con, \"SELECT COUNT(*) FROM table_name\")\n# and much more possible\n</pre>\n\n\n* R native format\n <pre>\nsave(myobject1, myobject2, ..., file=\"D:/temp/mydata.rda\")\nload(file=\"D:/temp/mydata.rda\")\n# note that the load command recreates the \"mydata\" object without prompting\n# you can also use save.image() to save a whole workspace\n</pre>\n\n\n* Other data-moving techniques\nTo export the definition of an R object (which you can then re-import using \"object = THISTHING\"):\n <pre>\ndput(object, \"\")\n</pre>\n\nTo read a tabular object with a header row from the clipboard\n <pre>\nobject = read.table(\"clipboard\", header=T)\n</pre>\n\n----\n\n=== Samsung SSD 840 Series Information ===\n\n* [http://thessdreview.com/our-reviews/samsung-840-series-240gb-ssd-review-the-worlds-first-tlc-ssd-takes-the-stage/4/ Samsung 840 Series 250GB SSD Review ? The Worlds First TLC SSD Takes Center Stage]\n\n* [http://www.techspot.com/review/578-samsung-840-pro-ssd/ Samsung 840 Pro SSD Review]\n\n----\n\n=== Gnuplot Tips ===\n\n\n* How to unset key [http://people.duke.edu/~hpgavin/gnuplot.html]\n <pre>\n      Create a title:                  > set title \"Force-Deflection Data\" \n      Put a label on the x-axis:       > set xlabel \"Deflection (meters)\"\n      Put a label on the y-axis:       > set ylabel \"Force (kN)\"\n      Change the x-axis range:         > set xrange [0.001:0.005]\n      Change the y-axis range:         > set yrange [20:500]\n      Have Gnuplot determine ranges:   > set autoscale\n      Move the key:                    > set key 0.01,100\n      Delete the key:                  > unset key\n      Put a label on the plot:         > set label \"yield point\" at 0.003, 260 \n      Remove all labels:               > unset label\n      Plot using log-axes:             > set logscale\n      Plot using log-axes on y-axis:   > unset logscale; set logscale y \n      Change the tic-marks:            > set xtics (0.002,0.004,0.006,0.008)\n      Return to the default tics:      > unset xtics; set xtics auto\n</pre>\n\n* Mouse and hotkey support in interactive terminals\n\n: Interaction with the current plot via mouse and hotkeys is supported for the X11, OS/2 Presentation Manager, ggi and Windows terminals. See `mouse input` for more information on mousing. See help for bind for information on hotkeys. Also see the documentation for individual mousing terminals `ggi`, `pm`, `windows` and `x11`.\n\n: Here are briefly some useful hotkeys. Hit \'h\' in the interactive interval for help. Hit \'m\' to switch mousing on/off. Hit \'g\' for grid, \'l\' for log and \'e\' for replot. Hit \'r\' for ruler to measure peak distances (linear scale) or peak ratios (log scale), and \'5\' for polar coordinates inside a map. Zoom by mouse (MB3), and move in the zoom history by \'p\', \'u\', \'n\'; hit \'a\' for autoscale. Use other mouse buttons to put current mouse coordinates to clipboard (double click of MB1), add temporarily or permanently labels to the plot (middle mouse button MB2). Rotate a 3D surface by mouse. Hit spacebar to switch to the gnuplot command window.\n\n: Sample script: mousevariables.dem\n\n* [http://www.gnuplot.info/docs_4.0/gnuplot.html#Mouse_and_hotkey_support_in_interactive_terminals Mouse and hotkey support in interactive terminals -- Gnuplot info]\n\n=== NetApp Storage System Management Software ===\n\n* NetApp OnCommand System Manager [http://www.netapp.com/us/products/management-software/system-manager.aspx]\n:\n\n== ## bNote-2013-03-15 ==\n\n=== DailyPlanning 2013-03-15 ===\n\n* list of candidate tasks\n\n:- [V] HML basic concept study, 오늘 AP 주제에 대해 lightreading\n\n:- [V] 엄교수님께 특허 일정 전달\n\n:- Real IO trace 확보 작업\n::- [V] 김혁호 책임과 미팅 > 13:30 미팅 수행 (업무요청하기로 함)\n::- NetApp, Dell, EMC 측과 연락\n::- 지근영 대리에게 연락\n\n:- IOWA:: Bayesian Network study\n:- IOWA:: Neural Network study\n:- IOWA:: HMM study\n:- IOWA:: SVM, K-means study (R 기반? 혹은 Python 기반?)\n\n:- MSR IO trace 분석 >> IOWA의 X\'s, Y 분석 방안 고민\n\n:- page cache to be revisited\n\n:- Data Placement 이슈: Media Difference (RAM,SSD,HDD) 외에 어떤 이슈가 있는가? Hadoop 같은 경우 노드 간 수평적 이동 이슈 있음. 매우 중요.\n\n:- 계획 외 업무들\n::- 이주평전문님의 본사팀과의 Conference Call 위해, 상무님 회의 대신 참석 (차주 화요일: 소장님께보고, 목요일: 부원장님께보고), 액션아이템 이전문님과 팀원께 전달.\n::- AP 세미나 참석 (최희열 전문)\n::- 팀 미팅: 소장님보고 자료 대응 방안 논의 -> IO Prediction 기반의 time 차원 제어로 공간적인 IO 속도 제약 극복 (마치 SS랩의 cooperative caching case처럼)\n\n\n=== HML Study:: \"Reducing the Dimensionality of Data with Neural Networks\" ===\n\n* Gradient descent [http://en.wikipedia.org/wiki/Gradient_descent]\n\n:- Gradient descent is a first-order optimization algorithm [http://en.wikipedia.org/wiki/First-order_approximation]\n\n:- Gradient descent to find the local minimum, gradient ascent to find the local maximum\n:: Gradient descent를 이용하여 function의 local \'\'\'minimum\'\'\'을 찾아내기 위해서는, 현재 지점에서의 function의 \'\'\'negative\'\'\' of the gradient (or of the approximate gradient)에 비례하는 taking steps를 한다. 만약 \'\'\'positive\'\'\' of the gradient에 비례하여 taking step한다면 그 function의 local \'\'\'maximum\'\'\'에 다가가게 된다. 이러한 절차는 gradient ascent라고 한다.\n\n\n* Gradient [http://en.wikipedia.org/wiki/Gradient]\n: Vector calculus에서, scalar field의 gradient는 다음 조건을 만족하는 vector field이다.\n:: direction은 scalar field의 증가분 (rate of increase)이 가장 최대가 되는 방향이다\n:: magnitude는 그 증가분이 된다 {{ In vector calculus, the gradient of a scalar field is a vector field that points in the direction of the greatest rate of increase of the scalar field, and whose magnitude is that rate of increase. }}\n\n\n* Orders of approximation [http://en.wikipedia.org/wiki/First-order_approximation]\n: terms for how precise an approximation is.\n: to indicate progressively more refined approximations: in increasing order of precision, a zeroth order approximation, a first order approximation, a second order approximation, and so forth\n: (Formally) an nth order of approximation\n:: one where the order of magnitude of the error is at most x^n, 혹은 big O notation으로 나타낸다면, error는 O(x^n) 이다.\n: detailed explanation with examples\n::- Zeroth-order (constant; a flat line with no slope; a polynomial of degree 0)\n::: x = [0, 1, 2]\n::: y = [3, 3, 5]\n::: y ~ f(x) = 3.67\n::- First-order (a linear approximation; straight line with a slope; a polynomial of degree 1)\n::: x = [0, 1, 2]\n::: y = [3, 3, 5]\n::: y ~ f(x) = x + 2.67\n::- Second-order (a quadratic polynomial; geometrically, a parabola; a polynomial of degree 2)\n::: x = [0, 1, 2]\n::: y = [3, 3, 5]\n::: y ~ f(x) = x^2 - x + 3\n\n=== Memo ===\n\n* 엄교수님과 연락 내용\n\n:- 엄교수님께 특허 일정 전달 (2013-03-15, 10\n:: 교수님, 안녕하세요? 기술원의 정명준전문입니다. 특허일정을 알아본 결과 3월 29일까지 직무발명서 시스템 등록을 하면 된다고 합니다. 앞으로 2주 정도 여유가 있네요 ^^ 그동안 실험결과를 정리하고 핵심아이디어 및 청구항을 잘 정리하면 될 것 같습니다. 차주 목요일 쯤에 한 번 조박사와 통화하여 기술상세/청구항/기존특허비교/침해적발등을 같이 논의해보면 어떨까합니다만 교수님 보시기에는 어떠신지요? 오늘도 멋진 하루 보내시구요, 항상 감사합니다. 정명준 드림.\n\n=== 연락처 (자주 사용하는) ===\n\n* 기술원 이주평 전문 : 01025984182, 010-2598-4182, #9956 : jupyung.lee@samsung.com\n* 기술원 신현정 전문 : 0173249294, 017-324-9294, #9747 : pharoah@samsung.com\n* 기술원 서정민 전문 : 01025441231, 010-2544-1231, #9817 : tony.seo@samsung.com\n* 기술원 구본철 전문 : 01091905907, 010-9190-5907, #9704 : bc.gu@samsung.com \n* 기술원 유개원 전문 : : gaewon.you@samsung.com\n* 기술원 최희열 전문 : 01096236578, 010-9623-6578, #9692 : heeyoul.choi@samsung.com\n* 기술원 문민영 전문 : , , #9716 :\n* 기술원 최영상 전문 : , , #9951 :\n* 기술원 박상도 전문 : , , #9586 :\n* 기술원 전바롬 전문 : , , #9547 :\n* 기술원 송인철 전문 : , , #9962 :\n* 기술원 박정현 연구원 : , , #9238 :\n\n* 기술원 심은수 상무 : 01020518077, 010-2051-8077, #9950 : eunsoo.shim@samsung.com\n* 기술원 서영완 전문 : 01030020208, 010-3002-0208, #9843 : sywpro@samsung.com\n* 기술원 유연아 사원 : 01090338452, 010-9033-8452, #9858 : yeonah78.yu@samsung.com\n\n* 삼성 SDS ESDM 인프라그룹 이형주 차장님: _ : hj001.lee@partner.samsung.com\n\n* 기술원 에어컨 안나올 때 (기술원 통합 방재 센터, 과장, 지원팀 > 환경안전그룹): #9120 :\n* VDI (SBC) 문제 있을 때 (VDI HelpDesk): #8272 : \n* 네트워크 안될 때 (방화벽 등) 어디로?: :\n* CLMS 시스템 문의 - 한지연 선임 / 서초 인사 CI 그룹: :\n\n* 서울대 컴퓨터공학부 엄현상 교수님 : 0162324667, 016-232-4667, 02-880-6755 : hseom@cse.snu.ac.kr\n* 서울대 컴퓨터공학부 조인순 박사 : 01051317886, 010-5131-7886, 02-880-9330 : insoonjo@gmail.com\n* 서울대 컴퓨터공학부 성민영 석사과정 : 01047245304, 010-4724-5304 : mysung@dcslab.snu.ac.kr\n\n=== HML (Hierarchical Machine Learning) AP (Advanced Program) ===\n\n* 세미나 일정\n\n{| border=\"1\"\n| 이름\n| 논문제목\n| 날짜\n|-\n| 최희열\n| Reducing the dimensionality of data with neural networks [http://www.cs.toronto.edu/~hinton/science.pdf]\n| 03월 15일 \n|-\n| 민윤홍	\n| A fast learning algorithm for deep belief nets [http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf]\n| 03월 22일 \n|-\n| 성재모	\n| Graphical Models \n| 03월 29일 \n|-\n| 정명준	\n| Hierarchical Temporal Memory including HTM Cortical Learning Algorithms \n| 04월 05일 \n|-\n| 박상도 	\n| How to Grow a Mind: Statistics, Structure, and Abstraction	\n| 04월 12일\n|-\n| 전바롬	\n| Learning Hierarchical Models of Scenes, Objects, and Parts\n| 04월 19일\n|-\n| 이호섭\n| Building high-level features using large scale unsupervised learning\n| 4월 26일\n|-\n| 박정현\n| High-Performance Neural Networks for Visual Object Classification\n| 05월 03일\n|-\n| 이호식\n| Deep Neural Networks for Acoustic Modeling in Speech Recognition\n| 5월 10일\n|-\n| 이예하\n| Unsupervised feature learning for audio classification using convolutioinal deep belief networks\n| 05월 24일\n|-\n| 송인철\n| Multimodal Deep Learning\n| 05월 31일\n|-\n|}\n\n== ## bNote-2013-03-14 ==\n\n\n=== Hidden Markov model (HMM) ===\n\n[[Hidden Markov model (HMM)]]\n\n== ## bNote-2013-03-13 ==\n\n\n=== Supercom Usage Statistics ===\n\n <pre>\n----------------------------------------------------------\n| During : 20130304 ~ 20130310                            |\n| Username : a1mjjung , Application(Total jobs) : matlab(1)\n----------------------------------------------------------\nTotal RUN time : 2 min 16 secs\nAverage RUN time : 2 min 16 secs\nMaximum RUN time : 2 min 16 secs\nAverage Wait time 1 secs\nMaximum Wait time 1 secs\n ---------------------------------------------------------\n|                       Application(Total jobs) : unix(14)\n----------------------------------------------------------\nTotal RUN time : 13 min 32 secs\nAverage RUN time : 58 secs\nMaximum RUN time : 3 min 23 secs\nAverage Wait time 1 secs\nMaximum Wait time 2 secs\n ---------------------------------------------------------\n\n</pre>\n\n=== ACM Transactions on Storage ===\n\n삼성 SDS 강석우 상무님 요청으로 우리 팀이 Review하게 됨.\n\n* [http://mc.manuscriptcentral.com/tos Welcome to the ACM Transactions on Storage manuscript submission site]\n\n== ## bNote-2013-03-12 ==\n\n\n=== SNIA Real IO Traces ===\n\n\n----\n==== Microsoft Production MSNStorageFileServer ( msnfs ) ====\n\n* Summary (Reads/Writes - All)\n: 2008-03-10 01:00 + 6 hours\n: Total # of IOs\n:: = 29,345,085 (total)\n:: = 19,729,611 (reads) + 9,615,474 (writes)\n:: = 29345085 = 19729611 + 9615474\n: Average IOPS\n:: = 1358.56 (= 29345085 / (6 * 3600))\n: Average interval time between IOs\n:: = 736 micro-seconds (= (6 * 3600 * 10^6 ) / 29345085)\n\n\n* Summary (Reads)\n <pre>\na1mjjung@secm:[R] $ pwd\n/home/X0101/a1mjjung/x/iowa.sidewinder/sidewinder/microsoft_msn_filesrvr_6h/MSNStorageFileServer/tdir/iowa-mw30m/R\n\na1mjjung@secm:[R] $ grep __valu__sig__ f030.infile_R.iowa.anal_s0010 \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  17\n__valu__sig__ _n_o_sigaddrs :  173401\n__valu__sig__ _sigioc_acc :  5005517\n__valu__sig__ _sigaddrs_efficiency :  28.8667135714\n__valu__sig__ _n_o_addr_total :  5053016\n__valu__sig__ _ioc_total :  19729611\n</pre>\n\n\n* Trace Log Field Information\n <pre>\n\n       1,         2,                 3,        4,      5,          6,      7,           8,       9,       10,          11,      12,       13,         14,       15\nDiskRead, TimeStamp, ProcessName (PID), ThreadID, IrpPtr, ByteOffset, IOSize, ElapsedTime, DiskNum, IrpFlags, DiskSvcTime, I/O Pri,  VolSnap, FileObject, FileName\n\nDiskWrite, TimeStamp, ProcessName (PID), ThreadID, IrpPtr, ByteOffset, IOSize, ElapsedTime, DiskNum, IrpFlags, DiskSvcTime, I/O Pri, VolSnap, FileObject, FileName\n</pre>\n\n\n* [[Trace Data Preprocessing Screenshot - Microsoft Production Trace - MSN FileServer]]\n\n\n----\n\n==== MSR Cambridge IO Traces ====\n\n\n* Summary\n: 22.4GB Trace Data from Data center servers\n:: \'\'\'13 servers, 36 volumes, 179 disks, 1 week\'\'\'\n\n\n* Backgrounds\n: Many enterprise servers are less I/O intensive than TPC benchmarks, which are specifically designed to stress the system under test. Enterprise workloads also show significant variation in usage over time, for example due to diurnal patterns.\n: In order to understand better the I/O patterns generated by standard data center servers, we instrumented the core servers in our building\'s data center to generate per volume block-level traces for one week.\n\n\n* References\n:* \"Write Off-Loading: Practical Power Management for Enterprise Storage\" - FAST 2008 [http://www.usenix.org/event/fast08/tech/narayanan.html]\n::- Messages from this paper\n::: The traditional view is that enterprise workloads make spinning disks down ineffective because idle periods are too short. We analyzed block-level traces from 36 volumes in an enterprise data center for one week and concluded that significant idle periods exist, and that they can be further increased by modifying the read/write patterns using write off-loading. Write off-loading allows write requests on spun-down disks to be temporarily redirected to persistent storage elsewhere in the data center.\n:* \"Self-organizing Storage (SOS) Project - Software\" \n::- Tools: nfsdump/nfsscan [http://www.eecs.harvard.edu/sos/software/index.html]\n::- SOS Project Traces [http://www.eecs.harvard.edu/sos/traces.html]\n\n\n* Trace Log Field Names\n: Timestamp, Hostname, DiskNumber, Type(Read/Write), Offset, Size, ResponseTime\n:- Timestamp: the time the I/O was issued in \"Windows filetime\"\n:- Hostname: the hostname (should be the same as that in the trace file name)\n:- DiskNumber: the disknumber (should be the same as in the trace file name)\n:- Type: \"Read\" or \"Write\"\n:- Offset: starting offset of the I/O in bytes (from the start of the logical disk)\n:- Size: transfer size of the I/O request in bytes\n:- ResponseTime: time taken by the I/O to complete, in \"Windows filetime\"\n\n\n* Trace Log Example\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ head -100 rsrch_0.csv \n\n128166372003061629,rsrch,0,Read,7014609920,24576,41286\n128166372016382155,rsrch,0,Write,1317441536,8192,1963\n128166372026382245,rsrch,0,Write,2436440064,4096,1835\n128166372036348580,rsrch,0,Write,3196526592,57344,35436\n128166372036379390,rsrch,0,Write,3154132992,4096,4626\n128166372036382264,rsrch,0,Write,3154124800,4096,1752\n128166372053100669,rsrch,0,Write,7609925632,10240,2053\n128166372053101032,rsrch,0,Write,15282630656,16384,1691\n128166372053101054,rsrch,0,Write,7612473344,16384,1668\n</pre>\n\n\n* IO Trace Nodes\n{| border=\"1\"\n| Node\n| Description\n| # of volumes\n| # of IOs\n|-\n| usr\n| User home directories\n| 3\n|-\n| proj\n| Project directories\n| 5\n|-\n| prn\n| Print server\n| 2\n|-\n| hm\n| Hardware monitoring\n| 2\n|-\n| rsrch\n| Research projects\n| 3\n|-\n| prxy\n| Firewall/WebProxy\n| 2\n|-\n| src1\n| Source control\n| 3\n|-\n| src2\n| Source control\n| 3\n|-\n| stg\n| Web staging\n| 2\n|-\n| ts\n| Terminal server\n| 1\n|-\n| web\n| Web/SQL server\n| 4\n|-\n| mds\n| Media server\n| 2\n|-\n| wdev\n| Test web server\n| 4\n|-\n|}\n\n\n* # of IOs\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ wc -l *.csv\n    3993316 hm_0.csv\n     609311 hm_1.csv\n    1211034 mds_0.csv\n    1637711 mds_1.csv\n    5585886 prn_0.csv\n   11233411 prn_1.csv\n    4224524 proj_0.csv\n   23639742 proj_1.csv\n   29266482 proj_2.csv\n    2244644 proj_3.csv\n    6465639 proj_4.csv\n   12518968 prxy_0.csv\n  168638964 prxy_1.csv\n    1433655 rsrch_0.csv\n      13780 rsrch_1.csv\n     207587 rsrch_2.csv\n   37415613 src1_0.csv\n   45746222 src1_1.csv\n    1907773 src1_2.csv\n    1557814 src2_0.csv\n     657774 src2_1.csv\n    1156885 src2_2.csv\n    2030915 stg_0.csv\n    2196861 stg_1.csv\n    1801734 ts_0.csv\n    2237889 usr_0.csv\n   45283980 usr_1.csv\n   10570046 usr_2.csv\n    1143261 wdev_0.csv\n       1055 wdev_1.csv\n     181266 wdev_2.csv\n        682 wdev_3.csv\n    2029945 web_0.csv\n     160891 web_1.csv\n    5175368 web_2.csv\n      31380 web_3.csv\n  434212008 total\n</pre>\n\n\n* List of traces\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ l\ntotal 22956880\ndrwxrwxr-x 2 a1mjjung X0101       4096 Mar 12 16:07 ./\ndrwxrwxr-x 3 a1mjjung X0101       4096 Mar 12 16:04 ../\n-r--r--r-- 1 a1mjjung X0101       1262 Oct 31  2008 DISCLAIMER.txt\n-r--r--r-- 1 a1mjjung X0101       1712 Oct 31  2008 MD5.txt\n-r--r--r-- 1 a1mjjung X0101       1815 Oct 31  2008 README.txt\n-r--r--r-- 1 a1mjjung X0101  202270441 Oct 30  2008 hm_0.csv\n-r--r--r-- 1 a1mjjung X0101   29765989 Oct 30  2008 hm_1.csv\n-r--r--r-- 1 a1mjjung X0101   63955502 Oct 30  2008 mds_0.csv\n-r--r--r-- 1 a1mjjung X0101   88337038 Oct 30  2008 mds_1.csv\n-r--r--r-- 1 a1mjjung X0101  291595716 Oct 30  2008 prn_0.csv\n-r--r--r-- 1 a1mjjung X0101  597136927 Oct 30  2008 prn_1.csv\n-r--r--r-- 1 a1mjjung X0101  233038754 Oct 30  2008 proj_0.csv\n-r--r--r-- 1 a1mjjung X0101 1305533029 Oct 30  2008 proj_1.csv\n-r--r--r-- 1 a1mjjung X0101 1614727432 Oct 30  2008 proj_2.csv\n-r--r--r-- 1 a1mjjung X0101  119913539 Oct 30  2008 proj_3.csv\n-r--r--r-- 1 a1mjjung X0101  350117046 Oct 30  2008 proj_4.csv\n-r--r--r-- 1 a1mjjung X0101  658840568 Oct 30  2008 prxy_0.csv\n-r--r--r-- 1 a1mjjung X0101 9043988744 Oct 30  2008 prxy_1.csv\n-r--r--r-- 1 a1mjjung X0101   77717781 Oct 31  2008 rsrch_0.csv\n-r--r--r-- 1 a1mjjung X0101     755814 Oct 31  2008 rsrch_1.csv\n-r--r--r-- 1 a1mjjung X0101   11154823 Oct 31  2008 rsrch_2.csv\n-r--r--r-- 1 a1mjjung X0101 2077380082 Oct 31  2008 src1_0.csv\n-r--r--r-- 1 a1mjjung X0101 2536095762 Oct 31  2008 src1_1.csv\n-r--r--r-- 1 a1mjjung X0101  101236500 Oct 31  2008 src1_2.csv\n-r--r--r-- 1 a1mjjung X0101   82511780 Oct 31  2008 src2_0.csv\n-r--r--r-- 1 a1mjjung X0101   35607343 Oct 31  2008 src2_1.csv\n-r--r--r-- 1 a1mjjung X0101   63026546 Oct 31  2008 src2_2.csv\n-r--r--r-- 1 a1mjjung X0101  105682669 Oct 31  2008 stg_0.csv\n-r--r--r-- 1 a1mjjung X0101  116358242 Oct 31  2008 stg_1.csv\n-r--r--r-- 1 a1mjjung X0101   93309044 Oct 31  2008 ts_0.csv\n-r--r--r-- 1 a1mjjung X0101  118478959 Oct 31  2008 usr_0.csv\n-r--r--r-- 1 a1mjjung X0101 2451360295 Oct 31  2008 usr_1.csv\n-r--r--r-- 1 a1mjjung X0101  574047026 Oct 31  2008 usr_2.csv\n-r--r--r-- 1 a1mjjung X0101   60262085 Oct 31  2008 wdev_0.csv\n-r--r--r-- 1 a1mjjung X0101      56014 Oct 31  2008 wdev_1.csv\n-r--r--r-- 1 a1mjjung X0101    9593588 Oct 31  2008 wdev_2.csv\n-r--r--r-- 1 a1mjjung X0101      35650 Oct 31  2008 wdev_3.csv\n-r--r--r-- 1 a1mjjung X0101  107571350 Oct 31  2008 web_0.csv\n-r--r--r-- 1 a1mjjung X0101    8507311 Oct 31  2008 web_1.csv\n-r--r--r-- 1 a1mjjung X0101  276121116 Oct 31  2008 web_2.csv\n-r--r--r-- 1 a1mjjung X0101    1649607 Oct 31  2008 web_3.csv\n</pre>\n\n\n <pre>\na1mjjung@secm:[MSR-Cambridge] $ wc -l *.csv\n    3993316 hm_0.csv\n     609311 hm_1.csv\n    1211034 mds_0.csv\n    1637711 mds_1.csv\n    5585886 prn_0.csv\n   11233411 prn_1.csv\n    4224524 proj_0.csv\n   23639742 proj_1.csv\n   29266482 proj_2.csv\n    2244644 proj_3.csv\n    6465639 proj_4.csv\n   12518968 prxy_0.csv\n  168638964 prxy_1.csv\n    1433655 rsrch_0.csv\n      13780 rsrch_1.csv\n     207587 rsrch_2.csv\n   37415613 src1_0.csv\n   45746222 src1_1.csv\n    1907773 src1_2.csv\n    1557814 src2_0.csv\n     657774 src2_1.csv\n    1156885 src2_2.csv\n    2030915 stg_0.csv\n    2196861 stg_1.csv\n    1801734 ts_0.csv\n    2237889 usr_0.csv\n   45283980 usr_1.csv\n   10570046 usr_2.csv\n    1143261 wdev_0.csv\n       1055 wdev_1.csv\n     181266 wdev_2.csv\n        682 wdev_3.csv\n    2029945 web_0.csv\n     160891 web_1.csv\n    5175368 web_2.csv\n      31380 web_3.csv\n  434212008 total\n</pre>\n\n=== Real IO Trace 수집 (삼성 SDS 강석우 상무) ===\n\n* 삼성SDS 강석우 상무 (클라우드 플랫폼 팀장)\n\n* 삼성 SDS 박성록 수석보 (클라우드 플랫폼 운영그룹)\n\n* 진행 현황\n\n:* EMC\n::- 박정원 과장에게 연락함. 담당자인 이임호 부장 소개해줌.\n::- 이임호 부장은 아직 연락 못함\n\n:* Dell\n::- 지근영 대리와 통화/메일 (TraceLog 요청사항을 Dell에게 전달하겠다고 함)\n:::- Trace Log 데이터 예제 전달\n\n:* NetApp\n::- 김주영 과장과 통화/메일\n:::- Trace Log 데이터 예제 전달\n\n:* Supercom 센터\n\n\n* 스토리지 상주 지원 인력\n:* EMC\n::- 노영변부장 : 010-5384-6426 (EMC KOREA 삼성 PMO)\n::- 박정원과장 : 010-9052-7805 (EMC KOREA 센터 상주지원)(연락하였음)\n:::- Phone call, IO trace 수집에 대해 설명 -> 담당자 연결 시켜줌 (이임호 부장)\n::- 이임호 부장: 010-3203-7823 (EMC 삼성전담, 프리세일즈 기술컨설턴트)\n:::- Not yet connected (another phone call)\n:* Dell\n::- 이정민차장 : 010-2908-0759 \n::- 지근영대리 : 010-9530-2819 (한빛마이크로 센터상주지원)\n:::- DELL 기술지원 업무(수원ICT센터) 상주 : (6층 전산실 DELL EQL스토리지 기술지원)\n:::- 연락처: <dell.korea@samsung.com> (443-803  경기?수원시?영통구?매탄3동 410-1 삼성SDS 수원ICT S/W연구소 4층)\n:::- 3/12 연락하였음. (전화/메신저/메일로 상황 설명 하였으며, 현재 Dell에 요청 전달된 상태임)\n:* NetApp(상주지원 없음)\n::- 최병석이사 : 010-8998-7138(NetApp Korea)\n::- 김주영과장 : 010-9577-4272 (아리라)\n:::- Email sent, Phone call\n\n <pre>\n\n\n------- Original Message -------\n\nSender : 강석우<sukwkang@samsung.com> 상무/팀장/클라우드플랫폼팀/삼성SDS\n\nDate : 2013-03-08 14:06 (GMT+09:00)\n\nTitle : Re: Re: Fwd: 스토리지 벤더 현황입니다.\n\n \n\n아마 스토리지 벤더 별로 자체 테스트 시스템이 있기 때문에 테스트 시스템에서 로그수집이 가능할 겁니다. 아니면 본사에서 이미 가지고 있을수도 있구요.\n\n \n\n강석우 드림\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-03-08 14:02 (GMT+09:00)\n\nTitle : Re: Fwd: 스토리지 벤더 현황입니다.\n\n \n\n대단히 감사합니다, 강 상무님.\n\n \n\n심은수 드림\n\n \n\n------- Original Message -------\n\nSender : 강석우<sukwkang@samsung.com> 상무/팀장/클라우드플랫폼팀/삼성SDS\n\nDate : 2013-03-08 13:58 (GMT+09:00)\n\nTitle : Fwd: 스토리지 벤더 현황입니다.\n\n \n\n심상무님,\n\n \n\n아래의 스토리지 벤더에 연락해서 요청을 하시면 됩니다. SDS의 클라우드 팀 강석우 상무 소개로 연락했다고 말씀하시구요. 만약 협조를 잘 안하면 저에게 다시 연락주세요. 제가 협조하도록 만들겠습니다. :-)\n\n \n\n강석우 드림\n\n \n\n------- Original Message -------\n\nSender : 박성록<rocky@samsung.com> 수석보/클라우드플랫폼운영그룹/삼성SDS\n\nDate : 2013-03-08 13:48 (GMT+09:00)\n\nTitle : 스토리지 벤더 현황입니다.\n\n \n\n \n\n안녕하십니까?  클라우드플랫폼운영그룹 박성록수석보입니다.\n\n \n\n스토리지 상주 지원 인력입니다.\n\n1. EMC \n\n  - 노영변부장 : 010-5384-6426 (EMC KOREA 삼성 PMO)\n\n  - 박정원과장 : 010-9052-7805  (EMC KOREA 센터 상주지원)\n\n \n\n2. Dell\n\n  - 이정민차장 : 010-2908-0759 \n\n  - 지근영대리 : 010-9530-2819 (한빛마이크로 센터상주지원)\n\n \n\n3. NetApp(상주지원 없음)\n\n - 최병석이사 : 010-8998-7138(NetApp Korea)\n\n - 김주영과장 : 010-9577-4272 (아리라)\n  \n</pre>\n\n=== Multimedia Streaming vs. IOWA-based PDP ===\n\n* eMBMS: Evolved Multimedia Broadcast Multicast Service\n: Expway\'s eMBMS [http://blog.expway.com/ Expway\'s eMBMS Solution Allows Mobile Operators to off-Load Mobile Traffic by 20%]\n:- Key Technical Features\n::- FLUTE (File Delivery over Unidirectional Transport) protocol\n::- Forward Error Correction\n::- File Repair\n::- Service Announcement\n::- DASH Video Protocol\n:- Mobile Traffic Prediction (in 2016)\n::- 70% of mobile traffic will be video\n::- 10% of all TV viewing will be on tablets\n::- This equauls to 25 million DVDs sent every single hour\n:- Expway Company\n::- 7 years of experience focused on mobile broadcast software\n:::- Robust and Mature Products\n:::- Optimized Bandwidth Usage\n:::- Low Footprint Terminal Stack\n\n\n\n* DASH: Dynamic Adaptive Streaming over HTTP (a.k.a MPEG-DASH)\n: Internet 상으로 media content에 대한 고품질 streaming을 가능하게 하는 기술 (기존 HTTP 웹서버들로부터 deliver됨)\n: 컨텐츠를 small HTTP-based file segement들로 쪼개어 다룬다는 점에서 Apple의 HTTP Live Streaming (HLS)와 유사하다고 볼 수 있음. (컨텐츠 예: movie, sports event의 live broadcast 등)\n\n\n* HTTP Live Streaming (HLS): HTTP-based media streaming communications protocol (QuickTime과 iOS software의 일부로 Apple이 구현함)\n:- 동작원리\n:: overall stream을 작은 HTTP-based file downloads로 쪼개어 다룬다 (각각의 download는 overall potentially unbounded transport stream의 하나의 short chunk를 담당). stream 세션 시작 시에는, available한 variouis sub-stream들에 대한 metadata를 포함하고 있는 extended M2U (m3u8) playlist를 download한다.\n:- 장점\n:: 표준화된 HTTP transaction만을 사용하기 때문에, HLS는 일반 HTTP traffic을 허용하는 firewall, proxy server들은 모두 통과 가능 (RTP와 같은 UDP 기반 프로토콜은 그렇지 못함)하며, 널리 사용 가능한 CDN 인프라를 통해서 쉽게 deliver될 수 있음.\n:- 특징\n:: AES와 같은 암호화 메커니즘 및 HTTPS 기반의 secure key distribution 방법, simple DRM 시스템을 제공함\n:: HLS의 이후 버전에서는 [[trick mode]][http://en.wikipedia.org/wiki/Trick_mode] 기반의 fast-forward/rewind 및 subtitle의 통합도 지원할 예정임 (2013-03-12 현재)\n:- 표준화\n:: Apple에서는 HLS (HTTP Live Streaming)를 Internet Draft로 작성하였음. (first stage in the process of submitting it to the IETF, as an Informational Request For Comments)\n\n\n\n=== Technical Articles ===\n\n* 상무님께서 보내주신 \"Future of Cloud Storage\" 메일에 대한 신전문님 정리\n\n아래 글들을 읽은 소감 or 요약입니다.\n글들이 이것저것 다양하네요.\n\n* [http://gigaom.com/2011/10/29/the-future-beyond-the-cloud-is-in-our-hands/ The future beyond the cloud is in our hands]\n\n: \"Cloud is all\"의 관점은 mobile device가 ubiquitous, unlimited, low-cost conntection 인데요.. 이런 생각은 현재의 mobile network의 한계와 mobile device의 빠른 발전까지 고려치 않은 비전이라는 비판입니다.\n\n:# 첫째, LTE 같은 mobile network이 향후 cloud를 모두 책임지지는 못한다는 것이고요. (WAN에서의 Bandwidth란 이미 wireless network을 사용하고 있는 user가 쓰는 용량으로 계산된 것이므로)\n:# 둘째, 향후 mobile은 n-core의 multi-gigaherts procesor와 1TB 이상의 local storage 이므로 이를 cloud에서 활용하자는 얘기입니다.\n\n: 예전의 장수석님의 Mobile Cloud 과제가 생각납니다. ^^; Mobile의 능력과 wireless network를 활용하자는 겁니다.\n::- what if those devices can talk to one another in a peer-to-peer or mesh network? \n::- What’s the aggregate power and capability of billions of these things, especially if there will be ways for them to work with and talk to one another both alone and in conjunction with cloud-based services?\n\n: 예로 든 것이 Amazon Silk과 Google의 Offline Mail입니다.\n::- Silk는 클라우드를 이용해 acceleration하는 브라우저입니다. 클라우드에서 사용자의 웹 패턴을 분석하여 미리 preloading하고 mobile에 최적화하여 속도를 높이는 건데요.역으로 mobile에 맞게 웹페이지를 작게 축소해서 mobile에 가져오기 때문에 클라우드가 동작하지 않더라도 속도를 유지시킬 수 있다고 합니다. 클라우드와 mobile간에 서로 보완하는 거지요.\n::- offline Mail은 말그대로 메일서버가 되지 않아도 전송외에 모든 메일 관리가 가능하도록 mobile에 데이터를 미리 다 가져다 놓는 겁니다.\n \n\n* [http://blog.cloudhq.net/post/36559564253/the-future-of-cloud-storage The Future of Cloud Storage]\n\n: 정확히는 이런 문제점이 나올 거다라는 cloud issue에 관한 prediction이라고 볼 수 있습니다.\n\n:# End of Files and Folders : 클라우드 서비스들이 전통적인 File이나 folder 개념을 사용하지 않고 자기만의 interface, 데이터 분류 방식을 쓰므로 나중에 cloud 간의 호환성 문제가 발생함\n:# End of Free Storage: cloud storage 시장이 mature된다면 결국 free storage는 없어질 거다. 지금은 홍보용인 거다.\n:# Data ownership Troubles : 클라우드에 데이터가 올라간 순간 사용자는 data에 대한 control를 잃어버린다.\n:# Encryption will become necessary : encryption이 필수..\n\n\n* [http://www.cs.berkeley.edu/~kubitron/talks/printable/CISCO-cloudstore.pdf Lessons from the OceanStore Project - UC Berkeley]\n\n: 첫번째 글에서 좀 더 나아간 형태..Cloud 시대를 맞이한 P2P의 재조명 되겠습니다. 과거 P2P가 공짜 미디어를 훔치는 수단이었다면 새로운 P2P는 extreme scale를 제공할 수 잇는 system의 새 디자인으로 사용할 수 있다는 얘기입니다. 여러 Client뿐 아니라 여러 Cloue Storage Provider들도 같이 참여하면 서로 윈윈할 수 있다는 얘기입니다.\n\n \n* [http://www.cloudsigma.com/blog/13-the-future-of-cloud-storage The Future of Cloud Storage (and what is wrong with the present)]\n: SAN도 local Storage도 이제 끝났다.  Distributed Replicated Block Device(DRBD) 라고 얘기하고 있습니다만, converged server+storage 형태로 Server 노드에 Storage까지 합체한 형태로 죽 붙이고 replication을 다른 node에 함으로써 latency, fail over 등의 Converged architecture장점을 얘기하고 있네요. Open Solution으로는 sheepdog이 있고 상용화버전으로는 Amplidata가 있다고 합니다.\n::>> open source로서 Linux Kernel (2.6.33 version 부터) 에 구현되어 있는 DRBD도 있음. (아래 그림 참고) HA (High Availability) 제공에 초점이 맞춰져 있고, replication mode도 fully-synchronous와 asynchronous mode, 그리고, 그 사이에, protection level과 performance 간의 tradeoff를 고려한, semi-synchronous mode (memory synchronous mode 라고도 합니다)가 지원됨.  이 Linux의 DRBD는, 우리 RACS 1 의 기술이 networked storage로 확장될 때 매우 유용한 기술적 base가 될 수 있을 것 같습니다. (마치 Local Disk Array에 대한 RACS 1이 Linux MD를 활용하여 구현되고 있는 것처럼, Networked Replicated Disk Array 기술 구현 시에 Linux DRBD를 잘 활용할 수도 있을 것임) [http://www.ibm.com/developerworks/linux/library/l-drbd/index.html High availability with the Distributed Replicated Block Device]\n\n <pre>\n\n------- Original Message -------\n\nDate : 2013-03-11 10:44 (GMT+09:00)\nTitle : Fwd: future of cloud storage\n\nFYI, \n\n저희 궁극의 시나리오가 \'무한대의 local 저장용량을 제공하는\' pervasive storage 쪽으로 잡히면서,\n상무님이 cloud storage의 핵심기술에 대한 깊은 이해를 요구하고 계십니다.\n아래 메일도 참고하시고 시간을 내어 관련 기술에 대한 study를 진행하도록 하겠습니다.\n\n\n------- Original Message -------\nDate : 2013-03-08 20:55 (GMT+09:00)\nTitle : future of cloud storage\n\n이 전문,\n\n\n아마 이 전문도 구글 검색하면 금방 찾을 글들일텐데, 하여간 내가 본 것들입니다.\n바로 아래 것은 많은 시사점을 주는 것 같습니다.\n\nhttp://gigaom.com/2011/10/29/the-future-beyond-the-cloud-is-in-our-hands/\n\n \n그 외의 글들.\n\nhttp://blog.cloudhq.net/post/36559564253/the-future-of-cloud-storage\n\nhttp://www.cs.berkeley.edu/~kubitron/talks/printable/CISCO-cloudstore.pdf\n\nhttp://www.cloudsigma.com/blog/13-the-future-of-cloud-storage\n\n \n우리가 pervasive storage로 서비스 시나리오를 잡은 만큼, 그 분야의 서비스/기술 발전 전망을 할 수 있어야겠습니다.\n</pre>\n\n== ## bNote-2013-03-11 ==\n\n\n=== Akamai - CDN Acceleration ===\n\n* 관련 기사들\n:# [[아카마이, \"쌩쌩 웹사이트 만들려면\"]] [http://www.bloter.net/archives/141513 bloter.net, 2013-01-24]\n:# [[아카마이, \'블레이즈\' 인수 - 인텔리전트 플랫폼 강화]] [http://www.bloter.net/archives/95561 bloter.net, 2012-02-09]\n:# [[아카마이, \"CDN 넘어 하이퍼커넥티드로\"]] [http://www.bloter.net/archives/92711 bloter.net, 2012-01-19]\n:# [[아카마이, CDN 장악 가속화 ... 코텐도 인수설]] [http://www.bloter.net/archives/85622 bloter.net, 2011-11-28]\n:# [[아카마이, \"스트리밍/보안 서비스를 빠르고 안전하게\"]] [http://www.bloter.net/archives/67854 bloter.net, 2011-07-13]\n:# [[네트워크 업계 \"기다렸다, 런던올림픽\"]] [http://www.bloter.net/archives/92823 bloter.net, 2012-01-22]\n\n\n=== 과제 목표 설정 ===\n\n* IOWA-based PDP 과제 목표 metric\n: 경쟁사, 기술원 현수준, 기술원 목표수준, 접근방식 등\n:- 필수 고려 사항\n:: 어째서 그러한 목표 수준을 잡았는지?\n:- 점검 사항\n:: EMC FAST 등 Automatic Tiering 기술의 현수준 파악 필요\n:- 접근 방식의 독창성/진보성\n::- ProactiveDP가 MWC 2013에 언급된 eMBMS, DASH 등과 어떤 차별점을 갖는가?\n:::- eMBMS (Evolved Multimedia Broadcast Multicast Service): LTE를 이용해 수많은 사용자에게 방송 컨텐츠를 동시에 효과적으로 배포하는 기술임. 스트리밍 전송 및 비 피크타임에 전송해 단말에 저장된 형태로 있다가 사용자가 원할 때 시청. <span style=\"color:blue\">level of intelligence</span>가 중요한 비교점이 될 수 있음.\n:::- DASH (Dynamic Adaptive Streaming over HTTP)\n::- CDN (Content Delivery Network)에서 Akamai와는 어떻게 차별되나?\n\n* 기술 진화 고민\n: evolution of technology as a driving force from old-age to the pervasive storage\n\n=== IOWA: bpo_a.20130305_104633.real_whole_trace.log ===\n\n* Trace information\n:- Machine under IOTracing: radiohead (Linux 3.2.0-34)\n:- Tracing time: 72 hours\n:- Trace log file: /x/var/iowa/sidewinder/iowa/preproc/tdir/myrealtrace/bpo_a.20130305_104633.real_whole_trace.log\n\n==== further Write pattern analysis (LBA-to-name processing, for top 18 addresses) ====\n\n* IO statistical summary\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  1115\n__valu__sig__ _n_o_sigaddrs :  18\n__valu__sig__ _sigioc_acc :  40010\n__valu__sig__ _sigaddrs_efficiency :  2222.77777778\n__valu__sig__ _n_o_addr_total :  40979\n__valu__sig__ _ioc_total :  157632\n</pre>\n\n\n* top 18 addresses\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__list__hitnum_and_addrs__.log.v1 | sort -n  | tail -20\n1093 : [1266683272]\n1105 : [1267153912]\n1113 : [1661223296]\n\n---- top 18 addr starts below ----\n1115 : [12856320, 12857344]\n1237 : [2480]\n1686 : [1266681984]\n1693 : [1266683032]\n1730 : [1266683160]\n1732 : [2352]\n1941 : [1661223328]\n2077 : [2048]\n2110 : [1267095912]\n2188 : [327569400]\n2265 : [1266683048]\n2309 : [327568408]\n2342 : [327568936]\n2364 : [1661223320]\n2395 : [1661223136]\n2526 : [1401209744]\n7185 : [1661223128]\n</pre>\n\n\n* Bar Graph for Hits_per_Addr (MyRealTrace, Radiohead, 72h)\n<!-- [[File:Myrealtrace on radiohead 72h cdst hits per addr impulses.v01.png | 500px]] -->\n[[File:Myrealtrace on radiohead 72h cdst hits per addr impulses.v01.png]]\n\n\n===== analysis table =====\n\n{| border=\"1\"\n| address\n| # of hits\n| device node\n| process accessed\n| periodicity (1000 IOs)\n| corresponding file/dir\n| notes\n|-\n| 1661223128\n| 7185\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.964271213967\n| /home/hendrix/.config/google-chrome/Default/Cookies-journal\n| inode (39585621)\n|-\n| 1401209744\n| 2526\n| (8,1) /dev/sda1\n| BrowserBlocking\n| 0.926207876573\n| /home/hendrix/.config/google-chrome/Default/Local Storage/https_plus.google.com_0.localstorage-journal\n| inode (39714913)\n|-\n| 1661223136\n| 2395\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.957927324401\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 1661223320\n| 2364\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.957927324401\n| /home/hendrix/.config/google-chrome/Default/Cookies-journal\n| inode (39585621)\n|-\n| 327568936\n| 2342\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.938895655704\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 327568408\n| 2309\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.951583434836\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 1266683048\n| 2265\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.957927324401\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 327569400\n| 2188\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.894488428745\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 1267095912\n| 2110\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.951583434836\n| /home/hendrix/.config/google-chrome/Default/Cookies-journal\n| inode (39585621)\n|-\n| 2048\n| 2077\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.94523954527\n| INVALID BLOCK\n| inode (N/A)\n|-\n| 1661223328\n| 1941\n| (8,1) /dev/sda1\n| Chrome_DBThread\n| 0.957927324401\n| /home/hendrix/.config/google-chrome/Default/Cookies\n| inode (39585620)\n|-\n| 2352\n| 1732\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.957927324401\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 1266683160\n| 1730\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.938895655704\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 1266683032\n| 1693\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.964271213967\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 1266681984\n| 1686\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.957927324401\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 2480\n| 1237\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.951583434836\n| INODE NOT FOUND\n| inode (N/A)\n|-\n| 12856320\n| 1115\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.94523954527\n| /home/hendrix/.cache/google-chrome/Default/Cache/data_1\n| inode (39585855)\n|-\n| 12857344\n| 1115\n| (8,1) /dev/sda1\n| flush-8:0\n| 0.94523954527\n| /home/hendrix/.cache/google-chrome/Default/Cache/index\n| inode (39585853)\n|-\n|}\n\n===== analysis result (processing output) =====\n\n <pre>\nblusjune@radiohead:[top_hot_18] $ l\ntotal 16\ndrwxrwxr-x 2 blusjune blusjune 4096 Mar 11 18:44 ./\ndrwxrwxr-x 3 blusjune blusjune 4096 Mar 11 14:49 ../\n-rwxr-xr-x 1 blusjune blusjune 2566 Mar 11 18:40 .bdx.0100.y.lba_to_name.sh*\n-rw-rw-r-- 1 blusjune blusjune 1768 Mar 11 18:44 .conf.lba_to_name.sh\n\nblusjune@radiohead:[top_hot_18] $ _BDX \nBDX[ /x/var/iowa/tdir/s05/w_ptrn_analysis/top_hot_18 ]# 0100 : lba_to_name\n\n\'_conf__target_lba_list\' is from:\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ tail -29 __simout.sigio_25.iowsz_100.t1_1000.__list__hitnum_and_addrs__.log.v2\n708 : [1795164168]\n840 : [12856336]\n841 : [1270876464]\n842 : [1661223968]\n913 : [1266682992]\n929 : [1270876456]\n943 : [1266681864]\n949 : [1266751536]\n956 : [1266751544]\n1093 : [1266683272]\n1105 : [1267153912]\n1113 : [1661223296]\n1115 : [12856320, 12857344]\n1237 : [2480]\n1686 : [1266681984]\n1693 : [1266683032]\n1730 : [1266683160]\n1732 : [2352]\n1941 : [1661223328]\n2077 : [2048]\n2110 : [1267095912]\n2188 : [327569400]\n2265 : [1266683048]\n2309 : [327568408]\n2342 : [327568936]\n2364 : [1661223320]\n2395 : [1661223136]\n2526 : [1401209744]\n7185 : [1661223128]\n\n#>> configuration started\n#<< _conf__target_dev (e.g., /dev/sda) : /dev/sda\n#<< _conf__target_dev_part (e.g., /dev/sda1) : /dev/sda1\n----\n/dev/sda1   *        2048  3907026943  1953512448   83  Linux\nBlock size:               4096\n----\n#<< _conf__lba_fs_start: 2048\n#<< _conf__fblk_size: 4096\n#<< _conf__sector_size [512]: \n#>> configuration completed\n\n#>> START Processing\n/dev/sda1: 1795164168 -> _EXCEPTION_ # inode for 224395265 is NOT FOUND -- Skip processing\n/dev/sda1: 12856336 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_1 # DEV:LBA:FBLK:INODE( /dev/sda1 : 12856336 : 1606786 : 39585855 )\n/dev/sda1: 1270876464 -> _EXCEPTION_ # inode for 158859302 is NOT FOUND -- Skip processing\n/dev/sda1: 1661223968 -> /home/blusjune/.config/google-chrome/Default/Local Storage/https_plus.google.com_0.localstorage # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223968 : 207652740 : 39714912 )\n/dev/sda1: 1266682992 -> _EXCEPTION_ # inode for 158335118 is NOT FOUND -- Skip processing\n/dev/sda1: 1270876456 -> _EXCEPTION_ # inode for 158859301 is NOT FOUND -- Skip processing\n/dev/sda1: 1266681864 -> _EXCEPTION_ # inode for 158334977 is NOT FOUND -- Skip processing\n/dev/sda1: 1266751536 -> /home/blusjune/.config/google-chrome # DEV:LBA:FBLK:INODE( /dev/sda1 : 1266751536 : 158343686 : 39585586 )\n/dev/sda1: 1266751544 -> /home/blusjune/.config/google-chrome/Default # DEV:LBA:FBLK:INODE( /dev/sda1 : 1266751544 : 158343687 : 39585590 )\n/dev/sda1: 1266683272 -> _EXCEPTION_ # inode for 158335153 is NOT FOUND -- Skip processing\n/dev/sda1: 1267153912 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_0 # DEV:LBA:FBLK:INODE( /dev/sda1 : 1267153912 : 158393983 : 39585854 )\n/dev/sda1: 1661223296 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_0 # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223296 : 207652656 : 39585854 )\n/dev/sda1: 12857344 -> /home/blusjune/.cache/google-chrome/Default/Cache/index # DEV:LBA:FBLK:INODE( /dev/sda1 : 12857344 : 1606912 : 39585853 )\n/dev/sda1: 12856320 -> /home/blusjune/.cache/google-chrome/Default/Cache/data_1 # DEV:LBA:FBLK:INODE( /dev/sda1 : 12856320 : 1606784 : 39585855 )\n/dev/sda1: 2480 -> _EXCEPTION_ # inode for 54 is NOT FOUND -- Skip processing\n/dev/sda1: 1266681984 -> _EXCEPTION_ # inode for 158334992 is NOT FOUND -- Skip processing\n/dev/sda1: 1266683032 -> _EXCEPTION_ # inode for 158335123 is NOT FOUND -- Skip processing\n/dev/sda1: 1266683160 -> _EXCEPTION_ # inode for 158335139 is NOT FOUND -- Skip processing\n/dev/sda1: 2352 -> _EXCEPTION_ # inode for 38 is NOT FOUND -- Skip processing\n/dev/sda1: 1661223328 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223328 : 207652660 : 39585620 )\n/dev/sda1: 2048 -> _EXCEPTION_ # fsblock 0 seems INVALID BLOCK -- Skip processing\n/dev/sda1: 1267095912 -> /home/blusjune/.config/google-chrome/Default/Cookies-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1267095912 : 158386733 : 39585621 )\n/dev/sda1: 327569400 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 327569400 : 40945919 : 39585620 )\n/dev/sda1: 1266683048 -> _EXCEPTION_ # inode for 158335125 is NOT FOUND -- Skip processing\n/dev/sda1: 327568408 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 327568408 : 40945795 : 39585620 )\n/dev/sda1: 327568936 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 327568936 : 40945861 : 39585620 )\n/dev/sda1: 1661223320 -> /home/blusjune/.config/google-chrome/Default/Cookies-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223320 : 207652659 : 39585621 )\n/dev/sda1: 1661223136 -> /home/blusjune/.config/google-chrome/Default/Cookies # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223136 : 207652636 : 39585620 )\n/dev/sda1: 1401209744 -> /home/blusjune/.config/google-chrome/Default/Local Storage/https_plus.google.com_0.localstorage-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1401209744 : 175150962 : 39714913 )\n/dev/sda1: 1661223128 -> /home/blusjune/.config/google-chrome/Default/Cookies-journal # DEV:LBA:FBLK:INODE( /dev/sda1 : 1661223128 : 207652635 : 39585621 )\n\nblusjune@radiohead:[top_hot_18] $ \n\n\n</pre>\n\n\n=== LBA-to-name processing ===\n\n* .bd/x/exphist info.\n: bd/x/exphist/sidewinder/sidewinder-20130311_190748/tools/lba_to_name/\n\n <pre>\nblusjune@jimi-hendrix:[lba_to_name] $ pwd\n/home/blusjune/bd/x/exphist/sidewinder/sidewinder-20130311_190748/tools/lba_to_name\nblusjune@jimi-hendrix:[lba_to_name] $ l\ntotal 16\ndrwxrwxr-x 2 blusjune blusjune 4096 Mar 11 19:03 ./\ndrwxrwxr-x 4 blusjune blusjune 4096 Mar 11 19:07 ../\n-rwxr-xr-x 1 blusjune blusjune 3869 Mar 11 19:03 .bdx.0100.y.lba_to_name.sh*\n-rw-rw-r-- 1 blusjune blusjune  300 Mar 11 19:03 .conf.lba_to_name.sh\n</pre>\n\n\n* References\n: [http://www.expertslogin.com/general/how-to-fix-repair-bad-blocks-in-linux/ How To Fix / Repair Bad Blocks In Linux] (B.GOOD)\n\n\n* How to find the corresponding location (file/dir) for some LBA (e.g., 1661223128)?\n\n: step 1: We need to find the block size of the filesystem\n <pre>\nblusjune@radiohead:[google] $ sudo tune2fs -l /dev/sda1  | grep Block\nBlock count:              488378112\nBlock size:               4096\nBlocks per group:         32768\n</pre>\n\n: step 2: check the starting sector of the partition (file system)\n <pre>\nblusjune@radiohead:[google] $ sudo fdisk -lu /dev/sda\n\nDisk /dev/sda: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x00001eb9\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *        2048  3907026943  1953512448   83  Linux\n\n</pre>\n\n: step 3: File the filesystem block that contains this target LBA\n <pre>\nb = (int) ((L-S) * 512 / B)\n\n, where\nb = File system block number (we need this answer)\nB = File system block size in bytes (4096 - we got this in step 1)\nL = target LBA (1661223128 - given)\nS = Starting sector of the partition as shown by fdisk -lu (2048 - we got this in step 2)\n\n, so the answer b is 207652635 (207,652,635)\n\n</pre>\n\n: step 4: Check the file/dir name with given b (207652635)\n <pre>\nblusjune@radiohead:[s05] $ sudo debugfs /dev/sda1 \ndebugfs 1.42 (29-Nov-2011)\n\ndebugfs:  testb 207652635\nBlock 207652635 marked in use\n\ndebugfs:  icheck 207652635\nBlock	Inode number\n207652635	39585621\n\ndebugfs:  ncheck 39585621\nInode	Pathname\n39585621	/home/blusjune/.config/google-chrome/Default/Cookies-journal\n\ndebugfs:\n</pre>\n\n== ## bNote-2013-03-08 ==\n\n=== lsof command guide/examples ===\n\n* [http://www.ibm.com/developerworks/aix/library/au-lsof.html Finding open files with lsof]\n\n:- Sean A. Walberg, Senior Network Engineer\n:- Summary:  Learn more about your system by seeing which files are open. Knowing which files an application has open, or which application has a particular file open, enables you to make better decisions as a system administrator. For instance, you shouldn\'t unmount a file system while files on it are open. Using lsof, you can check for open files and stopped processes before unmounting, as needed. Likewise, if you find an unknown file, you can find the application holding it open.\n\n\n=== debugfs command guide/examples ===\n\n* [http://www.cs.montana.edu/courses/309/topics/4-disks/debugfs_example.html debugfs examples - original article from montana.edu]\n\n <pre>\ndebugfs Command Examples\n\n\n# Use debufs to prowl around a file system.\n\n> debugfs /dev/hda6\ndebugfs 1.19, 13-Jul-2000 for EXT2 FS 0.5b, 95/08/09\n\n# list files\n\ndebugfs:  ls\n2790777 (12) .   32641 (12) ..   2790778 (12) dir1   2790781 (16) file1\n2790782 (4044) file2\n\n#  List the files with a long listing\n\n#  Format is:\n# Field 1:  Inode number.\n# Field 2:  First one or two digits is the type of node:\n#    2 = Character device\n#    4 = Directory\n#    6 = Block device\n#    10 = Regular file\n#    12 = Symbolic link\n#  \n#    The Last four digits are the Linux permissions\n# 3. Owner uid\n# 4. Group gid\n# 5. Size in bytes.\n# 6. Date \n# 7. Time of last creation.\n# 8. Filename.\n\ndebugfs:  ls -l\n2790777  40700   2605   2601    4096  5-Nov-2001 15:30 .\n 32641   40755   2605   2601    4096  5-Nov-2001 14:25 ..\n2790778  40700   2605   2601    4096  5-Nov-2001 12:43 dir1\n2790781 100600   2605   2601      14  5-Nov-2001 15:29 file1\n2790782 100600   2605   2601      14  5-Nov-2001 15:30 file2\n\n# dump the contents of file1\n\ndebugfs: cat file1\nThis is file1 \n\n# dump an inode to a file (same as cat, but to a file) and using\n#  instead of the file name.\n\ndebugfs: dump <2790782> file1-debugfs\n\n# dump the contents of an inode\n\ndebugfs: stat file1 \nInode: 2790782   Type: regular    Mode:  0600   Flags: 0x0   Generation: 46520506\nUser:  2605   Group:  2601   Size: 14\nFile ACL: 0    Directory ACL: 0\nLinks: 1   Blockcount: 8\nFragment:  Address: 0    Number: 0    Size: 0\nctime: 0x3be712ea -- Mon Nov  5 15:30:02 2001\natime: 0x3be712ea -- Mon Nov  5 15:30:02 2001\nmtime: 0x3be712ea -- Mon Nov  5 15:30:02 2001\nBLOCKS:\n5603924\nTOTAL: 1\n\n# Dump an directory inode and look at it.\n\ndebugfs: dump dir1 dir1-debugfs\n\n# Leave debugfs or use another xterm to look at the contents\n# using od or xxd.  The format of a directory (ext2 version 2.0) is:\n\n# Field 1. Four byte inode number.\n# Field 2. Two byte directory entry length.\n# Field 3. Two byte file name length. \n# Field 5. Filename (1-255 characters).\n# Pad.     The filename is padded to be a multiple of 4 bytes long.\n\n\n# use -c to see the file names and single byte values\n# You can see the file names and identify the locatin of\n# the other fields.  Of importance, the length of the \n# entries (octal); . (4-5), .. (20-21), file3 (34-35),\n# file4 (54-55), .file4.swp (74-75),  ...\n\n> od -c dir1-dump  \n0000000   z 225   *  \\0  \\f  \\0 001 002   .  \\0  \\0  \\0   y 225   *  \\0\n0000020  \\f  \\0 002 002   .   .  \\0  \\0 202 225   *  \\0 020  \\0 005 001\n0000040   f   i   l   e   3  \\0  \\0  \\0 201 225   *  \\0   ? 017 005 001\n0000060   f   i   l   e   4  \\0  \\0  \\0 177 225   *  \\0   ? 017  \\n 001\n0000100   .   f   i   l   e   4   .   s   w   p  \\0  \\0 200 225   *  \\0\n0000120   ? 017 006 001   f   i   l   e   4   ~   .   s   w   p   x  \\0\n0000140  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0\n*\n0010000\n\n# use -d to see the two byte values\n\n> od -d dir1-dump  \n0000000 38266    42    12   513    46     0 38265    42\n0000020    12   514 11822     0 38274    42    16   261\n0000040 26982 25964    51     0 38273    42  4056   261\n0000060 26982 25964    52     0 38271    42  4040   266\n0000100 26158 27753 13413 29486 28791     0 38272    42\n0000120  4020   262 26982 25964 32308 29486 28791   120\n0000140     0     0     0     0     0     0     0     0\n*\n0010000\n\n# You can see that the lengths of the entries are:\n#    . = 12, .. = 12, file3 = 16, file4 = 4096\n# Whoa! what happened there.  The file .file4.swp\n# and any other files in the directory have been deleted,\n# so the length of the entry goes to the end of the block\n#\n# use -l to see the four byte values.  We can see the inode\n# values of the files.\n\n> od -l dir1-dump  \n0000000     2790778    33619980          46     2790777\n0000020    33685516       11822     2790786    17104912\n0000040  1701603686          51     2790785    17108952\n0000060  1701603686          52     2790783    17436616\n0000100  1818846766  1932407909       28791     2790784\n0000120    17174452  1701603686  1932426804     7893111\n0000140           0           0           0           0\n*\n0010000\n\n\n-------------------------------------------------------------------<\n\n\n\n#\n# You inadvertently delete a file you want back.  The file was named\n# /home/harkin/test/file2.  Immediately do the following.\n#\n\n\n> umount /home\n\n# so that you don\'t create a new file that overwrites the inode\n# or use one of the file blocks.\n\n# Execute df to find out what partition /home is on\n\n>df \nFilesystem           1k-blocks      Used Available Use% Mounted on\n/dev/hda1              1011928    507860    452664  53% /\n/dev/hda8             27364092   1890176  24083896   8% /home\n/dev/hda5              8064272   3492760   4161860  46% /usr\n/dev/hda7              1011928     87956    872568  10% /var\nclowns:/db/boze       17783240  10494056   7183568  60% /home/bozo/db\n\n# Get the data on the /home filesystem\n\ntune2fs -l /dev/hda8 | grep \"Block size\"\n\n   Block size:               4096\n\n# So the block size is 4096 bytes.\n\n# Create a file system to duplicate the /home file system in case\n# you screw up royally.  This disk should be exactly the same size\n# as the file system you are backing up.  Fortunately there is an\n# unused disk /dev/hdb.\n\n> fdisk /dev/hdb\nCommand (m for help): n\nCommand action\n   l   logical (5 or over)\n   p   primary partition (1-4)\np\n\n+27364092K\nw\n\n# copy /home to the backup location\n\ndd if=/dev/hda8 of=/dev/hdb1 bs=4096\n\n# Now use debugfs to try to fix things.  We need to try to\n# find the inode of the deleted file.  Use lsdel to \n# list all of the deleted inodes on the file system.\n\ndebugfs -w            # to allow writing\ndebugfs:  lsdel\n3061 deleted inodes found.\n Inode  Owner  Mode    Size    Blocks    Time deleted\n                     .\n                     .\n3296723   2605 100600    652    1/   1 Fri Nov  2 07:30:33 2001\n3296724   2605 100600   1545    1/   1 Fri Nov  2 07:30:33 2001\n3296725   2605 100600    355    1/   1 Fri Nov  2 07:30:33 2001\n3296731   2605 100600    440    1/   1 Fri Nov  2 07:30:33 2001\n3296732   2605 100600   3536    1/   1 Fri Nov  2 07:30:33 2001\n3296733   2605 100600   2365    1/   1 Fri Nov  2 07:30:33 2001\n3296734   2605 100600    443    1/   1 Fri Nov  2 07:30:33 2001\n3296850   2605 100600   2046    1/   1 Fri Nov  2 07:30:33 2001\n3296851   2605 100600    729    1/   1 Fri Nov  2 07:30:33 2001\n3296852   2605 100600    850    1/   1 Fri Nov  2 07:30:33 2001\n3296853   2605 100600   3251    1/   1 Fri Nov  2 07:30:33 2001\n3296854   2605 100600   3733    1/   1 Fri Nov  2 07:30:33 2001\n3296855   2605 100600   3109    1/   1 Fri Nov  2 07:30:33 2001\n3296856   2605 100600   3211    1/   1 Fri Nov  2 07:30:33 2001\n652818   2605 100600 171791   43/  43 Fri Nov  2 16:07:33 2001\n897613   2605 100600   2096    1/   1 Mon Nov  5 07:49:28 2001\n979218   2605 100600   3797    1/   1 Mon Nov  5 07:49:29 2001\n979219   2605 100600   4096    1/   1 Mon Nov  5 07:49:29 2001\n179573   2605 100600   9113    3/   3 Mon Nov  5 12:41:16 2001\n636513   2605 100600   1327    1/   1 Mon Nov  5 12:41:16 2001\n636520   2605 100600     20    1/   1 Mon Nov  5 12:41:16 2001\n1338319   2605 100600   6998    2/   2 Mon Nov  5 12:48:55 2001\n \n# Based on the time and date, the inode to restore is 179573, 636513 \n# or 636520.  Try to figure out which one.\n\ndebugfs:cat <179573> \n   .\n   .\n\ndebugfs:cat <636513>\n   .\n\n# This is rather inconvenient.  If the directory where the files were\n# deleted from still exists, use the cd command to get there and then\n# use ls -d  which lists the files in the directory only, including\n# those with the deleted flag set. \n\n1566721  (12) .    32641  (12) ..    1566788  (60) 530\n1566790 (48) file1   1566791 (24) file2\n<1566747>  (20) file3\n\nThe inode numbers in brackets are deleted files.  A better looking display\ncomes with ls -ld.\n\n\n# So now you know which inode you need to restore.\n# To restore the file, you need to modify the inode, not the \n# directory entry.  This can be done with the modify_inode (mi)\n# command.  Specifically, change the deletion time to zero\n# and the link count to 1.\n\ndebugfs: mi <636513>\ndebugfs:  mi <148003>\n                              Mode    [0100644] \n                           User ID    [510] \n                          Group ID    [510] \n                              Size    [8123] \n                     Creation time    [904216575] \n                 Modification time    [904234782] \n                       Access time    [904234782] \n                     Deletion time    [904236721] 0\n                        Link count    [0] 1\n                       Block count    [16] \n                        File flags    [0x0] \n                         Reserved1    [0] \n                          File acl    [0] \n                     Directory acl    [0] \n                  Fragment address    [0] \n                   Fragment number    [0] \n                     Fragment size    [0] \n                   Direct Block #0    [100321] \n                   Direct Block #1    [100322] \n                   Direct Block #2    [100323] \n                   Direct Block #3    [100324] \n                   Direct Block #4    [200456] \n                   Direct Block #5    [200457] \n                   Direct Block #6    [200675] \n                   Direct Block #7    [200675] \n                   Direct Block #8    [304568] \n                   Direct Block #9    [0] \n                  Direct Block #10    [0] \n                  Direct Block #11    [0] \n                    Indirect Block    [0] \n             Double Indirect Block    [0] \n             Triple Indirect Block    [0] \n\n# It has been recovered.\n\n# This won\'t work for files with indirect blocks and you might find that\n# one or more blocks have been reused already.  If so, you can\n# recover as much data as possible by dumping the blocks to a file.\n\ndebugfs: dump <100321> /tmp > file1.000\ndebugfs: dump <100322> /tmp >> file1.000\n\n# and so on. For files that are longer than 12 blocks, you have to \n# trace the indirect, double-indirect and triple-indirect blocks.\n\n</pre>\n\n=== blktrace advanced ===\n\n* legacy \'blktrace\' data output\n <pre>\nDev_ID CPU_ID   SN   Timestamp      PID   Phz Act Address   Offset ProcessName\n------------------------------------------------------------------------------------  \n  8,16   1      929  2200.865379372 26328  A   R 3188196112 + 8 <- (8,17) 3188194064\n  8,17   1      930  2200.865379890 26328  Q   R 3188196112 + 8 [mysqld]\n  8,17   1      931  2200.865380598 26328  G   R 3188196112 + 8 [mysqld]\n  8,17   1      932  2200.865381014 26328  P   N [mysqld]\n  8,17   1      933  2200.865381784 26328  I   R 3188196112 + 8 [mysqld]\n  8,17   1      934  2200.865382107 26328  U   N [mysqld] 1\n  8,17   1      935  2200.865382605 26328  D   R 3188196112 + 8 [mysqld]\n  8,17   1      936  2200.871162161     0  C   R 3188196112 + 8 [0]\n  8,16   1      937  2200.871189524 26328  A   R 3188589744 + 8 <- (8,17) 3188587696\n  8,17   1      938  2200.871190517 26328  Q   R 3188589744 + 8 [mysqld]\n  8,17   1      939  2200.871192023 26328  G   R 3188589744 + 8 [mysqld]\n  8,17   1      940  2200.871192992 26328  P   N [mysqld]\n  8,17   1      941  2200.871194468 26328  I   R 3188589744 + 8 [mysqld]\n  8,17   1      942  2200.871195233 26328  U   N [mysqld] 1\n</pre>\n\n* legacy \'lsof\' data output\n <pre>\nCOMMAND     PID            USER   FD      TYPE             DEVICE SIZE/OFF       NODE NAME\n-------------------------------------------------------------------------------------------------\ninit          1            root  cwd       DIR                8,1     4096          2 /\ninit          1            root  rtd       DIR                8,1     4096          2 /\ninit          1            root  txt       REG                8,1   163096   57147454 /sbin/init\ninit          1            root  mem       REG                8,1    52120   96997047 /lib/x86_64-linux-gnu/libnss_files-2.15.so\ninit          1            root  mem       REG                8,1    47680   96993415 /lib/x86_64-linux-gnu/libnss_nis-2.15.so\ninit          1            root  mem       REG                8,1    97248   96997056 /lib/x86_64-linux-gnu/libnsl-2.15.so\ninit          1            root  mem       REG                8,1    35680   96997048 /lib/x86_64-linux-gnu/libnss_compat-2.15.so\ninit          1            root  mem       REG                8,1  1811128   96993409 /lib/x86_64-linux-gnu/libc-2.15.so\ninit          1            root  mem       REG                8,1    31752   96993413 /lib/x86_64-linux-gnu/librt-2.15.so\ninit          1            root  mem       REG                8,1   135366   96993411 /lib/x86_64-linux-gnu/libpthread-2.15.so\ninit          1            root  mem       REG                8,1   276392   96996820 /lib/x86_64-linux-gnu/libdbus-1.so.3.5.8\ninit          1            root  mem       REG                8,1    38888   96996879 /lib/x86_64-linux-gnu/libnih-dbus.so.1.0.0\ninit          1            root  mem       REG                8,1    96240   96996881 /lib/x86_64-linux-gnu/libnih.so.1.0.0\ninit          1            root  mem       REG                8,1   149280   96997051 /lib/x86_64-linux-gnu/ld-2.15.so\ninit          1            root    0u      CHR                1,3      0t0       1029 /dev/null\ninit          1            root    1u      CHR                1,3      0t0       1029 /dev/null\ninit          1            root    2u      CHR                1,3      0t0       1029 /dev/null\ninit          1            root    3r     FIFO                0,8      0t0       3001 pipe\ninit          1            root    4w     FIFO                0,8      0t0       3001 pipe\ninit          1            root    5r     0000                0,9        0       6797 anon_inode\ninit          1            root    6r     0000                0,9        0       6797 anon_inode\ninit          1            root    7u     unix 0xffff88020e5cc680      0t0       7152 socket\ninit          1            root    8u     unix 0xffff8802127caa40      0t0      10936 socket\ninit          1            root    9u     unix 0xffff8802116623c0      0t0       1999 socket\ninit          1            root   10u     unix 0xffff880211663400      0t0      10692 socket\ninit          1            root   12w      REG               8,18     2664   12845346 /var/log/upstart/mysql.log.1 (deleted)\ninit          1            root   14u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   16u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   17u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   18u      CHR                5,2      0t0       7184 /dev/ptmx\ninit          1            root   20w      REG               8,18     1342   12845172 /var/log/upstart/modemmanager.log.1 (deleted)\ninit          1            root   21u      CHR                5,2      0t0       7184 /dev/ptmx\nkthreadd      2            root  cwd       DIR                8,1     4096          2 /\nkthreadd      2            root  rtd       DIR                8,1     4096          2 /\nkthreadd      2            root  txt   unknown                                        /proc/2/exe\nksoftirqd     3            root  cwd       DIR                8,1     4096          2 /\nksoftirqd     3            root  rtd       DIR                8,1     4096          2 /\nksoftirqd     3            root  txt   unknown                                        /proc/3/exe\n...\napache2    2241            root  mem       REG                8,1  1852792   96996819 /lib/x86_64-linux-gnu/libcrypto.so.1.0.0\napache2    2241            root  mem       REG                8,1   374608   96996818 /lib/x86_64-linux-gnu/libssl.so.1.0.0\napache2    2241            root  mem       REG                8,1  1030512   96997045 /lib/x86_64-linux-gnu/libm-2.15.so\napache2    2241            root  mem       REG                8,1    66784   96996836 /lib/x86_64-linux-gnu/libbz2.so.1.0.4\napache2    2241            root  mem       REG                8,1  1518928   64756408 /usr/lib/x86_64-linux-gnu/libdb-5.1.so\napache2    2241            root  mem       REG                8,1   105288   96993414 /lib/x86_64-linux-gnu/libresolv-2.15.so\napache2    2241            root  mem       REG                8,1  8644728   65276931 /usr/lib/apache2/modules/libphp5.so\napache2    2241            root  mem       REG                8,1    34824   65276243 /usr/lib/apache2/modules/mod_negotiation.so\napache2    2241            root  mem       REG                8,1    18432   65276567 /usr/lib/apache2/modules/mod_mime.so\napache2    2241            root  mem       REG                8,1    10240   65276556 /usr/lib/apache2/modules/mod_env.so\napache2    2241            root  mem       REG                8,1    10240   65276178 /usr/lib/apache2/modules/mod_dir.so\napache2    2241            root  mem       REG                8,1    92720   96996948 /lib/x86_64-linux-gnu/libz.so.1.2.3.4\napache2    2241            root  mem       REG                8,1    22528   65276571 /usr/lib/apache2/modules/mod_deflate.so\napache2    2241            root  mem       REG                8,1    26624   65275788 /usr/lib/apache2/modules/mod_cgi.so\napache2    2241            root  mem       REG                8,1    34824   65276563 /usr/lib/apache2/modules/mod_autoindex.so\napache2    2241            root  mem       REG                8,1    10248   65275257 /usr/lib/apache2/modules/mod_authz_user.so\napache2    2241            root  mem       REG                8,1    10248   65276546 /usr/lib/apache2/modules/mod_authz_host.so\napache2    2241            root  mem       REG                8,1    10248   65275256 /usr/lib/apache2/modules/mod_authz_groupfile.so\napache2    2241            root  mem       REG                8,1     6152   65275193 /usr/lib/apache2/modules/mod_authz_default.so\napache2    2241            root  mem       REG                8,1    10248   65276547 /usr/lib/apache2/modules/mod_authn_file.so\napache2    2241            root  mem       REG                8,1    10248   65276545 /usr/lib/apache2/modules/mod_auth_basic.so\napache2    2241            root  mem       REG                8,1    14336   65275224 /usr/lib/apache2/modules/mod_alias.so\napache2    2241            root  mem       REG                8,1    14768   96993408 /lib/x86_64-linux-gnu/libdl-2.15.so\napache2    2241            root  mem       REG                8,1    18896   96996944 /lib/x86_64-linux-gnu/libuuid.so.1.3.0\napache2    2241            root  mem       REG                8,1   170024   96996871 /lib/x86_64-linux-gnu/libexpat.so.1.5.2\napache2    2241            root  mem       REG                8,1    43288   96997046 /lib/x86_64-linux-gnu/libcrypt-2.15.so\napache2    2241            root  mem       REG                8,1  1811128   96993409 /lib/x86_64-linux-gnu/libc-2.15.so\napache2    2241            root  mem       REG                8,1   135366   96993411 /lib/x86_64-linux-gnu/libpthread-2.15.so\napache2    2241            root  mem       REG                8,1   234720   64752153 /usr/lib/libapr-1.so.0.4.6\napache2    2241            root  mem       REG                8,1   142840   64752206 /usr/lib/libaprutil-1.so.0.3.12\napache2    2241            root  mem       REG                8,1   247896   96996910 /lib/x86_64-linux-gnu/libpcre.so.3.12.1\napache2    2241            root  mem       REG                8,1   149280   96997051 /lib/x86_64-linux-gnu/ld-2.15.so\napache2    2241            root  DEL       REG                0,4               11842 /dev/zero\napache2    2241            root    0r      CHR                1,3      0t0       1029 /dev/null\napache2    2241            root    1w      CHR                1,3      0t0       1029 /dev/null\napache2    2241            root    2w      REG               8,18    12640   12845390 /var/log/apache2/error.log\napache2    2241            root    3u     IPv4              14201      0t0        TCP *:http (LISTEN)\napache2    2241            root    4r     FIFO                0,8      0t0    7004026 pipe\napache2    2241            root    5w     FIFO                0,8      0t0    7004026 pipe\napache2    2241            root    6w      REG               8,18        0   12845132 /var/log/apache2/other_vhosts_access.log\napache2    2241            root    7w      REG               8,18   127987   12845338 /var/log/apache2/access.log\n\n</pre>\n\n:* [Q] what does \'node\' information mean in \'lsof\' output?\n\n\n* converged iotrace\n: join the information from the legacy blktrace data and lsof, inotify data\n\n\n\n=== SNIA IO Trace Data Files ===\n\n\n[http://iotta.snia.org/traces SNIA I/O Trace Data Files]\n\nThe categories (or types) of I/O traces include:\n\n* Application Traces [This category is currently empty]\n: Application Traces record calls made by a specific application.\n* Block I/O Traces\n: Block I/O Traces typically include block level (e.g., at the logical volume manager, disk driver, etc. level) and block protocol (e.g., SCSI, ATA, Fibre Channel) traces.\n* Historical Traces [This category is currently empty]\n: Historical Traces include all traces that 10 or more years of age.\n* NFS Traces\n: Network File System Traces are typically those for NFS and CIFS and which reflect the protocol used by such network file systems.\n* Parallel Traces\nParallel traces, generally taken from supercomputers, record the system calls made by multiple computers running in parallel.\nSSSI WIOCP Metrics\nSSSI WIOCP, the SNIA Solid State Storage Initiative (SSSI) Workload I/O Capture Program (WIOCP), collects already-summarized empirical metrics separately for both monitored devices and processes/applications.\nStatic Snapshots\nStatic Snapshots are traces taken statically of a file system rather than of system calls.\nSystem Call Traces\nSystem Call I/O Traces typically reflect operating system calls to the file system.\nTools\nHere you can find the tools used for reading the various trace files.\n\n\n==== MSR Cambridge Traces ====\n\n* [http://iotta.snia.org/traces/388 List of Traces]\n\n* MSR Cambridge Traces 1\n: 1-week block I/O Traces of enterprise servers at MSR Cambridge.\n: The citation for the MSRC traces can be found [http://static.usenix.org/event/fast08/tech/narayanan.html FAST 2008, \"Write Off-loading: Practical Power Management for Enterprise Storage\", Dushyanth Narayanan, Austin Donnelly, and Antony Rowstron, Microsoft Research Ltd.]\n\n\n\n==== Microsoft Enterprise Traces ====\n\nTraces collected at Microsoft using the event tracing for Windows framework.\n\n* [http://iotta.snia.org/traces/130 List of Traces]\n\n* TPCC Traces 1\n: TPC-C benchmark traces collected at Microsoft using the event tracing for Windows framework\n\n* TPCC Traces 2\n: TPC-C benchmark traces collected at Microsoft using the event tracing for Windows framework\n: These six 6-minute long traces were collected at various points during a TPC-C run, all of which were during periods of steady-state activity.\n\n* TPCE Traces\n: TPC-E benchmark traces collected at Microsoft using the event tracing for Windows framework\n: These 6 traces were collected during a ~ 84-minute TPC-E run which included a ~ 20-minute warm-up time.\n\n* Exchange Server Traces\n: Production traces collected at Microsoft using the event tracing for Windows framework\n: Collected for Exchange Server for a duration of 24 hours. The single tarball includes 96 trace files, each with a duration of 15 minutes.\n\n\n\n==== Microsoft Production Server Traces ====\n\n* [http://iotta.snia.org/traces/158 List of Traces]\n\n* BuildServer00 ~ BuildServer07\n: Traces of the 25 hours activity on the Microsoft Build Server\n* Development Tools Release\n: Collected for Developers Tools Release Server for a duration of 24 hours\n* Display Ads Data Server, Display Ads Payload Server\n: Collected over a period of 24 hours for Display Ads Data/Platform payload server\n* Live Maps Back End\n: Collected for LiveMaps back-end server for a duration of 24 hours\n* MSN Storage CFS\n: Collected for MSN Storage Metadata Server for a duration of 6 hours\n* MSN Storage File Server\n: Collected for MSN Storage file server for a duration of 6 hours\n* Radius Authentication\n: Collected for RADIUS authentication server\n* Radius Back End SQL Server\n: Collected for RADIUS back-end server\n\n=== 서울대 장병탁 교수님 세미나 ===\n\n* Hypernetwork ML/AI 기술\n:- [http://bi.snu.ac.kr/Courses/g-ai06_2/book-ch4-hypernetmemory-part3.pdf The Hypernetwork Model of Memory)]\n:- [http://bi.snu.ac.kr/Publications/Theses/BS12f_ChunHS.pdf 하이퍼네트워크 연상메모리 기반의 이미지-텍스트 교차검색 (Image-Text Crossmodal Retrieval via Hypernetwork Memory]\n:: 2nd wrong answer: (하이퍼네트워크 메모리 기반의 이미지-텍스트 교차검색)\n:: 1st wrong answer: (하이퍼네트워크 기반의 이미지-텍스트 연상 교차 검색)\n\n=== 상무님께서 보내주신 Storage 미래 관련 글들 ===\n\n\n* 기타\n\n: [http://wwpi.com/index.php?option=com_content&view=article&id=8158]\n\n: [http://lib.stanford.edu/files/pasig-jan2012/11B7%20Francis%20PASIG_2011_Francis_final.pdf]\n\n: [http://www.datarecoverygroup.com/articles/data-storage-history-and-future Data Storage History and Future]\n\n\n\n\n* 스토리지 미디어의 발전 전망\n: 우리가 스토리지 미디어를 개발하지는 않지만, 미디어의 발전 전망을 고려해서 소프트웨어의 미래를 전망해야겠지요.\n\n: [http://www-03.ibm.com/ibm/history/ibm100/us/en/icons/racetrack/ IBM100] \n\n: [http://static.usenix.org/events/fast02/coufal.pdf FAST 2002]\n\n\n\n\n* DNA를 데이터 스토리지로 이용하는 것에 관한 또 다른 글입니다. 장점/비용 이슈 언급됨.\n\n: [http://www.lifehacker.com.au/2013/01/is-dna-the-future-of-data-storage/ DNA를 data storage로 이용하기]\n\n \n* Storage의 미래\n\n: [http://blogs.computerworld.com/data-storage/20865/future-data-storage-revealed Future data storage revealed]\n\n: [http://blogs.computerworld.com/data-storage/21537/top-10-storage-predictions-back-future Top 10 storage predictions]\n\n: [http://blogs.computerworld.com/data-storage/21360/will-private-cloud-kill-storage-area-network Will Private Cloud Kill SAN?]\n\n== ## bNote-2013-03-07 ==\n\n\n=== Find X\'s ===\n\n==== System Event (esp., file system change) Tracing/Monitoring/Collecting ====\n\n* LTTng\n* DTrace\n* FTrace\n* Strace\n* SystemTap\n* inotify ***\n* FAM (File Alteration Monitor) [http://oss.sgi.com/projects/fam/]\n* Gamin (File and directory monitoring system defined to be a subset of the FAM system [http://people.gnome.org/~veillard/gamin/overview.html]\n\n----\n\n==== inotify ====\n\n* inotify - monitoring file system events\n: The inotify API provides a mechanism for monitoring file system events. Inotify can be used to monitor individual files, or to monitor directories. When a directory is monitored, inotify will return events for the directory itself, and for files inside the directory.\n: The following system calls are used with this API: inotify_init(2) (or inotify_init1(2)), inotify_add_watch(2), inotify_rm_watch(2), read(2), and close(2).\n\n* /proc interfaces\n: /proc/sys/fs/inotify/max_queued_events\n: /proc/sys/fs/inotify/max_user_instances\n: /proc/sys/fs/inotify/max_user_watches\n\n* Versions\n: Inotify was merged into the 2.6.13 Linux kernel. The required library interfaces were added to glibc in version 2.4. (IN_DONT_FOLLOW, IN_MASK_ADD, and IN_ONLYDIR were only added in version 2.5.)\n\n* Check whether inotify is enabled or not\n $ grep INOTIFY_USER /boot/config-$(uname -r)\n CONFIG_INOTIFY_USER=y\n\n* Installation on Ubuntu by apt-get\n: aptitude install inotify-tools python-inotifyx libinotifytools0-dev\n\n* inotify는 다음 event들에 대해서만 detection 가능함\n:* access\n:* modify\n:* attrib\n:: watched file에 대한 메타데이터가 변경되거나, watched directory 내의 file이 변경된 경우, \'attrib\' event가 발생\n:* close_write\n:* close_nowrite\n:* close\n:* open\n:* moved_to\n:* moved_from\n:* move\n:* move_self\n:: 이 event 이후에는 file or directory는 no longer being watched된다... 는데, delete event의 경우와 무엇이 다른가?\n:* create\n:* delete\n:* delete_self\n:* unmount\n\n\n\n\n* References\n# [http://www.infoq.com/articles/inotify-linux-file-system-event-monitoring InfoQ -- Inotify: Efficient, Real-time Linux File System Event Monitoring]\n# [http://www.ibm.com/developerworks/linux/library/l-ubuntu-inotify/index.html Monitor file system activity with inotify (B.GOOD)]\n# [http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=90586523eb4b349806887c62ee70685a49415124 git.kernel.org -- fsnotify: unified filesystem notification backend, 2009-05-21~2009-06-11]\n# [http://stackoverflow.com/questions/9614184/how-to-trace-per-file-io-operations-in-linux How to trace per-file IO operations in Linux? -- /proc/PID/fd/, systemtap, strace, fanotify]\n# [http://stackoverflow.com/questions/1835947/how-do-i-program-for-linuxs-new-fanotify-file-system-monitoring-feature Stackoverflow -- How do I program for Linux\'s new \'fanotify\' file system monitoring feature?]\n# [http://stackoverflow.com/questions/8381566/best-way-to-monitor-file-system-changes-in-linux Stackoverflow -- Best way to monitor file system changes in Linux]\n# [http://ubuntuforums.org/showthread.php?t=663950 python inotify example -- Ubuntu Forums]\n# [http://pyinotify.sourceforge.net/ Pyinotify: monitor filesystem events with Python under Linux - Brief Tutorial]\n# [http://github.com/seb-m/pyinotify pyinotify github]\n\n=== directory-file-addr spatial locality ===\n\n* Directory Hierarchy\n\n <pre>\nblusjune@jimi-hendrix:[dir_file_addr_spatial_locality] $ find r0\nr0\nr0/d1\nr0/d1/d13\nr0/d1/d11\nr0/d1/d12\nr0/d2\nr0/d2/d21\nr0/d2/d21/d212\nr0/d2/d21/d211\nr0/d2/d21/d213\nr0/d2/d22\nr0/d2/d22/d221\nr0/d2/d22/d222\n</pre>\n\n== ## bNote-2013-03-06 ==\n\n=== LBA-to-name processing (DELETEME) ===\n\nDone actually 2013-03-11 14:35. [http://www.expertslogin.com/general/how-to-fix-repair-bad-blocks-in-linux/ How To Fix / Repair Bad Blocks In Linux B.GOOD]\n\n\n* How to find the corresponding location (file/dir) for some LBA (e.g., 1661223128)?\n\n: step 1: We need to find the block size of the filesystem\n <pre>\nblusjune@radiohead:[google] $ sudo tune2fs -l /dev/sda1  | grep Block\nBlock count:              488378112\nBlock size:               4096\nBlocks per group:         32768\n</pre>\n\n: step 2: check the starting sector of the partition (file system)\n <pre>\nblusjune@radiohead:[google] $ sudo fdisk -lu /dev/sda\n\nDisk /dev/sda: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x00001eb9\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *        2048  3907026943  1953512448   83  Linux\n\n</pre>\n\n: step 3: File the filesystem block that contains this target LBA\n <pre>\nb = (int) ((L-S) * 512 / B)\n\n, where\nb = File system block number (we need this answer)\nB = File system block size in bytes (4096 - we got this in step 1)\nL = target LBA (1661223128 - given)\nS = Starting sector of the partition as shown by fdisk -lu (2048 - we got this in step 2)\n\n, so the answer b is 207652635 (207,652,635)\n\n</pre>\n\n: step 4: Check the file/dir name with given b (207652635)\n <pre>\nblusjune@radiohead:[s05] $ sudo debugfs /dev/sda1 \ndebugfs 1.42 (29-Nov-2011)\n\ndebugfs:  testb 207652635\nBlock 207652635 marked in use\n\ndebugfs:  icheck 207652635\nBlock	Inode number\n207652635	39585621\n\ndebugfs:  ncheck 39585621\nInode	Pathname\n39585621	/home/blusjune/.config/google-chrome/Default/Cookies-journal\n\ndebugfs:\n</pre>\n\n=== IOWA.sim.iox (myreal_72h) ===\n\n* base trace log on radiohead\n:- name: bpo_a.20130305_104633.real_whole_trace.log\n:- size: 197,372,058 bytes (197372058)\n\n <pre>\nblusjune@radiohead:[s05] $ pwd\n/x/var/iowa/sidewinder/iowa/iowa.sim.iox/tdir/s05\n\nblusjune@radiohead:[s05] $ l\ntotal 210572\ndrwxrwxr-x  2 blusjune blusjune      4096 Mar  6 11:07 ./\ndrwxrwxr-x 10 blusjune blusjune      4096 Mar  6 19:51 ../\nlrwxrwxrwx  1 blusjune blusjune        38 Mar  5 10:43 .bdx.0100.y.proc_after_trace_s10.sh -> ../.bdx.0100.y.proc_after_trace_s10.sh\n-rw-rw-r--  1 root     root     197372058 Mar  5 13:34 bpo_a.20130305_104633.real_whole_trace.log\n-rw-rw-r--  1 blusjune blusjune   4945483 Mar  6 11:06 tracelog.myrealtrace.log.A.addr\n-rw-rw-r--  1 blusjune blusjune   1081666 Mar  6 11:06 tracelog.myrealtrace.log.R.addr\n-rw-rw-r--  1 blusjune blusjune   3863817 Mar  6 11:06 tracelog.myrealtrace.log.W.addr\n-rw-rw-r--  1 blusjune blusjune   8339772 Mar  6 11:06 tracelog.myrealtrace.log.p1.out\n</pre>\n\n\n* R.addr analysis\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_200532.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  1\n__valu__sig__ _n_o_sigaddrs :  43683\n__valu__sig__ _sigioc_acc :  43683\n__valu__sig__ _sigaddrs_efficiency :  1.0\n__valu__sig__ _n_o_addr_total :  43683\n__valu__sig__ _ioc_total :  43683\n</pre>\n\n\n* W.addr analysis\n:- Used as weekly report item (2013-03-06), and lead to a patent\n::- just 18 addresses cover 25% of IO (40,010 IOs out of 157,632 IOs)\n::- x 2222.8 caching efficiency\n\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_202654.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  1115\n__valu__sig__ _n_o_sigaddrs :  18\n__valu__sig__ _sigioc_acc :  40010\n__valu__sig__ _sigaddrs_efficiency :  2222.77777778\n__valu__sig__ _n_o_addr_total :  40979\n__valu__sig__ _ioc_total :  157632\n</pre>\n\n\n* Processes Contributed to the IO Workload\n\n <pre>\na1mjjung@secm:[myrealtrace.radiohead.v01] $ cat bpo_a.20130305_104633.real_whole_trace.log  | grep \'\\[\' | sed -e \'s/.*\\(\\[.*\\]\\)/\\1/g\' | sort -u\n[(null)]\n[(null)] 1\n[(null)] 3\n[(null)] 4\n[(null)] 6\n[(null)] 7\n[0]\n[BrowserBlocking]\n[BrowserBlocking] 1\n[Chrome_CacheThr]\n[Chrome_CacheThr] 1\n[Chrome_CacheThr] 10\n[Chrome_CacheThr] 2\n[Chrome_CacheThr] 3\n[Chrome_CacheThr] 6\n[Chrome_CacheThr] 9\n[Chrome_ChildIOT]\n[Chrome_DBThread]\n[Chrome_DBThread] 1\n[Chrome_DBThread] 2\n[Chrome_DBThread] 3\n[Chrome_DBThread] 4\n[Chrome_DBThread] 5\n[Chrome_DBThread] 6\n[Chrome_FileThre]\n[Chrome_FileThre] 1\n[Chrome_FileThre] 2\n[Chrome_FileThre] 3\n[Chrome_FileThre] 4\n[Chrome_HistoryT]\n[Chrome_HistoryT] 1a1mjjung@secm:[myrealtrace.radiohead.v01] $ cat bpo_a.20130305_104633.real_whole_trace.log  | grep \'\\[\' | sed -e \'s/.*\\(\\[.*\\]\\)/\\1/g\' | sort -u\n[(null)]\n[(null)] 1\n[(null)] 3\n[(null)] 4\n[(null)] 6\n[(null)] 7\n[0]\n[BrowserBlocking]\n[BrowserBlocking] 1\n[Chrome_CacheThr]\n[Chrome_CacheThr] 1\n[Chrome_CacheThr] 10\n[Chrome_CacheThr] 2\n[Chrome_CacheThr] 3\n[Chrome_CacheThr] 6\n[Chrome_CacheThr] 9\n[Chrome_ChildIOT]\n[Chrome_DBThread]\n[Chrome_DBThread] 1\n[Chrome_DBThread] 2\n[Chrome_DBThread] 3\n[Chrome_DBThread] 4\n[Chrome_DBThread] 5\n[Chrome_DBThread] 6\n[Chrome_FileThre]\n[Chrome_FileThre] 1\n[Chrome_FileThre] 2\n[Chrome_FileThre] 3\n[Chrome_FileThre] 4\n[Chrome_HistoryT]\n[Chrome_HistoryT] 1\n[Chrome_HistoryT] 10\n[Chrome_HistoryT] 11\n[Chrome_HistoryT] 12\n[Chrome_HistoryT] 14\n[Chrome_HistoryT] 15\n[Chrome_HistoryT] 16\n[Chrome_HistoryT] 2\n[Chrome_HistoryT] 3\n[Chrome_HistoryT] 4\n[Chrome_HistoryT] 5\n[Chrome_HistoryT] 6\n[Chrome_HistoryT] 7\n[Chrome_HistoryT] 8\n[Chrome_HistoryT] 9\n[Chrome_IOThread]\n[Chrome_IOThread] 1\n[Chrome_ProcessL]\n[Chrome_SafeBrow]\n[Chrome_SafeBrow] 1\n[Chrome_SafeBrow] 4\n[Chrome_SafeBrow] 5\n[Chrome_SyncThre]\n[Chrome_SyncThre] 1\n[Chrome_SyncThre] 10\n[Chrome_SyncThre] 11\n[Chrome_SyncThre] 12\n[Chrome_SyncThre] 13\n[Chrome_SyncThre] 14\n[Chrome_SyncThre] 15\n[Chrome_SyncThre] 16\n[Chrome_SyncThre] 2\n[Chrome_SyncThre] 3\n[Chrome_SyncThre] 4\n[Chrome_SyncThre] 5\n[Chrome_SyncThre] 6\n[Chrome_SyncThre] 7\n[Chrome_SyncThre] 8\n[Chrome_SyncThre] 9\n[Chrome_WebKitTh]\n[Chrome_WebKitTh] 1\n[MediaStreamDevi]\n[MediaStreamDevi] 1\n[NetworkChangeNo]\n[NetworkChangeNo] 1\n[WorkerPool/2786]\n[WorkerPool/2786] 1\n[WorkerPool/2787]\n[WorkerPool/2787] 1\n[WorkerPool/2788]\n[WorkerPool/2788] 1\n[WorkerPool/2839]\n[WorkerPool/2839] 1\n[WorkerPool/2840]\n[WorkerPool/2840] 1\n[WorkerPool/2840] 2\n[WorkerPool/2981]\n[WorkerPool/2981] 1\n[WorkerPool/2982]\n[WorkerPool/2982] 1\n[_BDX]\n[_BDX] 1\n[apache2]\n[apache2] 1\n[apache2ctl]\n[apache2ctl] 1\n[bash]\n[bash] 1\n[bash] 2\n[bash] 3\n[bash] 4\n[bash] 5\n[blktrace]\n[cat]\n[cat] 1\n[checkarray]\n[checkarray] 1\n[chrome]\n[chrome] 1\n[chrome] 2\n[chrome] 3\n[chrome] 4\n[console-kit-dae]\n[console-kit-dae] 1\n[console-kit-dae] 4\n[cp]\n[cp] 1\n[cron]\n[dbus-launch]\n[dbus-launch] 1\n[env]\n[env] 1\n[expr]\n[expr] 2\n[flush-8:0]\n[flush-8:0] 1\n[flush-8:0] 10\n[flush-8:0] 11\n[flush-8:0] 12\n[flush-8:0] 13\n[flush-8:0] 14\n[flush-8:0] 15\n[flush-8:0] 16\n[flush-8:0] 2\n[flush-8:0] 3\n[flush-8:0] 4\n[flush-8:0] 5\n[flush-8:0] 6\n[flush-8:0] 7\n[flush-8:0] 8\n[flush-8:0] 9\n[git-pull]\n[git-pull] 1\n[git]\n[git] 1\n[gnome-pty-helpe]\n[gnome-pty-helpe] 1\n[gnome-terminal]\n[gnome-terminal] 1\n[gnome-terminal] 2\n[gnome-terminal] 3\n[gnome-terminal] 4\n[gnome-terminal] 5\n[gnome-terminal] 7\n[google-chrome]\n[google-chrome] 1\n[inotify_reader]\n[inotify_reader] 1\n[jbd2/sda1-8]\n[jbd2/sda1-8] 1\n[jbd2/sda1-8] 16\n[jbd2/sda1-8] 2\n[jbd2/sda1-8] 3\n[jbd2/sda1-8] 4\n[jbd2/sda1-8] 5\n[kworker/0:1]\n[kworker/0:2]\n[kworker/2:1]\n[kworker/2:2]\n[kworker/3:0]\n[kworker/4:0]\n[kworker/4:1]\n[kworker/4:2]\n[kworker/5:0]\n[kworker/5:1]\n[kworker/5:2]\n[kworker/7:2]\n[libvirtd]\n[libvirtd] 1\n[libvirtd] 2\n[ls]\n[ls] 1\n[mysqld]\n[nmbd]\n[nmbd] 2\n[nmbd] 4\n[nmbd] 5\n[nmbd] 6\n[python]\n[python] 1\n[python] 2\n[python] 3\n[python] 4\n[python] 5\n[rserver]\n[rserver] 1\n[rserver] 2\n[rserver] 3\n[rserver] 6\n[rserver] 8\n[rsession]\n[rsession] 1\n[run-parts]\n[run-parts] 1\n[sh]\n[sh] 1\n[smbd]\n[smbd] 1\n[smbd] 2\n[smbd] 3\n[sshd]\n[sshd] 1\n[sshd] 4\n[sshd] 5\n[swapper/0]\n[udevd]\n[udevd] 1\n[update-motd-fsc]\n[update-motd-fsc] 1\n[update-motd-upd]\n[update-motd-upd] 1\n[upstart-udev-br]\n[upstart-udev-br] 1\n[xdg-settings]\n[xdg-settings] 1\n[Chrome_HistoryT] 10\n[Chrome_HistoryT] 11\n[Chrome_HistoryT] 12\n[Chrome_HistoryT] 14\n[Chrome_HistoryT] 15\n[Chrome_HistoryT] 16\n[Chrome_HistoryT] 2\n[Chrome_HistoryT] 3\n[Chrome_HistoryT] 4\n[Chrome_HistoryT] 5\n[Chrome_HistoryT] 6\n[Chrome_HistoryT] 7\n[Chrome_HistoryT] 8\n[Chrome_HistoryT] 9\n[Chrome_IOThread]\n[Chrome_IOThread] 1\n[Chrome_ProcessL]\n[Chrome_SafeBrow]\n[Chrome_SafeBrow] 1\n[Chrome_SafeBrow] 4\n[Chrome_SafeBrow] 5\n[Chrome_SyncThre]\n[Chrome_SyncThre] 1\n[Chrome_SyncThre] 10\n[Chrome_SyncThre] 11\n[Chrome_SyncThre] 12\n[Chrome_SyncThre] 13\n[Chrome_SyncThre] 14\n[Chrome_SyncThre] 15\n[Chrome_SyncThre] 16\n[Chrome_SyncThre] 2\n[Chrome_SyncThre] 3\n[Chrome_SyncThre] 4\n[Chrome_SyncThre] 5\n[Chrome_SyncThre] 6\n[Chrome_SyncThre] 7\n[Chrome_SyncThre] 8\n[Chrome_SyncThre] 9\n[Chrome_WebKitTh]\n[Chrome_WebKitTh] 1\n[MediaStreamDevi]\n[MediaStreamDevi] 1\n[NetworkChangeNo]\n[NetworkChangeNo] 1\n[WorkerPool/2786]\n[WorkerPool/2786] 1\n[WorkerPool/2787]\n[WorkerPool/2787] 1\n[WorkerPool/2788]\n[WorkerPool/2788] 1\n[WorkerPool/2839]\n[WorkerPool/2839] 1\n[WorkerPool/2840]\n[WorkerPool/2840] 1\n[WorkerPool/2840] 2\n[WorkerPool/2981]\n[WorkerPool/2981] 1\n[WorkerPool/2982]\n[WorkerPool/2982] 1\n[_BDX]\n[_BDX] 1\n[apache2]\n[apache2] 1\n[apache2ctl]\n[apache2ctl] 1\n[bash]\n[bash] 1\n[bash] 2\n[bash] 3\n[bash] 4\n[bash] 5\n[blktrace]\n[cat]\n[cat] 1\n[checkarray]\n[checkarray] 1\n[chrome]\n[chrome] 1\n[chrome] 2\n[chrome] 3\n[chrome] 4\n[console-kit-dae]\n[console-kit-dae] 1\n[console-kit-dae] 4\n[cp]\n[cp] 1\n[cron]\n[dbus-launch]\n[dbus-launch] 1\n[env]\n[env] 1\n[expr]\n[expr] 2\n[flush-8:0]\n[flush-8:0] 1\n[flush-8:0] 10\n[flush-8:0] 11\n[flush-8:0] 12\n[flush-8:0] 13\n[flush-8:0] 14\n[flush-8:0] 15\n[flush-8:0] 16\n[flush-8:0] 2\n[flush-8:0] 3\n[flush-8:0] 4\n[flush-8:0] 5\n[flush-8:0] 6\n[flush-8:0] 7\n[flush-8:0] 8\n[flush-8:0] 9\n[git-pull]\n[git-pull] 1\n[git]\n[git] 1\n[gnome-pty-helpe]\n[gnome-pty-helpe] 1\n[gnome-terminal]\n[gnome-terminal] 1\n[gnome-terminal] 2\n[gnome-terminal] 3\n[gnome-terminal] 4\n[gnome-terminal] 5\n[gnome-terminal] 7\n[google-chrome]\n[google-chrome] 1\n[inotify_reader]\n[inotify_reader] 1\n[jbd2/sda1-8]\n[jbd2/sda1-8] 1\n[jbd2/sda1-8] 16\n[jbd2/sda1-8] 2\n[jbd2/sda1-8] 3\n[jbd2/sda1-8] 4\n[jbd2/sda1-8] 5\n[kworker/0:1]\n[kworker/0:2]\n[kworker/2:1]\n[kworker/2:2]\n[kworker/3:0]\n[kworker/4:0]\n[kworker/4:1]\n[kworker/4:2]\n[kworker/5:0]\n[kworker/5:1]\n[kworker/5:2]\n[kworker/7:2]\n[libvirtd]\n[libvirtd] 1\n[libvirtd] 2\n[ls]\n[ls] 1\n[mysqld]\n[nmbd]\n[nmbd] 2\n[nmbd] 4\n[nmbd] 5\n[nmbd] 6\n[python]\n[python] 1\n[python] 2\n[python] 3\n[python] 4\n[python] 5\n[rserver]\n[rserver] 1\n[rserver] 2\n[rserver] 3\n[rserver] 6\n[rserver] 8\n[rsession]\n[rsession] 1\n[run-parts]\n[run-parts] 1\n[sh]\n[sh] 1\n[smbd]\n[smbd] 1\n[smbd] 2\n[smbd] 3\n[sshd]\n[sshd] 1\n[sshd] 4\n[sshd] 5\n[swapper/0]\n[udevd]\n[udevd] 1\n[update-motd-fsc]\n[update-motd-fsc] 1\n[update-motd-upd]\n[update-motd-upd] 1\n[upstart-udev-br]\n[upstart-udev-br] 1\n[xdg-settings]\n[xdg-settings] 1\n\n</pre>\n\n\n=== IOWA.sim.iox (tpcc_250gb_48h) ===\n\n* R.addr analysis\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_182112.sigio_25.iowsz_100.t1_10000] $ cat __simout.sigio_25.iowsz_100.t1_10000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  7\n__valu__sig__ _n_o_sigaddrs :  126606\n__valu__sig__ _sigioc_acc :  1169938\n__valu__sig__ _sigaddrs_efficiency :  9.24077847811\n__valu__sig__ _n_o_addr_total :  1691608\n__valu__sig__ _ioc_total :  4113312\n</pre>\n\n\n* W.addr analysis\n\n <pre>\na1mjjung@secm:[iowa.sim.iox.20130306_190100.sigio_25.iowsz_100.t1_1000] $ cat __simout.sigio_25.iowsz_100.t1_1000.__valu__sig__.log \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  601\n__valu__sig__ _n_o_sigaddrs :  46304\n__valu__sig__ _sigioc_acc :  47940036\n__valu__sig__ _sigaddrs_efficiency :  1035.33249827\n__valu__sig__ _n_o_addr_total :  4910080\n__valu__sig__ _ioc_total :  191622453\n</pre>\n\n x39.02 = ( total_#_of_IOs / total_#_of_addrs_hit_actually )\n x1035.33 = ( 25%_sig_IOs / 25%_sig_addrs )\n\n=== R \'e1071\' package install (command line) ===\n\n <pre>\na1mjjung@secm:[e1071] $ R CMD INSTALL e1071\nJob <13412785> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura007>>\nWarning: invalid package \'e1071\'\nError: ERROR: no packages specified\na1mjjung@secm:[e1071] $ cd ..\nla1mjjung@secm:[e1071_build] $ l\ntotal 280\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:45 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\na1mjjung@secm:[e1071_build] $ R CMD INSTALL e1071\nJob <13412786> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n* installing to library \'/home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15\'\n* installing *source* package \'e1071\' ...\n** package \'e1071\' successfully unpacked and MD5 sums checked\nchecking for C++ compiler default output file name... a.out\nchecking whether the C++ compiler works... yes\nchecking whether we are cross compiling... no\nchecking for suffix of executables... \nchecking for suffix of object files... o\nchecking whether we are using the GNU C++ compiler... yes\nchecking whether g++ accepts -g... yes\n** libs\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c Rsvm.c -o Rsvm.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cmeans.c -o cmeans.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cshell.c -o cshell.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c floyd.c -o floyd.o\ng++ -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c svm.cpp -o svm.o\ng++ -shared -L/usr/local/lib64 -o e1071.so Rsvm.o cmeans.o cshell.o floyd.o svm.o\ninstalling to /home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15/e1071/libs\n** R\n** inst\n** preparing package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n   \'svmdoc.Rnw\' \n** testing if installed package can be loaded\n\n* DONE (e1071)\na1mjjung@secm:[e1071_build] $ R\nJob <13412787> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n\nR version 2.15.2 (2012-10-26) -- \"Trick or Treat\"\nCopyright (C) 2012 The R Foundation for Statistical Computing\nISBN 3-900051-07-0\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType \'license()\' or \'licence()\' for distribution details.\n\nR is a collaborative project with many contributors.\nType \'contributors()\' for more information and\n\'citation()\' on how to cite R or R packages in publications.\n\nType \'demo()\' for some demos, \'help()\' for on-line help, or\n\'help.start()\' for an HTML browser interface to help.\nType \'q()\' to quit R.\n\n> ?svm\nNo documentation for \'svm\' in specified packages and libraries:\nyou could try \'??svm\'\n> library(e1071) \nLoading required package: class\n> ?svm\n> \nSave workspace image? [y/n/c]: y\na1mjjung@secm:[e1071_build] $ pwd\n/home/X0101/a1mjjung/x/R/e1071_build\na1mjjung@secm:[e1071_build] $ ls -alF\ntotal 284\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:47 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\n-rw------- 1 a1mjjung X0101     25 Mar  6 18:47 .Rhistory\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:46 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\n\n</pre>\n\n\n\n\n=== 3rd meeting with Dr. CHOI ===\n\n\n\n=== 수퍼컴 (supercom) ===\n\n\n* account\n: ID: a1mjjung\n: PW: wjdaudwns\n: IP address: 202.20.183.10 (ssh)\n\n* password change\n: 한지연 사원 (jiyoun92.han@partner.samsung.com) (031-280-8147)\n\n* python 2.7.x from 2.6.x\n: [a1mjjung@login03 ~]$ /apps/Python/Python-2.7.3/bin/python\n\n== ## bNote-2013-03-05 ==\n\n=== 최희열 전문과 2차 미팅 ===\n\n* IO trace (궤적)의 변화 예측\n: IO 궤적의 변화를 신속 정확하게 예측하는 것의 중요성\n:: 언제 hot data가 fast-tier에 올라가야 할지,\n:: 현재의 hot data 말고 새로운 hot data는 언제 올라가야할지?\n:: 현재 fast-tier에 올라가 있는 hot data가 언제 unload되어야 할지?\n\ncached/tiered hot data가 계속 hot storage area에 \n: IO hot zone 궤적 변화의 전조로 사용될 수 있을만한 시스템 정보/상태 변화\n: IO hot zone 궤적이 정말로 변화하는 것일지 아니면 기존 궤적에서의 fluctuation인지?\n\n* Challenging Problems\n\n: 단위 시간 당 hot zone의 갯수가 항상 유지되는 것은 아님. 즉, hot zone의 갯수는 1개로 유지된 채, hot zone의 위치만 이동하는 형태는 아닐 수 있음. hot zone의 갯수가 1개였다가 4개로, 7개로, 줄어서 5개로, 다시 줄어서 2개로, 이런 식으로\n\n== ## bNote-2013-03-05 ==\n\n=== IOWA to ML Formulation ===\n\n* IO trace (궤적)의 변화 예측\n: IO 궤적의 변화를 신속 정확하게 예측하는 것의 중요성\n:: 언제 hot data가 fast-tier에 올라가야 할지,\n:: 현재의 hot data 말고 새로운 hot data는 언제 올라가야할지?\n:: 현재 fast-tier에 올라가 있는 hot data가 언제 unload되어야 할지?\n\ncached/tiered hot data가 계속 hot storage area에 \n: IO hot zone 궤적 변화의 전조로 사용될 수 있을만한 시스템 정보/상태 변화\n: IO hot zone 궤적이 정말로 변화하는 것일지 아니면 기존 궤적에서의 fluctuation인지?\n\n* Challenging Problems\n\n: 단위 시간 당 hot zone의 갯수가 항상 유지되는 것은 아님. 즉, hot zone의 갯수는 1개로 유지된 채, hot zone의 위치만 이동하는 형태는 아닐 수 있음. hot zone의 갯수가 1개였다가 4개로, 7개로, 줄어서 5개로, 다시 줄어서 2개로, 이런 식으로','utf-8'),(67,'== 20130530_134618 ==\n=== Introduction to Association Rule Mining ===\n\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n\n\n\n=== Frequent patterns/items mining examples with R::arules ===\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n* References\n:# [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))]\n:# [http://www.analysis-of-patterns.net/files/bgoethals.pdf Frequent Pattern Mining Slides - Universiteit Antwerpen]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning - Wikipedia]\n:# [http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf Frequent Pattern Mining in Web Log Data]\n:# [http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010 Temporal Pattern Mining Tutorial KDD 2010]\n:# [http://cran.r-project.org/web/packages/arules/index.html arules: Mining Association Rules and Frequent Itemsets]\n:# [http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf Introduction to arules - A computational environment for mining association rules and frequent item sets -- http://cran.r-project.org/]\n:# [http://lyle.smu.edu/IDA/arules/ Intelligent Data Analysis Lab @ SMU]\n:# [http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))]\n:# [http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/ Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning]\n:# [http://en.wikipedia.org/wiki/Sequence_mining Sequence mining]\n:# [http://en.wikipedia.org/wiki/GSP_Algorithm GSP Algorithm]\n:# [http://en.wikipedia.org/wiki/Apriori_algorithm Apriori algorithm]\n:# [http://en.wikipedia.org/wiki/Market_basket_analysis Market basket analysis]\n:# [http://en.wikipedia.org/wiki/Time_series Time series]\n\n\n----','utf-8'),(68,'== Introduction to Association Rule Mining ==\n\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n\n\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* [http://www.analysis-of-patterns.net/files/bgoethals.pdf Frequent Pattern Mining Slides - Universiteit Antwerpen]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning - Wikipedia]\n:# [http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf Frequent Pattern Mining in Web Log Data]\n:# [http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010 Temporal Pattern Mining Tutorial KDD 2010]\n:# [http://cran.r-project.org/web/packages/arules/index.html arules: Mining Association Rules and Frequent Itemsets]\n:# [http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf Introduction to arules - A computational environment for mining association rules and frequent item sets -- http://cran.r-project.org/]\n:# [http://lyle.smu.edu/IDA/arules/ Intelligent Data Analysis Lab @ SMU]\n:# [http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))]\n:# [http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/ Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers]\n:# [http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning]\n:# [http://en.wikipedia.org/wiki/Sequence_mining Sequence mining]\n:# [http://en.wikipedia.org/wiki/GSP_Algorithm GSP Algorithm]\n:# [http://en.wikipedia.org/wiki/Apriori_algorithm Apriori algorithm]\n:# [http://en.wikipedia.org/wiki/Market_basket_analysis Market basket analysis]\n:# [http://en.wikipedia.org/wiki/Time_series Time series]\n\n\n----','utf-8'),(69,'== Introduction to Association Rule Mining ==\n\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n\n\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(70,'== Introduction to Association Rule Mining ==\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(71,'\n== 20130530_144254 ==\n\n=== Introductory Articles to R Statistical Computing Software ===\n\n* http://www.cran.r-project.org/doc/manuals/R-intro.pdf\n: An Introduction to R -- Notes on R: A Programming Environment for Data Analysis and Graphics (Version 3.0.1 (2013-05-16))\n\n\n\n\n== 20130528_213910 ==\n\n=== suppress the command output in R ===\n\n* use sink() function, please!\n <pre>\nsink(file=\"arm_inspect.650k-support_0.012-225x488.log\") # enabling sink operation \n# (to redirect all the stdout to the file specified)\n\ninspect(f_650k_0012)\n\nsink() # disabling sink operation\n# (after this command, you can see the output message to stdout)\n</pre>\n\n\n* References\n\n:* https://stat.ethz.ch/pipermail/r-help/2007-August/138070.html\n:: R sink behavior (stat.ethz.ch)\n\n:* http://stat.ethz.ch/R-manual/R-patched/library/base/html/sink.html\n:: Send R Output to a File\n\n\n\n\n=== defining function in R (user-defined function in R) ===\n\n* use allocation operator \'<-\' to define my custom function\n <pre>\niowa_arm_f <- function (opt_dsrc, opt_support) {\n	print(\">> IOWA ARM: started\");\n	sink(file=\"/dev/null\");\n	t <- as(opt_dsrc, \"transactions\");\n	f <- eclat(opt_dsrc, parameter=list(support=opt_support, tidLists=T));\n	sink();\n	print(\">> IOWA ARM: finished\");\n	dim(tidLists(f));\n	return(f);\n}\n</pre>\n\n* use the function as usual\n <pre>\nf <- iowa_arm_f(ciop_d010, 0.012);\ndim(tidLists(f));\n</pre>\n\n* References\n\n:* http://www.statmethods.net/management/userfunctions.html\n:: User-written Functions -- Quick-R ((B.GOOD))\n\n\n\n== 20130524_103604 ==\n\n\n=== source() ===\n\n <pre>\nsource(\'data_set.R\');\n</pre>\n\n=== arules eclat() algorithm for association rules mining ===\n\n <pre>\nlibrary(arules);\ndata <- list(\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_1187\", \"_addr_23813\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\n			 c(\"_addr_23813\", \"_addr_51276758\"),\n			 c(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\n			 c(\"_addr_51276758\", \"_addr_319275\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\n			 c(\"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\n			 c(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\n			 c(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\n			 c(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\n			 c(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\n			 c(\"_addr_319275\", \"_addr_51276758\")\n			 );\n\nt <- as(data, \"transactions\");\nf <- eclat(data, parameter=list(tidLists=T, support=0.25))\ndim(tidLists(f))\nas(tidLists(f), \"list\")\nimage(tidLists(f))\ninspect(f)\n</pre>\n\n\n\n\n=== read.table(), write.table(), unlist() ===\n\n <pre>\nblusjune@jimi-hendrix:[basics] $ cat filein.L20 \n\n248425 , 100276969472\n248772 , 78847959040\n249197 , 139913195520\n251828 , 75536334848\n253182 , 76534030336\n254083 , 143595069440\n254961 , 143598755840\n255185 , 150857949184\n255393 , 118433374208\n255755 , 100324941824\n256025 , 85407301632\n258666 , 95264866304\n259078 , 142196629504\n261133 , 88597774336\n263287 , 97312505856\n264135 , 112585678848\n267323 , 96259047424\n267351 , 140665122816\n267634 , 139540049920\n268982 , 117314224128\n</pre>\n\n <pre>\n> datain;\n\n   timestamp      address\n   1     248425 100276969472\n   2     248772  78847959040\n   3     249197 139913195520\n   4     251828  75536334848\n   5     253182  76534030336\n   6     254083 143595069440\n   7     254961 143598755840\n   8     255185 150857949184\n   9     255393 118433374208\n   10    255755 100324941824\n   11    256025  85407301632\n   12    258666  95264866304\n   13    259078 142196629504\n   14    261133  88597774336\n   15    263287  97312505856\n   16    264135 112585678848\n   17    267323  96259047424\n   18    267351 140665122816\n   19    267634 139540049920\n   20    268982 117314224128\n</pre>\n\n <pre>\ndatain <- read.table(\'filein.L20\', col.names=c(\"timestamp\", \"address\"), sep=\",\", header=F);\ndataout <- datain %% 10;\ndataout_ul <- unlist(dataout, use.names=F)\nwrite.table(dataout, file=\"fileout.L20\", append=F, quote=T, sep=\" , \", row.names=F, col.names=T);\n</pre>\n\n\n\n\n=== scan() ===\n\n <pre>\ndatain_double <- scan(file=\"d010\", what=double(), sep=\"\", strip.white=T);\ndatain_char <- scan(file=\"d010\", what=\"character\", sep=\"\\n\", strip.white=T);\ndatain_char <- scan(file=\"d010\", what=\"char\", sep=\";\", strip.white=T);\n</pre>\n\n\n\n\n* Case 1: \'d010\' - data file of wrong format\n\n <pre>\nblusjune@jimi-hendrix:[basics] $ cat d010\n8 4 0 5 7;\n11 5 8;\n8 6 3 5 1;\n2 11 1;\n4 6 3 12 4;\n6 7 0 10 4 7;\n7 6 3;\n3 10 7 6 7 6;\n7 10;\n11 10\n</pre>\n\n </pre>\n> scan(file=\'d010\', what=\"numeric\", sep=\";\\n\", strip.white=T)\nError in scan(file = \"d010\", what = \"numeric\", sep = \";\\n\", strip.white = T) : \n  invalid \'sep\' value: must be one byte\n\n> scan(file=\'d010\', what=\"numeric\", sep=\"\\n\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7;\"    \"11 5 8;\"       \"8 6 3 5 1;\"    \"2 11 1;\"      \n [5] \"4 6 3 12 4;\"   \"6 7 0 10 4 7;\" \"7 6 3;\"        \"3 10 7 6 7 6;\"\n [9] \"7 10;\"         \"11 10\"        \n\n> scan(file=\'d010\', what=\"numeric\", sep=\";\", strip.white=T)\nRead 19 items\n [1] \"8 4 0 5 7\"    \"\"             \"11 5 8\"       \"\"             \"8 6 3 5 1\"   \n [6] \"\"             \"2 11 1\"       \"\"             \"4 6 3 12 4\"   \"\"            \n[11] \"6 7 0 10 4 7\" \"\"             \"7 6 3\"        \"\"             \"3 10 7 6 7 6\"\n[16] \"\"             \"7 10\"         \"\"             \"11 10\"       \n\n> scan(file=\'d010\', what=\"numeric\", sep=\"\", strip.white=T)\nRead 40 items\n [1] \"8\"   \"4\"   \"0\"   \"5\"   \"7;\"  \"11\"  \"5\"   \"8;\"  \"8\"   \"6\"   \"3\"   \"5\"  \n[13] \"1;\"  \"2\"   \"11\"  \"1;\"  \"4\"   \"6\"   \"3\"   \"12\"  \"4;\"  \"6\"   \"7\"   \"0\"  \n[25] \"10\"  \"4\"   \"7;\"  \"7\"   \"6\"   \"3;\"  \"3\"   \"10\"  \"7\"   \"6\"   \"7\"   \"6;\" \n[37] \"7\"   \"10;\" \"11\"  \"10\" \n\n</pre>\n\n\n\n\n* Case 2: \'d020\' - data file of good format\n\n <pre>\nblusjune@jimi-hendrix:[basics] $ cat d020\n8 4 0 5 7; 11 5 8; 8 6 3 5 1; 2 11 1; 4 6 3 12 4; 6 7 0 10 4 7; 7 6 3; 3 10 7 6 7 6; 7 10; 11 10\n</pre>\n\n <pre>\n> scan(file=\'d020\', what=\"numeric\", sep=\"\", strip.white=T)\nRead 40 items\n [1] \"8\"   \"4\"   \"0\"   \"5\"   \"7;\"  \"11\"  \"5\"   \"8;\"  \"8\"   \"6\"   \"3\"   \"5\"  \n[13] \"1;\"  \"2\"   \"11\"  \"1;\"  \"4\"   \"6\"   \"3\"   \"12\"  \"4;\"  \"6\"   \"7\"   \"0\"  \n[25] \"10\"  \"4\"   \"7;\"  \"7\"   \"6\"   \"3;\"  \"3\"   \"10\"  \"7\"   \"6\"   \"7\"   \"6;\" \n[37] \"7\"   \"10;\" \"11\"  \"10\" \n\n> scan(file=\'d020\', what=\"numeric\", sep=\";\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7\"    \"11 5 8\"       \"8 6 3 5 1\"    \"2 11 1\"       \"4 6 3 12 4\"  \n [6] \"6 7 0 10 4 7\" \"7 6 3\"        \"3 10 7 6 7 6\" \"7 10\"         \"11 10\"     \n</pre>\n\n\n\n\n* Case 3: \'d030\' - data file of good format\n\n <pre>\n8 4 0 5 7\n11 5 8\n8 6 3 5 1\n2 11 1\n4 6 3 12 4\n6 7 0 10 4 7\n7 6 3\n3 10 7 6 7 6\n7 10\n11 10\n</pre>\n\n <pre>\n> scan(file=\'d030\', what=\'numeric\', sep=\"\", strip.white=T)\nRead 40 items\n [1] \"8\"  \"4\"  \"0\"  \"5\"  \"7\"  \"11\" \"5\"  \"8\"  \"8\"  \"6\"  \"3\"  \"5\"  \"1\"  \"2\"  \"11\"\n[16] \"1\"  \"4\"  \"6\"  \"3\"  \"12\" \"4\"  \"6\"  \"7\"  \"0\"  \"10\" \"4\"  \"7\"  \"7\"  \"6\"  \"3\" \n[31] \"3\"  \"10\" \"7\"  \"6\"  \"7\"  \"6\"  \"7\"  \"10\" \"11\" \"10\"\n\n> scan(file=\'d030\', what=\'numeric\', sep=\";\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7\"    \"11 5 8\"       \"8 6 3 5 1\"    \"2 11 1\"       \"4 6 3 12 4\"  \n [6] \"6 7 0 10 4 7\" \"7 6 3\"        \"3 10 7 6 7 6\" \"7 10\"         \"11 10\"       \n\n> scan(file=\'d030\', what=\'numeric\', sep=\"\\n\", strip.white=T)\nRead 10 items\n [1] \"8 4 0 5 7\"    \"11 5 8\"       \"8 6 3 5 1\"    \"2 11 1\"       \"4 6 3 12 4\"  \n [6] \"6 7 0 10 4 7\" \"7 6 3\"        \"3 10 7 6 7 6\" \"7 10\"         \"11 10\"  \n</pre>\n\n\n\n\n== 20130502_032415 ==\n\n=== R script ===\n\n <pre>\na1mjjung@secm:[r_stat] $ cat > iowa_anal_1010.R << EOF\n#iowa_anal_1010.R\n#20130430_140506\n\n\nlibrary(e1071);\nlibrary(arules);\nlibrary(scatterplot3d);\n\n\nrm(list=ls());\n\n\nmyd <- read.table(\'r_stat_infile\', col.names=c(\"_hitcnt_\", \"_addr_\", \"_mwid_\"));\nmyd_mwid <- unlist(myd[3]);\nmyd_addr <- unlist(myd[2]);\nmyd_hitcnt <- unlist(myd[1]);\n\n\nmymat <- cbind(myd_mwid, myd_addr, myd_hitcnt);\ncolnames(mymat) <- c(\"_mwid_\", \"_addr_\", \"_hcnt_\");\nnocl <- 22; itmax <- 100; (cl1 <- kmeans(mymat, nocl, iter.max=itmax));\n\n\npng(\'output_col_by_hitcnt.png\', width=1024, height=2048);\nplot(myd_mwid, myd_addr, col=myd_hitcnt);\ndev.off();\npng(\'output_col_by_cluster.png\', width=1024, height=2048);\nplot(myd_mwid, myd_addr, col=cl1$cluster);\ndev.off();\nEOF\n\n</pre>\n\n\n\n== 20130428_233708 ==\n\n\n\n=== R (r_stat) references ===\n\n\n* manual/introduction to R\n:- [http://cran.r-project.org/doc/manuals/R-intro.pdf R introduction]\n:- [http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf arules (Association Rules Mining)]\n:- [http://www.cl.cam.ac.uk/~av308/vlachos_msc_thesis.pdf using SVM]\n\n\n* \'%in%\' and example() function\n\n <pre>\nOn 8 May 2011 21:18, Berwin A Turlach <[hidden email]> wrote:\n> G\'day Dan,\n>\n> On Sun, 8 May 2011 05:06:27 -0400\n> Dan Abner <[hidden email]> wrote:\n>\n>> Hello everyone,\n>>\n>> I am attempting to use the %in% operator with the ! to produce a NOT\n>> IN type of operation. Why does this not work? Suggestions?\n\nAlternatively,\n\nexample(`%in%`)\n\nor\n\n`%ni%` = Negate(`%in%`)\n\nHTH,\n\nbaptiste\n</pre>\n\n\n\n\n=== kmeans-svm #2 :: applying the kmeans result to svm (as a guideline for supervising) ((B.GOOD)) ===\n\n <pre>\n\n# combining the columns xy (time_t1, file_id) and cl4 (cluster_id)\nxy_cl4 <- cbind(xy, cl4$cluster);\n\n# assign the column names\ncolnames(xy_cl4) <- c(\"time_t1\", \"file_id\", \"cluster_id\");\n\n# prepare the x\'s and y for svm\nsmv_x <- subset(xy_cl4, select= -cluster_id);\nsmv_y <- subset(xy_cl4, select= cluster_id);\n\n# load the library \'e1071\'\nlibrary(e1071);\n\n# execute svm analysis to get model \'m\'\nm <- svm(svm_x, svm_y);\n\n# predict a new value with the model \'m\' and x\'s data\npred <- predict(m, svm_x, decision.values = T);\n\n# check the prediction result by comparing the predicted value \'pred\' with original value \'svm_y\'\ntable(pred, svm_y);\n\n# plot the result\nplot(cmdscale(dist(xy_cl4[, -3])), col=pred, pch=c(\"o\", \"+\")[1:120 %in% m$index + 1]);\n\n# write the resultant xy_cl4 table to the file(.csv)\nwrite.csv(xy_cl4, file=\"xy_cl4.csv\", row.names=T, col.names=T);\n\n</pre>\n\n\n\n\n=== kmeans-svm #1 :: kmeans example ((B.GOOD)) ===\n\n <pre>\n\n# clear all the data in memory\nrm(list=ls());\n\n# read the data from file (.csv format)\nmyd <- read.csv(\'traces/iowa_v3.csv\', header=T);\n\n# unlist() to convert \'list\' type data to \'vector\' type data for further numerical calculation\nmyd_c1 <- unlist(myd[1]); # field: time_t0\nmyd_c2 <- unlist(myd[2]); # field: time_t1\nmyd_c3 <- unlist(myd[3]); # field: prog\nmyd_c4 <- unlist(myd[4]); # field: file\n\n# create 2-D matrix (x-y) for 2-D kmeans analysis\nxy <- cbind(myd_c2, myd_c4); # combining columns: \'time_t1\' and \'file\'\n\n# assign the column names for x-label and y-label for better look of plot()\ncolnames(xy) <- c(\"time_t1\", \"file_id\");\n\n# load the library \'e1071\'\nlibrary(e1071);\n\n# execute kmeans analysis until we get satisfactory clustering performance value (value = between_SS / total_SS)\n# number of clusters is set to \'6\' huristically after plotting data xy to see the outlook\n# number of maximum iterations is set to \'100\' huristically after executing kmeans analysis several times\ncl1 <- kmeans(xy, 6, iter.max=100); # initial trial, the clustering performance value is ... (cannot evaluate)\ncl2 <- kmeans(xy, 6, iter.max=100); # second trial, the clustering performance value is higher than the first trial\ncl3 <- kmeans(xy, 6, iter.max=100); # third trial, the clustering performance value is higher than the second trial\n(cl4 <- kmeans(xy, 6, iter.max=100)); # fourth trial, the clustering performance value is not going higher than the last trial\n\n# 2-D plotting\nplot(xy, col=cl4$cluster);\n\n</pre>\n\n\n\n\n* most outer parentheses are used to see the result of execution\n\n <pre>\n\n> (cl4 <- kmeans(xy, 6, iter.max=100))\nK-means clustering with 6 clusters of sizes 19, 22, 19, 15, 20, 25\n\nCluster means:\n   time_t1    file_id\n1 10.52632 32.2105263\n2 14.40909 43.3181818\n3 18.57895 33.5263158\n4 22.00000  0.2666667\n5  6.50000 11.7000000\n6  2.00000  1.0000000\n\nClustering vector:\n  time_t11   time_t12   time_t13   time_t14   time_t15   time_t16   time_t17 \n         6          6          6          6          5          5          5 \n  time_t18   time_t19  time_t110  time_t111  time_t112  time_t113  time_t114 \n         5          1          2          1          1          2          2 \n time_t115  time_t116  time_t117  time_t118  time_t119  time_t120  time_t121 \n         2          2          3          3          3          3          4 \n time_t122  time_t123  time_t124  time_t125  time_t126  time_t127  time_t128 \n         4          4          6          6          6          6          6 \n time_t129  time_t130  time_t131  time_t132  time_t133  time_t134  time_t135 \n         5          5          5          5          1          1          1 \n time_t136  time_t137  time_t138  time_t139  time_t140  time_t141  time_t142 \n         1          2          2          2          2          2          3 \n time_t143  time_t144  time_t145  time_t146  time_t147  time_t148  time_t149 \n         3          3          4          4          4          6          6 \n time_t150  time_t151  time_t152  time_t153  time_t154  time_t155  time_t156 \n         6          6          6          5          5          5          5 \n time_t157  time_t158  time_t159  time_t160  time_t161  time_t162  time_t163 \n         1          1          1          1          2          2          2 \n time_t164  time_t165  time_t166  time_t167  time_t168  time_t169  time_t170 \n         2          3          3          3          3          4          4 \n time_t171  time_t172  time_t173  time_t174  time_t175  time_t176  time_t177 \n         4          6          6          6          6          6          5 \n time_t178  time_t179  time_t180  time_t181  time_t182  time_t183  time_t184 \n         5          5          5          1          1          1          1 \n time_t185  time_t186  time_t187  time_t188  time_t189  time_t190  time_t191 \n         2          2          2          2          3          3          3 \n time_t192  time_t193  time_t194  time_t195  time_t196  time_t197  time_t198 \n         3          4          4          4          6          6          6 \n time_t199 time_t1100 time_t1101 time_t1102 time_t1103 time_t1104 time_t1105 \n         6          6          5          5          5          5          1 \ntime_t1106 time_t1107 time_t1108 time_t1109 time_t1110 time_t1111 time_t1112 \n         1          1          1          2          2          2          2 \ntime_t1113 time_t1114 time_t1115 time_t1116 time_t1117 time_t1118 time_t1119 \n         3          3          3          3          4          4          4 \ntime_t1120 \n         6 \n\nWithin cluster sum of squares by cluster:\n[1]  51.89474 212.09091 117.36842  14.93333  53.20000 122.00000\n (between_SS / total_SS =  98.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"        \n> \n\n</pre>\n\n\n\n\n=== example() function in R ((B.GOOD)) ===\n\nexample() function is very very very useful to grasp the quick overview of some \'concept\' or \'function\' in R\n\n <pre>\n\nexample(svm)\nexample(kmeans)\nexample(\'%in%\')\nexample(rm)\nexample(plot)\n\n</pre>\n\n\n\n\n=== convert list to vector ===\n\nNote: the first line of \'iowa_v3.csv\' file should be the header information line (not the empty line containing just \',\')\n\n <pre>\nx_as_list_type <- read.csv(\'traces/iowa_v3.csv\', header=T)\nx_as_int_vector <- unlist(x_as_list_type, use.name=F)\n</pre>\n\n\n\n\n== 20130329_111903 ==\n\n=== R script file execution from command line ===\n\n$ cat > ex1.R << EOF\n <pre>\nlibrary(\"arules\")\ndata(\"Epub\")\nEpub\nsummary(Epub)\nquit(save=\"no\")\n\n## NOTE:\n# the last line \'quit(save=\"no\")\' is very important\n# to avoid the hang-like situation of R\nEOF\n</pre>\n\n$ R < ex1.R\n <pre>\nJob <13833365> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura009>>\n\nR version 2.15.2 (2012-10-26) -- \"Trick or Treat\"\nCopyright (C) 2012 The R Foundation for Statistical Computing\nISBN 3-900051-07-0\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType \'license()\' or \'licence()\' for distribution details.\n\nR is a collaborative project with many contributors.\nType \'contributors()\' for more information and\n\'citation()\' on how to cite R or R packages in publications.\n\nType \'demo()\' for some demos, \'help()\' for on-line help, or\n\'help.start()\' for an HTML browser interface to help.\nType \'q()\' to quit R.\n\n[Previously saved workspace restored]\n\n> Loading required package: Matrix\nLoading required package: lattice\n\nAttaching package: \'arules\'\n\nThe following object(s) are masked from \'package:base\':\n\n    %in%, write\n\n> > transactions in sparse format with\n 15729 transactions (rows) and\n 936 items (columns)\n> transactions as itemMatrix in sparse format with\n 15729 rows (elements/itemsets/transactions) and\n 936 columns (items) and a density of 0.001758755 \n\nmost frequent items:\ndoc_11d doc_813 doc_4c6 doc_955 doc_698 (Other) \n    356     329     288     282     245   24393 \n\nelement (itemset/transaction) length distribution:\nsizes\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n11615  2189   854   409   198   121    93    50    42    34    26    12    10 \n   14    15    16    17    18    19    20    21    22    23    24    25    26 \n   10     6     8     6     5     8     2     2     3     2     3     4     5 \n   27    28    30    34    36    38    41    43    52    58 \n    1     1     1     2     1     2     1     1     1     1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.646   2.000  58.000 \n\nincludes extended item information - examples:\n   labels\n1 doc_11d\n2 doc_13d\n3 doc_14c\n\nincludes extended transaction information - examples:\n      transactionID           TimeStamp\n10792  session_4795 2003-01-02 10:59:00\n10793  session_4797 2003-01-02 21:46:01\n10794  session_479a 2003-01-03 00:50:38\n> \n</pre>\n\n\n=== paste() example ===\n\n* example 1\n <pre>\n> 1:6\n[1] 1 2 3 4 5 6\n> paste(1:6)\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n> paste(\"A\", 1:6)\n[1] \"A 1\" \"A 2\" \"A 3\" \"A 4\" \"A 5\" \"A 6\"\n> paste(\"A\", 1:6, sep=\"\")\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\" \"A6\"\n> 2:7\n[1] 2 3 4 5 6 7\n> seq(8,3,by=-1)\n[1] 8 7 6 5 4 3\n> paste(\"A\", 1:6, 2:7, seq(8,3,by=-1), sep=\"\")\n[1] \"A128\" \"A237\" \"A346\" \"A455\" \"A564\" \"A673\"\n> paste(\"A\", 1:6, 2:7, seq(8,3,by=-1), sep=\"-\")\n[1] \"A-1-2-8\" \"A-2-3-7\" \"A-3-4-6\" \"A-4-5-5\" \"A-5-6-4\" \"A-6-7-3\"\n> paste(\"A\", 1:6, 2:7, seq(8,3,by=-1), sep=\"/\")\n[1] \"A/1/2/8\" \"A/2/3/7\" \"A/3/4/6\" \"A/4/5/5\" \"A/5/6/4\" \"A/6/7/3\"\n</pre>\n\n* example 2\n <pre>\n> stopifnot(identical(str1 <- paste(\"A\", 1:6, sep=\"\"), str2 <- paste0(\"A\", 1:6)))\n> str1\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\" \"A6\"\n> str2\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\" \"A6\"\n\n> paste(\"Today is\", date())\n[1] \"Today is Fri Mar 29 11:22:15 2013\"\n</pre>\n\n\n\n\n== 20130327_183541 ==\n\n=== list() examples ===\n\n <pre>\n> myl = list(apple=1, banana=2, cherry=3, durian=4, elderberry=5)\n> myl\n$apple\n[1] 1\n\n$banana\n[1] 2\n\n$cherry\n[1] 3\n\n$durian\n[1] 4\n\n$elderberry\n[1] 5\n\n> myl$apple\n[1] 1\n> myl$banana\n[1] 2\n> myl$cherry\n[1] 3\n> myl$durian\n[1] 4\n> myl$elderberry\n[1] 5\n> \n</pre>\n\n\n=== read-from/save-to a file ===\n\n <pre>\n# create a formatted transaction data\n> myd <- paste(\"apple,banana\", \"apple\", \"banana,cherry\", \"banana,cherry,durian\", sep=\"\\n\")> cat(myd)\napple,banana\napple\nbanana,cherry\n\n# write the transaction data to the file\n> write(myd, file = \"myd_basket_2\") \n\n# read the transaction data from the file, and save it to a variable\n> myt <- read.transactions(\"myd_basket_2\", format = \"basket\", sep=\",\")\n\n# inspect the transaction variable\n> inspect(myt)\n  items   \n1 {apple, \n   banana}\n2 {apple} \n3 {banana,\n   cherry}\n4 {banana,\n   cherry,\n   durian}\n</pre>\n\n\n\n\n=== clear workspace (delete data) ===\n\n* References\n* [https://stat.ethz.ch/pipermail/r-help/2007-August/137694.html Clear Workspace in R]\n* [http://stackoverflow.com/questions/11761992/remove-data-from-workspace Advanced method to clear data in R]\n\n* simply, remove three data \'data_1\', \'data_2\', \'data_3\'\n <pre>\nrm(data_1, data_2, data_3)\n</pre>\n\n* remove all the data searchable by ls()\n <pre>\nrm(list = ls())\n# \'list\' is a name of parameter to be passed into \'rm()\' function,\n# so it cannot be changed, it should be \"list\" literally.\n</pre>\n\n* remove all objects whose name begins with the string \"tmp\"\n <pre>\nrm(list = ls()[grep(\"^tmp\", ls())])\nrm(list = ls(pattern=\"^tmp\"))	# making use of the \'pattern\' argument\n</pre>\n\n== 20130313_172639 ==\n\n\n=== SVM example ===\n\n <pre>\n     data(iris)\n     attach(iris)\n     \n     ## classification mode\n     # default with factor response:\n     model <- svm(Species ~ ., data = iris)\n     \n     # alternatively the traditional interface:\n     x <- subset(iris, select = -Species)\n     y <- Species\n     model <- svm(x, y) \n     \n     print(model)\n     summary(model)\n     \n     # test with train data\n     pred <- predict(model, x)\n     # (same as:)\n     pred <- fitted(model)\n     \n     # Check accuracy:\n     table(pred, y)\n     \n     # compute decision values and probabilities:\n     pred <- predict(model, x, decision.values = TRUE)\n     attr(pred, \"decision.values\")[1:4,]\n     \n     # visualize (classes by color, SV by crosses):\n     plot(cmdscale(dist(iris[,-5])),\n          col = as.integer(iris[,5]),\n          pch = c(\"o\",\"+\")[1:150 %in% model$index + 1])\n</pre>\n\n\n=== cmdscale (Classical MultiDimensional Scaling) ===\n\n* Description\n: Classical multidimensional scaling of a data matrix. (a.k.a. principal coordinates analysis (Gower, 1966)\n\n* Usage\n: cmdscale(d, k=2, eig=FALSE, add=FALSE, x.ret=FALSE)\n\n* Arguments\n: \'\'\'d [mandatory]\'\'\': a distance structure such as that returned by \'dist\' or a full symmetric matrix containing the dissimilarities\n: k [optional]: the maximum dimension of the space which the data are to be represented in; must be in {1, 2, ..., n-1}\n: eig [optional]: indicates whether eigenvalues should be returned\n: add [optional]: logical indicating if an additive constant c* should be computed, and added to the non-diagonal dissimilarities such that the modified dissimilarities are Euclidean\n: x.ret [optional]: indicates whether the doubly centered symmetric distance matrix should be returned\n\n* Details\n: Multidimensional scaling takes a set of dissimilarities and returns a set of points such that the distances between the points are approximately equal to the dissimilarities\n\n\n=== dist (Distance matrix computation) ===\n\nThis computes and returns the distance matrix computed by using the specified distance measure to compute the distances between the rows of a data matrix\n\n* Usage\n: dist(x, method = \"euclidean\", diag = FALSE, upper = FALSE, p = 2)\n\n* Arguments\n: x\n:: a numeric matrix, data frame or \'dist\' object\n: method\n:: the distance measure to be used. this must be one of \"euclidean\", \"maximum\", \"manhattan\", \"canberra\", \"binary\", or \"minkowski\" (any unambiguous substring can be given)\n: diag\n\n* Examples (by blusjune)\n\n <pre>\n> mat_a\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    1    1\n[2,]    2    2    2    2\n[3,]    3    3    3    3\n[4,]    0    0    0    0\n> dist(mat_a)\n  1 2 3\n2 2    \n3 4 2  \n4 2 4 6\n> cmdscale(dist(mat_a))\n     [,1]          [,2]\n[1,]    1  5.809542e-08\n[2,]   -1  3.057654e-09\n[3,]   -3  9.172961e-09\n[4,]    3 -9.172961e-09\n> dist(cmdscale(dist(mat_a)))\n  1 2 3\n2 2    \n3 4 2  \n4 2 4 6\n</pre>\n\n <pre>\n> mat_b\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    2    2    2\n[3,]    0    0    0\n> dist(mat_b)\n         1        2\n2 1.732051         \n3 1.732051 3.464102\n> cmdscale(dist(mat_b))\n              [,1]         [,2]\n[1,]  7.412908e-33 1.564993e-08\n[2,] -1.732051e+00 2.477578e-09\n[3,]  1.732051e+00 2.477578e-09\n> dist(cmdscale(dist(mat_b)))\n         1        2\n2 1.732051         \n3 1.732051 3.464102\n\n> ((1-2)**2 + (1-2)**2 + (1-2)**2) ** 0.5\n[1] 1.732051\n> ((2-0)**2 + (2-0)**2 + (2-0)**2) ** 0.5\n[1] 3.464102\n</pre>\n\n\n== 20130306_185022 ==\n\n=== R package (\'e1071\') installation from command line ===\n\n: Assumes that already downloaded and unpacked properly the \'e1071_1.6-1.tar.gz\' file\n\n <pre>\n\n1mjjung@secm:[e1071] $ R CMD INSTALL e1071\nJob <13412785> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura007>>\nWarning: invalid package \'e1071\'\nError: ERROR: no packages specified\n\n\n\n\na1mjjung@secm:[e1071] $ cd ..\nla1mjjung@secm:[e1071_build] $ l\ntotal 280\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:45 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\n\n\n\n\na1mjjung@secm:[e1071_build] $ R CMD INSTALL e1071\nJob <13412786> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n* installing to library \'/home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15\'\n* installing *source* package \'e1071\' ...\n** package \'e1071\' successfully unpacked and MD5 sums checked\nchecking for C++ compiler default output file name... a.out\nchecking whether the C++ compiler works... yes\nchecking whether we are cross compiling... no\nchecking for suffix of executables... \nchecking for suffix of object files... o\nchecking whether we are using the GNU C++ compiler... yes\nchecking whether g++ accepts -g... yes\n** libs\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c Rsvm.c -o Rsvm.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cmeans.c -o cmeans.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c cshell.c -o cshell.o\ngcc -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -std=gnu99 -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c floyd.c -o floyd.o\ng++ -I/apps/R_project/R-2.15.2/lib64/R/include -DNDEBUG  -I/usr/local/include    -fpic  -g -O2 -fopenmp -m64 -I/apps/intel/ComposerXE/composerxe-2011.5.220/mkl/include  -c svm.cpp -o svm.o\ng++ -shared -L/usr/local/lib64 -o e1071.so Rsvm.o cmeans.o cshell.o floyd.o svm.o\ninstalling to /home/X0101/a1mjjung/R/x86_64-unknown-linux-gnu-library/2.15/e1071/libs\n** R\n** inst\n** preparing package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n   \'svmdoc.Rnw\' \n** testing if installed package can be loaded\n\n* DONE (e1071)\n\n\n\n\na1mjjung@secm:[e1071_build] $ R\nJob <13412787> is submitted to queue <int>.\n<<Waiting for dispatch ...>>\n<<Starting on gaura010>>\n\nR version 2.15.2 (2012-10-26) -- \"Trick or Treat\"\nCopyright (C) 2012 The R Foundation for Statistical Computing\nISBN 3-900051-07-0\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType \'license()\' or \'licence()\' for distribution details.\n\nR is a collaborative project with many contributors.\nType \'contributors()\' for more information and\n\'citation()\' on how to cite R or R packages in publications.\n\nType \'demo()\' for some demos, \'help()\' for on-line help, or\n\'help.start()\' for an HTML browser interface to help.\nType \'q()\' to quit R.\n\n> ?svm\nNo documentation for \'svm\' in specified packages and libraries:\nyou could try \'??svm\'\n> library(e1071) \nLoading required package: class\n> ?svm\n> \nSave workspace image? [y/n/c]: y\n\n\n\n\na1mjjung@secm:[e1071_build] $ pwd\n/home/X0101/a1mjjung/x/R/e1071_build\n\na1mjjung@secm:[e1071_build] $ ls -alF\ntotal 284\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:47 ./\ndrwxrwxr-x 3 a1mjjung X0101   4096 Mar  6 18:44 ../\n-rw------- 1 a1mjjung X0101     25 Mar  6 18:47 .Rhistory\ndrwxr-xr-x 7 a1mjjung X0101   4096 Mar  6 18:46 e1071/\n-rw-r--r-- 1 a1mjjung X0101 258910 Mar  6 18:43 e1071_1.6-1.tar.gz\n\n</pre>\n\n\n\n== 20130304_190007 ==\n\n\n\n=== test type of some object ===\n\n <pre>\n> x <- scan(\"/tmp/scan.txt\", what=list(NULL, name=character()))\n> x <- x[sapply(x, length) > 0]\n> is.vector(x)\n\n\n> x = mat.or.vec(100,1)\n> if (is.integer(x) == TRUE) { print (\"YES\") } else { print (\"NO\") }\n[1] \"NO\"\n> if (is.vector(x) == TRUE) { print (\"YES, vector\") } else { print (\"NO, NOT vector\") }\n[1] \"YES, vector\"\n</pre>\n\n\n\n\n=== Data import (load data from a file) ===\n\n* scan()\n <pre>\n > x1 <- scan(\"/etc/hosts\", what=character())\n\n > x1     \n [1] \"127.0.0.1\"       \"localhost\"       \"#127.0.1.1\"      \"stones\"         \n [5] \"#\"               \"The\"             \"following\"       \"lines\"          \n [9] \"are\"             \"desirable\"       \"for\"             \"IPv6\"           \n[13] \"capable\"         \"hosts\"           \"::1\"             \"ip6-localhost\"  \n[17] \"ip6-loopback\"    \"fe00::0\"         \"ip6-localnet\"    \"ff00::0\"        \n[21] \"ip6-mcastprefix\" \"ff02::1\"         \"ip6-allnodes\"    \"ff02::2\"        \n[25] \"ip6-allrouters\"  \"10.0.2.15\"       \"stones-eth0\"     \"#192.168.1.109\" \n[29] \"stones\"          \"hd-master-01\"    \"#192.168.1.110\"  \"pavement\"       \n[33] \"hd-slave-0001\"   \"192.168.1.112\"   \"stones\"          \"hd-master-01\"   \n[37] \"hd-slave-0001\"   \"kandinsky\"       \"192.168.1.110\"   \"pavement\"       \n[41] \"hd-slave-0002\"  \n</pre>\n\n* read.table()\n <pre>\n> iot_r <- read.table(\'tracelog.msn_filesrvr.R\')  \n</pre>\n\n\n\n=== function defintion, for loop in R ===\n\n <pre>\n> avg_smoothing <- function(src, srcl, sf) {\n    tgtl = srcl + 1 - sf\n    tgt <- mat.or.vec(tgtl, 1)\n    for (i in 1:tgtl) {\n        tgt[i] = mean(src[i:(i+sf-1)])\n    }\n    return (tgt)\n}\n\n> vec1 <- rnorm(100, mean=10, sd=1)\n> vec1_sf2 <- avg_smoothing(vec1, 100, 2)\n> vec1_sf4 <- avg_smoothing(vec1, 100, 4)\n> vec1_sf8 <- avg_smoothing(vec1, 100, 8)\n\n> plot(vec1, col=\"gray\", type=\"l\")\n> points(vec1_sf2, col=\"red\", type=\"l\")\n> points(vec1_sf4, col=\"blue\", type=\"l\")\n> points(vec1_sf8, col=\"green\", type=\"l\")\n</pre>\n\n\n\n\n== 20130127_225539 ==\n\n=== R Installation ===\n\n* to install R statistical computing software\n** me@matrix$ sudo apt-get install r-base\n\n* to start R from command line, just type \'R\' in your terminal\n** me@matrix$ R\n\n* RStudio download [http://www.rstudio.com/ide/download/]\n** RStudio Desktop [http://www.rstudio.com/ide/download/desktop]\n**: If you run R on your desktop\n** RStudio Server [http://www.rstudio.com/ide/download/server]\n**: If you run R on a Linux server and want to enable users to remotely access RStudio using a web browser\n*** RStudio Server Getting Started [http://www.rstudio.com/ide/docs/server/getting_started]\n\n <pre>\nsudo apt-get install r-base\nwget http://download2.rstudio.org/rstudio-server-0.97.312-amd64.deb\nsudo gdebi rstudio-server-0.97.312-amd64.deb \nbsc.adm.create_me\ngoogle-chrome http://kandinsky:8787 # type ID/PW (me?!)\n</pre>\n\n\n* Debian Packages of R Software [http://cran.r-project.org/bin/linux/debian/README.html]\n\n=== R Guide/Tutorial/Example ===\n\n* R Tutorial [http://www.r-tutor.com/]\n** R Tutorial:: Data Import [http://www.r-tutor.com/r-introduction/data-frame/data-import]\n\n\n* Example R scripts [http://people.eng.unimelb.edu.au/aturpin/R/index.html]]\n\n\n* R by example [http://www.mayin.org/ajayshah/KB/R/index.html]\n\n\n* R example:: Kmeans [http://stat.ethz.ch/R-manual/R-patched/library/stats/html/kmeans.html]\n\n\n* R package install howto\n; e1071\n: Misc Functions of the Department of Statistics (e1071), TU Wien\n:* package-installation and loading\n install.packages(\"e1071\") # installing the package \'e1071\'\n library(\"e1071\") # loading the installed package \'e1071\'\n\n\n* Importing SAS, SPSS and Stata files into R [http://staff.washington.edu/glynn/r_import.pdf]\n\n\n=== R Troubleshooting ===\n\n* Problems importing csv file/converting from integer to double in R [http://stackoverflow.com/questions/8381839/problems-importing-csv-file-converting-from-integer-to-double-in-r]\n\n\n=== Misc. ===\n\n* WEKA Tutorial [http://www.cs.utexas.edu/users/ml/tutorials/Weka-tut/]\n* weka is a metric prefix for 10^30','utf-8'),(72,'== Introduction to Association Rule Mining ==\n\n\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(73,'== Introduction to Association Rule Mining ==\n\n\n\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(74,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(75,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n\n\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(76,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n\n\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ((B.GOOD)) ===\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n----','utf-8'),(77,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010\n\n\n\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series','utf-8'),(78,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n== R package arules :: Eclat Algorithm ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n\n\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(79,'== Introduction to Association Rule Mining ==\n\n\n <pre>\n{|class=\"wikitable\" style=\"float: right; margin-left: 1em;\"\n|+ Example database with 4 items and 5 transactions\n|-\n! transaction ID !! milk !! bread !! butter !! beer\n|-\n| 1 || 1 || 1 || 0 || 0 \n|-\n| 2 || 0 || 0 || 1 || 0 \n|-\n| 3 || 0 || 0 || 0 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 \n|-\n| 5 || 0 || 1 || 0 || 0 \n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name=\"mining\" /> the problem of association rule mining is defined as: Let <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'. Let <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'. Each transaction in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>. A \'\'rule\'\' is defined as an implication of the form <math>X \\Rightarrow Y</math> where <math>X, Y \\subseteq I</math> and <math>X \\cap Y = \\emptyset</math>. The sets of items (for short \'\'itemsets\'\') <math>X</math> and <math>Y</math> are called \'\'antecedent\'\' (left-hand-side or LHS) and \'\'consequent\'\' (right-hand-side or RHS) of the rule respectively.\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer}\\}</math> and a small database containing the items (1 codes presence and 0 absence of an item in a transaction) is shown in the table to the right. An example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.\n\n</pre>\n\n: FIM/ARM (Frequent Itemset Mining / Association Rule Mining)\n\n\n== FIM/ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM/ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(80,': FIM/ARM (Frequent Itemset Mining / Association Rule Mining)\n\n== FIM/ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM/ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(81,'FIM/ARM (Frequent Itemset Mining / Association Rule Mining)\n\n== FIM/ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM/ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(82,'FIM/ARM (Frequent Itemset Mining / Association Rule Mining)\n\n\n\n== FIM/ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM/ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(83,'Frequent Itemset Mining (FIM) / Association Rule Mining (ARM)\n\n\n\n\n== FIM/ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM/ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(84,'#REDIRECT [[Frequent Itemset Mining (FIM) / Association Rule Mining (ARM)]]','utf-8'),(85,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(86,'#REDIRECT [[PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]','utf-8'),(87,'#REDIRECT [[Bnote Frequent Itemset Mining (FIM) / Association Rule Mining (ARM)]]','utf-8'),(88,'== Lessons learned ==\n\n* \"An Analysis of Traces from a Production MapReduce Cluster,\" CCGRID 2010\n\n* CCGRID 2010에 소개되었던 CMU의 연구인 [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5493490 An Analysis of Traces from a Production MapReduce Cluster // Kavulya, S. CMU // CCGRID 2010]에는 Yahoo!의 M45 Supercomputing Cluster에서의 10달 간의 MapReduce tracelog에 대한 분석 결과가 있다. 이를 보면, task type과 failure 와의 연관성이 있음을 알 수 있다.\n\n* Failure와 straggler 간의 관계는 다음과 같다. straggler는 아직 명확한 exception이 일어나서 task가 명시적인 failure로 mark가 된 상태는 아니지만, 통상적인 경우보다 꽤 느리게 task가 수행되고 있는 상태를 의미한다.\n\n\n== Excerpts from paper ==\n\n=== Data Summary ===\n\n{| class=\"wikitable\" border=\"1\" \n|+ Summary M45 dataset\n! Key\n! Value\n|-\n| Log Period\n|\n: Apr 25 - Nov 12, 2008\n: Jan 19 - Apr 24, 2009\n|-\n| Hadoop versions\n|\n: 0.16: Apr 2008 -\n: 0.17: Jun 2008 -\n: 0.18: Jan 2009 -\n|-\n| Number of active users\n| 31\n|-\n| Number of jobs\n| 171079\n|-\n| Successful jobs\n| 165948 (97%)\n|-\n| Failed jobs\n| 4100 (2.4%)\n|-\n| Cancelled jobs\n| 1031 (0.6%)\n|-\n| Average maps per job\n| 154 +/- 558 sigma\n|-\n| Average reduces per job\n| 19 +/- 145 sigma\n|-\n| Average nodes per job\n| 27 +/- 22 sigma\n|-\n| Maximum nodes per job\n| 299\n|-\n| Average job duration\n| 1214 +/- 13875 seconds\n|-\n| Maximum job duration\n| 6.84 days\n|-\n| Node days used\n| 132624\n|}\n\n\n=== Memo ===\n\n* There were 4100 failed jobs and 1031 incomplete jobs in our dataset. These failures accounted for 3% of the total jobs run.\n\n* We observed that:\n# <span style=\"color:red\">\'\'\'Most jobs fail within 150 seconds after the first aborted task\'\'\'</span>. Figure 3(c) shows that 90% of jobs exhibited an error latency of less than 150 seconds from the first aborted task to the last retry of that task (the default number of retries for aborted tasks was 4). We observed a maximum error latency of 4.3 days due to a copy failure in a single reduce task. Better diagnosis and recovery approaches are needed to reduce error latency in long-running tasks.\n# Most failures occurred during the map phase. Failures due to task exceptions in the <span style=\"color:red\">\'\'\'map phase\'\'\'</span> were the most prevalent - 36% of these failures were due to <span style=\"color:red\">\'\'\'array indexing errors\'\'\'</span> (see Figure 4). <span style=\"color:red\">\'\'\'IO exceptions\'\'\'</span> were common in the <span style=\"color:red\">\'\'\'reduce phase\'\'\'</span> accounting for 23% of failures. Configuration problems, such as missing files, led to failures during job initialization.\n# Application hangs and node failures were more prevalent in cancelled jobs. Task timeouts, which occur when a task hangs and fails to report progress for more than 10 minutes, and lost TaskTracker daemons due to node or process failures were more prevalent in cancelled jobs than in failed jobs.\n# Spurious exceptions exist in the logs. The logs contained spurious exceptions due to debugging statements that were turned on by default in certain versions of Hadoop. For example, a debug exception to troubleshoot a problem in the DFS client attributed to data buffering, and an exception due to a disabled feature that ignored the nonzero exit codes in Hadoop streaming, accounted for 90% of exceptions from January 21 to February 18, 2009. This motivates the need for error-log analysis tools that highlight important exceptions to users','utf-8'),(89,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(90,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n=== Pretty Good As An Introduction to Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(91,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(92,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(93,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(94,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n\n* \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) [http://www.jstatsoft.org/v14/i15/paper]\n:- UML class diagram of the \'\'\'arules\'\'\' package\n\n\n* Examples and resources on association rule mining with R (R-bloggers.com)\n\n: Below are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n: 1. My R example and document on association rule mining, redundancy removal and rule interpretation [http://www.rdatamining.com/examples/association-rules]\n::- Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies” [http://www.rdatamining.com/docs/RDataMining.pdf]\n\n: 2. Vignettes for mining and visualizing association rules\n::- Introduction to arules: A computational environment for mining association rules and frequent item sets [http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf]\n::- Visualizing Association Rules: Introduction to arulesViz [http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf]\n\n: 3. Some R implementations of association rule algorithms:\n::- “Data Mining Algorithms In R: Frequent Pattern Mining” [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining]\n\n: 4. Lecture Notes (slides in PDF) on the theory of association rules [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf][http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf]\n\n: 5. A book chapter on Association Rules [http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf]\n\n: 6. A comparison of over 20 interestingness measures for association rules:\n::- “Selecting the right objective measure for association analysis” [http://www.cse.msu.edu/~ptan/papers/IS.pdf]\n\n: 7. A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD)) [http://www.rdatamining.com/resources/onlinedocs]\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(95,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(96,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Examples and Resources on Association Rule Mining with R (R-bloggers.com)\n===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n* http://www.rdatamining.com/resources/onlinedocs\n: A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD))\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(97,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Examples and Resources on Association Rule Mining with R (R-bloggers.com)\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== Association Rule Mining with R ===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n* http://www.rdatamining.com/resources/onlinedocs\n: A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD))\n\n\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(98,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Examples and Resources on Association Rule Mining with R (R-bloggers.com)\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== Examples of Association Rule Mining with R ===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(99,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n: A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n\n=== Examples and Resources on Association Rule Mining with R (R-bloggers.com)\n\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== Examples of Association Rule Mining with R ===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(100,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n: A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== Examples of Association Rule Mining with R ===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique.\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(101,'\n== FIM / ARM - Apriori Algorithm (arules in R) ==\n\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ ls -alF .bdx.0* ex-010.R \n-rw-rw-r-- 1 blusjune blusjune 182 May 30 20:51 .bdx.0090.y.create_teste_datafile.sh\n-rw-rw-r-- 1 blusjune blusjune  11 May 30 20:51 .bdx.0100.y.exec_r_stat.sh\n-rwxr-xr-x 1 blusjune blusjune 821 May 30 20:52 ex-010.R*\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0090.y.create_teste_datafile.sh \n#!/bin/sh\n\n_datafile=\"teste\";\n\n_creat_datafile()\n{\n	echo \"create data file\";\n	cat > teste << EOF\nA,B,C\nB,C\nA,B,D\nA,B,C,D\nA\nB\nEOF\n}\n\nif [ ! -r $_datafile ]; then\n	_creat_datafile;\nfi\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat .bdx.0100.y.exec_r_stat.sh \n./ex-010.R\n</pre>\n\n <pre>\nblusjune@jimi-hendrix:[apriori] $ cat ex-010.R \n#!/home/blusjune/.b/x/sys/bin/x.0007/R -f\n## Apriori algorithm (for Association Rule Mining)\n##_ver=20130530_151301\n\n\nlibrary(arules);\nlibrary(arulesViz);\n\n\ntr <- read.transactions(\"teste\", format=\"basket\", sep=\",\");\ninspect(tr);\nimage(tr);\nitemFrequencyPlot(tr, support=0.1);\nlength(tr);\n\nrules <- apriori(tr, parameter=list(supp=0.5, conf=0.5));\ninspect(rules);\nsummary(rules);\ninterestMeasure(rules,\n				c(\"support\",\n				  \"chiSquare\",\n				  \"confidence\",\n				  \"conviction\",\n				  \"cosine\",\n				  \"coverage\",\n				  \"leverage\",\n				  \"lift\",\n				  \"oddsRatio\"\n				  ),\n				tr);\n\n# to calculate a single measure and add it to the quality slot:\nquality(rules) <- cbind(quality(rules), hyperConfidence = interestMeasure(rules, method=\"hyperConfidence\", Income));\ninspect(head(SORT(rules, by=\"hyperConfidence\")));\n</pre>\n\n\n\n\n== FIM / ARM - Eclat Algorithm (arules in R) ==\n\n* Frequent pattern mining examples using \'arules\' R package [http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm] [http://en.wikibooks.org/wiki/File:Ex-visualization1.jpg]\n\n <pre>\n## Create transaction data set.\ndata <- list(\n  c(\"a\",\"b\",\"c\"),\n  c(\"a\",\"b\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"b\",\"e\"),\n  c(\"b\",\"c\",\"e\"),\n  c(\"a\",\"d\",\"e\"),\n  c(\"a\",\"c\"),\n  c(\"a\",\"b\",\"d\"),\n  c(\"c\",\"e\"),\n  c(\"a\",\"b\",\"d\",\"e\")\n)\n \nt <- as(data, \"transactions\")\n \n## Mine itemsets with tidLists.\nf <- eclat(data, parameter = list(support = 0, tidLists = TRUE))\n \n## Get dimensions of the tidLists.\ndim(tidLists(f))\n \n## Coerce tidLists to list.\nas(tidLists(f), \"list\")\n \n## Inspect visually.\nimage(tidLists(f))\n \n##Show the Frequent itemsets and respectives supports\ninspect(f)\n</pre>\n\n== References ==\n\n=== Pretty Good As An Introduction to the Data Mining / FIM / ARM / R ===\n\n* http://www.rdatamining.com/resources/onlinedocs\n: R and Data Mining ((B.GOOD))\n: A collection of links to online resources on data mining (and R), not limitted to association rules ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds.html\n: Mining of Massive Datasets (Book) // Anand Rajaraman and Jeff Ullman, Stanford // ((B.GOOD))\n\n* http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n: Mining of Massive Datasets (Book) - Chapter 6 Frequent Itemsets\n\n=== On Wikibooks.org ===\n: ((B.GOOD))\n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Eclat Algorithm ((B.GOOD))\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Apriori_Algorithm\n: Data Mining Algorithms In R/Frequent Pattern Mining/The Apriori Algorithm ((B.GOOD))\n\n\n=== Good Slides for DM/FIM/ARM ===\n\n* http://www.ismll.uni-hildesheim.de/lehre/bi-09s/script/lecture13.pdf\n: Market Basket Analysis\n\n* http://paginas.fe.up.pt/~ec/files_1112/week_04_Association.pdf\n: Market Basket Analysisand Mining Association Rules\n\n* http://classes.engr.oregonstate.edu/eecs/fall2012/cs434/notes/associaterules.pdf\n: Frequent Pattern Mining: Association Rules (CS434)\n\n=== On CRAN (The Comprehensive R Archive Network) ===\n\n* http://cran.r-project.org/web/packages/arules/index.html\n: arules: Mining Association Rules and Frequent Itemsets\n\n* http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules - A computational environment for mining association rules and frequent item sets // Michael Hahsler, Bettina Grun, Kurt Hornik, Christian Buchta\n\n=== Christian Borgelt\'s Web (Data Analysis) ===\n\n: ((B.GOOD))\n\n* http://www.borgelt.net//index.html\n: Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//apriori.html\n: Apriori - Association Rule Inducation / Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//doc/apriori/apriori.html\n: Full description of the apriori program // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//eclat.html\n: Eclat - Frequent Item Set Mining // Christian Borgelt\'s Web Pages\n\n* http://www.borgelt.net//papers/cstat_02.pdf\n: Induction of Association Rules: Apriori Implementation // Christian Borgelt and Rudolf Kruse\n\n* http://www.borgelt.net//papers/fimi_03.pdf\n: Efficient Implementations of Apriori and Eclat\n\n\n=== Examples of Association Rule Mining with R ===\n\nBelow are some free online resources on association rule mining with R and also documents on the basic theory behind the technique. (R-bloggers.com)\n\n* http://www.jstatsoft.org/v14/i15/paper\n: \"arules - A Computational Environment for Mining Association Rules and Frequent Item Sets\", Michael Hahsler, Bettina Grun, Kurt Hornik ((B.GOOD)) : UML class diagram of the \'\'\'arules\'\'\' package\n\n* http://www.rdatamining.com/examples/association-rules\n: My R example and document on association rule mining, redundancy removal and rule interpretation\n\n* http://www.rdatamining.com/docs/RDataMining.pdf\n: Chapter 9 Association Rules in book “R and Data Mining: Examples and Studies”\n\n* http://cran.csiro.au/web/packages/arules/vignettes/arules.pdf\n: Introduction to arules: A computational environment for mining association rules and frequent item sets\n\n* http://cran.csiro.au/web/packages/arulesViz/vignettes/arulesViz.pdf\n: Visualizing Association Rules: Introduction to arulesViz \n\n* http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining\n: Some R implementations of association rule algorithms: “Data Mining Algorithms In R: Frequent Pattern Mining”\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap6_basic_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch6)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap7_extended_association_analysis.pdf\n: Lecture Notes (slides in PDF) on the theory of association rules (Ch7)\n\n* http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\n: A book chapter on Association Rules\n\n* http://www.cse.msu.edu/~ptan/papers/IS.pdf\n: A comparison of over 20 interestingness measures for association rules - “Selecting the right objective measure for association analysis”\n\n=== On Wikipedia ===\n\n* http://en.wikipedia.org/wiki/Association_rule_learning\n: Association rule learning - Wikipedia\n\n* http://en.wikipedia.org/wiki/Sequence_mining\n: Sequence mining\n\n* http://en.wikipedia.org/wiki/GSP_Algorithm\n: GSP Algorithm\n\n* http://en.wikipedia.org/wiki/Apriori_algorithm\n: Apriori algorithm\n\n* http://en.wikipedia.org/wiki/Market_basket_analysis\n: Market basket analysis\n\n* http://en.wikipedia.org/wiki/Time_series\n: Time series\n\n\n=== Misc. // Other Sites ===\n\n* http://jmlr.csail.mit.edu/papers/volume12/hahsler11a/hahsler11a.pdf\n: The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets -- Journal of Machine Learning Research 12 (2011) 2021-2025 ((B.GOOD))\n\n* http://lyle.smu.edu/IDA/arules/\n: Intelligent Data Analysis Lab @ SMU\n\n* http://www.r-bloggers.com/examples-and-resources-on-association-rule-mining-with-r/\n: Examples and resources on association rule mining with R -- R-bloggers -- R news and tutorials contributed by (425) R bloggers\n\n* http://www.analysis-of-patterns.net/files/bgoethals.pdf\n: Frequent Pattern Mining Slides - Universiteit Antwerpen\n\n* http://www.uni-obuda.hu/journal/Ivancsy_Vajk_5.pdf\n: Frequent Pattern Mining in Web Log Data\n\n* http://www.timeseriesknowledgemining.org/wiki/index.php?title=Temporal_Pattern_Mining_Tutorial_KDD_2010\n: Temporal Pattern Mining Tutorial KDD 2010','utf-8'),(102,'\n== ## bNote-2013-05-31 ==\n\n=== Samsung PC Desktop Specification ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information\n\n\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(103,'\n== ## bNote-2013-05-31 ==\n\n=== Samsung PC Desktop Specification ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(104,'\n== ## bNote-2013-05-31 ==\n\n=== Samsung PC Desktop Specification ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and 16 GB maximum supported memory size for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and 8 GB maximum supported memory size for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(105,'\n== ## bNote-2013-05-31 ==\n\n=== Samsung PC Desktop Specification ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(106,'\n== ## bNote-2013-05-31 ==\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(107,'\n== ## bNote-2013-05-31 ==\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* [http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid= DB-P100 Information (Intel Q43 Chipset)]\n: DB-P100 Information (Intel Q43 Chipset)\n\n* [http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm Intel Q43 Chipset]\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(108,'\n== ## bNote-2013-05-31 ==\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(109,'#REDIRECT [[((news.article)) 네트워크 업계 \"기다렸다, 런던올림픽\"]]','utf-8'),(110,'#REDIRECT [[(news.article) 네트워크 업계 \"기다렸다, 런던올림픽\"]]','utf-8'),(111,'#REDIRECT [[News 네트워크 업계 \"기다렸다, 런던올림픽\"]]','utf-8'),(112,'#REDIRECT [[(News) 네트워크 업계 \"기다렸다, 런던올림픽\"]]','utf-8'),(113,'#REDIRECT [[(News) 아카마이, \"CDN 넘어 하이퍼커넥티드로\"]]','utf-8'),(114,'#REDIRECT [[(News) 아카마이, \"스트리밍/보안 서비스를 빠르고 안전하게\"]]','utf-8'),(115,'#REDIRECT [[(News) 아카마이, \"쌩쌩 웹사이트 만들려면\"]]','utf-8'),(116,'#REDIRECT [[(News) 아카마이, \'블레이즈\' 인수 - 인텔리전트 플랫폼 강화]]','utf-8'),(117,'#REDIRECT [[(News) 아카마이, CDN 장악 가속화 ... 코텐도 인수설]]','utf-8'),(118,'\n== ## bNote-2013-05-31 ==\n\n=== blktrace 에서 GBL phase flag 의미 ===\n\n;Q\n: block layer의 request queue routine에 의해 I/O request가 handling 되기 시작\n\n;G\n: block layer 내에서 I/O request 핸들링 위한 resource 생성\n\n;I\n: I/O request가 queue에 삽입됨\n\n;D\n: I/O request가 device로 issue됨\n\n;C\n: I/O completed\n\n;R\n: Re-queue (device 단에서 처리 불가한 상황이거나 해서 다시 I/O queuing)\n\n;S\n: block layer 내에 I/O request 생성에 필요한 resource가 부족하여 잠시 기다려야 하는 상황\n\n;M, F\n: merging the I/O (그냥 M은 backward merge, F는 지금 도착한 I/O의 address가 이전 I/O의 address가 시작하는 곳 앞에 위치하는 경우 의미 - 둘 간의 차이에 그다지 신경쓸 필요 없음)\n\n: blktrace 2.0.0 기준\n\n\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(119,'\n== ## bNote-2013-05-31 ==\n\n=== blktrace 에서 GBL phase flag 의미 ===\n\n;Q\n: block layer의 request queue routine에 의해 I/O request가 handling 되기 시작\n\n;G\n: block layer 내에서 I/O request 핸들링 위한 resource 생성\n\n;I\n: I/O request가 queue에 삽입됨\n\n;D\n: I/O request가 device로 issue됨\n\n;C\n: I/O completed\n\n;R\n: Re-queue (device 단에서 처리 불가한 상황이거나 해서 다시 I/O queuing)\n\n;S\n: block layer 내에 I/O request 생성에 필요한 resource가 부족하여 잠시 기다려야 하는 상황\n\n;M, F\n: merging the I/O (그냥 M은 backward merge, F는 지금 도착한 I/O의 address가 이전 I/O의 address가 시작하는 곳 앞에 위치하는 경우 의미 - 둘 간의 차이에 그다지 신경쓸 필요 없음)\n\n(blktrace 2.0.0 기준)\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(120,'\n== ## bNote-2013-05-31 ==\n\n=== 수퍼컴 업무 협조 공문 ===\n\n <pre>\n\nI/O Workload 분석 & 시뮬레이션 환경 병렬화\n\n(과제명: Intelligent Data Management - PL: 이주평 전문, NPR 과제)\n\n---------------------------------------------------------\n\n \n\nIntelligent Data Management 과제 (구 Data-intensive Storage)에서는\n\nI/O 예측에 기반한 Proactive Data Placement 기술 연구를 위해\n\nI/O Workload 분석을 수행하고 있습니다.\n\n \n\n수십 GB에서 수 TB에 이르는 데이터에 대해\n\nData Mining / Machine Learning 기반 분석을 진행해야 하기 때문에\n\n고성능의 수퍼컴 활용 및 효과적인 병렬화가 필수적입니다.\n\n \n\n이에, 수퍼컴센터와의 협력을 통해\n\n분석/시뮬레이션 작업의 TAT 단축 및\n\n분석 체계 혁신을 이루고자 합니다.\n\n \n\n협업 대상 업무 및 진행 계획은 다음과 같습니다.\n\n - 슈퍼컴 시스템 로그 수집 건 (6월: 로그 수집 모듈 작성, 7월: 수집 작업)\n\n - 병렬화/RSP 적용 건 (6월 ~ 7월: 전처리 및 분석 모듈 병렬화 구축)\n\n\n\n상기 내용으로 수퍼컴 센터와 협업을 하고자 하오니\n\n검토 후 결재 부탁 드립니다.\n\n\n\n\n\n※ 추진 일정 등 자세한 내용은 첨부 파일을 참고하시기 바랍니다.\n\n첨부 1: 수퍼컴 센터 업무 지원 요청\n\n첨부 2: 사전 회의록 (5/20, 김혁호 책임, 정명준 전문)\n\n \n\n----- 이상 -----\n\n \n\n</pre>\n\n\n=== blktrace 에서 GBL phase flag 의미 ===\n\n;Q\n: block layer의 request queue routine에 의해 I/O request가 handling 되기 시작\n\n;G\n: block layer 내에서 I/O request 핸들링 위한 resource 생성\n\n;I\n: I/O request가 queue에 삽입됨\n\n;D\n: I/O request가 device로 issue됨\n\n;C\n: I/O completed\n\n;R\n: Re-queue (device 단에서 처리 불가한 상황이거나 해서 다시 I/O queuing)\n\n;S\n: block layer 내에 I/O request 생성에 필요한 resource가 부족하여 잠시 기다려야 하는 상황\n\n;M, F\n: merging the I/O (그냥 M은 backward merge, F는 지금 도착한 I/O의 address가 이전 I/O의 address가 시작하는 곳 앞에 위치하는 경우 의미 - 둘 간의 차이에 그다지 신경쓸 필요 없음)\n\n(blktrace 2.0.0 기준)\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(121,'\n== ## bNote-2013-05-31 ==\n\n=== 수퍼컴 업무 협조 공문 ===\n\n <pre>\n\nI/O Workload 분석 & 시뮬레이션 환경 병렬화\n\n(과제명: Intelligent Data Management / PL: 이주평 전문 / NPR 과제)\n\n---------------------------------------------------------\n\n \n\nIntelligent Data Management 과제 (구 Data-intensive Storage)에서는\n\nI/O 예측에 기반한 Proactive Data Placement 기술 연구를 위해\n\nI/O Workload 분석을 수행하고 있습니다.\n\n \n\n수십 GB에서 수 TB에 이르는 데이터에 대해\n\nData Mining / Machine Learning 기반 분석을 진행해야 하기 때문에\n\n고성능의 수퍼컴 활용 및 효과적인 병렬화가 필수적입니다.\n\n \n\n이에, 수퍼컴센터와의 협력을 통해\n\n분석/시뮬레이션 작업의 TAT 단축 및\n\n분석 체계 혁신을 이루고자 합니다.\n\n \n\n협업 대상 업무 및 진행 계획은 다음과 같습니다.\n\n - 슈퍼컴 시스템 로그 수집 건 (6월: 로그 수집 모듈 작성, 7월: 수집 작업)\n\n - 병렬화/RSP 적용 건 (6월 ~ 7월: 전처리 및 분석 모듈 병렬화 구축)\n\n\n\n상기 내용으로 수퍼컴 센터와 협업을 하고자 하오니\n\n검토 후 결재 부탁 드립니다.\n\n\n\n\n\n※ 추진 일정 등 자세한 내용은 첨부 파일을 참고하시기 바랍니다.\n\n첨부 1: 수퍼컴 센터 업무 지원 요청\n\n첨부 2: 사전 회의록 (5/20, 김혁호 책임, 정명준 전문)\n\n \n\n----- 이상 -----\n\n \n\n</pre>\n\n=== blktrace 에서 GBL phase flag 의미 ===\n\n;Q\n: block layer의 request queue routine에 의해 I/O request가 handling 되기 시작\n\n;G\n: block layer 내에서 I/O request 핸들링 위한 resource 생성\n\n;I\n: I/O request가 queue에 삽입됨\n\n;D\n: I/O request가 device로 issue됨\n\n;C\n: I/O completed\n\n;R\n: Re-queue (device 단에서 처리 불가한 상황이거나 해서 다시 I/O queuing)\n\n;S\n: block layer 내에 I/O request 생성에 필요한 resource가 부족하여 잠시 기다려야 하는 상황\n\n;M, F\n: merging the I/O (그냥 M은 backward merge, F는 지금 도착한 I/O의 address가 이전 I/O의 address가 시작하는 곳 앞에 위치하는 경우 의미 - 둘 간의 차이에 그다지 신경쓸 필요 없음)\n\n(blktrace 2.0.0 기준)\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(122,'\n== ## bNote-2013-05-31 ==\n\n=== 수퍼컴 업무 협조 공문 ===\n\n <pre>\n\nI/O Workload 분석 & 시뮬레이션 환경 병렬화\n\n(과제명: Intelligent Data Management / PL: 이주평 전문 / NPR 과제)\n\n---------------------------------------------------------\n\n \n\nIntelligent Data Management 과제 (구 Data-intensive Storage)에서는\n\nI/O 예측에 기반한 Proactive Data Placement 기술 연구를 위해\n\nI/O Workload 분석을 수행하고 있습니다.\n\n \n\n수십 GB에서 수 TB에 이르는 데이터에 대해\n\nData Mining / Machine Learning 기반 분석을 진행해야 하기 때문에\n\n고성능의 수퍼컴 활용 및 효과적인 병렬화가 필수적입니다.\n\n \n\n이에, 수퍼컴센터와의 협력을 통해\n\n분석/시뮬레이션 작업의 TAT 단축 및\n\n분석 체계 혁신을 이루고자 합니다.\n\n \n\n협업 대상 업무 및 진행 계획은 다음과 같습니다.\n\n - 슈퍼컴 시스템 로그 수집 건 (6월: 로그 수집 모듈 작성, 7월: 수집 작업)\n\n - 병렬화/RSP 적용 건 (6월 ~ 7월: 전처리 및 분석 모듈 병렬화 구축)\n\n\n\n상기 내용으로 수퍼컴 센터와 협업을 하고자 하오니\n\n검토 후 결재 부탁 드립니다.\n\n\n\n\n\n※ 추진 일정 등 자세한 내용은 첨부 파일을 참고하시기 바랍니다.\n\n첨부 1: 수퍼컴 센터 업무 지원 요청\n\n첨부 2: 사전 회의록 (5/20, 김혁호 책임, 정명준 전문)\n\n \n\n----- 이상 -----\n\n \n\n</pre>\n\n=== blktrace 에서 GBL phase flag 의미 ===\n\n;Q\n: block layer의 request queue routine에 의해 I/O request가 handling 되기 시작\n\n;G\n: block layer 내에서 I/O request 핸들링 위한 resource 생성\n\n;I\n: I/O request가 queue에 삽입됨\n\n;D\n: I/O request가 device로 issue됨\n\n;C\n: I/O completed\n\n;R\n: Re-queue (device 단에서 처리 불가한 상황이거나 해서 다시 I/O queuing)\n\n;S\n: block layer 내에 I/O request 생성에 필요한 resource가 부족하여 잠시 기다려야 하는 상황\n\n;M, F\n: merging the I/O (그냥 M은 backward merge, F는 지금 도착한 I/O의 address가 이전 I/O의 address가 시작하는 곳 앞에 위치하는 경우 의미 - 둘 간의 차이에 그다지 신경쓸 필요 없음)\n\n;P\n: Device로 내려가는 I/O를 마개로 막아놓고 (Plug) Merging을 하겠다는 의미\n\n;U\n: Unplug를 함으로써 Merge된 I/O들이 밑으로 (Device쪽으로) 내려갈 수 있게 됨\n\n\n(blktrace 2.0.0 기준)\n\n=== Samsung PC Desktop Information (DB-P100) ===\n\n* http://www.micnc.co.kr/board/kboard.php?board=product1_2&act=view&no=11&page=4&search_mode=&search_word=&cid=\n: DB-P100 Information (Intel Q43 Chipset)\n\n* http://www.intel.com/cd/products/services/emea/eng/chipsets/403198.htm\n: Intel Q43 Chipset\n:- Dual-Channel DDR2 memory support\n:: Delivers up to 12.8 GB/s (DDR2 800 dual 6.4 GB/s) of bandwidth and \'\'\'16 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n:- Dual-Channel DDR3 memory support\n:: Delivers up to 17 GB/s (DDR3 1066 dual 8.5 GB/s) of bandwidth and \'\'\'8 GB maximum supported memory size\'\'\' for faster system responsiveness and support of 64-bit computing.\n\n== ## bNote-2013-05-23 ==\n\n=== Coaccess-based I/O Prediction ===\n\n <pre>\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_51276758\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\"),\nc(\"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_4489275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_51276758\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_1187\", \"_addr_4489275\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_4489275\", \"_addr_1187\", \"_addr_23813\", \"_addr_319275\"),\nc(\"_addr_319275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_23813\", \"_addr_4489275\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_1187\", \"_addr_319275\", \"_addr_51276758\"),\nc(\"_addr_4489275\", \"_addr_23813\", \"_addr_51276758\"),\nc(\"_addr_319275\", \"_addr_51276758\")\n</pre>\n\n <pre>\n   items            support\n---------------------------\n1  {_addr_23813,           \n    _addr_4489275}     0.35\n2  {_addr_1187,            \n    _addr_4489275}     0.30\n3  {_addr_1187,            \n    _addr_23813}       0.35\n4  {_addr_1187,            \n    _addr_319275}      0.25\n5  {_addr_23813,           \n    _addr_51276758}    0.35\n6  {_addr_319275,          \n    _addr_51276758}    0.40\n7  {_addr_23813,           \n    _addr_319275}      0.40\n8  {_addr_23813}       0.70\n9  {_addr_319275}      0.65\n10 {_addr_51276758}    0.60\n11 {_addr_1187}        0.50\n12 {_addr_4489275}     0.45\n\n</pre>\n\n=== block layer plug/unplug === \n\n* [[Linux kernel - block layer plug/unplug]] ((B.GOOD)) [http://nimhaplz.egloos.com/5598614]\n\n== ## bNote-2013-05-20 ==\n\n=== Real Traces Downloaded from SNIA ===\n\nREADME\n <pre>\ncat README\n## _timestamp=20130520_092713\n##\n## List of downloaded real trace files\n## Downloaded from SNIA (by Brian M. JUNG)\n\n\n\n\n## Real Tracelog - 1-week block I/O traces of enterprise servers at MSR Cambridge\n./MSR_Cambridge\n        ./MSR_Cambridge/msr-cambridge1.tar\n        ./MSR_Cambridge/msr-cambridge2.tar\n\n\n## Real Tracelog - Enterprise Workload collected at Microsoft:\n./MS_Enterprise\n        ## Real Tracelog - Enterprise Workload (Exchange Mail Server) at Microsoft:\n        ./MS_Enterprise/Exchange_Server\n                ./MS_Enterprise/Exchange_Server/Exchange-Server-Traces.tar\n\n        ## Real Tracelog - TPC-E Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCE\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-10-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-56-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.05-46-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-53-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-42-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCE/W2K8.TPCE.10-18-2007.06-28-PM.trace.csv.bz2\n\n        ## Real Tracelog - TPC-C Enterprise Benchmark Workload at Microsoft:\n        ./MS_Enterprise/TPCC\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-02-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-44-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-43-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-52-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-03-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-37-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.04-01-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-48-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-49-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.09-56-AM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.03-57-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.12-51-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.10-19-2007.01-32-PM.trace.csv.bz2\n                ./MS_Enterprise/TPCC/W2K8.TPCC.02-26-2008.10-09-AM.trace.csv.bz2\n\n\n## Real Tracelog - Microsoft Production Workload:\n##  Collected from several real production servers in Microsoft\n./MS_Production\n        ./MS_Production/BuildServer00.zip\n        ./MS_Production/MSNStorageFileServer.zip\n        ./MS_Production/DevelopmentToolsRelease.zip\n        ./MS_Production/BuildServer06.zip\n        ./MS_Production/BuildServer03.zip\n        ./MS_Production/BuildServer04.zip\n        ./MS_Production/BuildServer07.zip\n        ./MS_Production/BuildServer02.zip\n        ./MS_Production/BuildServer01.zip\n        ./MS_Production/BuildServer05.zip\n        ./MS_Production/RadiusAuthentication.zip\n        ./MS_Production/DisplayAdsPayload.zip\n        ./MS_Production/DisplayAdsDataServer.zip\n        ./MS_Production/MSNStorageCFS.zip\n        ./MS_Production/RadiusBackEndSQLServer.zip\n        ./MS_Production/LiveMapsBackEnd.zip\n\n\n## Tracelog for the following paper:\n##   \"I/O Deduplication: Utilizing Content Similarity to Improve I/O Performance\",\n##   Florida International University, FAST 2010\n##   (http://static.usenix.org/events/fast10/tech/slides/koller.pdf)\n./FIU_IODedup\n                ./FIU_IODedup/web-vm.tar.gz\n                ./FIU_IODedup/mail-02.tar.gz\n                ./FIU_IODedup/homes.tar.gz\n                ./FIU_IODedup/mail-03.tar.gz\n                ./FIU_IODedup/mail-01.tar.gz\n                ./FIU_IODedup/mail-04.tar.gz\n\n\n## Real Tracelog - Parallel supercomputer workloads, from UC Berkeley (1991, too old)\n./Parallel_Traces\n        ./Parallel_Traces/sprite.tar\n\n\n## reserved (empty) directory For our future REAL TRACE LOGS!!!\n./gaesung-gongdan\n\n</pre>\n\n <pre>\n[real_traces] $ du -sm *\n7086    FIU_IODedup\n5060    MSR_Cambridge\n21978   MS_Enterprise\n18645   MS_Production\n218     Parallel_Traces\n1       README\n1       gaesung-gongdan\n0       this_machine\n</pre>\n\n== ## bNote-2013-05-16 ==\n\n=== Real and Effective IDs ===\n\n* Real and Effective IDs \n: http://www.lst.de/~okir/blackhats/node23.html\n\nFirst, in order to really understand how this works, we need to step back for a moment and talk about the Unix user ID concept.\n\nAt the lowest level of the operating system, the kernel, users and groups aren\'t identified by names, but numbers. The kernel needs to be fast and robust, and data structures better be small, and moving around strings is anything but efficient. So, each user name and group name is mapped to a unique unsigned number, called user and group ID for short, or uid and gid. This mapping is done via the /etc/passwd and /etc/group files, respectively. The user and group ID 0 are commonly called root, but that\'s really just a convention. \n\nEach Unix process has a user ID and a group ID associated with it, and when trying to open a file for writing, for instance, these IDs are used to determine whether the process should be granted access or not. These IDs constitute the effective privilege of the process, because they determine what a process can do and what it cannot. Most of the time, these IDs will be referred to as the effective uid and gid.\n\nWhat happens when you invoke the passwd utility is that the effective uid of the process is set to 0, i.e. the uid of the root user. As a result, the program is permitted to modify the /etc/passwd file, and can thus replace the encrypted password in your account entry with the new one you just provided.\n\nIf you\'re familiar with the passwd utility, you\'ll know that as a normal user, you\'re only allowed to modify the password of your own account; it will not let you modify the password of any other account. So this begs the question, how does it know who invoked it?\n\nThat\'s where another pair of user and group ID comes in, called the real uid and gid, respectively. These IDs are used to track who a user really is, i.e. on what account he or she is logged in. This uid value is not changed when you invoke programs such as passwd. So the program simply needs to find out what user name corresponds to its real uid, and refuse to change any other account.\n\nMost of the time, the effective user ID of a process is just the same as the real ones, and there\'s no point in making a fuss of this minor distinction.\n\nThings start to get interesting when you invoke a setuid application, however. Assume you\'re logging into your normal user account, which has a user ID of 500. Now you invoke a setuid root application. Because it\'s setuid root, the operating system will set the the effective user ID of the process to that of the root user (0). The real user ID, however, remains unchanged. This allows the application to learn the identity of the user who invoked it, and to continue to access files etc with the privilege of the invoking user.\n\nIf you think about a large company where employees have different levels of access to different locations, you could compare the real user ID to the name badges people wear, and the effective user ID to the set of keys they\'ve been given.\n\nI have to confess that for the sake of simplicity, I have so far avoided to talk about an additional bunch of group IDs a process usually lugs around; these are called the supplementary gids. A user can be a member of several groups at the same time, for instance a software engineer working on project Fourtytwo may be a member of both the eng and fourtytwo groups at the same time. This set of all groups is transported in an additional group vector associated with each process, called the supplementary group ID; what we\'ve called ``the\'\' gid all the time is really the primary gid in this case.\n\nIn the real vs effective scheme of things we\'ve talked about, supplementary gids are neither fish nor meat. There isn\'t a real and effective set of supplementary gids; there\'s just one single set. And on one hand, they are part of the effective privilege of a process, because they\'re used in determining whether e.g. a process is given access to a file. But on the other hand, invoking a setgid application doesn\'t alter this group vector the least.\n\nThe set of supplementary gids isn\'t usually much of an issue for setuid applications, however.\n\n\n=== Machine Learning Books ===\n\n <pre>\n이미 아시는 분들도 있으시겠지만 추가 정보를\n\n도움 되실 분들을 위해 공유하여 드립니다.\n\n \n\nl  온라인 무료 강의\n\nn  Machine Learning (Stanford U.)\n\nu  Descriptions \n\n·          강사: Andrew Ng \n\n·          무료 웹사이트: https://www.coursera.org/#course/ml\n\n·          알기 쉽게 잘 설명함.\n\n \n\nl  전공 서적\n\nn  Machine Learning (저자 : Tom M. Mitchell, ISBN : 0-07-115467-1)\n\nu  Descriptions \n\n·          PDF 파일 다운로드 가능: http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf\n\n·          신경망 (Neural Network) 설명 우수.\n\n \n\nn  Machine Learning : a Probabilistic Perspective (K.P. Murphy, ISBN 978-0-262-01802-9)\n\nu  Descriptions \n\n·          Deep Learning 외 최신 기술 설명.\n\n·          구입처: Yes24 외  http://www.yes24.com/24/goods/7107202?scode=032&OzSrank=1 (Price: 89,000원)\n\n·          SAIT 도서관 1권 소장 중 (등록번호: 028247, 청구기호: BC Q325.5 M978m)\n\n·          표지 이미지\n\n\n\n \n\n \n\n감사합니다.\n\n노요한 드림\n\nYohan J. Roh, Ph.D. \n\nResearch Staff Member, Data Analytics Group,\n\nSamsung Advanced Institute of Technology\n\nPhone: +82-10-7222-7137\n\nTel: +82-31-280-9728   Fax: +82-31-280-9860 \n\nE-mail: yohan.roh@samsung.com\n\n \n\n \n\n \n\nFrom: 구본철 [mailto:bc.gu@samsung.com] \nSent: Monday, May 13, 2013 11:54 AM\nTo: 심은수; 전바롬; 최영상; 김현준; 문민영; 민윤홍; 박상도; 서영완; 서정민; 성재모; 송인철; 신현정; 유개원; 이선재; 이예하; 이주평; 이호섭; 이호식; 정명준; 최희열; 박정현; 우경구; 박문호; 성영경; 홍석진; 감혜진; 권근주; 김예훈; 김중회; 김하영; 노요한; 라나; 박진만; 박형민; 유상현; 유승우; 이지현; 이천희; 이호동; 전주혁; 조백환; 채승철; 하미숙; 강효아; 이기용; 장주호\nSubject: Re: AI 책\n\n \n\n안녕하세요, 구본철입니다.\n\n이미 알고 계신 분들도 많겠지만, 아래 책의 저자 중 한명인 Peter Norvig은\n\n구글의 Sebastian Thrun과 \"Introduction to Artificial Intelligence\"라는 동영상 강의를 공개한 것으로도 유명합니다.\n\n강의 내용이 해당 책과 연관이 많기 때문에 함께 보시면 더 좋을 것이라고 생각되네요. (동영상 다운로드 가능)\n\n(https://www.udacity.com/course/cs271)\n\n그리고, 2nd edition의 경우 기술원 도서관에서도 소장하고 있습니다.\n\n현재 제가 대출 중이므로 내용이 궁금하신 분은 제게 문의해주세요.\n\n그럼 좋은 하루 되시길 바랍니다.\n\n감사합니다.\n\n \n\np.s. Coursera의 AI 카테고리(https://www.coursera.org/courses?cats=cs-ai)도 참고하세요^^\n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-05-13 08:33 (GMT+09:00)\n\nTitle : AI 책\n\n \n\nAI에 관한 좋은 introduction 책을 소개합니다.\n\n다양한 주제에 대하여 매우 좋은 introduction / overview를 제공합니다.\n\ntable of contents는 아마존에서 확인할 수 있습니다.\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?s=books&ie=UTF8&qid=1368401154&sr=1-1&keywords=artificial+intelligence\n\n \n\n국내에서는 아래에서 구입할 수 있습니다.\n\nhttp://www.yes24.com/24/Goods/4870172?Acode=101\n\n \n\n감사합니다.\n\n</pre>\n\n== ## bNote-2013-05-15 ==\n\n\n=== blktrace enhancement ===\n\n\n=== Save the private SAVL ===\n\n\n=== SAIT-India 과제 ===\n\n\n: 지난 번 인디아과제 관련, FreeNAS 기반으로 Storage System Prototyping하는 activity에 대해 신전문님께 말씀드렸었습니다만, FreeBSD 기반이었다는 것이 맘에 계속 걸렸었습니다. 다행히도, 새롭게 추천드리고 싶은 Open source 기반 프로젝트를 하나 찾았습니다. ^^\n\n: OpenMediaVault를 강력히 추천합니다. (http://www.openmediavault.org/)\n\n* Quick Summary\n:- Debian Linux에서 구동되는 FreeNAS-alternative Storage System (NAS) project\n:- FreeNAS 개발자들이 참여\n:- Licence는 GPLv3를 따르는 것으로 보임\n:- 현재 0.4.24 버전까지 나왔음\n\n* Features\n:- Debian Linux Kernel 기반\n:- Web기반 관리 제공\n:- Link aggregation, Wake on LAN 기능\n:- Linux의 Native File System 모두 지원\n:- LVM 등 볼륨관리, Software RAID, JBOD 등 어레이관리 지원\n:- Per-volume Quota 지원\n:- Security feature로서 ACL 지원\n:- S.M.A.R.T. 및 SNMP (read-only) 지원\n:- Network File System으로서 NFS v3,v4 (Linux), SMB/CIFS (Windows) 지원\n\n@ Plugins\n:- UPS\n:- iSCSI target\n:- 등등\n\n== ## bNote-2013-05-10 ==\n\n=== 수퍼컴센터와 협력 방안 (Real Trace 확보) ===\n\n <pre>\n\n배경 및 목적\n  ① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n  ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n추진 방향\n  ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n  ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n     예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n추진 효과\n  ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n  ② Workload에 최적화된 Data Management 알고리즘 개발로\n     데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n  ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n\n추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                        : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform) : 2주\n      [Step 3] I/O Workload Outlook 분석              : 4주\n      [Step 4] Dominant I/O 패턴 추출                  : 8주\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주\n      [Step 6] Data Placement 알고리즘 구현 및 검증      : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n         (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        → 22주에서 약 15주 정도로 단축예상\n              ※ 7주 감소 효과 = 2주(@3단계)+2주(@4단계)+3주(@5단계)\n</pre>\n\n== ## bNote-2013-05-09 ==\n\n=== Cloud Computing 기술 동향 센싱 ===\n\n\n\n== ## bNote-2013-05-07 ==\n\n\n=== IOWA-based Proactive Data Placement Roadmap ===\n\n: \'13: 주기성 기반 Low-overhead I/O Prediction\n: \'14~\'15: 전조 기반 High-accuracy I/O Prediction\n: \'15: I/O Prediction 기반 분산 Data Placement\n\n\n=== dirtifying 843t with iometer ===\n\n==== Test environment ====\n\n* 메모리사 IOmeter 설정\n:- No. of outstanding I/Os: 32 개\n:- 4KB random writes, 1-hour\n\n* Partitioning: \n: 480GB 크기의 843T를 약 240GB 크기를 갖는 2개의 partition으로 나누어\n: Partition 1을 Iometer로 dirtyfing하여 100% (240GB) 채워놓고,\n: Partition 2를 Iometer로 dirtifying하면서 동시에 성능 metric 측정.\n\n* NCQ: enabled (queue_depth = 31)\n\n* 현재까지의 실험 진행 상황\n: uFLIP에 의한 dirtyfing은 취소 (480GB짜리 SSD에 대해서, uFLIP에 의한 dirtifying은 너무 느려서 중간에 취소하였음)\n: iometer에 의한 dirtifying 진행 중 (메모리사 설정 이용)\n\n* 지금까지 확인된 내용 (iostat 명령어를 이용하여 확인하였음)\n: latency를 높이는 쪽으로 가기 위해서는 file system을 bypass하여 raw block layer를 바로 이용하는 것이 좋지만, throughput을 높이는 방향으로 가기 위해서는 file system (ext4)에서 I/O를 handling하는 방식을 참고할 필요가 있겠다는 생각이 들었음.\n: iometer가 발생시키는 I/O workload가 file system을 통과하게 되면, 4KB 단위 I/O들이 상당히 merge되어 throughput 값이 높아지는 효과 발생 (file vs. raw = 112,171KB/s vs. 3,632KB/s), 그러나 예상되는 바처럼 latency특성은 저하됨 (file vs. raw = 221.00 writes/sec vs. 908.00 writes/sec)\n\n\n----\n* iostat-20130507_141457-oio32.log\n\n* I/O through the file system (ext4 - measured by iostat at initial-mid time)\n: Write IOPS = 221.00 (iosize: about 1,015KB)\n: Write Throughput = 112,172.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     9.00    0.00  221.00     0.00 112172.00  1015.13     1.00    4.54    0.00    4.54   4.51  99.60\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iostat at final time)\n: Write IOPS = 523.00 (iosize: about 8KB)\n: Write Throughput = 2092 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  523.00     0.00  2092.00     8.00     0.98    1.88    0.00    1.88   1.88  98.40\n</pre>\n\n\n* I/O through the file system (ext4 - measured by iometer at final time)\n: Total I/Os per Second: 439.97\n: Total MBs per Second: 1.72\n: Average I/O Response Time (ms): 290.9243\n: Maximum I/O Response Time (ms): 1791.3428\n\n\n----\n* I/O to block layer (bypassing file system)\n: Write IOPS = 908.00 (iosize: 4KB)\n: Write Throughput = 3,632.00 KB/s\n <pre>\nDevice:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nsdd1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00\nsdd2              0.00     0.00    0.00  908.00     0.00  3632.00     8.00     1.99    2.19    0.00    2.19   1.10  99.60\n</pre>\n\n== ## bNote-2013-05-02 ==\n\n=== R (r_stat) 3D plot ===\n\n* [http://www.statmethods.net/graphs/scatterplot.html Simple Scatterplot // Quick-R]\n* [http://www.r-bloggers.com/turning-your-data-into-a-3d-chart/ Turning your data into a 3d chart // R-bloggers]\n* [http://www.ejwagenmakers.com/misc/Plotting_3d_in_R.pdf R graph gallery // Compilation by Eric Lecoutre // 2003-12-12]','utf-8'),(123,'== ## bNote-2013-02-28 ==\n\n=== 수원임직원주차장 방문예약 ===\n[https://www.sdigitalcity.com/security/visit/login.do 수원임직원주차장 방문예약]\n\n== ## bNote-2013-02-26 ==\n\n=== iowa.sidewinder ===\n\n* construction: filebench workload generator\n\n== ## bNote-2013-02-21 ==\n\n\n=== Daily.Kickoff ===\n\n* acsptrn outlook program\n:- kag (K-address-group)\n::- can \'K\' be a signature of a IO pattern?\n:::- what kind of implication can be extracted from \'KAG\'?\n::: : what size of chunk can have a meaning?\n::: : unit of tiering IO? unit of proactive caching? unit of IO dump size to the SSD?\n::- hamming distance (proper choice! why? difference value means the # of items to be added in cache) (older elements identified as a different one can be evicted or not, according to the cache space condition)\n:- outlook seek distance stat-dist\n:: - can \'seek distance\' be a metric for identifying hot zone?\n\n* cache simulation program\n:- hit ratio calculation, performance gain calculation\n\n* ML approach planning\n:- what to do for proactive data placement?\n::- what to do for static(?) macro insight?\n::- what to do for dynamic(?) micro prediction? (periodicity?)\n\n== ## bNote-2013-02-20 ==\n\n=== IOWA planning with Yanpei Chen Paper (SOSP 2011) ===\n\n\n\n\n\n\n\n=== Intel releases SSD cache acceleration software for Linux servers ===\n\n인텔이 Linux Server를 위한 CAS (Cache Acceleration Software) 기술을 Release했다는 소식입니다.\n\n이 CAS라는 기술은 작년에 인수한 Nevex사의 CacheWorks 기술에 기반하고 있습니다.\n\n특징으로 볼 수 있는 것은 기존 block cache의 문제점으로 지적되었던 DRAM에도 올라간 data가 SSD cache에도 올라가는 현상을 피하기 위해 초보적인 수준의 intelligence를 도입했다는 점입니다.\n\n(very hot data는 DRAM에 보내되, 덜 hot하지만 still I/O active data는 SSD로 나누어 보낸다는 것이 핵심입니다) \n\n\'\'\'Intel releases SSD cache acceleration software for Linux servers\'\'\'\n\nSSD-based CAS for Linux servers can offer up to 18 times the performance for read-intensive applications, Intel says. \n\n[http://www.infoworld.com/d/open-source-software/intel-releases-ssd-cache-acceleration-software-linux-servers-212673?page=0,0 InfoWorld News FEBRUARY 12, 2013]\n\n\n발췌 내용:\n...\nTo ensure it does not duplicate work already being performed in DRAM, the CAS software takes control of all data access. The hottest data is placed on DRAM, while less hot, but still I/O active data, is placed on SSD.\n\nIntel\'s CAS software provides cooperative integration between the standard server DRAM cache and the Intel SSD cache, creating a multi-level cache that optimizes the use of system memory and automatically determines the best cache level for active data.\n...\n\n== ## bNote-2013-02-19 ==\n\n\n=== Workload generation and tracing (with filebench, blktrace) ===\n\n* target workloads\n:- fileserver.f\n:- mongo.f\n:- netsfs.f\n:- networkfs.f\n:- oltp.f\n:- tpcso.f\n:- varmail.f\n:- videoserver.f\n:- webserver.f\n\n\n* video server workload (running time: 3600 seconds) (20130219_143400)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\n#>> cleaning up \'30.tmp/\' directory\n#>> ready for generating workload specified in \'20.conf/.tconf.sh\'\n#>> please run iotrace (blktrace) in \'34.iotrace/\'\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\n#>> [20130219_143400] waiting for iotrace getting started \n.................................\n----------------------------------------------------\n#>> Ok, now start workload generation\nstarted: 20130219_143433\n----------------------------------------------------\nFilebench Version 1.4.9.1\n 3599: 0.000: Allocated 170MB of shared memory\n 3599: 0.001: Eventgen rate taken from variable\n 3599: 0.001: Video Server Version 3.0 personality successfully loaded\n 3599: 0.001: Creating/pre-allocating files and filesets\n 3599: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n 3599: 0.002: Re-using fileset passivevids.\n 3599: 0.002: Creating fileset passivevids...\n 3599: 2227.782: Preallocated 104 of 194 of fileset passivevids in 2228 seconds\n 3599: 2227.800: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n 3599: 2227.800: Re-using fileset activevids.\n 3599: 2227.800: Creating fileset activevids...\n 3599: 2227.805: Preallocated 32 of 32 of fileset activevids in 1 seconds\n 3599: 2227.805: waiting for fileset pre-allocation to finish\n 3974: 4324.570: Starting 1 vidreaders instances\n 3974: 4324.599: Starting 1 vidwriter instances\n 3976: 4324.780: Starting 1 vidwriter threads\n 3975: 4324.790: Starting 48 vidreaders threads\n 3599: 4325.877: Running...\n 3599: 7926.189: Run took 3600 seconds...\n 3599: 7926.234: Per-Operation Breakdown\nserverlimit          423882ops      118ops/s   0.0mb/s    305.7ms/op    60431us/op-cpu [0ms - 2292ms]\nvidreader            423983ops      118ops/s  29.4mb/s    407.1ms/op    39674us/op-cpu [0ms - 2292ms]\nreplaceinterval      17ops        0ops/s   0.0mb/s  10000.1ms/op        0us/op-cpu [10000ms - 10000ms]\nwrtclose             17ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               17ops        0ops/s  47.2mb/s 134156.2ms/op 17297647us/op-cpu [3919ms - 421911ms]\nwrtopen              18ops        0ops/s   0.0mb/s     26.3ms/op     1111us/op-cpu [0ms - 247ms]\nvidremover           18ops        0ops/s   0.0mb/s  60936.6ms/op   281667us/op-cpu [69ms - 181905ms]\n 3599: 7926.234: IO Summary: 424053 ops, 117.782 ops/s, (118/0 r/w),  76.6mb/s,      0us cpu/op, 415.0ms latency\n 3599: 7926.234: Shutting down processes\n----------------------------------------------------\nfinished: 20130219_164641\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n\n* video server workload (running time: 60 seconds) (20130219_111030)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\n#>> cleaning up \'30.tmp/\' directory\n#>> ready for generating workload specified in \'20.conf/.tconf.sh\'\n#>> please run iotrace (blktrace) in \'34.iotrace/\'\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\n#>> [20130219_111030] waiting for iotrace getting started \n...........\n----------------------------------------------------\n#>> Ok, now start workload generation\nstarted: 20130219_111041\n----------------------------------------------------\nFilebench Version 1.4.9.1\n 2322: 0.000: Allocated 170MB of shared memory\n 2322: 0.001: Eventgen rate taken from variable\n 2322: 0.001: Video Server Version 3.0 personality successfully loaded\n 2322: 0.001: Creating/pre-allocating files and filesets\n 2322: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n 2322: 0.720: Removed any existing fileset passivevids in 1 seconds\n 2322: 0.720: making tree for filset /mnt/hdd_4/filebench/passivevids\n 2322: 0.721: Creating fileset passivevids...\n 2322: 5398.571: Preallocated 104 of 194 of fileset passivevids in 5398 seconds\n 2322: 5398.580: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n 2322: 5441.697: Removed any existing fileset activevids in 44 seconds\n 2322: 5441.698: making tree for filset /mnt/hdd_4/filebench/activevids\n 2322: 5441.698: Creating fileset activevids...\n 2322: 7548.330: Preallocated 32 of 32 of fileset activevids in 2107 seconds\n 2322: 7548.330: waiting for fileset pre-allocation to finish\n 2939: 9496.045: Starting 1 vidreaders instances\n 2939: 9496.077: Starting 1 vidwriter instances\n 2941: 9496.275: Starting 1 vidwriter threads\n 2940: 9496.294: Starting 48 vidreaders threads\n 2322: 9497.515: Running...\n 2322: 9557.521: Run took 60 seconds...\n 2322: 9557.540: Per-Operation Breakdown\nserverlimit          22911ops      382ops/s   0.0mb/s    116.4ms/op   129288us/op-cpu [0ms - 5662ms]\nvidreader            23044ops      384ops/s  95.8mb/s     11.3ms/op     3815us/op-cpu [0ms - 2135ms]\nreplaceinterval      0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtclose             0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtopen              1ops        0ops/s   0.0mb/s      0.1ms/op        0us/op-cpu [0ms - 0ms]\nvidremover           1ops        0ops/s   0.0mb/s   5112.0ms/op   110000us/op-cpu [5111ms - 5111ms]\n 2322: 9557.540: IO Summary: 23046 ops, 384.062 ops/s, (384/0 r/w),  95.8mb/s,      0us cpu/op,  11.6ms latency\n 2322: 9557.540: Shutting down processes\n----------------------------------------------------\nfinished: 20130219_135000\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n\n* video server workload (running time: 3600 seconds) (20130218_161548)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\nrm: cannot remove `wloadsrc.f\': No such file or directory\n#>> [20130218_161548] waiting for iotrace getting started \n........\n----------------------------------------------------\n#>> Ok, now start workload generation\n----------------------------------------------------\n20130218_161556\nFilebench Version 1.4.9.1\n30523: 0.000: Allocated 170MB of shared memory\n30523: 0.001: Eventgen rate taken from variable\n30523: 0.001: Video Server Version 3.0 personality successfully loaded\n30523: 0.001: Creating/pre-allocating files and filesets\n30523: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n30523: 5.754: Removed any existing fileset passivevids in 6 seconds\n30523: 5.754: making tree for filset /mnt/hdd_4/filebench/passivevids\n30523: 5.755: Creating fileset passivevids...\n30523: 5296.248: Preallocated 104 of 194 of fileset passivevids in 5291 seconds\n30523: 5296.261: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n30523: 5296.331: Removed any existing fileset activevids in 1 seconds\n30523: 5296.331: making tree for filset /mnt/hdd_4/filebench/activevids\n30523: 5296.331: Creating fileset activevids...\n30523: 8052.228: Preallocated 32 of 32 of fileset activevids in 2756 seconds\n30523: 8052.228: waiting for fileset pre-allocation to finish\n31345: 10248.763: Starting 1 vidreaders instances\n31345: 10248.804: Starting 1 vidwriter instances\n31346: 10248.868: Starting 48 vidreaders threads\n31347: 10249.084: Starting 1 vidwriter threads\n30523: 10250.088: Running...\n30523: 13850.427: Run took 3600 seconds...\n30523: 13850.506: Per-Operation Breakdown\nserverlimit          1228002ops      341ops/s   0.0mb/s    105.3ms/op    71013us/op-cpu [0ms - 2410ms]\nvidreader            1228102ops      341ops/s  85.3mb/s    140.0ms/op    46691us/op-cpu [0ms - 1919ms]\nreplaceinterval      0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtclose             0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtopen              1ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nvidremover           1ops        0ops/s   0.0mb/s     68.8ms/op    70000us/op-cpu [68ms - 68ms]\n30523: 13850.506: IO Summary: 1228104 ops, 341.104 ops/s, (341/0 r/w),  85.3mb/s,      0us cpu/op, 140.0ms latency\n30523: 13850.506: Shutting down processes\n20130218_200648\n----------------------------------------------------\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n== ## bNote-2013-02-18 ==\n\n=== Misc. ===\n\n* [https://h30613.www3.hp.com/media/files/downloads/Non-FilmedSessions/TB2216_Becoming.pdf Become a Flash expert in minutes! SAMSUNG]\n\n* [http://static.usenix.org/event/lsf08/tech/FS_shepler.pdf Filebench Spencer Shepler]\n\n\n=== blktrace knowledge ===\n\n\n <pre>\n  8,48   4     1657     0.426331441  3443  A   W 2593896448 + 1024 <- (8,49) 2593894400\n  8,48   4     1658     0.426332295  3443  Q   W 2593896448 + 1024 [flush-8:48]\n  8,48   4     1659     0.426334674  3443  G   W 2593896448 + 1024 [flush-8:48]\n  8,48   4     1660     0.426503502  3443  A   W 2593897472 + 1024 <- (8,49) 2593895424\n  8,48   4     1661     0.426503961  3443  Q   W 2593897472 + 1024 [flush-8:48]\n  8,48   4     1662     0.426505840  3443  G   W 2593897472 + 1024 [flush-8:48]\n  8,48   4     1663     0.426666260  3443  A   W 2593898496 + 1024 <- (8,49) 2593896448\n  8,48   4     1664     0.426666707  3443  Q   W 2593898496 + 1024 [flush-8:48]\n  8,48   4     1665     0.426668036  3443  G   W 2593898496 + 1024 [flush-8:48]\n  8,48   4     1666     0.426829266  3443  A   W 2593899520 + 1024 <- (8,49) 2593897472\n  8,48   4     1667     0.426829714  3443  Q   W 2593899520 + 1024 [flush-8:48]\n  8,48   4     1668     0.426831154  3443  G   W 2593899520 + 1024 [flush-8:48]\n</pre>\n\n <pre>\n2593896448 + 1024 <- (8,49) 2593894400\n2593896448 - 2593894400 = 2048\n\n2593897472 + 1024 <- (8,49) 2593895424\n2593897472 - 2593895424 = 2048\n\n2593898496 + 1024 <- (8,49) 2593896448\n2593898496 - 2593896448 = 2048\n\n2593899520 + 1024 <- (8,49) 2593897472\n2593899520 - 2593897472 = 2048\n</pre>\n\n\'\'\'2048\'\'\' is the offset of the partition (/dev/sdd1) from the start point of the block device (/dev/sdd)\n\n <pre>\nblusjune@radiohead:[20130219_143400--videoserver] $ sudo fdisk -l /dev/sdd\n[sudo] password for blusjune: \n\nDisk /dev/sdd: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x000c526d\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdd1            2048  3907026943  1953512448   83  Linux\n</pre>\n\n== ## bNote-2013-02-15 ==\n\n\n\n\n=== Hierarchical Temporal Memory ===\n\n* Numenta Hierarchical Temporal Memory - including HTM Cortical Learning Algorithms Version 0.2.1, September 12, 2011 [https://www.numenta.com/htm-overview/education/HTM_CorticalLearningAlgorithms.pdf Numenta Hierarchical Temporal Memory]\n\n# HTM Overview\n# HTM Cortical Learning Algorithms\n# Spatial Pooling Implementation and Pseudocode\n# Temporal Pooling Implementation and Pseudocode\n# Appendix A: A Comparison between Biological Neurons and HTM Cells\n# Appendix B: A Comparison of Layers in the Neocortex and an HTM Region\n\n==== About Numenta ====\n\n: Numenta, Inc. (www.numenta.com) was formed in 2005 // to develop HTM technology for both commercial and scientific use\n: Use of Numenta\'s software and intellectual property is free for research purposes.\n: Numenta will generate revenue by selling support, licensing software, and licensing intellectual property for commercial deployments.\n: Numenta is based in Redwood City, California. (Privately funded)\n\n\n==== Chapter 1. HTM Overview ====\n\n: Hierarchical Temporal Memory (HTM) is a machine learning technology that aims to capture the structural and algorithmic properties of the neocortex.\n\n: The neocortex is the seat of intelligent thought in the mammalian brain.\n\n: HTM\n\n=== SSD dirty script (iometer) ===\n\n[http://www.iometer.org/doc/downloads.html IOmeter dirty SSD script]\n\n\n=== BDP 근거 ===\n\n* 48시간 TPC-C 벤치마크한 blktrace log: 54.4GB\n* 1차 pre-processing: 52.2 GB\n\n== ## bNote-2013-02-14 ==\n\n\n=== 특허화 ===\n\n==== Patent #3 ====\n\n* (data + io insight) packaging 관련 특허\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n==== Patent #4 ====\n\n* IOWA based Proactive Data Placement 자체에 대한 특허\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 clue를 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보이는 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# clue, as simple as possible\n::::# clue, as specific as possible\n::::# clue, efficiently traversable - corresponding clue case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# clue, easily extensible - clue 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n==== Patent #5 ====\n\n* SSD Retention Time Controlling for Caching/Tiering 관련 특허\n\n\n==== Patent #6 ====\n\n* HML (Hierarchical ML)에서 FPGA, ASIC 등을 이용해서 Acceleration할 수 있는 방법은 없을까?\n\n== ## bNote-2013-02-12 ==\n\n\n\n=== IBdM 팀 회의 (15:40 ~ 17:00, 실험실) ===\n\n* 과제목표 (현재안): Intelligent Big Data Management 문제를 “H-ML을 통해” 남들보다 (EMC보다) 훨씬 잘 풀겠다!\n:- 요구사항\n::- 성공적으로 연구를 진행했을 경우의 최종 “기대치” 가 order-of-magnitude 높아야 함, 해결하는 문제가 수년간의 심도깊은 연구를 요구하여야 함\n::- I/O 성능 혁신 <- Data Placement 문제 : H-ML을 통한 Proactive data placement [70x improvement]\n::- 저장 효율 혁신 <- Data Reduction 문제 : H-ML을 통한 Clustered Deduplication [XXXX improvement] “Big Data를 Small Data로 만드는 기법”\n\n* Big Data Era에는 기존 Data Management 문제가 어떻게 달라지나?\n:- data양 폭증, IT budget증가 제한적 -> data reduction을 통한 IT 비용 절감 -> Data Deduplication 기술\n:- data capacity planning이 더 어려워짐 -> 필요에 따라 on-demand로 용량/성능 증설 요구 -> Scale-out 기술\n:- 비정형 data의 증가 -> NoSQL (?), Stream Processing (?)\n:- 비정형 data를 표현하는 데이터의 요구 증가 -> tagged data(metadata) 관리기술\n:: (data에 tag되어 data의 과거 access이력, access pattern, 복제/분열 history 등 intelligence정보를 저장)\n:- decentralized data(?) -> 자동화된 데이터 관리 중요 -> self-managed architecture 기술\n\n* 우리의 차별화\n:- Tool : hierarchical machine learning\n:- Input Data : 풍부한 tagged data (I/O workload, File I/O log, Application-level log, …)\n:- Motto\n::- Big Data의 근본적 이해를 통한 Big Data 관리기법 지능화\n::- Big Data의 생성, 확장, 분열, 복제, 소멸 등 full-cycle에 대한 근본적 이해\n\n* Action Item\n:- H-ML 기법 적용을 위한 data placement 문제 formulate, 기대치 estimation [융]\n:- H-ML 기법 적용을 위한 deduplication 문제 formulate, 기대치 estimation [신,구]\n:- “big data era에는 기존 data 관리 문제가 어떻게 달라지나?” [all]\n\n=== IOWA based Proactive Data Placement ===\n\n심상무님 지시.\n\n---- \n <pre>\n\nTM-20130212-IOWAPDP\n\n[ IO Workload Analysis 기반의\nProactive Data Placement ]\n\n\n\n2013-02-12\n\n\n\nData-intensive Storage\nIntelligence Group\nIntelligent Computing Lab\n</pre>\n\n----\n==== expected benefit ====\n\n <pre>\n이전에 얘기한 바가 있는데, proactive data placement와\nreactive data placement의 논리적인 비교를 통해\n원리상 기대되는 benefit의 크기 규모를 얘기할 수 있을까요?\n\n> 네, IO 성능 Penalty 최소화 측면과 SSD cache media 효율성 향상 측면에서\n  가늠해볼 수 있습니다. \n  \n  가) IO penalty 최소화\n    : 기존 방식은 미리 cache (혹은 fast-tier)에 proactive하게 올린다는 개념이 없었기 때문에\n      일단 최초 한 번은 HDD로부터 access되는 것이 필요하며,\n      그만큼 성능 penalty를 겪을 수 밖에 없습니다.\n      문제는 이렇게 성능 penalty를 겪는 경우가 얼마나 많이 존재하느냐입니다.\n\n      특히, \n      many-times-of-rehit 되는 small-number-of-data와 ................... (1)\n      small-times-of-rehit 되는 large-number-of-data 의 ................... (2)\n      분포 양상이 중요한데,\n      (1), (2) 각각이 전체 IO 에 얼만큼의 contribution을 해내는지가\n      proactive 방식과 reactive 방식과의 성능 차이에 영향을 미칩니다.\n\n      7200rpm의 HDD의 Read latency는 8500us,\n      SLC 기반의 NAND의 Read latency는 25us 라고 하고,\n      총 1,000 번의 IO access가 있었다고 했을 때,\n      A, B, C 각 workload 경우에 따른 IO 처리 소요 시간을 계산함으로써\n      reactive data placement 대비 proactive data placement의\n      성능 향상 정도를 가늠해보겠습니다.\n\n      (참고로, 아래 성능 비교 계산은 reactive Vs. proactive의 \n       핵심 logic에만 집중한 highly-abstracted 계산으로서,\n       블럭 레이어에서의 IO scheduling, HDD의 read-ahead, SATA/PCIe 버스 구조,\n       스토리지 디바이스 내부 구조 등 세부 시스템 적인 고려는 전혀 되어 있지 않음.\n       또한 cache media로 사용되는 SSD의 용량은 충분히 크다고 가정하였음)\n\n      A) 1개의 addr가 1,000번의 access를 다 받아낸 경우 (active range: 1) // 극단적 (1)의 경우\n         reactive 방식: 1 x 8500us + 999 x 25us = 33,475us\n         proactive 방식 (예측 적중률 100%): 1000 x 25us = 25,000us // IO 시간 1.3배 단축\n         proactive 방식 (예측 적중률 100% 미만): 1 x 8500us + 999 x 25us = 33,475us // IO 시간 단축 없음\n\n      B) 50개 addr가 400번의 access를, 150개 addr가 600번의 access를 받은 경우 (active range: 200)\n         reactive 방식: (50 x 8500us + 350 x 25us) + (150 x 8500us + 450 x 25us) = 1,720,000us\n         proactive 방식 (예측 적중률 100%): 1000 x 25us = 25,000us // IO 시간 68배 단축\n         proactive 방식 (예측 적중률 90%): 900 x 25us + 100 x 8500us = 872,500us // IO 시간 1.97배 단축\n\n      C) 1,000개 addr가 1번씩 다 access된 경우 (active range: 1,000) // 극단적 (2)의 경우\n         reactive 방식: 1000 x 8500us = 8,500,000us\n         proactive 방식 (100% 예측 적중시): 1000 x 25us = 25,000us // IO 시간 340배 단축\n   \n  나) SSD cache 미디어 효율성 향상\n    : 기존에는 미래에 다가올 IO에 대한 예측을 가지고 있지 않기 때문에\n      무조건 hit된 data들을 cache에 올리게 되며, 이후 자주 access될 data들과\n      그렇지 않은 data들이 모두 cache되므로, cache 공간의 낭비가 커집니다.\n      특히, data 규모가 커져서 SSD cache를 쓸 필요가 있는 경우,\n      SSD에 대한 write 및 erase를 많이 유발하게 되는 기존 방식은\n      SSD cache의 성능을 저하 시키는 중요한 요인이 될 수 있습니다.\n      이 측면에 대한 수치적 검증은, SSDsim 등을 이용하거나,\n      실제 SSD에 주어진 workload를 인가하여 IO 성능을 측정함으로써\n      실험적으로 증명해볼 수 있을 것 같습니다.\n</pre>\n\n----\n\n==== what is the heart of proactive data placement? ====\n <pre>\n대개 caching 알고리즘들은 데이터 I/O 패턴만 이용해서 cache에 남겨둘 데이터를 고르잖아요.\nproactive data placement 기술의 핵심은 미래의 데이터 I/O 패턴을 예측하는 것이라고 할 수 있나요?\n\n> 네, 맞습니다.\n  proactive data placement 기술의 핵심은 다가올 data IO 패턴 예측입니다.\n\n  상무님 말씀하신 것처럼,\n  기존의 cache 알고리즘들은 이미 cache된 data들에 대한\n  replacement (즉 eviction) 알고리즘이라고 보시면 됩니다.\n  즉, 일단은 access된 data들을 모두 cache에 올려두었다가,\n  그 cahced data들 중에서 \'미래에 덜 필요할 것\'같은 데이터를 골라내는 것입니다.\n  그러나 cache되어야 할 data를 예측하여 \'proactive 하게 미리\'\n  cache에 가져다놓는 것은 고려되어 있지 않습니다.\n  (이렇게 하지 못한 이유는, CPU L2/L3 cache와 DRAM 간의 cache이다보니\n   복잡한 알고리즘을 돌릴 시간 및 공간이 절대적으로 부족하기 때문이었습니다)\n	\n  기존 cache 알고리즘의 장점은 cache라는 공간에 data를 placement하는 알고리즘은\n  극도로 심플하지만, (별도의 알고리즘 없이 access된 data는 무조건 cache에 두므로)\n  일단 한 번이라도 data가 access되어야만 caching이 될 수 있다는 한계가 있습니다.\n\n  앞에서 이미 잠시 언급하였습니다만,\n  small-number-of-data가 many-times-of-rehit 이 된다면,\n  reactive data placement 방식과 proactive data placement 방식 간에\n  성능 차이가 크지 않을 수 있습니다.\n  그러나, active data range가 매우 넓은 경우에는\n  small-number-of-data가 many-times-of-rehit 되는 경우 뿐만 아니라,\n  large-number-of-data들이 small-times-of-rehit되는 경향 (long-tail)이\n  보여질 수 있기 때문에 proactive data placement에 의한 이득이 커질 수 있습니다.\n  (Active range가 매우 커지는 Big data 상황에서는\n   이렇게 될 가능성이 더 커질 것으로 보입니다)\n\n</pre>\n\n----\n\n==== Patent: IOWA based PDP ====\n <pre>\n미래의 데이터 I/O 패턴을 예측하는데 과거의 데이터 I/O 패턴만 볼 것이 아니라,\n데이터 자체의 특성, 파일 단위의 특성 (예를 들면, 이미지인지, 서버 로그인지, conf 파일인지),\nblock 자체의 특성 (뭐가 있을까요?), creator (user, process)의 종류,\nlast change/update time, 현재 active한 process들의 종류 등,\n다양한 데이터를 이용하는 것이 어떨까요?\n이런 방식이 새로운 아이디어라면 특허 출원을 검토해 주세요.\n\n> 네, 상무님, 다양한 데이터를 이용하는 것이 필요합니다.\n  이에, trace 기반의 access 패턴 뿐만 아니라 호스트 시스템에서 얻을 수 있는\n  정보들을 최대한 활용하고자 하고 있습니다.\n  그리고 이 부분에서 ML 기술의 적극적인 도입이 필요합니다.\n\n  (이 정보들이 IO 패턴 발굴 및 예측에 도움을 줄 수 있는지 유효성을 검증하고자\n   일부 정보들을 활용하여 이미 실험을 시작하고 있습니다.\n   현재는 IO 유발자 정보를 고려하여 실험 중입니다.)\n\n  고려중인 정보/접근 방식들은 다음과 같습니다.\n\n  - IO 액세스 패턴\n    - trace 기반의 address 별 access pattern\n      : periodicity 뿐만 아니라 recency, 누적 access count,\n        read/write intensiveness, access randomness, data usage-cycle 패턴\n        (read-modify-write(update)-read 등) 등\n      : 수집 시 blktrace 등 IO trace 툴 활용\n    - 시스템 meta 정보 기반 access pattern\n      : inode 등에서 얻어낼 수 있는 last create/update/access time 등\n      : 수집 시 inode 등 file, filesystem 정보 활용\n\n  - IO 유발자 정보\n    - user ID, group ID, process ID 등 creator 정보 및 현재 해당 file을 이용하는 \n      process / class 등\n      : 수집 시 debugfs, proc, sysfs 등의 메커니즘 활용\n      : IO 유발자가 어느 category에 속하는지 category 별로 분석\n        (web, DB, auth, monitoring, kernel thread (journald, kworker 등), ...)\n\n<!--\n  - State machine 기반의 macro IO pattern analsysis\n    : 유의미한 parameter 몇 가지로 정의되는 state machine을 정의하고,\n      이 state 간의 transition 확률을 이용하여 다가올 macro IO 패턴을 예측\n-->\n\n  특허 출원에 대해 말씀드리자면, 위의 기법 각각에 대한 특허에 대해서는\n  일부 시스템 등록, 혹은 현재 직무발명서 작성 중이었습니다만,\n  말씀 주신 바처럼 현재 저희가 진행하고 있는\n  IOWA 기반의 proactive data placement 자체에 대한\n  특허 가능성도 검토하도록 하겠습니다.\n\n  참고로, 2/12일 (오늘) 현재, 삼성 특허 검색 사이트에서, 검색식\n  ((proactive data placement) OR (data preplacement))\n  (cach* OR tiering)\n  ((IO OR I/O OR workload) analy*)\n  으로 찾아본 결과, 검색된 관련 특허는 없었습니다만,\n  검색 범위를 더 넓혀서 추가로 더 찾아보겠습니다.\n\n</pre>\n\n----\n\n==== Realistic sample trace log collection ====\n<pre>\n이렇게 다양한 데이터를 이용해서 데이터 I/O 패턴을 예측하려면 많은 sample log들을 모아서\n예측 함수를 만들어야 합니다. 분산 스토리지가 특정 용도에 이용되고 있다면\n(예로, 스트리밍 서버, 웹 서버, VDI 서버 등), 그것별로 sample log들을 많이 모아야 할 것 같습니다.\n우리가 어떻게 이런 다양한 로그들을 모을 수 있을까요?\n\n> 네, 전적으로 동감합니다. 수집 현황/계획은 아래와 같습니다.\n\n  1. Web Server IO Log\n    : 메모리사에서도 SAVL 타겟으로 우선적으로 관심을 가졌던 workload로서\n      SPECweb benchmark을 통해 realistic workload를 얻을 수 있습니다.\n      (메모리 사 응용기술팀으로부터 시스템 구축에 필요한 파일 받아옴)\n\n  2. DB IO Log\n    : TPC-C benchmark을 통해 realistic workload를 얻을 수 있습니다.\n      (이미 환경 구축 완료 및 사용 중)\n\n  3. Supercomputing IO Log\n    : 현재 기술원 3층의 수퍼컴퓨터 관리팀과 접촉 중\n      (Matlab, R, MapReduce-like RSP framework에 대한 workload 존재)\n\n  4. Streaming IO Log\n    : 아직 확보 방안 마련되지 않음\n		\n  5. VDI IO Log\n    : 기술원 정보전략팀과 의논 가능\n      (\'12년에 1차로 trace log 확보를 시도해본 적 있으나\n       정보전략팀과 SDS와의 관계가 어려워져 실패)\n\n</pre>\n\n----\n\n==== problem formulation considering on-line learning ====\n<pre>\n\n[ 지시 1 ]\n\n앞 경우는 sample log들을 모아서 예측 함수를 만드는 것이고,\n실제 deploy되었을 때는 dynamic 하게 예측 함수가 adaption할 수 있어야 하는데,\n그런 adaptation을 on-line learning으로 볼 수 있습니다.\n최희열 전문과 협력해서 위 문제들을 formulation 하기 바랍니다.\n\n> 네, 위의 내용들을 정리하여 최희열 전문과 함께 위의 문제들을 명확히 정리해보겠습니다.\n\n</pre>\n\n----\n\n== ## bNote-2013-02-08 ==\n\n=== Git Server on My Local Machine ===\n\n==== 4.4 Git on the Server - Setting Up the Server ====\n\n* http://git-scm.com/book/en/Git-on-the-Server-Setting-Up-the-Server\n: Setting up the server // git-scm.com\n\n* http://git-scm.com/book/ch4-4.html\n: 4.4 Git on the Server - Setting Up the Server // git-scm.com\n\n\n: Let\'s walk through setting up SSH access on the server side. In this example, you\'ll use the authorized_keys method for authenticating your users. We also assume you\'re running a standard Linux distribution like Ubuntu. First, you create a \'git\' user and a .ssh directory for that user.\n\n <pre>\n$ sudo adduser git\n$ su git\n$ cd\n$ mkdir .ssh\n</pre>\n\n: Next, you need to add some developer SSH public keys to the authorized_keys file for that user. Let\'s assume you\'ve received a few keys by e-mail and saved them to temporary files. Again, the public keys look something like this:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCB007n/ww+ouN4gSLKssMxXnBOvf9LGt4L\nojG6rs6hPB09j9R/T17/x4lhJA0F3FR1rP6kYBRsWj2aThGw6HXLm9/5zytK6Ztg3RPKK+4k\nYjh6541NYsnEAZuXz0jTTyAUfrtU3Z5E003C4oxOj6H0rfIF1kKI9MAQLMdpGW1GYEIgS9Ez\nSdfd8AcCIicTDWbqLAcU4UpkaX8KyGlLwsNuuGztobF8m72ALC/nLF6JLtPofwFBlgc+myiv\nO7TCUSBdLQlgMVOFq1I2uPWQOkOWQAHukEOmfjy2jctxSDBQ220ymjaNsHT4kgtZg2AYYgPq\ndAv8JggJICUvax2T9va5 gsg-keypair\n</pre>\n\n: You just append them to your authorized_keys file:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.josie.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.jessica.pub >> ~/.ssh/authorized_keys\n</pre>\n\n: Now, you can set up an empty repository for them by running git init with the --bare option, which initializes the repository without a working directory:\n\n <pre>\n$ cd /opt/git\n$ mkdir project.git\n$ cd project.git\n$ git --bare init\n</pre>\n\n: Then, John, Josie, or Jessica can push the first version of their project into that repository by adding it as a remote and pushing up a branch. Note that someone must shell onto the machine and create a bare repository every time you want to add a project. Let\'s use gitserver as the hostname of the server on which you\'ve set up your \'git\' user and repository. If you\'re running it internally, and you set up DNS for gitserver to point to that server, then you can use the commands pretty much as is:\n\n: # on Johns computer\n <pre>\n$ cd myproject\n$ git init\n$ git add .\n$ git commit -m \'initial commit\'\n$ git remote add origin git@gitserver:/opt/git/project.git\n$ git push origin master\n</pre>\n\nAt this point, the others can clone it down and push changes back up just as easily:\n\n <pre>\n$ git clone git@gitserver:/opt/git/project.git\n$ cd project\n$ vim README\n$ git commit -am \'fix for the README file\'\n$ git push origin master\n</pre>\n\n: With this method, you can quickly get a read/write Git server up and running for a handful of developers.\n\n: As an extra precaution, you can easily restrict the \'git\' user to only doing Git activities with a limited shell tool called git-shell that comes with Git. If you set this as your \'git\' user\'s login shell, then the \'git\' user cannot have normal shell access to your server. To use this, specify git-shell instead of bash or csh for your user\'s login shell. To do so, you will likely have to edit your /etc/passwd file:\n\n <pre>\n$ sudo vim /etc/passwd\n</pre>\n\n: At the bottom, you should find a line that looks something like this:\n\n <pre>\ngit:x:1000:1000::/home/git:/bin/sh\n</pre>\n\n: Change /bin/sh to /usr/bin/git-shell (or run which git-shell to see where it’s installed). The line should look something like this:\n\n <pre>\ngit:x:1000:1000::/home/git:/usr/bin/git-shell\n</pre>\n\nNow, the \'git\' user can only use the SSH connection to push and pull Git repositories and cannot shell onto the machine. If you try, you will see a login rejection like this:\n\n <pre>\n$ ssh git@gitserver\nfatal: What do you think I am? A shell?\nConnection to gitserver closed.\n</pre>\n\n==== Install GitWeb ====\n\n* [http://blog.countableset.ch/2012/04/29/ubuntu-12-dot-04-installing-gitolite-and-gitweb/ Installing gitolite and gitweb on Ubuntu 12.04]\n\nNote this is only tested on Ubuntu 12.04 Server with apache2, I\'m sure it would work on the desktop version also.\n\n===== Installing Git =====\n <pre>\nsudo apt-get install git-core\n</pre>\nOptional, setup git global settings:\n\n <pre>\ngit config --global user.name \"Your Name\"\ngit config --global user.email your@email.com\n</pre>\n\n===== SSH Key =====\nGenerate ssh public/private key on the machine you would like to access git repo from and copy it to the server into the /tmp/ directory, for reference here is the command:\n\n <pre>\nssh-keygen -t rsa -C \"name@computer\"\n</pre>\n\n===== Installing Gitolite =====\n\n <pre>\nsudo apt-get install gitolite\n</pre>\n\nCreate a user to access gitolite with, in this case I chose git since I don\'t like to type:\n\n <pre>\nsudo adduser \\\n    --system \\\n    --shell /bin/bash \\\n    --gecos \'git version control\' \\\n    --group \\\n    --disabled-password \\\n    --home /home/git \\\n    git\n</pre>\n\nNow login to the newly created user, and set the path, and move to its home directory:\n\n <pre>\nsudo su git\necho \"PATH=$HOME/bin:$PATH\" > ~/.bashrc\ncd\n</pre>\n\nRun the gitolite setup command with the public key you copied to the tmp directory to initialized the location for gitolite use. Then change the $REPO_UMASK to 0027 when it opens the .gitolite.rc for editing during the installation process. If it didn\'t open the file for any reason just open it up in vim (Note this is only if you\'d like to use gitweb):\n\n <pre>\ngl-setup /tmp/rachel.pub\n# change $REPO_UMASK = 0077; to $REPO_UMASK = 0027; # gets you \'rwxr-x---\'\n</pre>\n\nAfterward, it has made the gitolite-admin.git, testing.git repo and all other necessary files. Check to see that everything works by cloning the repo on the machine with the public/private key.\n\n <pre>\ngit clone git@<server>:gitolite-admin.git\n</pre>\n\nHere is a resource about the syntax for the config file and adding users.\n\n===== Install Gitweb =====\nThis is the tricky bit... Install gitweb and the highlight app. Gitweb is located at \'/usr/share/gitweb\'\n\n <pre>\nsudo apt-get install highlight gitweb\n</pre>\n\nEdit the gitweb config to the locations of the project list and repos, and add the highlighting bit at the end of the file:\n\n <pre>\nsudo vim /etc/gitweb.conf\n# change $projectroot to /home/git/repositories\n# change $projects_list to /home/git/projects.list\n# Add Highlighting at the end\n$feature{\'highlight\'}{\'default\'} = [1];\n</pre>\n\nChange gitolite instance to allow access for gitweb. First append www-data to git group so gitweb can access the repos, then change the permissions for git repos and the projects list, finally restart apache:\n\n <pre>\nsudo usermod -a -G git www-data\nsudo chmod g+r /home/git/projects.list\nsudo chmod -R g+rx /home/git/repositories\nsudo service apache2 restart\n</pre>\n\nFinally you need to tell gitolite which repo you want to show up in gitweb. To do this edit the gitolite.conf file from the gitolite-admin.git repo:\n\n <pre>\nrepo testing\n  RW+ = @all\n  R = gitweb\n</pre>\n\n\n\n\n== ## bNote-2013-02-07 ==\n\n=== World Largest Data Centers ===\n\n* [http://wikibon.org/blog/inside-ten-of-the-worlds-largest-data-centers/ Inside Ten of the World’s Largest Data Centers, on March 25, 2010]\n\n\n=== Enterprise Storage Related Articles ===\n\n----\n==== The future of enterprise storage? ====\n: Henry Baltazar / Senior Analyst / Storage & Systems / The 451 Group\n: [http://www.snia.org/sites/default/files2/sdc_archives/2010_presentations/general_session/HenryBaltazar_Cloud_Convergence_Consolidation.pdf Clouds, Convergence, and Consolidation - SNIA SDC 2010]\n\n----\n==== Tiering vs. Caching ====\n; Automated Storage Tiering\n: Has become a popular technology in the past two years with multiple vendors embedding this feature within their storage systems.\n: Allows storage systems to migrate \'hot\' data to high speed flash-based storage tiers, while simultaneously moving stale data to inexpensive SATA hard drives.\n; Storage Caching\n: Primarily being driven by NetApp (i.e. Flash Cache) and a few startups, storage systems with caching use solid state storage as a memory extension technology.\n: Caching proponents claim that the migration required to do tiering is inefficient and reacts too slowly to eliminate hotspots.\n----\n\n=== Comparison of NAS vs. SAN // Pain Points of Storage System ===\n\n어제 회의의 Action Items 중 하나로, 제가 맡았던 부분인\n\"현재 Arch.가 야기하는 기술적 Poin Points 추가 정리\" 관련 메일입니다.\n\n내용은 다음과 같이 크게 두 가지로 구분됩니다.\n\n1. NAS vs. SAN 비교\n\n2. 기존 Storage System의 Pain Points (HP StorageWorks 제품 Feature를 통해 바라본)\n  -> 여기에는 이전문님께서 말씀하시곤 했던 ILM(Info. Lifecycle Mgmt.)에 \n     대한 언급도 나오네요. (더 자세한 내용은 첨부파일에 있습니다)\n\n// 아래 내용들은  HP에서 만든 \"Storage Overview and Architecture (HP, Arvind Shrivastava)\" 자료를 참고하였습니다. (첨부 1)  다소 NAS에 불리한 점들이 많이 부각되어 보입니다만, 다르게 보면, \"기존 NAS의 구조적 한계점이 이러했으니 Performance Drop 없이도 Scale-out 되는 NAS로 가기 위해서는 이러한 단점들이 극복되어야하겠다\" 라는 내용으로 받아들일 수도 있겠습니다.\n\n\n==== Comparisons of SAN, NAS (and DAS) ====\n\n{| border=1\n|+ \'\'\'DAS vs. NAS vs. SAN\'\'\'\n| Direct Attached Storage (DAS)\n| Network Attached Storage (NAS)\n| Storage Area Network (SAN)\n|-\n|\n*+ Low cost solution\n*+ Simple to configure\n*- De-centralized storage management\n*- No storage consolidation\n*- No high availability\n*- Low performance\n|\n*+ Heterogeneous environment\n*+ Centralized storage management\n*- Low performance\n*- Limited scalability\n*- Network congestion during backups & restore\n*- Ethernet limitations\n|\n*+ Heterogeneous environment\n*+ Centralized storage management\n*+ High degree of fault tolerance\n*+ Best and superior performance\n*+ Storage consolidation\n*+ Fast and efficient backups and restores\n*+ Dynamic scalability\n*- Expensive solution for small setups\n|-\n|}\n\n\n\n{| border=1\n|+ \'\'\'NAS vs. SAN\'\'\'\n| Items\n| style=\"width:40%\" | Network Attached Storage (NAS)\n| style=\"width:40%\" | Storage Area Network (SAN)\n|-\n| Network\n| IP\n| Fibre Channel\n|-\n| Reliability\n| Unreliable\n: >> FCoE, iSCSI 도 있는데 정말 그렇게 Unreliable인걸까요? FC에 비해 상대적으로 그렇다는 의미로 받아들이면 될까요?\n| Reliable\n|-\n| CPU overhead\n| Extremely High\n: >> \"Extremely High\" 까지일 줄은 몰랐네요. Block IO에 비해 File System Operation까지 해야 하는 NAS의 경우 CPU overhead가 더 있을 수 있겠다는 생각은 했었지만, 생각보다 overhead가 꽤 있는지도 모르겠습니다. (NetApp NAS, EMC Isilon 확인 필요).\n| Extremely Low\n: >> 반대로, SAN이 상대적으로 low-overhead라고 해서 \"Extremely\" 까지일 줄은...\n|-\n| Blocks\n| Large number of small blocks\n| Large blocks of data\n|-\n| Backup\n| LAN backup\n| LAN-free backup\n|-\n| Access and control by application\n| Applications driven by universal access to files\n| Applications managing own data specs\n|-\n| Typical usage\n|\n* Office applications file serving\n* File sharing between office users and CAD users\n* Content management for web servers\n* Consolidation of \'home directories\' for controlled and centralized backup\n* Boot device for diskless server farms\n* Rich media such as audio or video streaming\n|\n* Physical storage consolidation of any application mix\n* Physical storage sharing for cluster configurations\n* Sharing of backup devices (libraries)\n* Long distance data mirroring and/or data replication on storage device level\n* Redundant, high speed disk access for large database servers\n|}\n\n<br/>\n----\n\n==== HP Storage Product - StorageWorks ====\n\n\'\'\'Pain Points\'\'\'\n* HP의 storage 제품인 StorageWorks에서 언급된 pain points를 요약해보면 다음과 같습니다.\n\n# 네트워크 트래픽 증가로 인한 스토리지 시스템 성능 저하 (여기서 WAN을 언급하는 것으로 보아 물리적으로 멀리 떨어진 스토리지 간의 트래픽을 언급하는 것으로 보입니다)\n# 저장된 data의 대부분을 차지하는 (operational data에 비해 3~4배 이상 크기) reference data의 지속적 증가 (여기서 reference data라는 것은, 당장 operation되고 있지는 않지만, 이전에 생성되어 사용된 적은 있으나 폐기되지 않고 저장되어 있는, 그렇게 계속 쌓여가는 data들을 의미하는 것으로 보여집니다. 한마디로, operational data가 hot-data라면, reference data는 cold-data라고 볼 수 있을 것 같습니다)\n# 관리되고 저장된 data의 증가에 따라 속도 저하 및 TCO 증가\n# hetero한 사양 및 상태로 여러 곳에 흩어져있는 storage system의 관리\n\n\n\n\'\'\'HP StorageWorks - Reference Information Storage System\'\'\'\n:* 일종의 ILM (Information Lifecycle Management)\n:* Cost 절감 위해 application server/storage로부터 잘 사용되지 않는 static data 및 중복 data 감소.\n:* data의 integrity 보장.\n:* Power indexing 및 Grid computing을 통해 훨씬 빠른 search/retrieval을 달성하겠다.\n\n[[File:20130207 HP Storage Product Slides Scale is the problem .png|500px]]\n\n\n* Changing digital storage demands \n:* Majority is reference information (operational data 보다는 reference 정보가 저장된 data의 대부분 차지)\n\n[[File:20130207 HP Storage Product Slides - Changing digital storage demands .png|500px]] \n\n\n* WAN acceleration\n:* WAN을 통해 연결되는 스토리지 트래픽 감소 기술\n(100배 이상 빠른 throughput 달성 목표)\n{| border=1\n| Problems\n| style=\"width:40%\" | HP\'s solution\n| style=\"width:40%\" | What it does\n|-\n| Not enough bandwidth\n|\n* Scalable data referencing (SDR)\n|\n* Remove repeated bytes from the WAN for all TCP traffic\n|-\n| TCP chattiness and inefficiency\n|\n* Virtual Window Expansion\n* High speed TCP optimizations\n|\n* Optimizes performance of TCP\n* Enables much higher throughput on \"LFNs\"\n|-\n| Application chattiness and inefficiency\n| Transaction prediction (CIFS, MAPI)\n| Optimizes the performance of specific applications\n|}\n\n== ## bNote-2013-02-06 ==\n\n\n=== Defining: Target Segment, Product, Architecture ===\n금일 회의때 논의한 내용을 정리합니다.\n\n\n\'\'\'Consensus\'\'\'\n\n* Big data 시장만 보면 현재로서는 시장이 크지 않다. \n\n* HW Box를 같이 팔아야 한다. 그렇지 않으면 돈이 안된다.\n\n* Google, NHN등 big data center player에게 팔것은 아니다.\n\n* 삼성이 Enterprise Storage 사업 진입한다고 가정 (사업 진입유무를 우리가 고민하지 말자)\n\n* Large-scale system에서 data/node scale이 증가할수록 manageability issue가 커진다.\n\n* storage 신사업 초기에는 기존 infra에 대한 당사 제품의 compatibility가 매우 중요할 것이다. 이 부분은 신사업추진단이 고민할 것이고 우리는 2nd/3rd generation에 해당하는 innovative architecture에 집중한다.\n\n* new architecture는 고객의 현재/미래의 pain point에 기반해야 한다.\n\n \n\'\'\'Pain Point (고객입장)\'\'\'\n\n* \"Google같은 flexible/efficient data center 를 우리 회사 scale에 맞게 내부에서 소유하고 관리하고 싶습니다.\" (Google-in-a-box)\n\n* \"여러 인프라에 흩어진 기업데이터를 통합 관리하고 싶습니다.\" (for bigdata analytics?)\n\n* \"어떠한 경우에도 data loss가 없고 available한 불멸의 스토리지를 소유하고 싶습니다.\"\n\n* \"우리가 소유한 모든 infra를 통합관리하는 극강의 manageability가 필요합니다. 충분히 자동화되고 self-healing되어 관리자 1명당 관리노드의 갯수를 혁신적으로 올릴 수 있으면 좋겠습니다.\" (ML, Brain-inspired computing 적용가능성 있음)\n\n* replication 기술 (InformationWeek 자료에서 니즈 언급)\n\n\n\'\'\'Big Question\'\'\'\n\n* SAN ? NAS ? SAN+NAS ?\n\n* Server+Storage ?\n\n* up to which scale does the distribute architecture need to be expanded?\n\n* 고객이 별도의 big data platform (HW+SW) 을 구매할까? (in addition to already owned infra)\n\n* SW-defined storage 시대가 올까?\n\n* 왜 Google/NHN은 직접 data center를 buildup 했을까? 왜 EMC를 안쓸까?\n\n* InformationWeek 자료의 기술선호도가 2018년에는 어떻게 바뀔까?\n\n* Replication 기술이 중요한만큼 매우 어려울까?\n\n* 2018년에는 Cloud Storage 의존성이 어떻게 될까?\n\n* Data center model이 어떻게 다를까? : Google vs. Amazon (Fusion-IO구매) vs. Apple (Isilon구매)\n\n* service fee 로 먹고사는 biz model이 나올 수 있나?\n\n* Hadoop + Dedup? (data가 있는곳에 processing을! 이라는 hadoop philosophy가 dedup에 의해 어떤 영향을 받는가?)\n\n* block I/F, file I/F 이후 new interface가 어떻게 확산될까? 왜 object I/F는 확산되지 못했을까? \n\n* brain-like한 I/F는 어떤 형태가 될까?\n\n\n\'\'\'Action Items\'\'\'\n\n* 이: Google의 data center evolution history\n\n* 서: SAN/NAS, Converged/Storage, 분산 scale(100s? 1000? 10000?)\n\n* 신+구: 분산-scale storage에 분산-scale dedup이 적용되면 architecture가 어떻게 바뀌나? dedup manager의 위치가 어디가 되어야 하나?\n\n* 융: 현 architecture가 야기하는 기술적인 Pain Point 추가정리\n\n\n\'\'\'Check this out\'\'\'\n* 451 report provided by Seo-C: Automatic Tiering is the very hot technology\n\n== ## bNote-2013-02-05 ==\n\n\n=== IOWA:: machine learning ===\n\nk-addr-group\n\ndistance calculation and clustering\n\nhamming distance\n\n\n[[ bsc.iowa.lsp.addr.access_ptrn ]]\n\n\n[http://en.wikipedia.org/wiki/Levenshtein_distance Levenshtein distance]\n\n\n <pre>\n__k_addr_group__ 0 : [720616224, 3282792808, 3282793000, 3282793096]\n__k_addr_group__ 1 : [728218248, 730235992, 817521776, 3765476016]\n__k_addr_group__ 2 : [721421456, 724475672, 724475864, 3712136296]\n__k_addr_group__ 3 : [724475960, 828267560, 895981360, 3277437312]\n__k_addr_group__ 4 : [725469848, 726381104, 726381496, 726793728]\n__k_addr_group__ 5 : [723112360, 730730224, 3677713128, 3773683448]\n__k_addr_group__ 6 : [725235304, 828274704, 896778072, 3277440104]\n__k_addr_group__ 7 : [720987560, 724481496, 724727104, 725008864]\n__k_addr_group__ 8 : [545503608, 896780128, 3069345832, 3076725824]\n__k_addr_group__ 9 : [721386480, 721386960, 3752324048, 3767545408]\n__k_addr_group__ 10 : [725420616, 726671672, 727735192, 3066446120]\n__k_addr_group__ 11 : [721030800, 721030896, 721411176, 827916488]\n...\n__k_addr_group__ 245 : [725218552, 822170744, 870855200, 3642881480]\n__k_addr_group__ 246 : [457025608, 722055120, 3685634288, 3779707504]\n__k_addr_group__ 247 : [722055216, 722055312, 722055376, 725157784]\n__k_addr_group__ 248 : [721285200, 721285296, 728117992, 728118184]\n__k_addr_group__ 249 : [725218680, 725218808, 725218936, 725219032]\n</pre>\n\n\n\n=== Python:: numerical sort for string list ===\n\n <pre>\n\n>>> typeof(mlist)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name \'typeof\' is not defined\n>>> help(mlist)\n\n>>> mlist = {}\n>>> mlist[0] = []\n>>> mlist[1] = []\n>>> mlist[0] = [\"11243\", \"92\", \"2132\", \"42\"]\n>>> mlist[1] = [\"243\", \"892\", \"19132\", \"3242\"]\n>>> mlist[0] = [ int(x) for x in mlist[0] ]\n>>> mlist[0]\n[11243, 92, 2132, 42]\n>>> mlist[1]\n[\'243\', \'892\', \'19132\', \'3242\']\n>>> mlist[1] = [ int(x) for x in mlist[1] ]\n>>> mlist[1]\n[243, 892, 19132, 3242]\n>>> mlist   \n{0: [11243, 92, 2132, 42], 1: [243, 892, 19132, 3242]}\n>>> mlist[0].sort()\n>>> mlist\n{0: [42, 92, 2132, 11243], 1: [243, 892, 19132, 3242]}\n>>> mlist[1].sort()\n>>> mlist\n{0: [42, 92, 2132, 11243], 1: [243, 892, 3242, 19132]}\n>>> \n\n</pre>\n\n== ## bNote-2013-02-04 ==\n\n\n\n=== CWD ===\n\n <pre>\nb@ub04:[anal] $ pwd\n/x/iowa/now/tracelog_analysis.v02.Q_phase.2_mysqld/anal\nb@ub04:[anal] $ l\ntotal 36\ndrwxrwxr-x 8 b b 4096 Feb  4 19:23 ./\ndrwxrwxr-x 5 b b 4096 Feb  4 19:23 ../\n-rw-rw-r-- 1 b b  127 Jan 30 19:42 .bdx.0100.y.exec_iowa_for_all_cases.sh\ndrwxrwxr-x 2 b b 4096 Jan 30 19:38 .tools/\nlrwxrwxrwx 1 b b   27 Jan 30 15:55 .tracelog.A.addr -> ../preproc/.tracelog.A.addr\nlrwxrwxrwx 1 b b   27 Jan 30 18:48 .tracelog.R.addr -> ../preproc/.tracelog.R.addr\nlrwxrwxrwx 1 b b   27 Jan 30 18:48 .tracelog.W.addr -> ../preproc/.tracelog.W.addr\ndrwxrwxr-x 2 b b 4096 Jan 30 19:39 t00.R.xLBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:43 t01.R.1LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t02.R.100LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t03.R.10000LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t04.R.20000LBA/\n</pre>\n\n=== IOWA.data.tpcc_250gb_48h.mysql.R.addr:: Offset 100K, Len 1K ===\n\n* [[IOWA.data.tpcc_250gb_48h.mysql.R.addr:: Offset 100K, Len 1K]]\n\n=== Frequent Item Detection/Discovery ===\n\n==== Stanford Open Book - Mining of Massive Datasets ((B.GOOD)) ====\n\n* MMD Book [http://i.stanford.edu/~ullman/mmds.html]\n\n: The book has now been published by Cambridge University Press. The publisher is offering a 20% discount to anyone who buys the hardcopy Here. By agreement with the publisher, you can still download it free from this page. Cambridge Press does, however, retain copyright on the work, and we expect that you will obtain their permission and acknowledge our authorship if you republish parts or all of it. We are sorry to have to mention this point, but we have evidence that other items we have published on the Web have been appropriated and republished under other names. It is easy to detect such misuse, by the way, as you will learn in Chapter 3.\n: Anand Rajaraman (@anand_raj) and Jeff Ullman\n\n: Ch 3. Finding Similar Items [http://i.stanford.edu/~ullman/mmds/ch3.pdf]\n:: 3.1 Applications of Near-Neighbor Search\n::: 3.1.1 Jaccard Similarity of Sets\n:: 3.2 Shingling of Documents\n:: 3.3 Similarity-preserving Summaries of Sets\n:: 3.4 Locality-sensitive Hashing for Documents\n:: 3.5 Distance Measures\n:: 3.6 The Theory of Locality-sensitive Functions\n:: 3.7 LSH Families for Other Distance Measures\n:: 3.8 Applications of Locality-sensitive Hashing\n:: 3.9 Methods for High Degrees of Similarity\n\n: Ch 6. Frequent Itemsets [http://i.stanford.edu/~ullman/mmds/ch6.pdf]\n:: 6.1 The Market-Basket Model\n:: 6.2 Market Baskets and the A-Priori Algorithm\n:: 6.3 Handling Larger Datasets in Main Memory\n:: 6.4 Limited-pass Algorithms\n:: 6.5 Counting Frequent Items in a Stream\n\n==== Orange - open source data visualization and analysis tool ====\n\nOpen source data visualization and analysis for novice and experts. Data mining through visual programming or Python scripting. Components for machine learning. Add-ons for bioinformatics and text mining. Packed with features for data analytics.\n\n\n==== SAX - Symbolic Aggregate Approximation ====\n\nSAX (Symbolic Aggregate approXimation) Homepage [http://www.cs.ucr.edu/~eamonn/SAX.htm]\n\nSAX is the first symbolic representation for time series that allows for dimensionality reduction and indexing with a lower-bounding distance measure. In classic data mining tasks such as clustering, classification, index, etc., SAX is as good as well-known representations such as Discrete Wavelet Transform (DWT) and Discrete Fourier Transform (DFT), while requiring less storage space. In addition, the representation allows researchers to avail of the wealth of data structures and algorithms in bioinformatics or text mining, and also provides solutions to many challenges associated with current data mining tasks. One example is motif discovery, a problem which we defined for time series data. There is great potential for extending and applying the discrete representation on a wide class of data mining tasks.\n\n\n==== The Amadeus motif discovery platform ====\n\nAmadeus Home [http://acgt.cs.tau.ac.il/amadeus/]\n\nAmadeus is a user-friendly software platform for genome-scale detection of known and novel motifs (recurring patterns) in DNA sequences, applicable to a wide range of motif discovery tasks. Amadeus outperforms extant tools and sets a new standard for motif discovery software in terms of accuracy, running time, range of application, wealth of output information and ease of use.\n\n==== Motif Detection ====\n\n* Python for Bioinformatics [http://telliott99.blogspot.kr/2009/06/motif-discovery.html]\n* MOODS: Motif Occurrence Detection Suite - ALGOrithmic Data ANalysis [http://www.cs.helsinki.fi/group/pssmfind/]\n* CMU CS Lecture -- Motif Detection [http://www.cs.cmu.edu/~epxing/Class/10810-05/Lecture6.pdf]\n\n==== Biopython ====\n\nBiopython is a set of freely available tools for biological computation written in Python by an international team of developers. [http://biopython.org/wiki/Main_Page Biopython]\n\n* Required Python Modules [http://www.scipy.org/more_about_SciPy NumPy / SciPy (SciPy.org)]\n** NumPy: N-dimensional Array Manipulations\n** SciPy: Scientific tools for Python\n\n* References\n# [http://biopython.org/DIST/docs/tutorial/Tutorial.pdf Biopython Tutorial]\n\n<br/>\n----\n\n=== RAM-SSD 하이브리드 Page Cache 아키텍쳐 :: WIPS 의견 대응 ===\n <pre>\n접수번호 : RN-201301-004-1\n\n명칭 : RAM-SSD 하이브리드 Page Cache 아키텍쳐\n\n기술 핵심 : RAM과 SSD를 이용하여 디스크 data를 caching하는 구조가\npage cache layer에서 동작하도록 함으로써 hot access를 check하고\naccess 정도에 따라 RAM 또는 SSD에 저장(tier-1~2)하거나 disk에서 access되도록 하는 것\n\n구성 : A) page cache에서 동작하는 RAN-SSD hybrid cache architecture\n      B) tier-1 page cache(RAM), tier-2 page cache(SSD)\n      C) page에 대한 hit pattern으로 page들을 tier-1,2로 class 지정\n\n\n--\n안녕하세요? 정명준입니다.\n다루는 내용이 많았음에도 깔끔하게 잘 정리해주시느라 고생이 많으셨습니다. ^_^\n꼭 추가되었으면 하는 내용과, 서술의 톤이 살짝 바뀌면 좋을만한 부분이 있어\n아래 보내주신 메일 본문에 추가하였습니다.\n(제 의견은 파란색으로 표시되어있습니다)\n\n1) \'기술 핵심\' 부분\n\n	\"RAM과 SSD를 이용하여 디스크 data를 caching하는 구조가 page cache layer에서 동작하도록 함으로써\" \n		-> 본 발명의 핵심 부분인 부분을 잘 적어주셨습니다.\n\n	\"hot access를 check하고 access 정도에 따라\"\n		-> 이 부분은 \"access pattern을 점검하여 rehit-potential에 따라\" 라는 표현으로 바뀌면 더욱 좋겠습니다.\n		기존에는 recently hot (recency) 위주로 cache될지 evict될지를 판단하였는데,\n		본 발명에서는 rehit potential을 보고 판단하겠다는 점이 다릅니다.\n		(once-hot data 라든지, periodicity 등을 파악)\n\n	\"RAM 또는 SSD에 저장(tier-1~2)하거나 disk에서 access되도록 하는 것\"\n		-> 여기에 \"page chunk 단위로 page tiering을 한다\"는 언급이 되었으면 좋겠습니다. (청구의 범위를 너무 제한하는 것이 아니라면요)\n\n\n2) \'구성\' 부분\n\n	아래 \'가~바\' 부분이 본 발명에서 청구하고자 하는 주요 내용들입니다.\n	이 중에서 \'가\' 부분이 보내주신 메일에서 A, B, C로 적어주신 \'구성\' 부분인데요,\n	대표 청구항을 적자면 아래 \'가~바\' 부분이 모두 필요할 것 같습니다.\n\n	가) RAM-SSD hybrid page cache architecture\n		page cache layer에서 동작하고;\n		tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는;\n		page tiering 기반의 hybrid page cache 아키텍쳐.\n\n	나) rehit-potential aware page classification, and page class based cache data placement\n\n	다) page tiering operation in host side\n\n	라) page tiering operation in SSD side\n		chunk I/O in SSD in {log-structured and/or parallelism-maximized} manner\n\n	마) TRIM ahead 메커니즘 (page chunk writes latency를 줄이기 위해 미리 공간 확보하는)\n\n	바) victim page chunk selection in SSD\n		oldest-first\n		most-mark-first\n		with upper-tier-backup\n		\n\n오후에 전화로 말씀 나누면서 설명 드리겠습니다.		\n감사합니다.\n정명준 드림.\n</pre>\n\n\n=== 서울대 2013-02-01 Meeting Minutes ===\n\n1. 실험 관련\n: HDD 개수와 map slot의 개수를 align하여 실험 진행 후 기존 결과와 함께 분석\n: <span style=\"color:blue\">>>네, 본 실험에서 나타난 \'재미있는\' 현상이 Hadoop 권장 환경 구성과 맞지 않아서 발생한 것이 아니라 Hadoop 시스템 근원적으로 가지고 있는 \'한계\' 때문이라는 것으로 주장이 되려면, HDD 수와 execution slot 간 alignment는 필수적일 것 같습니다. 잘 부탁드리겠습니다.[1]</span>\n\n: In-memory buffer 크기 변화를 통한 spill file 개수 및 phase elapsed time의 관계를 exponential expression 으로 분석\n: <span style=\"color:blue\">>> 아, 제 발음이 좋지 않았던 것 같습니다. ^^;; exponential expression이라기 보다는 extrapolation이 가능한 equation으로 나오면 좋겠다는 것으로 이해해주시면 좋겠습니다.  아마 이게 되려면 충분한 경우에 대해서 실험이 되어야 할 것인데, equation이 나오기만 한다면 무척 중요한 의미를 갖게 될 것 같습니다. 전체적인 phase elapsed time 뿐만 아니라 p.15에 색깔로 구분된 각 영역들(map - map/shuffle - shuffle - reduce)의 변화 양상도 잘 설명이 될 수 있다면 좋겠습니다.[2]</span>\n\n2. SSD 관련\n: <span style=\"color:blue\">>> 특히 보내주신 발표 슬라이드의 p.18에 언급된 SSD 사용 시 9.5% 감소 부분은 충격이었습니다. 이 현상을 유발한 원인이 명확하게 규명되고 (1), 그 규명된 원인이 기존에 알려지지 않았던 것이며 (2), 그것을 해소할 수 있는 방법도 같이 제시될 수 있다면 (3), 본 산학 과제가 더욱 큰 의미를 갖게 될 것 같습니다.[3]</span>\n\n: random write는 피하는 방향으로 SSD을 통한 하둡 수행 성능 향상이 나올 수 있도록 I/O contention의 오버헤드가 SSD 성능과 상쇄될 수 있도록 detail하게 실험.\n: network i/o와 disk i/o의 trade-off\n: <span style=\"color:blue\">>> 네, combine 등을 통해서 network overhead를 local overhead (local computation 및 local I/O)로 trade-off 시키게 되지만, 그 local overhead를 SSD로 상쇄시킬 수 있는 best 방법 및 예상 효과가 나온다면 매우 의미가 있을 것 같습니다.[4]</span>','utf-8'),(124,'== ## bNote-2013-02-28 ==\n\n=== 수원임직원주차장 방문예약 ===\n[https://www.sdigitalcity.com/security/visit/login.do 수원임직원주차장 방문예약]\n\n== ## bNote-2013-02-26 ==\n\n=== iowa.sidewinder ===\n\n* construction: filebench workload generator\n\n== ## bNote-2013-02-21 ==\n\n\n=== Daily.Kickoff ===\n\n* acsptrn outlook program\n:- kag (K-address-group)\n::- can \'K\' be a signature of a IO pattern?\n:::- what kind of implication can be extracted from \'KAG\'?\n::: : what size of chunk can have a meaning?\n::: : unit of tiering IO? unit of proactive caching? unit of IO dump size to the SSD?\n::- hamming distance (proper choice! why? difference value means the # of items to be added in cache) (older elements identified as a different one can be evicted or not, according to the cache space condition)\n:- outlook seek distance stat-dist\n:: - can \'seek distance\' be a metric for identifying hot zone?\n\n* cache simulation program\n:- hit ratio calculation, performance gain calculation\n\n* ML approach planning\n:- what to do for proactive data placement?\n::- what to do for static(?) macro insight?\n::- what to do for dynamic(?) micro prediction? (periodicity?)\n\n== ## bNote-2013-02-20 ==\n\n=== IOWA planning with Yanpei Chen Paper (SOSP 2011) ===\n\n\n\n\n\n\n\n=== Intel releases SSD cache acceleration software for Linux servers ===\n\n인텔이 Linux Server를 위한 CAS (Cache Acceleration Software) 기술을 Release했다는 소식입니다.\n\n이 CAS라는 기술은 작년에 인수한 Nevex사의 CacheWorks 기술에 기반하고 있습니다.\n\n특징으로 볼 수 있는 것은 기존 block cache의 문제점으로 지적되었던 DRAM에도 올라간 data가 SSD cache에도 올라가는 현상을 피하기 위해 초보적인 수준의 intelligence를 도입했다는 점입니다.\n\n(very hot data는 DRAM에 보내되, 덜 hot하지만 still I/O active data는 SSD로 나누어 보낸다는 것이 핵심입니다) \n\n\'\'\'Intel releases SSD cache acceleration software for Linux servers\'\'\'\n\nSSD-based CAS for Linux servers can offer up to 18 times the performance for read-intensive applications, Intel says. \n\n[http://www.infoworld.com/d/open-source-software/intel-releases-ssd-cache-acceleration-software-linux-servers-212673?page=0,0 InfoWorld News FEBRUARY 12, 2013]\n\n\n발췌 내용:\n...\nTo ensure it does not duplicate work already being performed in DRAM, the CAS software takes control of all data access. The hottest data is placed on DRAM, while less hot, but still I/O active data, is placed on SSD.\n\nIntel\'s CAS software provides cooperative integration between the standard server DRAM cache and the Intel SSD cache, creating a multi-level cache that optimizes the use of system memory and automatically determines the best cache level for active data.\n...\n\n== ## bNote-2013-02-19 ==\n\n\n=== Workload generation and tracing (with filebench, blktrace) ===\n\n* target workloads\n:- fileserver.f\n:- mongo.f\n:- netsfs.f\n:- networkfs.f\n:- oltp.f\n:- tpcso.f\n:- varmail.f\n:- videoserver.f\n:- webserver.f\n\n\n* video server workload (running time: 3600 seconds) (20130219_143400)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\n#>> cleaning up \'30.tmp/\' directory\n#>> ready for generating workload specified in \'20.conf/.tconf.sh\'\n#>> please run iotrace (blktrace) in \'34.iotrace/\'\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\n#>> [20130219_143400] waiting for iotrace getting started \n.................................\n----------------------------------------------------\n#>> Ok, now start workload generation\nstarted: 20130219_143433\n----------------------------------------------------\nFilebench Version 1.4.9.1\n 3599: 0.000: Allocated 170MB of shared memory\n 3599: 0.001: Eventgen rate taken from variable\n 3599: 0.001: Video Server Version 3.0 personality successfully loaded\n 3599: 0.001: Creating/pre-allocating files and filesets\n 3599: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n 3599: 0.002: Re-using fileset passivevids.\n 3599: 0.002: Creating fileset passivevids...\n 3599: 2227.782: Preallocated 104 of 194 of fileset passivevids in 2228 seconds\n 3599: 2227.800: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n 3599: 2227.800: Re-using fileset activevids.\n 3599: 2227.800: Creating fileset activevids...\n 3599: 2227.805: Preallocated 32 of 32 of fileset activevids in 1 seconds\n 3599: 2227.805: waiting for fileset pre-allocation to finish\n 3974: 4324.570: Starting 1 vidreaders instances\n 3974: 4324.599: Starting 1 vidwriter instances\n 3976: 4324.780: Starting 1 vidwriter threads\n 3975: 4324.790: Starting 48 vidreaders threads\n 3599: 4325.877: Running...\n 3599: 7926.189: Run took 3600 seconds...\n 3599: 7926.234: Per-Operation Breakdown\nserverlimit          423882ops      118ops/s   0.0mb/s    305.7ms/op    60431us/op-cpu [0ms - 2292ms]\nvidreader            423983ops      118ops/s  29.4mb/s    407.1ms/op    39674us/op-cpu [0ms - 2292ms]\nreplaceinterval      17ops        0ops/s   0.0mb/s  10000.1ms/op        0us/op-cpu [10000ms - 10000ms]\nwrtclose             17ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               17ops        0ops/s  47.2mb/s 134156.2ms/op 17297647us/op-cpu [3919ms - 421911ms]\nwrtopen              18ops        0ops/s   0.0mb/s     26.3ms/op     1111us/op-cpu [0ms - 247ms]\nvidremover           18ops        0ops/s   0.0mb/s  60936.6ms/op   281667us/op-cpu [69ms - 181905ms]\n 3599: 7926.234: IO Summary: 424053 ops, 117.782 ops/s, (118/0 r/w),  76.6mb/s,      0us cpu/op, 415.0ms latency\n 3599: 7926.234: Shutting down processes\n----------------------------------------------------\nfinished: 20130219_164641\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n\n* video server workload (running time: 60 seconds) (20130219_111030)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\n#>> cleaning up \'30.tmp/\' directory\n#>> ready for generating workload specified in \'20.conf/.tconf.sh\'\n#>> please run iotrace (blktrace) in \'34.iotrace/\'\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\n#>> [20130219_111030] waiting for iotrace getting started \n...........\n----------------------------------------------------\n#>> Ok, now start workload generation\nstarted: 20130219_111041\n----------------------------------------------------\nFilebench Version 1.4.9.1\n 2322: 0.000: Allocated 170MB of shared memory\n 2322: 0.001: Eventgen rate taken from variable\n 2322: 0.001: Video Server Version 3.0 personality successfully loaded\n 2322: 0.001: Creating/pre-allocating files and filesets\n 2322: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n 2322: 0.720: Removed any existing fileset passivevids in 1 seconds\n 2322: 0.720: making tree for filset /mnt/hdd_4/filebench/passivevids\n 2322: 0.721: Creating fileset passivevids...\n 2322: 5398.571: Preallocated 104 of 194 of fileset passivevids in 5398 seconds\n 2322: 5398.580: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n 2322: 5441.697: Removed any existing fileset activevids in 44 seconds\n 2322: 5441.698: making tree for filset /mnt/hdd_4/filebench/activevids\n 2322: 5441.698: Creating fileset activevids...\n 2322: 7548.330: Preallocated 32 of 32 of fileset activevids in 2107 seconds\n 2322: 7548.330: waiting for fileset pre-allocation to finish\n 2939: 9496.045: Starting 1 vidreaders instances\n 2939: 9496.077: Starting 1 vidwriter instances\n 2941: 9496.275: Starting 1 vidwriter threads\n 2940: 9496.294: Starting 48 vidreaders threads\n 2322: 9497.515: Running...\n 2322: 9557.521: Run took 60 seconds...\n 2322: 9557.540: Per-Operation Breakdown\nserverlimit          22911ops      382ops/s   0.0mb/s    116.4ms/op   129288us/op-cpu [0ms - 5662ms]\nvidreader            23044ops      384ops/s  95.8mb/s     11.3ms/op     3815us/op-cpu [0ms - 2135ms]\nreplaceinterval      0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtclose             0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtopen              1ops        0ops/s   0.0mb/s      0.1ms/op        0us/op-cpu [0ms - 0ms]\nvidremover           1ops        0ops/s   0.0mb/s   5112.0ms/op   110000us/op-cpu [5111ms - 5111ms]\n 2322: 9557.540: IO Summary: 23046 ops, 384.062 ops/s, (384/0 r/w),  95.8mb/s,      0us cpu/op,  11.6ms latency\n 2322: 9557.540: Shutting down processes\n----------------------------------------------------\nfinished: 20130219_135000\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n\n* video server workload (running time: 3600 seconds) (20130218_161548)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\nrm: cannot remove `wloadsrc.f\': No such file or directory\n#>> [20130218_161548] waiting for iotrace getting started \n........\n----------------------------------------------------\n#>> Ok, now start workload generation\n----------------------------------------------------\n20130218_161556\nFilebench Version 1.4.9.1\n30523: 0.000: Allocated 170MB of shared memory\n30523: 0.001: Eventgen rate taken from variable\n30523: 0.001: Video Server Version 3.0 personality successfully loaded\n30523: 0.001: Creating/pre-allocating files and filesets\n30523: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n30523: 5.754: Removed any existing fileset passivevids in 6 seconds\n30523: 5.754: making tree for filset /mnt/hdd_4/filebench/passivevids\n30523: 5.755: Creating fileset passivevids...\n30523: 5296.248: Preallocated 104 of 194 of fileset passivevids in 5291 seconds\n30523: 5296.261: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n30523: 5296.331: Removed any existing fileset activevids in 1 seconds\n30523: 5296.331: making tree for filset /mnt/hdd_4/filebench/activevids\n30523: 5296.331: Creating fileset activevids...\n30523: 8052.228: Preallocated 32 of 32 of fileset activevids in 2756 seconds\n30523: 8052.228: waiting for fileset pre-allocation to finish\n31345: 10248.763: Starting 1 vidreaders instances\n31345: 10248.804: Starting 1 vidwriter instances\n31346: 10248.868: Starting 48 vidreaders threads\n31347: 10249.084: Starting 1 vidwriter threads\n30523: 10250.088: Running...\n30523: 13850.427: Run took 3600 seconds...\n30523: 13850.506: Per-Operation Breakdown\nserverlimit          1228002ops      341ops/s   0.0mb/s    105.3ms/op    71013us/op-cpu [0ms - 2410ms]\nvidreader            1228102ops      341ops/s  85.3mb/s    140.0ms/op    46691us/op-cpu [0ms - 1919ms]\nreplaceinterval      0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtclose             0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtopen              1ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nvidremover           1ops        0ops/s   0.0mb/s     68.8ms/op    70000us/op-cpu [68ms - 68ms]\n30523: 13850.506: IO Summary: 1228104 ops, 341.104 ops/s, (341/0 r/w),  85.3mb/s,      0us cpu/op, 140.0ms latency\n30523: 13850.506: Shutting down processes\n20130218_200648\n----------------------------------------------------\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n== ## bNote-2013-02-18 ==\n\n=== Misc. ===\n\n* [https://h30613.www3.hp.com/media/files/downloads/Non-FilmedSessions/TB2216_Becoming.pdf Become a Flash expert in minutes! SAMSUNG]\n\n* [http://static.usenix.org/event/lsf08/tech/FS_shepler.pdf Filebench Spencer Shepler]\n\n\n=== blktrace knowledge ===\n\n\n <pre>\n  8,48   4     1657     0.426331441  3443  A   W 2593896448 + 1024 <- (8,49) 2593894400\n  8,48   4     1658     0.426332295  3443  Q   W 2593896448 + 1024 [flush-8:48]\n  8,48   4     1659     0.426334674  3443  G   W 2593896448 + 1024 [flush-8:48]\n  8,48   4     1660     0.426503502  3443  A   W 2593897472 + 1024 <- (8,49) 2593895424\n  8,48   4     1661     0.426503961  3443  Q   W 2593897472 + 1024 [flush-8:48]\n  8,48   4     1662     0.426505840  3443  G   W 2593897472 + 1024 [flush-8:48]\n  8,48   4     1663     0.426666260  3443  A   W 2593898496 + 1024 <- (8,49) 2593896448\n  8,48   4     1664     0.426666707  3443  Q   W 2593898496 + 1024 [flush-8:48]\n  8,48   4     1665     0.426668036  3443  G   W 2593898496 + 1024 [flush-8:48]\n  8,48   4     1666     0.426829266  3443  A   W 2593899520 + 1024 <- (8,49) 2593897472\n  8,48   4     1667     0.426829714  3443  Q   W 2593899520 + 1024 [flush-8:48]\n  8,48   4     1668     0.426831154  3443  G   W 2593899520 + 1024 [flush-8:48]\n</pre>\n\n <pre>\n2593896448 + 1024 <- (8,49) 2593894400\n2593896448 - 2593894400 = 2048\n\n2593897472 + 1024 <- (8,49) 2593895424\n2593897472 - 2593895424 = 2048\n\n2593898496 + 1024 <- (8,49) 2593896448\n2593898496 - 2593896448 = 2048\n\n2593899520 + 1024 <- (8,49) 2593897472\n2593899520 - 2593897472 = 2048\n</pre>\n\n\'\'\'2048\'\'\' is the offset of the partition (/dev/sdd1) from the start point of the block device (/dev/sdd)\n\n <pre>\nblusjune@radiohead:[20130219_143400--videoserver] $ sudo fdisk -l /dev/sdd\n[sudo] password for blusjune: \n\nDisk /dev/sdd: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x000c526d\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdd1            2048  3907026943  1953512448   83  Linux\n</pre>\n\n== ## bNote-2013-02-15 ==\n\n\n\n\n=== Hierarchical Temporal Memory ===\n\n* Numenta Hierarchical Temporal Memory - including HTM Cortical Learning Algorithms Version 0.2.1, September 12, 2011 [https://www.numenta.com/htm-overview/education/HTM_CorticalLearningAlgorithms.pdf Numenta Hierarchical Temporal Memory]\n\n# HTM Overview\n# HTM Cortical Learning Algorithms\n# Spatial Pooling Implementation and Pseudocode\n# Temporal Pooling Implementation and Pseudocode\n# Appendix A: A Comparison between Biological Neurons and HTM Cells\n# Appendix B: A Comparison of Layers in the Neocortex and an HTM Region\n\n==== About Numenta ====\n\n: Numenta, Inc. (www.numenta.com) was formed in 2005 // to develop HTM technology for both commercial and scientific use\n: Use of Numenta\'s software and intellectual property is free for research purposes.\n: Numenta will generate revenue by selling support, licensing software, and licensing intellectual property for commercial deployments.\n: Numenta is based in Redwood City, California. (Privately funded)\n\n\n==== Chapter 1. HTM Overview ====\n\n: Hierarchical Temporal Memory (HTM) is a machine learning technology that aims to capture the structural and algorithmic properties of the neocortex.\n\n: The neocortex is the seat of intelligent thought in the mammalian brain.\n\n: HTM\n\n=== SSD dirty script (iometer) ===\n\n[http://www.iometer.org/doc/downloads.html IOmeter dirty SSD script]\n\n\n=== BDP 근거 ===\n\n* 48시간 TPC-C 벤치마크한 blktrace log: 54.4GB\n* 1차 pre-processing: 52.2 GB\n\n== ## bNote-2013-02-14 ==\n\n\n=== 특허화 ===\n\n==== Patent #3 ====\n\n* (data + io insight) packaging 관련 특허\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n==== Patent #4 ====\n\n* IOWA based Proactive Data Placement 자체에 대한 특허\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 clue를 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보이는 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# clue, as simple as possible\n::::# clue, as specific as possible\n::::# clue, efficiently traversable - corresponding clue case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# clue, easily extensible - clue 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n==== Patent #5 ====\n\n* SSD Retention Time Controlling for Caching/Tiering 관련 특허\n\n\n==== Patent #6 ====\n\n* HML (Hierarchical ML)에서 FPGA, ASIC 등을 이용해서 Acceleration할 수 있는 방법은 없을까?\n\n== ## bNote-2013-02-12 ==\n\n\n\n=== IBdM 팀 회의 (15:40 ~ 17:00, 실험실) ===\n\n* 과제목표 (현재안): Intelligent Big Data Management 문제를 “H-ML을 통해” 남들보다 (EMC보다) 훨씬 잘 풀겠다!\n:- 요구사항\n::- 성공적으로 연구를 진행했을 경우의 최종 “기대치” 가 order-of-magnitude 높아야 함, 해결하는 문제가 수년간의 심도깊은 연구를 요구하여야 함\n::- I/O 성능 혁신 <- Data Placement 문제 : H-ML을 통한 Proactive data placement [70x improvement]\n::- 저장 효율 혁신 <- Data Reduction 문제 : H-ML을 통한 Clustered Deduplication [XXXX improvement] “Big Data를 Small Data로 만드는 기법”\n\n* Big Data Era에는 기존 Data Management 문제가 어떻게 달라지나?\n:- data양 폭증, IT budget증가 제한적 -> data reduction을 통한 IT 비용 절감 -> Data Deduplication 기술\n:- data capacity planning이 더 어려워짐 -> 필요에 따라 on-demand로 용량/성능 증설 요구 -> Scale-out 기술\n:- 비정형 data의 증가 -> NoSQL (?), Stream Processing (?)\n:- 비정형 data를 표현하는 데이터의 요구 증가 -> tagged data(metadata) 관리기술\n:: (data에 tag되어 data의 과거 access이력, access pattern, 복제/분열 history 등 intelligence정보를 저장)\n:- decentralized data(?) -> 자동화된 데이터 관리 중요 -> self-managed architecture 기술\n\n* 우리의 차별화\n:- Tool : hierarchical machine learning\n:- Input Data : 풍부한 tagged data (I/O workload, File I/O log, Application-level log, …)\n:- Motto\n::- Big Data의 근본적 이해를 통한 Big Data 관리기법 지능화\n::- Big Data의 생성, 확장, 분열, 복제, 소멸 등 full-cycle에 대한 근본적 이해\n\n* Action Item\n:- H-ML 기법 적용을 위한 data placement 문제 formulate, 기대치 estimation [융]\n:- H-ML 기법 적용을 위한 deduplication 문제 formulate, 기대치 estimation [신,구]\n:- “big data era에는 기존 data 관리 문제가 어떻게 달라지나?” [all]\n\n=== IOWA based Proactive Data Placement ===\n\n심상무님 지시.\n\n---- \n <pre>\n\nTM-20130212-IOWAPDP\n\n[ IO Workload Analysis 기반의\nProactive Data Placement ]\n\n\n\n2013-02-12\n\n\n\nData-intensive Storage\nIntelligence Group\nIntelligent Computing Lab\n</pre>\n\n----\n==== expected benefit ====\n\n <pre>\n이전에 얘기한 바가 있는데, proactive data placement와\nreactive data placement의 논리적인 비교를 통해\n원리상 기대되는 benefit의 크기 규모를 얘기할 수 있을까요?\n\n> 네, IO 성능 Penalty 최소화 측면과 SSD cache media 효율성 향상 측면에서\n  가늠해볼 수 있습니다. \n  \n  가) IO penalty 최소화\n    : 기존 방식은 미리 cache (혹은 fast-tier)에 proactive하게 올린다는 개념이 없었기 때문에\n      일단 최초 한 번은 HDD로부터 access되는 것이 필요하며,\n      그만큼 성능 penalty를 겪을 수 밖에 없습니다.\n      문제는 이렇게 성능 penalty를 겪는 경우가 얼마나 많이 존재하느냐입니다.\n\n      특히, \n      many-times-of-rehit 되는 small-number-of-data와 ................... (1)\n      small-times-of-rehit 되는 large-number-of-data 의 ................... (2)\n      분포 양상이 중요한데,\n      (1), (2) 각각이 전체 IO 에 얼만큼의 contribution을 해내는지가\n      proactive 방식과 reactive 방식과의 성능 차이에 영향을 미칩니다.\n\n      7200rpm의 HDD의 Read latency는 8500us,\n      SLC 기반의 NAND의 Read latency는 25us 라고 하고,\n      총 1,000 번의 IO access가 있었다고 했을 때,\n      A, B, C 각 workload 경우에 따른 IO 처리 소요 시간을 계산함으로써\n      reactive data placement 대비 proactive data placement의\n      성능 향상 정도를 가늠해보겠습니다.\n\n      (참고로, 아래 성능 비교 계산은 reactive Vs. proactive의 \n       핵심 logic에만 집중한 highly-abstracted 계산으로서,\n       블럭 레이어에서의 IO scheduling, HDD의 read-ahead, SATA/PCIe 버스 구조,\n       스토리지 디바이스 내부 구조 등 세부 시스템 적인 고려는 전혀 되어 있지 않음.\n       또한 cache media로 사용되는 SSD의 용량은 충분히 크다고 가정하였음)\n\n      A) 1개의 addr가 1,000번의 access를 다 받아낸 경우 (active range: 1) // 극단적 (1)의 경우\n         reactive 방식: 1 x 8500us + 999 x 25us = 33,475us\n         proactive 방식 (예측 적중률 100%): 1000 x 25us = 25,000us // IO 시간 1.3배 단축\n         proactive 방식 (예측 적중률 100% 미만): 1 x 8500us + 999 x 25us = 33,475us // IO 시간 단축 없음\n\n      B) 50개 addr가 400번의 access를, 150개 addr가 600번의 access를 받은 경우 (active range: 200)\n         reactive 방식: (50 x 8500us + 350 x 25us) + (150 x 8500us + 450 x 25us) = 1,720,000us\n         proactive 방식 (예측 적중률 100%): 1000 x 25us = 25,000us // IO 시간 68배 단축\n         proactive 방식 (예측 적중률 90%): 900 x 25us + 100 x 8500us = 872,500us // IO 시간 1.97배 단축\n\n      C) 1,000개 addr가 1번씩 다 access된 경우 (active range: 1,000) // 극단적 (2)의 경우\n         reactive 방식: 1000 x 8500us = 8,500,000us\n         proactive 방식 (100% 예측 적중시): 1000 x 25us = 25,000us // IO 시간 340배 단축\n   \n  나) SSD cache 미디어 효율성 향상\n    : 기존에는 미래에 다가올 IO에 대한 예측을 가지고 있지 않기 때문에\n      무조건 hit된 data들을 cache에 올리게 되며, 이후 자주 access될 data들과\n      그렇지 않은 data들이 모두 cache되므로, cache 공간의 낭비가 커집니다.\n      특히, data 규모가 커져서 SSD cache를 쓸 필요가 있는 경우,\n      SSD에 대한 write 및 erase를 많이 유발하게 되는 기존 방식은\n      SSD cache의 성능을 저하 시키는 중요한 요인이 될 수 있습니다.\n      이 측면에 대한 수치적 검증은, SSDsim 등을 이용하거나,\n      실제 SSD에 주어진 workload를 인가하여 IO 성능을 측정함으로써\n      실험적으로 증명해볼 수 있을 것 같습니다.\n</pre>\n\n----\n\n==== what is the heart of proactive data placement? ====\n <pre>\n대개 caching 알고리즘들은 데이터 I/O 패턴만 이용해서 cache에 남겨둘 데이터를 고르잖아요.\nproactive data placement 기술의 핵심은 미래의 데이터 I/O 패턴을 예측하는 것이라고 할 수 있나요?\n\n> 네, 맞습니다.\n  proactive data placement 기술의 핵심은 다가올 data IO 패턴 예측입니다.\n\n  상무님 말씀하신 것처럼,\n  기존의 cache 알고리즘들은 이미 cache된 data들에 대한\n  replacement (즉 eviction) 알고리즘이라고 보시면 됩니다.\n  즉, 일단은 access된 data들을 모두 cache에 올려두었다가,\n  그 cahced data들 중에서 \'미래에 덜 필요할 것\'같은 데이터를 골라내는 것입니다.\n  그러나 cache되어야 할 data를 예측하여 \'proactive 하게 미리\'\n  cache에 가져다놓는 것은 고려되어 있지 않습니다.\n  (이렇게 하지 못한 이유는, CPU L2/L3 cache와 DRAM 간의 cache이다보니\n   복잡한 알고리즘을 돌릴 시간 및 공간이 절대적으로 부족하기 때문이었습니다)\n	\n  기존 cache 알고리즘의 장점은 cache라는 공간에 data를 placement하는 알고리즘은\n  극도로 심플하지만, (별도의 알고리즘 없이 access된 data는 무조건 cache에 두므로)\n  일단 한 번이라도 data가 access되어야만 caching이 될 수 있다는 한계가 있습니다.\n\n  앞에서 이미 잠시 언급하였습니다만,\n  small-number-of-data가 many-times-of-rehit 이 된다면,\n  reactive data placement 방식과 proactive data placement 방식 간에\n  성능 차이가 크지 않을 수 있습니다.\n  그러나, active data range가 매우 넓은 경우에는\n  small-number-of-data가 many-times-of-rehit 되는 경우 뿐만 아니라,\n  large-number-of-data들이 small-times-of-rehit되는 경향 (long-tail)이\n  보여질 수 있기 때문에 proactive data placement에 의한 이득이 커질 수 있습니다.\n  (Active range가 매우 커지는 Big data 상황에서는\n   이렇게 될 가능성이 더 커질 것으로 보입니다)\n\n</pre>\n\n----\n\n==== Patent: IOWA based PDP ====\n <pre>\n미래의 데이터 I/O 패턴을 예측하는데 과거의 데이터 I/O 패턴만 볼 것이 아니라,\n데이터 자체의 특성, 파일 단위의 특성 (예를 들면, 이미지인지, 서버 로그인지, conf 파일인지),\nblock 자체의 특성 (뭐가 있을까요?), creator (user, process)의 종류,\nlast change/update time, 현재 active한 process들의 종류 등,\n다양한 데이터를 이용하는 것이 어떨까요?\n이런 방식이 새로운 아이디어라면 특허 출원을 검토해 주세요.\n\n> 네, 상무님, 다양한 데이터를 이용하는 것이 필요합니다.\n  이에, trace 기반의 access 패턴 뿐만 아니라 호스트 시스템에서 얻을 수 있는\n  정보들을 최대한 활용하고자 하고 있습니다.\n  그리고 이 부분에서 ML 기술의 적극적인 도입이 필요합니다.\n\n  (이 정보들이 IO 패턴 발굴 및 예측에 도움을 줄 수 있는지 유효성을 검증하고자\n   일부 정보들을 활용하여 이미 실험을 시작하고 있습니다.\n   현재는 IO 유발자 정보를 고려하여 실험 중입니다.)\n\n  고려중인 정보/접근 방식들은 다음과 같습니다.\n\n  - IO 액세스 패턴\n    - trace 기반의 address 별 access pattern\n      : periodicity 뿐만 아니라 recency, 누적 access count,\n        read/write intensiveness, access randomness, data usage-cycle 패턴\n        (read-modify-write(update)-read 등) 등\n      : 수집 시 blktrace 등 IO trace 툴 활용\n    - 시스템 meta 정보 기반 access pattern\n      : inode 등에서 얻어낼 수 있는 last create/update/access time 등\n      : 수집 시 inode 등 file, filesystem 정보 활용\n\n  - IO 유발자 정보\n    - user ID, group ID, process ID 등 creator 정보 및 현재 해당 file을 이용하는 \n      process / class 등\n      : 수집 시 debugfs, proc, sysfs 등의 메커니즘 활용\n      : IO 유발자가 어느 category에 속하는지 category 별로 분석\n        (web, DB, auth, monitoring, kernel thread (journald, kworker 등), ...)\n\n<!--\n  - State machine 기반의 macro IO pattern analsysis\n    : 유의미한 parameter 몇 가지로 정의되는 state machine을 정의하고,\n      이 state 간의 transition 확률을 이용하여 다가올 macro IO 패턴을 예측\n-->\n\n  특허 출원에 대해 말씀드리자면, 위의 기법 각각에 대한 특허에 대해서는\n  일부 시스템 등록, 혹은 현재 직무발명서 작성 중이었습니다만,\n  말씀 주신 바처럼 현재 저희가 진행하고 있는\n  IOWA 기반의 proactive data placement 자체에 대한\n  특허 가능성도 검토하도록 하겠습니다.\n\n  참고로, 2/12일 (오늘) 현재, 삼성 특허 검색 사이트에서, 검색식\n  ((proactive data placement) OR (data preplacement))\n  (cach* OR tiering)\n  ((IO OR I/O OR workload) analy*)\n  으로 찾아본 결과, 검색된 관련 특허는 없었습니다만,\n  검색 범위를 더 넓혀서 추가로 더 찾아보겠습니다.\n\n</pre>\n\n----\n\n==== Realistic sample trace log collection ====\n<pre>\n이렇게 다양한 데이터를 이용해서 데이터 I/O 패턴을 예측하려면 많은 sample log들을 모아서\n예측 함수를 만들어야 합니다. 분산 스토리지가 특정 용도에 이용되고 있다면\n(예로, 스트리밍 서버, 웹 서버, VDI 서버 등), 그것별로 sample log들을 많이 모아야 할 것 같습니다.\n우리가 어떻게 이런 다양한 로그들을 모을 수 있을까요?\n\n> 네, 전적으로 동감합니다. 수집 현황/계획은 아래와 같습니다.\n\n  1. Web Server IO Log\n    : 메모리사에서도 SAVL 타겟으로 우선적으로 관심을 가졌던 workload로서\n      SPECweb benchmark을 통해 realistic workload를 얻을 수 있습니다.\n      (메모리 사 응용기술팀으로부터 시스템 구축에 필요한 파일 받아옴)\n\n  2. DB IO Log\n    : TPC-C benchmark을 통해 realistic workload를 얻을 수 있습니다.\n      (이미 환경 구축 완료 및 사용 중)\n\n  3. Supercomputing IO Log\n    : 현재 기술원 3층의 수퍼컴퓨터 관리팀과 접촉 중\n      (Matlab, R, MapReduce-like RSP framework에 대한 workload 존재)\n\n  4. Streaming IO Log\n    : 아직 확보 방안 마련되지 않음\n		\n  5. VDI IO Log\n    : 기술원 정보전략팀과 의논 가능\n      (\'12년에 1차로 trace log 확보를 시도해본 적 있으나\n       정보전략팀과 SDS와의 관계가 어려워져 실패)\n\n</pre>\n\n----\n\n==== problem formulation considering on-line learning ====\n<pre>\n\n[ 지시 1 ]\n\n앞 경우는 sample log들을 모아서 예측 함수를 만드는 것이고,\n실제 deploy되었을 때는 dynamic 하게 예측 함수가 adaption할 수 있어야 하는데,\n그런 adaptation을 on-line learning으로 볼 수 있습니다.\n최희열 전문과 협력해서 위 문제들을 formulation 하기 바랍니다.\n\n> 네, 위의 내용들을 정리하여 최희열 전문과 함께 위의 문제들을 명확히 정리해보겠습니다.\n\n</pre>\n\n----\n\n== ## bNote-2013-02-08 ==\n\n=== Git Server on My Local Machine ===\n\n==== Git on the Server - Setting Up the Server ====\n\n* http://git-scm.com/book/en/Git-on-the-Server-Setting-Up-the-Server\n: Setting up the server // git-scm.com\n\n* http://git-scm.com/book/ch4-4.html\n: 4.4 Git on the Server - Setting Up the Server // git-scm.com\n\n\n: Let\'s walk through setting up SSH access on the server side. In this example, you\'ll use the authorized_keys method for authenticating your users. We also assume you\'re running a standard Linux distribution like Ubuntu. First, you create a \'git\' user and a .ssh directory for that user.\n\n <pre>\n$ sudo adduser git\n$ su git\n$ cd\n$ mkdir .ssh\n</pre>\n\n: Next, you need to add some developer SSH public keys to the authorized_keys file for that user. Let\'s assume you\'ve received a few keys by e-mail and saved them to temporary files. Again, the public keys look something like this:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCB007n/ww+ouN4gSLKssMxXnBOvf9LGt4L\nojG6rs6hPB09j9R/T17/x4lhJA0F3FR1rP6kYBRsWj2aThGw6HXLm9/5zytK6Ztg3RPKK+4k\nYjh6541NYsnEAZuXz0jTTyAUfrtU3Z5E003C4oxOj6H0rfIF1kKI9MAQLMdpGW1GYEIgS9Ez\nSdfd8AcCIicTDWbqLAcU4UpkaX8KyGlLwsNuuGztobF8m72ALC/nLF6JLtPofwFBlgc+myiv\nO7TCUSBdLQlgMVOFq1I2uPWQOkOWQAHukEOmfjy2jctxSDBQ220ymjaNsHT4kgtZg2AYYgPq\ndAv8JggJICUvax2T9va5 gsg-keypair\n</pre>\n\n: You just append them to your authorized_keys file:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.josie.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.jessica.pub >> ~/.ssh/authorized_keys\n</pre>\n\n: Now, you can set up an empty repository for them by running git init with the --bare option, which initializes the repository without a working directory:\n\n <pre>\n$ cd /opt/git\n$ mkdir project.git\n$ cd project.git\n$ git --bare init\n</pre>\n\n: Then, John, Josie, or Jessica can push the first version of their project into that repository by adding it as a remote and pushing up a branch. Note that someone must shell onto the machine and create a bare repository every time you want to add a project. Let\'s use gitserver as the hostname of the server on which you\'ve set up your \'git\' user and repository. If you\'re running it internally, and you set up DNS for gitserver to point to that server, then you can use the commands pretty much as is:\n\n: # on Johns computer\n <pre>\n$ cd myproject\n$ git init\n$ git add .\n$ git commit -m \'initial commit\'\n$ git remote add origin git@gitserver:/opt/git/project.git\n$ git push origin master\n</pre>\n\nAt this point, the others can clone it down and push changes back up just as easily:\n\n <pre>\n$ git clone git@gitserver:/opt/git/project.git\n$ cd project\n$ vim README\n$ git commit -am \'fix for the README file\'\n$ git push origin master\n</pre>\n\n: With this method, you can quickly get a read/write Git server up and running for a handful of developers.\n\n: As an extra precaution, you can easily restrict the \'git\' user to only doing Git activities with a limited shell tool called git-shell that comes with Git. If you set this as your \'git\' user\'s login shell, then the \'git\' user cannot have normal shell access to your server. To use this, specify git-shell instead of bash or csh for your user\'s login shell. To do so, you will likely have to edit your /etc/passwd file:\n\n <pre>\n$ sudo vim /etc/passwd\n</pre>\n\n: At the bottom, you should find a line that looks something like this:\n\n <pre>\ngit:x:1000:1000::/home/git:/bin/sh\n</pre>\n\n: Change /bin/sh to /usr/bin/git-shell (or run which git-shell to see where it’s installed). The line should look something like this:\n\n <pre>\ngit:x:1000:1000::/home/git:/usr/bin/git-shell\n</pre>\n\nNow, the \'git\' user can only use the SSH connection to push and pull Git repositories and cannot shell onto the machine. If you try, you will see a login rejection like this:\n\n <pre>\n$ ssh git@gitserver\nfatal: What do you think I am? A shell?\nConnection to gitserver closed.\n</pre>\n\n==== Install GitWeb ====\n\n* [http://blog.countableset.ch/2012/04/29/ubuntu-12-dot-04-installing-gitolite-and-gitweb/ Installing gitolite and gitweb on Ubuntu 12.04]\n\nNote this is only tested on Ubuntu 12.04 Server with apache2, I\'m sure it would work on the desktop version also.\n\n===== Installing Git =====\n <pre>\nsudo apt-get install git-core\n</pre>\nOptional, setup git global settings:\n\n <pre>\ngit config --global user.name \"Your Name\"\ngit config --global user.email your@email.com\n</pre>\n\n===== SSH Key =====\nGenerate ssh public/private key on the machine you would like to access git repo from and copy it to the server into the /tmp/ directory, for reference here is the command:\n\n <pre>\nssh-keygen -t rsa -C \"name@computer\"\n</pre>\n\n===== Installing Gitolite =====\n\n <pre>\nsudo apt-get install gitolite\n</pre>\n\nCreate a user to access gitolite with, in this case I chose git since I don\'t like to type:\n\n <pre>\nsudo adduser \\\n    --system \\\n    --shell /bin/bash \\\n    --gecos \'git version control\' \\\n    --group \\\n    --disabled-password \\\n    --home /home/git \\\n    git\n</pre>\n\nNow login to the newly created user, and set the path, and move to its home directory:\n\n <pre>\nsudo su git\necho \"PATH=$HOME/bin:$PATH\" > ~/.bashrc\ncd\n</pre>\n\nRun the gitolite setup command with the public key you copied to the tmp directory to initialized the location for gitolite use. Then change the $REPO_UMASK to 0027 when it opens the .gitolite.rc for editing during the installation process. If it didn\'t open the file for any reason just open it up in vim (Note this is only if you\'d like to use gitweb):\n\n <pre>\ngl-setup /tmp/rachel.pub\n# change $REPO_UMASK = 0077; to $REPO_UMASK = 0027; # gets you \'rwxr-x---\'\n</pre>\n\nAfterward, it has made the gitolite-admin.git, testing.git repo and all other necessary files. Check to see that everything works by cloning the repo on the machine with the public/private key.\n\n <pre>\ngit clone git@<server>:gitolite-admin.git\n</pre>\n\nHere is a resource about the syntax for the config file and adding users.\n\n===== Install Gitweb =====\nThis is the tricky bit... Install gitweb and the highlight app. Gitweb is located at \'/usr/share/gitweb\'\n\n <pre>\nsudo apt-get install highlight gitweb\n</pre>\n\nEdit the gitweb config to the locations of the project list and repos, and add the highlighting bit at the end of the file:\n\n <pre>\nsudo vim /etc/gitweb.conf\n# change $projectroot to /home/git/repositories\n# change $projects_list to /home/git/projects.list\n# Add Highlighting at the end\n$feature{\'highlight\'}{\'default\'} = [1];\n</pre>\n\nChange gitolite instance to allow access for gitweb. First append www-data to git group so gitweb can access the repos, then change the permissions for git repos and the projects list, finally restart apache:\n\n <pre>\nsudo usermod -a -G git www-data\nsudo chmod g+r /home/git/projects.list\nsudo chmod -R g+rx /home/git/repositories\nsudo service apache2 restart\n</pre>\n\nFinally you need to tell gitolite which repo you want to show up in gitweb. To do this edit the gitolite.conf file from the gitolite-admin.git repo:\n\n <pre>\nrepo testing\n  RW+ = @all\n  R = gitweb\n</pre>\n\n\n\n\n== ## bNote-2013-02-07 ==\n\n=== World Largest Data Centers ===\n\n* [http://wikibon.org/blog/inside-ten-of-the-worlds-largest-data-centers/ Inside Ten of the World’s Largest Data Centers, on March 25, 2010]\n\n\n=== Enterprise Storage Related Articles ===\n\n----\n==== The future of enterprise storage? ====\n: Henry Baltazar / Senior Analyst / Storage & Systems / The 451 Group\n: [http://www.snia.org/sites/default/files2/sdc_archives/2010_presentations/general_session/HenryBaltazar_Cloud_Convergence_Consolidation.pdf Clouds, Convergence, and Consolidation - SNIA SDC 2010]\n\n----\n==== Tiering vs. Caching ====\n; Automated Storage Tiering\n: Has become a popular technology in the past two years with multiple vendors embedding this feature within their storage systems.\n: Allows storage systems to migrate \'hot\' data to high speed flash-based storage tiers, while simultaneously moving stale data to inexpensive SATA hard drives.\n; Storage Caching\n: Primarily being driven by NetApp (i.e. Flash Cache) and a few startups, storage systems with caching use solid state storage as a memory extension technology.\n: Caching proponents claim that the migration required to do tiering is inefficient and reacts too slowly to eliminate hotspots.\n----\n\n=== Comparison of NAS vs. SAN // Pain Points of Storage System ===\n\n어제 회의의 Action Items 중 하나로, 제가 맡았던 부분인\n\"현재 Arch.가 야기하는 기술적 Poin Points 추가 정리\" 관련 메일입니다.\n\n내용은 다음과 같이 크게 두 가지로 구분됩니다.\n\n1. NAS vs. SAN 비교\n\n2. 기존 Storage System의 Pain Points (HP StorageWorks 제품 Feature를 통해 바라본)\n  -> 여기에는 이전문님께서 말씀하시곤 했던 ILM(Info. Lifecycle Mgmt.)에 \n     대한 언급도 나오네요. (더 자세한 내용은 첨부파일에 있습니다)\n\n// 아래 내용들은  HP에서 만든 \"Storage Overview and Architecture (HP, Arvind Shrivastava)\" 자료를 참고하였습니다. (첨부 1)  다소 NAS에 불리한 점들이 많이 부각되어 보입니다만, 다르게 보면, \"기존 NAS의 구조적 한계점이 이러했으니 Performance Drop 없이도 Scale-out 되는 NAS로 가기 위해서는 이러한 단점들이 극복되어야하겠다\" 라는 내용으로 받아들일 수도 있겠습니다.\n\n\n==== Comparisons of SAN, NAS (and DAS) ====\n\n{| border=1\n|+ \'\'\'DAS vs. NAS vs. SAN\'\'\'\n| Direct Attached Storage (DAS)\n| Network Attached Storage (NAS)\n| Storage Area Network (SAN)\n|-\n|\n*+ Low cost solution\n*+ Simple to configure\n*- De-centralized storage management\n*- No storage consolidation\n*- No high availability\n*- Low performance\n|\n*+ Heterogeneous environment\n*+ Centralized storage management\n*- Low performance\n*- Limited scalability\n*- Network congestion during backups & restore\n*- Ethernet limitations\n|\n*+ Heterogeneous environment\n*+ Centralized storage management\n*+ High degree of fault tolerance\n*+ Best and superior performance\n*+ Storage consolidation\n*+ Fast and efficient backups and restores\n*+ Dynamic scalability\n*- Expensive solution for small setups\n|-\n|}\n\n\n\n{| border=1\n|+ \'\'\'NAS vs. SAN\'\'\'\n| Items\n| style=\"width:40%\" | Network Attached Storage (NAS)\n| style=\"width:40%\" | Storage Area Network (SAN)\n|-\n| Network\n| IP\n| Fibre Channel\n|-\n| Reliability\n| Unreliable\n: >> FCoE, iSCSI 도 있는데 정말 그렇게 Unreliable인걸까요? FC에 비해 상대적으로 그렇다는 의미로 받아들이면 될까요?\n| Reliable\n|-\n| CPU overhead\n| Extremely High\n: >> \"Extremely High\" 까지일 줄은 몰랐네요. Block IO에 비해 File System Operation까지 해야 하는 NAS의 경우 CPU overhead가 더 있을 수 있겠다는 생각은 했었지만, 생각보다 overhead가 꽤 있는지도 모르겠습니다. (NetApp NAS, EMC Isilon 확인 필요).\n| Extremely Low\n: >> 반대로, SAN이 상대적으로 low-overhead라고 해서 \"Extremely\" 까지일 줄은...\n|-\n| Blocks\n| Large number of small blocks\n| Large blocks of data\n|-\n| Backup\n| LAN backup\n| LAN-free backup\n|-\n| Access and control by application\n| Applications driven by universal access to files\n| Applications managing own data specs\n|-\n| Typical usage\n|\n* Office applications file serving\n* File sharing between office users and CAD users\n* Content management for web servers\n* Consolidation of \'home directories\' for controlled and centralized backup\n* Boot device for diskless server farms\n* Rich media such as audio or video streaming\n|\n* Physical storage consolidation of any application mix\n* Physical storage sharing for cluster configurations\n* Sharing of backup devices (libraries)\n* Long distance data mirroring and/or data replication on storage device level\n* Redundant, high speed disk access for large database servers\n|}\n\n<br/>\n----\n\n==== HP Storage Product - StorageWorks ====\n\n\'\'\'Pain Points\'\'\'\n* HP의 storage 제품인 StorageWorks에서 언급된 pain points를 요약해보면 다음과 같습니다.\n\n# 네트워크 트래픽 증가로 인한 스토리지 시스템 성능 저하 (여기서 WAN을 언급하는 것으로 보아 물리적으로 멀리 떨어진 스토리지 간의 트래픽을 언급하는 것으로 보입니다)\n# 저장된 data의 대부분을 차지하는 (operational data에 비해 3~4배 이상 크기) reference data의 지속적 증가 (여기서 reference data라는 것은, 당장 operation되고 있지는 않지만, 이전에 생성되어 사용된 적은 있으나 폐기되지 않고 저장되어 있는, 그렇게 계속 쌓여가는 data들을 의미하는 것으로 보여집니다. 한마디로, operational data가 hot-data라면, reference data는 cold-data라고 볼 수 있을 것 같습니다)\n# 관리되고 저장된 data의 증가에 따라 속도 저하 및 TCO 증가\n# hetero한 사양 및 상태로 여러 곳에 흩어져있는 storage system의 관리\n\n\n\n\'\'\'HP StorageWorks - Reference Information Storage System\'\'\'\n:* 일종의 ILM (Information Lifecycle Management)\n:* Cost 절감 위해 application server/storage로부터 잘 사용되지 않는 static data 및 중복 data 감소.\n:* data의 integrity 보장.\n:* Power indexing 및 Grid computing을 통해 훨씬 빠른 search/retrieval을 달성하겠다.\n\n[[File:20130207 HP Storage Product Slides Scale is the problem .png|500px]]\n\n\n* Changing digital storage demands \n:* Majority is reference information (operational data 보다는 reference 정보가 저장된 data의 대부분 차지)\n\n[[File:20130207 HP Storage Product Slides - Changing digital storage demands .png|500px]] \n\n\n* WAN acceleration\n:* WAN을 통해 연결되는 스토리지 트래픽 감소 기술\n(100배 이상 빠른 throughput 달성 목표)\n{| border=1\n| Problems\n| style=\"width:40%\" | HP\'s solution\n| style=\"width:40%\" | What it does\n|-\n| Not enough bandwidth\n|\n* Scalable data referencing (SDR)\n|\n* Remove repeated bytes from the WAN for all TCP traffic\n|-\n| TCP chattiness and inefficiency\n|\n* Virtual Window Expansion\n* High speed TCP optimizations\n|\n* Optimizes performance of TCP\n* Enables much higher throughput on \"LFNs\"\n|-\n| Application chattiness and inefficiency\n| Transaction prediction (CIFS, MAPI)\n| Optimizes the performance of specific applications\n|}\n\n== ## bNote-2013-02-06 ==\n\n\n=== Defining: Target Segment, Product, Architecture ===\n금일 회의때 논의한 내용을 정리합니다.\n\n\n\'\'\'Consensus\'\'\'\n\n* Big data 시장만 보면 현재로서는 시장이 크지 않다. \n\n* HW Box를 같이 팔아야 한다. 그렇지 않으면 돈이 안된다.\n\n* Google, NHN등 big data center player에게 팔것은 아니다.\n\n* 삼성이 Enterprise Storage 사업 진입한다고 가정 (사업 진입유무를 우리가 고민하지 말자)\n\n* Large-scale system에서 data/node scale이 증가할수록 manageability issue가 커진다.\n\n* storage 신사업 초기에는 기존 infra에 대한 당사 제품의 compatibility가 매우 중요할 것이다. 이 부분은 신사업추진단이 고민할 것이고 우리는 2nd/3rd generation에 해당하는 innovative architecture에 집중한다.\n\n* new architecture는 고객의 현재/미래의 pain point에 기반해야 한다.\n\n \n\'\'\'Pain Point (고객입장)\'\'\'\n\n* \"Google같은 flexible/efficient data center 를 우리 회사 scale에 맞게 내부에서 소유하고 관리하고 싶습니다.\" (Google-in-a-box)\n\n* \"여러 인프라에 흩어진 기업데이터를 통합 관리하고 싶습니다.\" (for bigdata analytics?)\n\n* \"어떠한 경우에도 data loss가 없고 available한 불멸의 스토리지를 소유하고 싶습니다.\"\n\n* \"우리가 소유한 모든 infra를 통합관리하는 극강의 manageability가 필요합니다. 충분히 자동화되고 self-healing되어 관리자 1명당 관리노드의 갯수를 혁신적으로 올릴 수 있으면 좋겠습니다.\" (ML, Brain-inspired computing 적용가능성 있음)\n\n* replication 기술 (InformationWeek 자료에서 니즈 언급)\n\n\n\'\'\'Big Question\'\'\'\n\n* SAN ? NAS ? SAN+NAS ?\n\n* Server+Storage ?\n\n* up to which scale does the distribute architecture need to be expanded?\n\n* 고객이 별도의 big data platform (HW+SW) 을 구매할까? (in addition to already owned infra)\n\n* SW-defined storage 시대가 올까?\n\n* 왜 Google/NHN은 직접 data center를 buildup 했을까? 왜 EMC를 안쓸까?\n\n* InformationWeek 자료의 기술선호도가 2018년에는 어떻게 바뀔까?\n\n* Replication 기술이 중요한만큼 매우 어려울까?\n\n* 2018년에는 Cloud Storage 의존성이 어떻게 될까?\n\n* Data center model이 어떻게 다를까? : Google vs. Amazon (Fusion-IO구매) vs. Apple (Isilon구매)\n\n* service fee 로 먹고사는 biz model이 나올 수 있나?\n\n* Hadoop + Dedup? (data가 있는곳에 processing을! 이라는 hadoop philosophy가 dedup에 의해 어떤 영향을 받는가?)\n\n* block I/F, file I/F 이후 new interface가 어떻게 확산될까? 왜 object I/F는 확산되지 못했을까? \n\n* brain-like한 I/F는 어떤 형태가 될까?\n\n\n\'\'\'Action Items\'\'\'\n\n* 이: Google의 data center evolution history\n\n* 서: SAN/NAS, Converged/Storage, 분산 scale(100s? 1000? 10000?)\n\n* 신+구: 분산-scale storage에 분산-scale dedup이 적용되면 architecture가 어떻게 바뀌나? dedup manager의 위치가 어디가 되어야 하나?\n\n* 융: 현 architecture가 야기하는 기술적인 Pain Point 추가정리\n\n\n\'\'\'Check this out\'\'\'\n* 451 report provided by Seo-C: Automatic Tiering is the very hot technology\n\n== ## bNote-2013-02-05 ==\n\n\n=== IOWA:: machine learning ===\n\nk-addr-group\n\ndistance calculation and clustering\n\nhamming distance\n\n\n[[ bsc.iowa.lsp.addr.access_ptrn ]]\n\n\n[http://en.wikipedia.org/wiki/Levenshtein_distance Levenshtein distance]\n\n\n <pre>\n__k_addr_group__ 0 : [720616224, 3282792808, 3282793000, 3282793096]\n__k_addr_group__ 1 : [728218248, 730235992, 817521776, 3765476016]\n__k_addr_group__ 2 : [721421456, 724475672, 724475864, 3712136296]\n__k_addr_group__ 3 : [724475960, 828267560, 895981360, 3277437312]\n__k_addr_group__ 4 : [725469848, 726381104, 726381496, 726793728]\n__k_addr_group__ 5 : [723112360, 730730224, 3677713128, 3773683448]\n__k_addr_group__ 6 : [725235304, 828274704, 896778072, 3277440104]\n__k_addr_group__ 7 : [720987560, 724481496, 724727104, 725008864]\n__k_addr_group__ 8 : [545503608, 896780128, 3069345832, 3076725824]\n__k_addr_group__ 9 : [721386480, 721386960, 3752324048, 3767545408]\n__k_addr_group__ 10 : [725420616, 726671672, 727735192, 3066446120]\n__k_addr_group__ 11 : [721030800, 721030896, 721411176, 827916488]\n...\n__k_addr_group__ 245 : [725218552, 822170744, 870855200, 3642881480]\n__k_addr_group__ 246 : [457025608, 722055120, 3685634288, 3779707504]\n__k_addr_group__ 247 : [722055216, 722055312, 722055376, 725157784]\n__k_addr_group__ 248 : [721285200, 721285296, 728117992, 728118184]\n__k_addr_group__ 249 : [725218680, 725218808, 725218936, 725219032]\n</pre>\n\n\n\n=== Python:: numerical sort for string list ===\n\n <pre>\n\n>>> typeof(mlist)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name \'typeof\' is not defined\n>>> help(mlist)\n\n>>> mlist = {}\n>>> mlist[0] = []\n>>> mlist[1] = []\n>>> mlist[0] = [\"11243\", \"92\", \"2132\", \"42\"]\n>>> mlist[1] = [\"243\", \"892\", \"19132\", \"3242\"]\n>>> mlist[0] = [ int(x) for x in mlist[0] ]\n>>> mlist[0]\n[11243, 92, 2132, 42]\n>>> mlist[1]\n[\'243\', \'892\', \'19132\', \'3242\']\n>>> mlist[1] = [ int(x) for x in mlist[1] ]\n>>> mlist[1]\n[243, 892, 19132, 3242]\n>>> mlist   \n{0: [11243, 92, 2132, 42], 1: [243, 892, 19132, 3242]}\n>>> mlist[0].sort()\n>>> mlist\n{0: [42, 92, 2132, 11243], 1: [243, 892, 19132, 3242]}\n>>> mlist[1].sort()\n>>> mlist\n{0: [42, 92, 2132, 11243], 1: [243, 892, 3242, 19132]}\n>>> \n\n</pre>\n\n== ## bNote-2013-02-04 ==\n\n\n\n=== CWD ===\n\n <pre>\nb@ub04:[anal] $ pwd\n/x/iowa/now/tracelog_analysis.v02.Q_phase.2_mysqld/anal\nb@ub04:[anal] $ l\ntotal 36\ndrwxrwxr-x 8 b b 4096 Feb  4 19:23 ./\ndrwxrwxr-x 5 b b 4096 Feb  4 19:23 ../\n-rw-rw-r-- 1 b b  127 Jan 30 19:42 .bdx.0100.y.exec_iowa_for_all_cases.sh\ndrwxrwxr-x 2 b b 4096 Jan 30 19:38 .tools/\nlrwxrwxrwx 1 b b   27 Jan 30 15:55 .tracelog.A.addr -> ../preproc/.tracelog.A.addr\nlrwxrwxrwx 1 b b   27 Jan 30 18:48 .tracelog.R.addr -> ../preproc/.tracelog.R.addr\nlrwxrwxrwx 1 b b   27 Jan 30 18:48 .tracelog.W.addr -> ../preproc/.tracelog.W.addr\ndrwxrwxr-x 2 b b 4096 Jan 30 19:39 t00.R.xLBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:43 t01.R.1LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t02.R.100LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t03.R.10000LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t04.R.20000LBA/\n</pre>\n\n=== IOWA.data.tpcc_250gb_48h.mysql.R.addr:: Offset 100K, Len 1K ===\n\n* [[IOWA.data.tpcc_250gb_48h.mysql.R.addr:: Offset 100K, Len 1K]]\n\n=== Frequent Item Detection/Discovery ===\n\n==== Stanford Open Book - Mining of Massive Datasets ((B.GOOD)) ====\n\n* MMD Book [http://i.stanford.edu/~ullman/mmds.html]\n\n: The book has now been published by Cambridge University Press. The publisher is offering a 20% discount to anyone who buys the hardcopy Here. By agreement with the publisher, you can still download it free from this page. Cambridge Press does, however, retain copyright on the work, and we expect that you will obtain their permission and acknowledge our authorship if you republish parts or all of it. We are sorry to have to mention this point, but we have evidence that other items we have published on the Web have been appropriated and republished under other names. It is easy to detect such misuse, by the way, as you will learn in Chapter 3.\n: Anand Rajaraman (@anand_raj) and Jeff Ullman\n\n: Ch 3. Finding Similar Items [http://i.stanford.edu/~ullman/mmds/ch3.pdf]\n:: 3.1 Applications of Near-Neighbor Search\n::: 3.1.1 Jaccard Similarity of Sets\n:: 3.2 Shingling of Documents\n:: 3.3 Similarity-preserving Summaries of Sets\n:: 3.4 Locality-sensitive Hashing for Documents\n:: 3.5 Distance Measures\n:: 3.6 The Theory of Locality-sensitive Functions\n:: 3.7 LSH Families for Other Distance Measures\n:: 3.8 Applications of Locality-sensitive Hashing\n:: 3.9 Methods for High Degrees of Similarity\n\n: Ch 6. Frequent Itemsets [http://i.stanford.edu/~ullman/mmds/ch6.pdf]\n:: 6.1 The Market-Basket Model\n:: 6.2 Market Baskets and the A-Priori Algorithm\n:: 6.3 Handling Larger Datasets in Main Memory\n:: 6.4 Limited-pass Algorithms\n:: 6.5 Counting Frequent Items in a Stream\n\n==== Orange - open source data visualization and analysis tool ====\n\nOpen source data visualization and analysis for novice and experts. Data mining through visual programming or Python scripting. Components for machine learning. Add-ons for bioinformatics and text mining. Packed with features for data analytics.\n\n\n==== SAX - Symbolic Aggregate Approximation ====\n\nSAX (Symbolic Aggregate approXimation) Homepage [http://www.cs.ucr.edu/~eamonn/SAX.htm]\n\nSAX is the first symbolic representation for time series that allows for dimensionality reduction and indexing with a lower-bounding distance measure. In classic data mining tasks such as clustering, classification, index, etc., SAX is as good as well-known representations such as Discrete Wavelet Transform (DWT) and Discrete Fourier Transform (DFT), while requiring less storage space. In addition, the representation allows researchers to avail of the wealth of data structures and algorithms in bioinformatics or text mining, and also provides solutions to many challenges associated with current data mining tasks. One example is motif discovery, a problem which we defined for time series data. There is great potential for extending and applying the discrete representation on a wide class of data mining tasks.\n\n\n==== The Amadeus motif discovery platform ====\n\nAmadeus Home [http://acgt.cs.tau.ac.il/amadeus/]\n\nAmadeus is a user-friendly software platform for genome-scale detection of known and novel motifs (recurring patterns) in DNA sequences, applicable to a wide range of motif discovery tasks. Amadeus outperforms extant tools and sets a new standard for motif discovery software in terms of accuracy, running time, range of application, wealth of output information and ease of use.\n\n==== Motif Detection ====\n\n* Python for Bioinformatics [http://telliott99.blogspot.kr/2009/06/motif-discovery.html]\n* MOODS: Motif Occurrence Detection Suite - ALGOrithmic Data ANalysis [http://www.cs.helsinki.fi/group/pssmfind/]\n* CMU CS Lecture -- Motif Detection [http://www.cs.cmu.edu/~epxing/Class/10810-05/Lecture6.pdf]\n\n==== Biopython ====\n\nBiopython is a set of freely available tools for biological computation written in Python by an international team of developers. [http://biopython.org/wiki/Main_Page Biopython]\n\n* Required Python Modules [http://www.scipy.org/more_about_SciPy NumPy / SciPy (SciPy.org)]\n** NumPy: N-dimensional Array Manipulations\n** SciPy: Scientific tools for Python\n\n* References\n# [http://biopython.org/DIST/docs/tutorial/Tutorial.pdf Biopython Tutorial]\n\n<br/>\n----\n\n=== RAM-SSD 하이브리드 Page Cache 아키텍쳐 :: WIPS 의견 대응 ===\n <pre>\n접수번호 : RN-201301-004-1\n\n명칭 : RAM-SSD 하이브리드 Page Cache 아키텍쳐\n\n기술 핵심 : RAM과 SSD를 이용하여 디스크 data를 caching하는 구조가\npage cache layer에서 동작하도록 함으로써 hot access를 check하고\naccess 정도에 따라 RAM 또는 SSD에 저장(tier-1~2)하거나 disk에서 access되도록 하는 것\n\n구성 : A) page cache에서 동작하는 RAN-SSD hybrid cache architecture\n      B) tier-1 page cache(RAM), tier-2 page cache(SSD)\n      C) page에 대한 hit pattern으로 page들을 tier-1,2로 class 지정\n\n\n--\n안녕하세요? 정명준입니다.\n다루는 내용이 많았음에도 깔끔하게 잘 정리해주시느라 고생이 많으셨습니다. ^_^\n꼭 추가되었으면 하는 내용과, 서술의 톤이 살짝 바뀌면 좋을만한 부분이 있어\n아래 보내주신 메일 본문에 추가하였습니다.\n(제 의견은 파란색으로 표시되어있습니다)\n\n1) \'기술 핵심\' 부분\n\n	\"RAM과 SSD를 이용하여 디스크 data를 caching하는 구조가 page cache layer에서 동작하도록 함으로써\" \n		-> 본 발명의 핵심 부분인 부분을 잘 적어주셨습니다.\n\n	\"hot access를 check하고 access 정도에 따라\"\n		-> 이 부분은 \"access pattern을 점검하여 rehit-potential에 따라\" 라는 표현으로 바뀌면 더욱 좋겠습니다.\n		기존에는 recently hot (recency) 위주로 cache될지 evict될지를 판단하였는데,\n		본 발명에서는 rehit potential을 보고 판단하겠다는 점이 다릅니다.\n		(once-hot data 라든지, periodicity 등을 파악)\n\n	\"RAM 또는 SSD에 저장(tier-1~2)하거나 disk에서 access되도록 하는 것\"\n		-> 여기에 \"page chunk 단위로 page tiering을 한다\"는 언급이 되었으면 좋겠습니다. (청구의 범위를 너무 제한하는 것이 아니라면요)\n\n\n2) \'구성\' 부분\n\n	아래 \'가~바\' 부분이 본 발명에서 청구하고자 하는 주요 내용들입니다.\n	이 중에서 \'가\' 부분이 보내주신 메일에서 A, B, C로 적어주신 \'구성\' 부분인데요,\n	대표 청구항을 적자면 아래 \'가~바\' 부분이 모두 필요할 것 같습니다.\n\n	가) RAM-SSD hybrid page cache architecture\n		page cache layer에서 동작하고;\n		tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는;\n		page tiering 기반의 hybrid page cache 아키텍쳐.\n\n	나) rehit-potential aware page classification, and page class based cache data placement\n\n	다) page tiering operation in host side\n\n	라) page tiering operation in SSD side\n		chunk I/O in SSD in {log-structured and/or parallelism-maximized} manner\n\n	마) TRIM ahead 메커니즘 (page chunk writes latency를 줄이기 위해 미리 공간 확보하는)\n\n	바) victim page chunk selection in SSD\n		oldest-first\n		most-mark-first\n		with upper-tier-backup\n		\n\n오후에 전화로 말씀 나누면서 설명 드리겠습니다.		\n감사합니다.\n정명준 드림.\n</pre>\n\n\n=== 서울대 2013-02-01 Meeting Minutes ===\n\n1. 실험 관련\n: HDD 개수와 map slot의 개수를 align하여 실험 진행 후 기존 결과와 함께 분석\n: <span style=\"color:blue\">>>네, 본 실험에서 나타난 \'재미있는\' 현상이 Hadoop 권장 환경 구성과 맞지 않아서 발생한 것이 아니라 Hadoop 시스템 근원적으로 가지고 있는 \'한계\' 때문이라는 것으로 주장이 되려면, HDD 수와 execution slot 간 alignment는 필수적일 것 같습니다. 잘 부탁드리겠습니다.[1]</span>\n\n: In-memory buffer 크기 변화를 통한 spill file 개수 및 phase elapsed time의 관계를 exponential expression 으로 분석\n: <span style=\"color:blue\">>> 아, 제 발음이 좋지 않았던 것 같습니다. ^^;; exponential expression이라기 보다는 extrapolation이 가능한 equation으로 나오면 좋겠다는 것으로 이해해주시면 좋겠습니다.  아마 이게 되려면 충분한 경우에 대해서 실험이 되어야 할 것인데, equation이 나오기만 한다면 무척 중요한 의미를 갖게 될 것 같습니다. 전체적인 phase elapsed time 뿐만 아니라 p.15에 색깔로 구분된 각 영역들(map - map/shuffle - shuffle - reduce)의 변화 양상도 잘 설명이 될 수 있다면 좋겠습니다.[2]</span>\n\n2. SSD 관련\n: <span style=\"color:blue\">>> 특히 보내주신 발표 슬라이드의 p.18에 언급된 SSD 사용 시 9.5% 감소 부분은 충격이었습니다. 이 현상을 유발한 원인이 명확하게 규명되고 (1), 그 규명된 원인이 기존에 알려지지 않았던 것이며 (2), 그것을 해소할 수 있는 방법도 같이 제시될 수 있다면 (3), 본 산학 과제가 더욱 큰 의미를 갖게 될 것 같습니다.[3]</span>\n\n: random write는 피하는 방향으로 SSD을 통한 하둡 수행 성능 향상이 나올 수 있도록 I/O contention의 오버헤드가 SSD 성능과 상쇄될 수 있도록 detail하게 실험.\n: network i/o와 disk i/o의 trade-off\n: <span style=\"color:blue\">>> 네, combine 등을 통해서 network overhead를 local overhead (local computation 및 local I/O)로 trade-off 시키게 되지만, 그 local overhead를 SSD로 상쇄시킬 수 있는 best 방법 및 예상 효과가 나온다면 매우 의미가 있을 것 같습니다.[4]</span>','utf-8'),(125,'== Git on the Server - Setting Up the Server ==\n\n* http://git-scm.com/book/en/Git-on-the-Server-Setting-Up-the-Server\n: Setting up the server // git-scm.com\n\n* http://git-scm.com/book/ch4-4.html\n: 4.4 Git on the Server - Setting Up the Server // git-scm.com\n\n\n: Let\'s walk through setting up SSH access on the server side. In this example, you\'ll use the authorized_keys method for authenticating your users. We also assume you\'re running a standard Linux distribution like Ubuntu. First, you create a \'git\' user and a .ssh directory for that user.\n\n <pre>\n$ sudo adduser git\n$ su git\n$ cd\n$ mkdir .ssh\n</pre>\n\n: Next, you need to add some developer SSH public keys to the authorized_keys file for that user. Let\'s assume you\'ve received a few keys by e-mail and saved them to temporary files. Again, the public keys look something like this:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCB007n/ww+ouN4gSLKssMxXnBOvf9LGt4L\nojG6rs6hPB09j9R/T17/x4lhJA0F3FR1rP6kYBRsWj2aThGw6HXLm9/5zytK6Ztg3RPKK+4k\nYjh6541NYsnEAZuXz0jTTyAUfrtU3Z5E003C4oxOj6H0rfIF1kKI9MAQLMdpGW1GYEIgS9Ez\nSdfd8AcCIicTDWbqLAcU4UpkaX8KyGlLwsNuuGztobF8m72ALC/nLF6JLtPofwFBlgc+myiv\nO7TCUSBdLQlgMVOFq1I2uPWQOkOWQAHukEOmfjy2jctxSDBQ220ymjaNsHT4kgtZg2AYYgPq\ndAv8JggJICUvax2T9va5 gsg-keypair\n</pre>\n\n: You just append them to your authorized_keys file:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.josie.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.jessica.pub >> ~/.ssh/authorized_keys\n</pre>\n\n: Now, you can set up an empty repository for them by running git init with the --bare option, which initializes the repository without a working directory:\n\n <pre>\n$ cd /opt/git\n$ mkdir project.git\n$ cd project.git\n$ git --bare init\n</pre>\n\n: Then, John, Josie, or Jessica can push the first version of their project into that repository by adding it as a remote and pushing up a branch. Note that someone must shell onto the machine and create a bare repository every time you want to add a project. Let\'s use gitserver as the hostname of the server on which you\'ve set up your \'git\' user and repository. If you\'re running it internally, and you set up DNS for gitserver to point to that server, then you can use the commands pretty much as is:\n\n: # on Johns computer\n <pre>\n$ cd myproject\n$ git init\n$ git add .\n$ git commit -m \'initial commit\'\n$ git remote add origin git@gitserver:/opt/git/project.git\n$ git push origin master\n</pre>\n\nAt this point, the others can clone it down and push changes back up just as easily:\n\n <pre>\n$ git clone git@gitserver:/opt/git/project.git\n$ cd project\n$ vim README\n$ git commit -am \'fix for the README file\'\n$ git push origin master\n</pre>\n\n: With this method, you can quickly get a read/write Git server up and running for a handful of developers.\n\n: As an extra precaution, you can easily restrict the \'git\' user to only doing Git activities with a limited shell tool called git-shell that comes with Git. If you set this as your \'git\' user\'s login shell, then the \'git\' user cannot have normal shell access to your server. To use this, specify git-shell instead of bash or csh for your user\'s login shell. To do so, you will likely have to edit your /etc/passwd file:\n\n <pre>\n$ sudo vim /etc/passwd\n</pre>\n\n: At the bottom, you should find a line that looks something like this:\n\n <pre>\ngit:x:1000:1000::/home/git:/bin/sh\n</pre>\n\n: Change /bin/sh to /usr/bin/git-shell (or run which git-shell to see where it’s installed). The line should look something like this:\n\n <pre>\ngit:x:1000:1000::/home/git:/usr/bin/git-shell\n</pre>\n\nNow, the \'git\' user can only use the SSH connection to push and pull Git repositories and cannot shell onto the machine. If you try, you will see a login rejection like this:\n\n <pre>\n$ ssh git@gitserver\nfatal: What do you think I am? A shell?\nConnection to gitserver closed.\n</pre>\n\n== Install GitWeb ==\n\n* [http://blog.countableset.ch/2012/04/29/ubuntu-12-dot-04-installing-gitolite-and-gitweb/ Installing gitolite and gitweb on Ubuntu 12.04]\n\nNote this is only tested on Ubuntu 12.04 Server with apache2, I\'m sure it would work on the desktop version also.\n\n=== Installing Git ===\n <pre>\nsudo apt-get install git-core\n</pre>\nOptional, setup git global settings:\n\n <pre>\ngit config --global user.name \"Your Name\"\ngit config --global user.email your@email.com\n</pre>\n\n=== SSH Key ===\nGenerate ssh public/private key on the machine you would like to access git repo from and copy it to the server into the /tmp/ directory, for reference here is the command:\n\n <pre>\nssh-keygen -t rsa -C \"name@computer\"\n</pre>\n\n==== Installing Gitolite ====\n\n <pre>\nsudo apt-get install gitolite\n</pre>\n\nCreate a user to access gitolite with, in this case I chose git since I don\'t like to type:\n\n <pre>\nsudo adduser \\\n    --system \\\n    --shell /bin/bash \\\n    --gecos \'git version control\' \\\n    --group \\\n    --disabled-password \\\n    --home /home/git \\\n    git\n</pre>\n\nNow login to the newly created user, and set the path, and move to its home directory:\n\n <pre>\nsudo su git\necho \"PATH=$HOME/bin:$PATH\" > ~/.bashrc\ncd\n</pre>\n\nRun the gitolite setup command with the public key you copied to the tmp directory to initialized the location for gitolite use. Then change the $REPO_UMASK to 0027 when it opens the .gitolite.rc for editing during the installation process. If it didn\'t open the file for any reason just open it up in vim (Note this is only if you\'d like to use gitweb):\n\n <pre>\ngl-setup /tmp/rachel.pub\n# change $REPO_UMASK = 0077; to $REPO_UMASK = 0027; # gets you \'rwxr-x---\'\n</pre>\n\nAfterward, it has made the gitolite-admin.git, testing.git repo and all other necessary files. Check to see that everything works by cloning the repo on the machine with the public/private key.\n\n <pre>\ngit clone git@<server>:gitolite-admin.git\n</pre>\n\nHere is a resource about the syntax for the config file and adding users.\n\n=== Install Gitweb ===\nThis is the tricky bit... Install gitweb and the highlight app. Gitweb is located at \'/usr/share/gitweb\'\n\n <pre>\nsudo apt-get install highlight gitweb\n</pre>\n\nEdit the gitweb config to the locations of the project list and repos, and add the highlighting bit at the end of the file:\n\n <pre>\nsudo vim /etc/gitweb.conf\n# change $projectroot to /home/git/repositories\n# change $projects_list to /home/git/projects.list\n# Add Highlighting at the end\n$feature{\'highlight\'}{\'default\'} = [1];\n</pre>\n\nChange gitolite instance to allow access for gitweb. First append www-data to git group so gitweb can access the repos, then change the permissions for git repos and the projects list, finally restart apache:\n\n <pre>\nsudo usermod -a -G git www-data\nsudo chmod g+r /home/git/projects.list\nsudo chmod -R g+rx /home/git/repositories\nsudo service apache2 restart\n</pre>\n\nFinally you need to tell gitolite which repo you want to show up in gitweb. To do this edit the gitolite.conf file from the gitolite-admin.git repo:\n\n <pre>\nrepo testing\n  RW+ = @all\n  R = gitweb\n</pre>','utf-8'),(126,'== Git on the Server - Setting Up the Server ==\n\n* http://git-scm.com/book/en/Git-on-the-Server-Setting-Up-the-Server\n: Setting up the server // git-scm.com\n\n* http://git-scm.com/book/ch4-4.html\n: 4.4 Git on the Server - Setting Up the Server // git-scm.com\n\n\n: Let\'s walk through setting up SSH access on the server side. In this example, you\'ll use the authorized_keys method for authenticating your users. We also assume you\'re running a standard Linux distribution like Ubuntu. First, you create a \'git\' user and a .ssh directory for that user.\n\n <pre>\n$ sudo adduser git\n$ su git\n$ cd\n$ mkdir .ssh\n</pre>\n\n: Next, you need to add some developer SSH public keys to the authorized_keys file for that user. Let\'s assume you\'ve received a few keys by e-mail and saved them to temporary files. Again, the public keys look something like this:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCB007n/ww+ouN4gSLKssMxXnBOvf9LGt4L\nojG6rs6hPB09j9R/T17/x4lhJA0F3FR1rP6kYBRsWj2aThGw6HXLm9/5zytK6Ztg3RPKK+4k\nYjh6541NYsnEAZuXz0jTTyAUfrtU3Z5E003C4oxOj6H0rfIF1kKI9MAQLMdpGW1GYEIgS9Ez\nSdfd8AcCIicTDWbqLAcU4UpkaX8KyGlLwsNuuGztobF8m72ALC/nLF6JLtPofwFBlgc+myiv\nO7TCUSBdLQlgMVOFq1I2uPWQOkOWQAHukEOmfjy2jctxSDBQ220ymjaNsHT4kgtZg2AYYgPq\ndAv8JggJICUvax2T9va5 gsg-keypair\n</pre>\n\n: You just append them to your authorized_keys file:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.josie.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.jessica.pub >> ~/.ssh/authorized_keys\n</pre>\n\n: Now, you can set up an empty repository for them by running git init with the --bare option, which initializes the repository without a working directory:\n\n <pre>\n$ cd /opt/git\n$ mkdir project.git\n$ cd project.git\n$ git --bare init\n</pre>\n\n: Then, John, Josie, or Jessica can push the first version of their project into that repository by adding it as a remote and pushing up a branch. Note that someone must shell onto the machine and create a bare repository every time you want to add a project. Let\'s use gitserver as the hostname of the server on which you\'ve set up your \'git\' user and repository. If you\'re running it internally, and you set up DNS for gitserver to point to that server, then you can use the commands pretty much as is:\n\n: # on Johns computer\n <pre>\n$ cd myproject\n$ git init\n$ git add .\n$ git commit -m \'initial commit\'\n$ git remote add origin git@gitserver:/opt/git/project.git\n$ git push origin master\n</pre>\n\nAt this point, the others can clone it down and push changes back up just as easily:\n\n <pre>\n$ git clone git@gitserver:/opt/git/project.git\n$ cd project\n$ vim README\n$ git commit -am \'fix for the README file\'\n$ git push origin master\n</pre>\n\n: With this method, you can quickly get a read/write Git server up and running for a handful of developers.\n\n: As an extra precaution, you can easily restrict the \'git\' user to only doing Git activities with a limited shell tool called git-shell that comes with Git. If you set this as your \'git\' user\'s login shell, then the \'git\' user cannot have normal shell access to your server. To use this, specify git-shell instead of bash or csh for your user\'s login shell. To do so, you will likely have to edit your /etc/passwd file:\n\n <pre>\n$ sudo vim /etc/passwd\n</pre>\n\n: At the bottom, you should find a line that looks something like this:\n\n <pre>\ngit:x:1000:1000::/home/git:/bin/sh\n</pre>\n\n: Change /bin/sh to /usr/bin/git-shell (or run which git-shell to see where it’s installed). The line should look something like this:\n\n <pre>\ngit:x:1000:1000::/home/git:/usr/bin/git-shell\n</pre>\n\nNow, the \'git\' user can only use the SSH connection to push and pull Git repositories and cannot shell onto the machine. If you try, you will see a login rejection like this:\n\n <pre>\n$ ssh git@gitserver\nfatal: What do you think I am? A shell?\nConnection to gitserver closed.\n</pre>\n\n== Install GitWeb ==\n\n* [http://blog.countableset.ch/2012/04/29/ubuntu-12-dot-04-installing-gitolite-and-gitweb/ Installing gitolite and gitweb on Ubuntu 12.04]\n\nNote this is only tested on Ubuntu 12.04 Server with apache2, I\'m sure it would work on the desktop version also.\n\n=== Installing Git ===\n <pre>\nsudo apt-get install git-core\n</pre>\nOptional, setup git global settings:\n\n <pre>\ngit config --global user.name \"Your Name\"\ngit config --global user.email your@email.com\n</pre>\n\n=== SSH Key ===\nGenerate ssh public/private key on the machine you would like to access git repo from and copy it to the server into the /tmp/ directory, for reference here is the command:\n\n <pre>\nssh-keygen -t rsa -C \"name@computer\"\n</pre>\n\n=== Installing Gitolite ===\n\n <pre>\nsudo apt-get install gitolite\n</pre>\n\nCreate a user to access gitolite with, in this case I chose git since I don\'t like to type:\n\n <pre>\nsudo adduser \\\n    --system \\\n    --shell /bin/bash \\\n    --gecos \'git version control\' \\\n    --group \\\n    --disabled-password \\\n    --home /home/git \\\n    git\n</pre>\n\nNow login to the newly created user, and set the path, and move to its home directory:\n\n <pre>\nsudo su git\necho \"PATH=$HOME/bin:$PATH\" > ~/.bashrc\ncd\n</pre>\n\nRun the gitolite setup command with the public key you copied to the tmp directory to initialized the location for gitolite use. Then change the $REPO_UMASK to 0027 when it opens the .gitolite.rc for editing during the installation process. If it didn\'t open the file for any reason just open it up in vim (Note this is only if you\'d like to use gitweb):\n\n <pre>\ngl-setup /tmp/rachel.pub\n# change $REPO_UMASK = 0077; to $REPO_UMASK = 0027; # gets you \'rwxr-x---\'\n</pre>\n\nAfterward, it has made the gitolite-admin.git, testing.git repo and all other necessary files. Check to see that everything works by cloning the repo on the machine with the public/private key.\n\n <pre>\ngit clone git@<server>:gitolite-admin.git\n</pre>\n\nHere is a resource about the syntax for the config file and adding users.\n\n=== Install Gitweb ===\nThis is the tricky bit... Install gitweb and the highlight app. Gitweb is located at \'/usr/share/gitweb\'\n\n <pre>\nsudo apt-get install highlight gitweb\n</pre>\n\nEdit the gitweb config to the locations of the project list and repos, and add the highlighting bit at the end of the file:\n\n <pre>\nsudo vim /etc/gitweb.conf\n# change $projectroot to /home/git/repositories\n# change $projects_list to /home/git/projects.list\n# Add Highlighting at the end\n$feature{\'highlight\'}{\'default\'} = [1];\n</pre>\n\nChange gitolite instance to allow access for gitweb. First append www-data to git group so gitweb can access the repos, then change the permissions for git repos and the projects list, finally restart apache:\n\n <pre>\nsudo usermod -a -G git www-data\nsudo chmod g+r /home/git/projects.list\nsudo chmod -R g+rx /home/git/repositories\nsudo service apache2 restart\n</pre>\n\nFinally you need to tell gitolite which repo you want to show up in gitweb. To do this edit the gitolite.conf file from the gitolite-admin.git repo:\n\n <pre>\nrepo testing\n  RW+ = @all\n  R = gitweb\n</pre>','utf-8'),(127,'== Git on the Server - Setting Up the Server ==\n\n: Let\'s walk through setting up SSH access on the server side. In this example, you\'ll use the authorized_keys method for authenticating your users. We also assume you\'re running a standard Linux distribution like Ubuntu. First, you create a \'git\' user and a .ssh directory for that user.\n\n <pre>\n$ sudo adduser git\n$ su git\n$ cd\n$ mkdir .ssh\n</pre>\n\n: Next, you need to add some developer SSH public keys to the authorized_keys file for that user. Let\'s assume you\'ve received a few keys by e-mail and saved them to temporary files. Again, the public keys look something like this:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCB007n/ww+ouN4gSLKssMxXnBOvf9LGt4L\nojG6rs6hPB09j9R/T17/x4lhJA0F3FR1rP6kYBRsWj2aThGw6HXLm9/5zytK6Ztg3RPKK+4k\nYjh6541NYsnEAZuXz0jTTyAUfrtU3Z5E003C4oxOj6H0rfIF1kKI9MAQLMdpGW1GYEIgS9Ez\nSdfd8AcCIicTDWbqLAcU4UpkaX8KyGlLwsNuuGztobF8m72ALC/nLF6JLtPofwFBlgc+myiv\nO7TCUSBdLQlgMVOFq1I2uPWQOkOWQAHukEOmfjy2jctxSDBQ220ymjaNsHT4kgtZg2AYYgPq\ndAv8JggJICUvax2T9va5 gsg-keypair\n</pre>\n\n: You just append them to your authorized_keys file:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.josie.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.jessica.pub >> ~/.ssh/authorized_keys\n</pre>\n\n: Now, you can set up an empty repository for them by running git init with the --bare option, which initializes the repository without a working directory:\n\n <pre>\n$ cd /opt/git\n$ mkdir project.git\n$ cd project.git\n$ git --bare init\n</pre>\n\n: Then, John, Josie, or Jessica can push the first version of their project into that repository by adding it as a remote and pushing up a branch. Note that someone must shell onto the machine and create a bare repository every time you want to add a project. Let\'s use gitserver as the hostname of the server on which you\'ve set up your \'git\' user and repository. If you\'re running it internally, and you set up DNS for gitserver to point to that server, then you can use the commands pretty much as is:\n\n: # on Johns computer\n <pre>\n$ cd myproject\n$ git init\n$ git add .\n$ git commit -m \'initial commit\'\n$ git remote add origin git@gitserver:/opt/git/project.git\n$ git push origin master\n</pre>\n\nAt this point, the others can clone it down and push changes back up just as easily:\n\n <pre>\n$ git clone git@gitserver:/opt/git/project.git\n$ cd project\n$ vim README\n$ git commit -am \'fix for the README file\'\n$ git push origin master\n</pre>\n\n: With this method, you can quickly get a read/write Git server up and running for a handful of developers.\n\n: As an extra precaution, you can easily restrict the \'git\' user to only doing Git activities with a limited shell tool called git-shell that comes with Git. If you set this as your \'git\' user\'s login shell, then the \'git\' user cannot have normal shell access to your server. To use this, specify git-shell instead of bash or csh for your user\'s login shell. To do so, you will likely have to edit your /etc/passwd file:\n\n <pre>\n$ sudo vim /etc/passwd\n</pre>\n\n: At the bottom, you should find a line that looks something like this:\n\n <pre>\ngit:x:1000:1000::/home/git:/bin/sh\n</pre>\n\n: Change /bin/sh to /usr/bin/git-shell (or run which git-shell to see where it’s installed). The line should look something like this:\n\n <pre>\ngit:x:1000:1000::/home/git:/usr/bin/git-shell\n</pre>\n\nNow, the \'git\' user can only use the SSH connection to push and pull Git repositories and cannot shell onto the machine. If you try, you will see a login rejection like this:\n\n <pre>\n$ ssh git@gitserver\nfatal: What do you think I am? A shell?\nConnection to gitserver closed.\n</pre>\n\n== Install GitWeb ==\n\n* [http://blog.countableset.ch/2012/04/29/ubuntu-12-dot-04-installing-gitolite-and-gitweb/ Installing gitolite and gitweb on Ubuntu 12.04]\n\nNote this is only tested on Ubuntu 12.04 Server with apache2, I\'m sure it would work on the desktop version also.\n\n=== Installing Git ===\n <pre>\nsudo apt-get install git-core\n</pre>\nOptional, setup git global settings:\n\n <pre>\ngit config --global user.name \"Your Name\"\ngit config --global user.email your@email.com\n</pre>\n\n=== SSH Key ===\nGenerate ssh public/private key on the machine you would like to access git repo from and copy it to the server into the /tmp/ directory, for reference here is the command:\n\n <pre>\nssh-keygen -t rsa -C \"name@computer\"\n</pre>\n\n=== Installing Gitolite ===\n\n <pre>\nsudo apt-get install gitolite\n</pre>\n\nCreate a user to access gitolite with, in this case I chose git since I don\'t like to type:\n\n <pre>\nsudo adduser \\\n    --system \\\n    --shell /bin/bash \\\n    --gecos \'git version control\' \\\n    --group \\\n    --disabled-password \\\n    --home /home/git \\\n    git\n</pre>\n\nNow login to the newly created user, and set the path, and move to its home directory:\n\n <pre>\nsudo su git\necho \"PATH=$HOME/bin:$PATH\" > ~/.bashrc\ncd\n</pre>\n\nRun the gitolite setup command with the public key you copied to the tmp directory to initialized the location for gitolite use. Then change the $REPO_UMASK to 0027 when it opens the .gitolite.rc for editing during the installation process. If it didn\'t open the file for any reason just open it up in vim (Note this is only if you\'d like to use gitweb):\n\n <pre>\ngl-setup /tmp/rachel.pub\n# change $REPO_UMASK = 0077; to $REPO_UMASK = 0027; # gets you \'rwxr-x---\'\n</pre>\n\nAfterward, it has made the gitolite-admin.git, testing.git repo and all other necessary files. Check to see that everything works by cloning the repo on the machine with the public/private key.\n\n <pre>\ngit clone git@<server>:gitolite-admin.git\n</pre>\n\nHere is a resource about the syntax for the config file and adding users.\n\n=== Install Gitweb ===\nThis is the tricky bit... Install gitweb and the highlight app. Gitweb is located at \'/usr/share/gitweb\'\n\n <pre>\nsudo apt-get install highlight gitweb\n</pre>\n\nEdit the gitweb config to the locations of the project list and repos, and add the highlighting bit at the end of the file:\n\n <pre>\nsudo vim /etc/gitweb.conf\n# change $projectroot to /home/git/repositories\n# change $projects_list to /home/git/projects.list\n# Add Highlighting at the end\n$feature{\'highlight\'}{\'default\'} = [1];\n</pre>\n\nChange gitolite instance to allow access for gitweb. First append www-data to git group so gitweb can access the repos, then change the permissions for git repos and the projects list, finally restart apache:\n\n <pre>\nsudo usermod -a -G git www-data\nsudo chmod g+r /home/git/projects.list\nsudo chmod -R g+rx /home/git/repositories\nsudo service apache2 restart\n</pre>\n\nFinally you need to tell gitolite which repo you want to show up in gitweb. To do this edit the gitolite.conf file from the gitolite-admin.git repo:\n\n <pre>\nrepo testing\n  RW+ = @all\n  R = gitweb\n</pre>','utf-8'),(128,'== Git on the Server - Setting Up the Server ==\n\n: Let\'s walk through setting up SSH access on the server side. In this example, you\'ll use the authorized_keys method for authenticating your users. We also assume you\'re running a standard Linux distribution like Ubuntu. First, you create a \'git\' user and a .ssh directory for that user.\n\n <pre>\n$ sudo adduser git\n$ su git\n$ cd\n$ mkdir .ssh\n</pre>\n\n: Next, you need to add some developer SSH public keys to the authorized_keys file for that user. Let\'s assume you\'ve received a few keys by e-mail and saved them to temporary files. Again, the public keys look something like this:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCB007n/ww+ouN4gSLKssMxXnBOvf9LGt4L\nojG6rs6hPB09j9R/T17/x4lhJA0F3FR1rP6kYBRsWj2aThGw6HXLm9/5zytK6Ztg3RPKK+4k\nYjh6541NYsnEAZuXz0jTTyAUfrtU3Z5E003C4oxOj6H0rfIF1kKI9MAQLMdpGW1GYEIgS9Ez\nSdfd8AcCIicTDWbqLAcU4UpkaX8KyGlLwsNuuGztobF8m72ALC/nLF6JLtPofwFBlgc+myiv\nO7TCUSBdLQlgMVOFq1I2uPWQOkOWQAHukEOmfjy2jctxSDBQ220ymjaNsHT4kgtZg2AYYgPq\ndAv8JggJICUvax2T9va5 gsg-keypair\n</pre>\n\n: You just append them to your authorized_keys file:\n\n <pre>\n$ cat /tmp/id_rsa.john.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.josie.pub >> ~/.ssh/authorized_keys\n$ cat /tmp/id_rsa.jessica.pub >> ~/.ssh/authorized_keys\n</pre>\n\n: Now, you can set up an empty repository for them by running git init with the --bare option, which initializes the repository without a working directory:\n\n <pre>\n$ cd /opt/git\n$ mkdir project.git\n$ cd project.git\n$ git --bare init\n</pre>\n\n: Then, John, Josie, or Jessica can push the first version of their project into that repository by adding it as a remote and pushing up a branch. Note that someone must shell onto the machine and create a bare repository every time you want to add a project. Let\'s use gitserver as the hostname of the server on which you\'ve set up your \'git\' user and repository. If you\'re running it internally, and you set up DNS for gitserver to point to that server, then you can use the commands pretty much as is:\n\n: # on Johns computer\n <pre>\n$ cd myproject\n$ git init\n$ git add .\n$ git commit -m \'initial commit\'\n$ git remote add origin git@gitserver:/opt/git/project.git\n$ git push origin master\n</pre>\n\nAt this point, the others can clone it down and push changes back up just as easily:\n\n <pre>\n$ git clone git@gitserver:/opt/git/project.git\n$ cd project\n$ vim README\n$ git commit -am \'fix for the README file\'\n$ git push origin master\n</pre>\n\n: With this method, you can quickly get a read/write Git server up and running for a handful of developers.\n\n: As an extra precaution, you can easily restrict the \'git\' user to only doing Git activities with a limited shell tool called git-shell that comes with Git. If you set this as your \'git\' user\'s login shell, then the \'git\' user cannot have normal shell access to your server. To use this, specify git-shell instead of bash or csh for your user\'s login shell. To do so, you will likely have to edit your /etc/passwd file:\n\n <pre>\n$ sudo vim /etc/passwd\n</pre>\n\n: At the bottom, you should find a line that looks something like this:\n\n <pre>\ngit:x:1000:1000::/home/git:/bin/sh\n</pre>\n\n: Change /bin/sh to /usr/bin/git-shell (or run which git-shell to see where it’s installed). The line should look something like this:\n\n <pre>\ngit:x:1000:1000::/home/git:/usr/bin/git-shell\n</pre>\n\nNow, the \'git\' user can only use the SSH connection to push and pull Git repositories and cannot shell onto the machine. If you try, you will see a login rejection like this:\n\n <pre>\n$ ssh git@gitserver\nfatal: What do you think I am? A shell?\nConnection to gitserver closed.\n</pre>\n\n== Install GitWeb ==\n\n* [http://blog.countableset.ch/2012/04/29/ubuntu-12-dot-04-installing-gitolite-and-gitweb/ Installing gitolite and gitweb on Ubuntu 12.04]\n\nNote this is only tested on Ubuntu 12.04 Server with apache2, I\'m sure it would work on the desktop version also.\n\n=== Installing Git ===\n <pre>\nsudo apt-get install git-core\n</pre>\nOptional, setup git global settings:\n\n <pre>\ngit config --global user.name \"Your Name\"\ngit config --global user.email your@email.com\n</pre>\n\n=== SSH Key ===\nGenerate ssh public/private key on the machine you would like to access git repo from and copy it to the server into the /tmp/ directory, for reference here is the command:\n\n <pre>\nssh-keygen -t rsa -C \"name@computer\"\n</pre>\n\n=== Installing Gitolite ===\n\n <pre>\nsudo apt-get install gitolite\n</pre>\n\nCreate a user to access gitolite with, in this case I chose git since I don\'t like to type:\n\n <pre>\nsudo adduser \\\n    --system \\\n    --shell /bin/bash \\\n    --gecos \'git version control\' \\\n    --group \\\n    --disabled-password \\\n    --home /home/git \\\n    git\n</pre>\n\nNow login to the newly created user, and set the path, and move to its home directory:\n\n <pre>\nsudo su git\necho \"PATH=$HOME/bin:$PATH\" > ~/.bashrc\ncd\n</pre>\n\nRun the gitolite setup command with the public key you copied to the tmp directory to initialized the location for gitolite use. Then change the $REPO_UMASK to 0027 when it opens the .gitolite.rc for editing during the installation process. If it didn\'t open the file for any reason just open it up in vim (Note this is only if you\'d like to use gitweb):\n\n <pre>\ngl-setup /tmp/rachel.pub\n# change $REPO_UMASK = 0077; to $REPO_UMASK = 0027; # gets you \'rwxr-x---\'\n</pre>\n\nAfterward, it has made the gitolite-admin.git, testing.git repo and all other necessary files. Check to see that everything works by cloning the repo on the machine with the public/private key.\n\n <pre>\ngit clone git@<server>:gitolite-admin.git\n</pre>\n\nHere is a resource about the syntax for the config file and adding users.\n\n=== Install Gitweb ===\nThis is the tricky bit... Install gitweb and the highlight app. Gitweb is located at \'/usr/share/gitweb\'\n\n <pre>\nsudo apt-get install highlight gitweb\n</pre>\n\nEdit the gitweb config to the locations of the project list and repos, and add the highlighting bit at the end of the file:\n\n <pre>\nsudo vim /etc/gitweb.conf\n# change $projectroot to /home/git/repositories\n# change $projects_list to /home/git/projects.list\n# Add Highlighting at the end\n$feature{\'highlight\'}{\'default\'} = [1];\n</pre>\n\nChange gitolite instance to allow access for gitweb. First append www-data to git group so gitweb can access the repos, then change the permissions for git repos and the projects list, finally restart apache:\n\n <pre>\nsudo usermod -a -G git www-data\nsudo chmod g+r /home/git/projects.list\nsudo chmod -R g+rx /home/git/repositories\nsudo service apache2 restart\n</pre>\n\nFinally you need to tell gitolite which repo you want to show up in gitweb. To do this edit the gitolite.conf file from the gitolite-admin.git repo:\n\n <pre>\nrepo testing\n  RW+ = @all\n  R = gitweb\n</pre>\n\n\n\n== References ==\n\n* http://git-scm.com/book/en/Git-on-the-Server-Setting-Up-the-Server\n: Setting up the server // git-scm.com\n\n* http://git-scm.com/book/ch4-4.html\n: 4.4 Git on the Server - Setting Up the Server // git-scm.com','utf-8'),(129,'== ## bNote-2013-02-28 ==\n\n=== 수원임직원주차장 방문예약 ===\n[https://www.sdigitalcity.com/security/visit/login.do 수원임직원주차장 방문예약]\n\n== ## bNote-2013-02-26 ==\n\n=== iowa.sidewinder ===\n\n* construction: filebench workload generator\n\n== ## bNote-2013-02-21 ==\n\n\n=== Daily.Kickoff ===\n\n* acsptrn outlook program\n:- kag (K-address-group)\n::- can \'K\' be a signature of a IO pattern?\n:::- what kind of implication can be extracted from \'KAG\'?\n::: : what size of chunk can have a meaning?\n::: : unit of tiering IO? unit of proactive caching? unit of IO dump size to the SSD?\n::- hamming distance (proper choice! why? difference value means the # of items to be added in cache) (older elements identified as a different one can be evicted or not, according to the cache space condition)\n:- outlook seek distance stat-dist\n:: - can \'seek distance\' be a metric for identifying hot zone?\n\n* cache simulation program\n:- hit ratio calculation, performance gain calculation\n\n* ML approach planning\n:- what to do for proactive data placement?\n::- what to do for static(?) macro insight?\n::- what to do for dynamic(?) micro prediction? (periodicity?)\n\n== ## bNote-2013-02-20 ==\n\n=== IOWA planning with Yanpei Chen Paper (SOSP 2011) ===\n\n\n\n\n\n\n\n=== Intel releases SSD cache acceleration software for Linux servers ===\n\n인텔이 Linux Server를 위한 CAS (Cache Acceleration Software) 기술을 Release했다는 소식입니다.\n\n이 CAS라는 기술은 작년에 인수한 Nevex사의 CacheWorks 기술에 기반하고 있습니다.\n\n특징으로 볼 수 있는 것은 기존 block cache의 문제점으로 지적되었던 DRAM에도 올라간 data가 SSD cache에도 올라가는 현상을 피하기 위해 초보적인 수준의 intelligence를 도입했다는 점입니다.\n\n(very hot data는 DRAM에 보내되, 덜 hot하지만 still I/O active data는 SSD로 나누어 보낸다는 것이 핵심입니다) \n\n\'\'\'Intel releases SSD cache acceleration software for Linux servers\'\'\'\n\nSSD-based CAS for Linux servers can offer up to 18 times the performance for read-intensive applications, Intel says. \n\n[http://www.infoworld.com/d/open-source-software/intel-releases-ssd-cache-acceleration-software-linux-servers-212673?page=0,0 InfoWorld News FEBRUARY 12, 2013]\n\n\n발췌 내용:\n...\nTo ensure it does not duplicate work already being performed in DRAM, the CAS software takes control of all data access. The hottest data is placed on DRAM, while less hot, but still I/O active data, is placed on SSD.\n\nIntel\'s CAS software provides cooperative integration between the standard server DRAM cache and the Intel SSD cache, creating a multi-level cache that optimizes the use of system memory and automatically determines the best cache level for active data.\n...\n\n== ## bNote-2013-02-19 ==\n\n\n=== Workload generation and tracing (with filebench, blktrace) ===\n\n* target workloads\n:- fileserver.f\n:- mongo.f\n:- netsfs.f\n:- networkfs.f\n:- oltp.f\n:- tpcso.f\n:- varmail.f\n:- videoserver.f\n:- webserver.f\n\n\n* video server workload (running time: 3600 seconds) (20130219_143400)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\n#>> cleaning up \'30.tmp/\' directory\n#>> ready for generating workload specified in \'20.conf/.tconf.sh\'\n#>> please run iotrace (blktrace) in \'34.iotrace/\'\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\n#>> [20130219_143400] waiting for iotrace getting started \n.................................\n----------------------------------------------------\n#>> Ok, now start workload generation\nstarted: 20130219_143433\n----------------------------------------------------\nFilebench Version 1.4.9.1\n 3599: 0.000: Allocated 170MB of shared memory\n 3599: 0.001: Eventgen rate taken from variable\n 3599: 0.001: Video Server Version 3.0 personality successfully loaded\n 3599: 0.001: Creating/pre-allocating files and filesets\n 3599: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n 3599: 0.002: Re-using fileset passivevids.\n 3599: 0.002: Creating fileset passivevids...\n 3599: 2227.782: Preallocated 104 of 194 of fileset passivevids in 2228 seconds\n 3599: 2227.800: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n 3599: 2227.800: Re-using fileset activevids.\n 3599: 2227.800: Creating fileset activevids...\n 3599: 2227.805: Preallocated 32 of 32 of fileset activevids in 1 seconds\n 3599: 2227.805: waiting for fileset pre-allocation to finish\n 3974: 4324.570: Starting 1 vidreaders instances\n 3974: 4324.599: Starting 1 vidwriter instances\n 3976: 4324.780: Starting 1 vidwriter threads\n 3975: 4324.790: Starting 48 vidreaders threads\n 3599: 4325.877: Running...\n 3599: 7926.189: Run took 3600 seconds...\n 3599: 7926.234: Per-Operation Breakdown\nserverlimit          423882ops      118ops/s   0.0mb/s    305.7ms/op    60431us/op-cpu [0ms - 2292ms]\nvidreader            423983ops      118ops/s  29.4mb/s    407.1ms/op    39674us/op-cpu [0ms - 2292ms]\nreplaceinterval      17ops        0ops/s   0.0mb/s  10000.1ms/op        0us/op-cpu [10000ms - 10000ms]\nwrtclose             17ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               17ops        0ops/s  47.2mb/s 134156.2ms/op 17297647us/op-cpu [3919ms - 421911ms]\nwrtopen              18ops        0ops/s   0.0mb/s     26.3ms/op     1111us/op-cpu [0ms - 247ms]\nvidremover           18ops        0ops/s   0.0mb/s  60936.6ms/op   281667us/op-cpu [69ms - 181905ms]\n 3599: 7926.234: IO Summary: 424053 ops, 117.782 ops/s, (118/0 r/w),  76.6mb/s,      0us cpu/op, 415.0ms latency\n 3599: 7926.234: Shutting down processes\n----------------------------------------------------\nfinished: 20130219_164641\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n\n* video server workload (running time: 60 seconds) (20130219_111030)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\n#>> cleaning up \'30.tmp/\' directory\n#>> ready for generating workload specified in \'20.conf/.tconf.sh\'\n#>> please run iotrace (blktrace) in \'34.iotrace/\'\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\n#>> [20130219_111030] waiting for iotrace getting started \n...........\n----------------------------------------------------\n#>> Ok, now start workload generation\nstarted: 20130219_111041\n----------------------------------------------------\nFilebench Version 1.4.9.1\n 2322: 0.000: Allocated 170MB of shared memory\n 2322: 0.001: Eventgen rate taken from variable\n 2322: 0.001: Video Server Version 3.0 personality successfully loaded\n 2322: 0.001: Creating/pre-allocating files and filesets\n 2322: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n 2322: 0.720: Removed any existing fileset passivevids in 1 seconds\n 2322: 0.720: making tree for filset /mnt/hdd_4/filebench/passivevids\n 2322: 0.721: Creating fileset passivevids...\n 2322: 5398.571: Preallocated 104 of 194 of fileset passivevids in 5398 seconds\n 2322: 5398.580: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n 2322: 5441.697: Removed any existing fileset activevids in 44 seconds\n 2322: 5441.698: making tree for filset /mnt/hdd_4/filebench/activevids\n 2322: 5441.698: Creating fileset activevids...\n 2322: 7548.330: Preallocated 32 of 32 of fileset activevids in 2107 seconds\n 2322: 7548.330: waiting for fileset pre-allocation to finish\n 2939: 9496.045: Starting 1 vidreaders instances\n 2939: 9496.077: Starting 1 vidwriter instances\n 2941: 9496.275: Starting 1 vidwriter threads\n 2940: 9496.294: Starting 48 vidreaders threads\n 2322: 9497.515: Running...\n 2322: 9557.521: Run took 60 seconds...\n 2322: 9557.540: Per-Operation Breakdown\nserverlimit          22911ops      382ops/s   0.0mb/s    116.4ms/op   129288us/op-cpu [0ms - 5662ms]\nvidreader            23044ops      384ops/s  95.8mb/s     11.3ms/op     3815us/op-cpu [0ms - 2135ms]\nreplaceinterval      0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtclose             0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtopen              1ops        0ops/s   0.0mb/s      0.1ms/op        0us/op-cpu [0ms - 0ms]\nvidremover           1ops        0ops/s   0.0mb/s   5112.0ms/op   110000us/op-cpu [5111ms - 5111ms]\n 2322: 9557.540: IO Summary: 23046 ops, 384.062 ops/s, (384/0 r/w),  95.8mb/s,      0us cpu/op,  11.6ms latency\n 2322: 9557.540: Shutting down processes\n----------------------------------------------------\nfinished: 20130219_135000\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n\n* video server workload (running time: 3600 seconds) (20130218_161548)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\nrm: cannot remove `wloadsrc.f\': No such file or directory\n#>> [20130218_161548] waiting for iotrace getting started \n........\n----------------------------------------------------\n#>> Ok, now start workload generation\n----------------------------------------------------\n20130218_161556\nFilebench Version 1.4.9.1\n30523: 0.000: Allocated 170MB of shared memory\n30523: 0.001: Eventgen rate taken from variable\n30523: 0.001: Video Server Version 3.0 personality successfully loaded\n30523: 0.001: Creating/pre-allocating files and filesets\n30523: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n30523: 5.754: Removed any existing fileset passivevids in 6 seconds\n30523: 5.754: making tree for filset /mnt/hdd_4/filebench/passivevids\n30523: 5.755: Creating fileset passivevids...\n30523: 5296.248: Preallocated 104 of 194 of fileset passivevids in 5291 seconds\n30523: 5296.261: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n30523: 5296.331: Removed any existing fileset activevids in 1 seconds\n30523: 5296.331: making tree for filset /mnt/hdd_4/filebench/activevids\n30523: 5296.331: Creating fileset activevids...\n30523: 8052.228: Preallocated 32 of 32 of fileset activevids in 2756 seconds\n30523: 8052.228: waiting for fileset pre-allocation to finish\n31345: 10248.763: Starting 1 vidreaders instances\n31345: 10248.804: Starting 1 vidwriter instances\n31346: 10248.868: Starting 48 vidreaders threads\n31347: 10249.084: Starting 1 vidwriter threads\n30523: 10250.088: Running...\n30523: 13850.427: Run took 3600 seconds...\n30523: 13850.506: Per-Operation Breakdown\nserverlimit          1228002ops      341ops/s   0.0mb/s    105.3ms/op    71013us/op-cpu [0ms - 2410ms]\nvidreader            1228102ops      341ops/s  85.3mb/s    140.0ms/op    46691us/op-cpu [0ms - 1919ms]\nreplaceinterval      0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtclose             0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtopen              1ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nvidremover           1ops        0ops/s   0.0mb/s     68.8ms/op    70000us/op-cpu [68ms - 68ms]\n30523: 13850.506: IO Summary: 1228104 ops, 341.104 ops/s, (341/0 r/w),  85.3mb/s,      0us cpu/op, 140.0ms latency\n30523: 13850.506: Shutting down processes\n20130218_200648\n----------------------------------------------------\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n== ## bNote-2013-02-18 ==\n\n=== Misc. ===\n\n* [https://h30613.www3.hp.com/media/files/downloads/Non-FilmedSessions/TB2216_Becoming.pdf Become a Flash expert in minutes! SAMSUNG]\n\n* [http://static.usenix.org/event/lsf08/tech/FS_shepler.pdf Filebench Spencer Shepler]\n\n\n=== blktrace knowledge ===\n\n\n <pre>\n  8,48   4     1657     0.426331441  3443  A   W 2593896448 + 1024 <- (8,49) 2593894400\n  8,48   4     1658     0.426332295  3443  Q   W 2593896448 + 1024 [flush-8:48]\n  8,48   4     1659     0.426334674  3443  G   W 2593896448 + 1024 [flush-8:48]\n  8,48   4     1660     0.426503502  3443  A   W 2593897472 + 1024 <- (8,49) 2593895424\n  8,48   4     1661     0.426503961  3443  Q   W 2593897472 + 1024 [flush-8:48]\n  8,48   4     1662     0.426505840  3443  G   W 2593897472 + 1024 [flush-8:48]\n  8,48   4     1663     0.426666260  3443  A   W 2593898496 + 1024 <- (8,49) 2593896448\n  8,48   4     1664     0.426666707  3443  Q   W 2593898496 + 1024 [flush-8:48]\n  8,48   4     1665     0.426668036  3443  G   W 2593898496 + 1024 [flush-8:48]\n  8,48   4     1666     0.426829266  3443  A   W 2593899520 + 1024 <- (8,49) 2593897472\n  8,48   4     1667     0.426829714  3443  Q   W 2593899520 + 1024 [flush-8:48]\n  8,48   4     1668     0.426831154  3443  G   W 2593899520 + 1024 [flush-8:48]\n</pre>\n\n <pre>\n2593896448 + 1024 <- (8,49) 2593894400\n2593896448 - 2593894400 = 2048\n\n2593897472 + 1024 <- (8,49) 2593895424\n2593897472 - 2593895424 = 2048\n\n2593898496 + 1024 <- (8,49) 2593896448\n2593898496 - 2593896448 = 2048\n\n2593899520 + 1024 <- (8,49) 2593897472\n2593899520 - 2593897472 = 2048\n</pre>\n\n\'\'\'2048\'\'\' is the offset of the partition (/dev/sdd1) from the start point of the block device (/dev/sdd)\n\n <pre>\nblusjune@radiohead:[20130219_143400--videoserver] $ sudo fdisk -l /dev/sdd\n[sudo] password for blusjune: \n\nDisk /dev/sdd: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x000c526d\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdd1            2048  3907026943  1953512448   83  Linux\n</pre>\n\n== ## bNote-2013-02-15 ==\n\n\n\n\n=== Hierarchical Temporal Memory ===\n\n* Numenta Hierarchical Temporal Memory - including HTM Cortical Learning Algorithms Version 0.2.1, September 12, 2011 [https://www.numenta.com/htm-overview/education/HTM_CorticalLearningAlgorithms.pdf Numenta Hierarchical Temporal Memory]\n\n# HTM Overview\n# HTM Cortical Learning Algorithms\n# Spatial Pooling Implementation and Pseudocode\n# Temporal Pooling Implementation and Pseudocode\n# Appendix A: A Comparison between Biological Neurons and HTM Cells\n# Appendix B: A Comparison of Layers in the Neocortex and an HTM Region\n\n==== About Numenta ====\n\n: Numenta, Inc. (www.numenta.com) was formed in 2005 // to develop HTM technology for both commercial and scientific use\n: Use of Numenta\'s software and intellectual property is free for research purposes.\n: Numenta will generate revenue by selling support, licensing software, and licensing intellectual property for commercial deployments.\n: Numenta is based in Redwood City, California. (Privately funded)\n\n\n==== Chapter 1. HTM Overview ====\n\n: Hierarchical Temporal Memory (HTM) is a machine learning technology that aims to capture the structural and algorithmic properties of the neocortex.\n\n: The neocortex is the seat of intelligent thought in the mammalian brain.\n\n: HTM\n\n=== SSD dirty script (iometer) ===\n\n[http://www.iometer.org/doc/downloads.html IOmeter dirty SSD script]\n\n\n=== BDP 근거 ===\n\n* 48시간 TPC-C 벤치마크한 blktrace log: 54.4GB\n* 1차 pre-processing: 52.2 GB\n\n== ## bNote-2013-02-14 ==\n\n\n=== 특허화 ===\n\n==== Patent #3 ====\n\n* (data + io insight) packaging 관련 특허\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n==== Patent #4 ====\n\n* IOWA based Proactive Data Placement 자체에 대한 특허\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 clue를 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보이는 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# clue, as simple as possible\n::::# clue, as specific as possible\n::::# clue, efficiently traversable - corresponding clue case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# clue, easily extensible - clue 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n==== Patent #5 ====\n\n* SSD Retention Time Controlling for Caching/Tiering 관련 특허\n\n\n==== Patent #6 ====\n\n* HML (Hierarchical ML)에서 FPGA, ASIC 등을 이용해서 Acceleration할 수 있는 방법은 없을까?\n\n== ## bNote-2013-02-12 ==\n\n\n\n=== IBdM 팀 회의 (15:40 ~ 17:00, 실험실) ===\n\n* 과제목표 (현재안): Intelligent Big Data Management 문제를 “H-ML을 통해” 남들보다 (EMC보다) 훨씬 잘 풀겠다!\n:- 요구사항\n::- 성공적으로 연구를 진행했을 경우의 최종 “기대치” 가 order-of-magnitude 높아야 함, 해결하는 문제가 수년간의 심도깊은 연구를 요구하여야 함\n::- I/O 성능 혁신 <- Data Placement 문제 : H-ML을 통한 Proactive data placement [70x improvement]\n::- 저장 효율 혁신 <- Data Reduction 문제 : H-ML을 통한 Clustered Deduplication [XXXX improvement] “Big Data를 Small Data로 만드는 기법”\n\n* Big Data Era에는 기존 Data Management 문제가 어떻게 달라지나?\n:- data양 폭증, IT budget증가 제한적 -> data reduction을 통한 IT 비용 절감 -> Data Deduplication 기술\n:- data capacity planning이 더 어려워짐 -> 필요에 따라 on-demand로 용량/성능 증설 요구 -> Scale-out 기술\n:- 비정형 data의 증가 -> NoSQL (?), Stream Processing (?)\n:- 비정형 data를 표현하는 데이터의 요구 증가 -> tagged data(metadata) 관리기술\n:: (data에 tag되어 data의 과거 access이력, access pattern, 복제/분열 history 등 intelligence정보를 저장)\n:- decentralized data(?) -> 자동화된 데이터 관리 중요 -> self-managed architecture 기술\n\n* 우리의 차별화\n:- Tool : hierarchical machine learning\n:- Input Data : 풍부한 tagged data (I/O workload, File I/O log, Application-level log, …)\n:- Motto\n::- Big Data의 근본적 이해를 통한 Big Data 관리기법 지능화\n::- Big Data의 생성, 확장, 분열, 복제, 소멸 등 full-cycle에 대한 근본적 이해\n\n* Action Item\n:- H-ML 기법 적용을 위한 data placement 문제 formulate, 기대치 estimation [융]\n:- H-ML 기법 적용을 위한 deduplication 문제 formulate, 기대치 estimation [신,구]\n:- “big data era에는 기존 data 관리 문제가 어떻게 달라지나?” [all]\n\n=== IOWA based Proactive Data Placement ===\n\n심상무님 지시.\n\n---- \n <pre>\n\nTM-20130212-IOWAPDP\n\n[ IO Workload Analysis 기반의\nProactive Data Placement ]\n\n\n\n2013-02-12\n\n\n\nData-intensive Storage\nIntelligence Group\nIntelligent Computing Lab\n</pre>\n\n----\n==== expected benefit ====\n\n <pre>\n이전에 얘기한 바가 있는데, proactive data placement와\nreactive data placement의 논리적인 비교를 통해\n원리상 기대되는 benefit의 크기 규모를 얘기할 수 있을까요?\n\n> 네, IO 성능 Penalty 최소화 측면과 SSD cache media 효율성 향상 측면에서\n  가늠해볼 수 있습니다. \n  \n  가) IO penalty 최소화\n    : 기존 방식은 미리 cache (혹은 fast-tier)에 proactive하게 올린다는 개념이 없었기 때문에\n      일단 최초 한 번은 HDD로부터 access되는 것이 필요하며,\n      그만큼 성능 penalty를 겪을 수 밖에 없습니다.\n      문제는 이렇게 성능 penalty를 겪는 경우가 얼마나 많이 존재하느냐입니다.\n\n      특히, \n      many-times-of-rehit 되는 small-number-of-data와 ................... (1)\n      small-times-of-rehit 되는 large-number-of-data 의 ................... (2)\n      분포 양상이 중요한데,\n      (1), (2) 각각이 전체 IO 에 얼만큼의 contribution을 해내는지가\n      proactive 방식과 reactive 방식과의 성능 차이에 영향을 미칩니다.\n\n      7200rpm의 HDD의 Read latency는 8500us,\n      SLC 기반의 NAND의 Read latency는 25us 라고 하고,\n      총 1,000 번의 IO access가 있었다고 했을 때,\n      A, B, C 각 workload 경우에 따른 IO 처리 소요 시간을 계산함으로써\n      reactive data placement 대비 proactive data placement의\n      성능 향상 정도를 가늠해보겠습니다.\n\n      (참고로, 아래 성능 비교 계산은 reactive Vs. proactive의 \n       핵심 logic에만 집중한 highly-abstracted 계산으로서,\n       블럭 레이어에서의 IO scheduling, HDD의 read-ahead, SATA/PCIe 버스 구조,\n       스토리지 디바이스 내부 구조 등 세부 시스템 적인 고려는 전혀 되어 있지 않음.\n       또한 cache media로 사용되는 SSD의 용량은 충분히 크다고 가정하였음)\n\n      A) 1개의 addr가 1,000번의 access를 다 받아낸 경우 (active range: 1) // 극단적 (1)의 경우\n         reactive 방식: 1 x 8500us + 999 x 25us = 33,475us\n         proactive 방식 (예측 적중률 100%): 1000 x 25us = 25,000us // IO 시간 1.3배 단축\n         proactive 방식 (예측 적중률 100% 미만): 1 x 8500us + 999 x 25us = 33,475us // IO 시간 단축 없음\n\n      B) 50개 addr가 400번의 access를, 150개 addr가 600번의 access를 받은 경우 (active range: 200)\n         reactive 방식: (50 x 8500us + 350 x 25us) + (150 x 8500us + 450 x 25us) = 1,720,000us\n         proactive 방식 (예측 적중률 100%): 1000 x 25us = 25,000us // IO 시간 68배 단축\n         proactive 방식 (예측 적중률 90%): 900 x 25us + 100 x 8500us = 872,500us // IO 시간 1.97배 단축\n\n      C) 1,000개 addr가 1번씩 다 access된 경우 (active range: 1,000) // 극단적 (2)의 경우\n         reactive 방식: 1000 x 8500us = 8,500,000us\n         proactive 방식 (100% 예측 적중시): 1000 x 25us = 25,000us // IO 시간 340배 단축\n   \n  나) SSD cache 미디어 효율성 향상\n    : 기존에는 미래에 다가올 IO에 대한 예측을 가지고 있지 않기 때문에\n      무조건 hit된 data들을 cache에 올리게 되며, 이후 자주 access될 data들과\n      그렇지 않은 data들이 모두 cache되므로, cache 공간의 낭비가 커집니다.\n      특히, data 규모가 커져서 SSD cache를 쓸 필요가 있는 경우,\n      SSD에 대한 write 및 erase를 많이 유발하게 되는 기존 방식은\n      SSD cache의 성능을 저하 시키는 중요한 요인이 될 수 있습니다.\n      이 측면에 대한 수치적 검증은, SSDsim 등을 이용하거나,\n      실제 SSD에 주어진 workload를 인가하여 IO 성능을 측정함으로써\n      실험적으로 증명해볼 수 있을 것 같습니다.\n</pre>\n\n----\n\n==== what is the heart of proactive data placement? ====\n <pre>\n대개 caching 알고리즘들은 데이터 I/O 패턴만 이용해서 cache에 남겨둘 데이터를 고르잖아요.\nproactive data placement 기술의 핵심은 미래의 데이터 I/O 패턴을 예측하는 것이라고 할 수 있나요?\n\n> 네, 맞습니다.\n  proactive data placement 기술의 핵심은 다가올 data IO 패턴 예측입니다.\n\n  상무님 말씀하신 것처럼,\n  기존의 cache 알고리즘들은 이미 cache된 data들에 대한\n  replacement (즉 eviction) 알고리즘이라고 보시면 됩니다.\n  즉, 일단은 access된 data들을 모두 cache에 올려두었다가,\n  그 cahced data들 중에서 \'미래에 덜 필요할 것\'같은 데이터를 골라내는 것입니다.\n  그러나 cache되어야 할 data를 예측하여 \'proactive 하게 미리\'\n  cache에 가져다놓는 것은 고려되어 있지 않습니다.\n  (이렇게 하지 못한 이유는, CPU L2/L3 cache와 DRAM 간의 cache이다보니\n   복잡한 알고리즘을 돌릴 시간 및 공간이 절대적으로 부족하기 때문이었습니다)\n	\n  기존 cache 알고리즘의 장점은 cache라는 공간에 data를 placement하는 알고리즘은\n  극도로 심플하지만, (별도의 알고리즘 없이 access된 data는 무조건 cache에 두므로)\n  일단 한 번이라도 data가 access되어야만 caching이 될 수 있다는 한계가 있습니다.\n\n  앞에서 이미 잠시 언급하였습니다만,\n  small-number-of-data가 many-times-of-rehit 이 된다면,\n  reactive data placement 방식과 proactive data placement 방식 간에\n  성능 차이가 크지 않을 수 있습니다.\n  그러나, active data range가 매우 넓은 경우에는\n  small-number-of-data가 many-times-of-rehit 되는 경우 뿐만 아니라,\n  large-number-of-data들이 small-times-of-rehit되는 경향 (long-tail)이\n  보여질 수 있기 때문에 proactive data placement에 의한 이득이 커질 수 있습니다.\n  (Active range가 매우 커지는 Big data 상황에서는\n   이렇게 될 가능성이 더 커질 것으로 보입니다)\n\n</pre>\n\n----\n\n==== Patent: IOWA based PDP ====\n <pre>\n미래의 데이터 I/O 패턴을 예측하는데 과거의 데이터 I/O 패턴만 볼 것이 아니라,\n데이터 자체의 특성, 파일 단위의 특성 (예를 들면, 이미지인지, 서버 로그인지, conf 파일인지),\nblock 자체의 특성 (뭐가 있을까요?), creator (user, process)의 종류,\nlast change/update time, 현재 active한 process들의 종류 등,\n다양한 데이터를 이용하는 것이 어떨까요?\n이런 방식이 새로운 아이디어라면 특허 출원을 검토해 주세요.\n\n> 네, 상무님, 다양한 데이터를 이용하는 것이 필요합니다.\n  이에, trace 기반의 access 패턴 뿐만 아니라 호스트 시스템에서 얻을 수 있는\n  정보들을 최대한 활용하고자 하고 있습니다.\n  그리고 이 부분에서 ML 기술의 적극적인 도입이 필요합니다.\n\n  (이 정보들이 IO 패턴 발굴 및 예측에 도움을 줄 수 있는지 유효성을 검증하고자\n   일부 정보들을 활용하여 이미 실험을 시작하고 있습니다.\n   현재는 IO 유발자 정보를 고려하여 실험 중입니다.)\n\n  고려중인 정보/접근 방식들은 다음과 같습니다.\n\n  - IO 액세스 패턴\n    - trace 기반의 address 별 access pattern\n      : periodicity 뿐만 아니라 recency, 누적 access count,\n        read/write intensiveness, access randomness, data usage-cycle 패턴\n        (read-modify-write(update)-read 등) 등\n      : 수집 시 blktrace 등 IO trace 툴 활용\n    - 시스템 meta 정보 기반 access pattern\n      : inode 등에서 얻어낼 수 있는 last create/update/access time 등\n      : 수집 시 inode 등 file, filesystem 정보 활용\n\n  - IO 유발자 정보\n    - user ID, group ID, process ID 등 creator 정보 및 현재 해당 file을 이용하는 \n      process / class 등\n      : 수집 시 debugfs, proc, sysfs 등의 메커니즘 활용\n      : IO 유발자가 어느 category에 속하는지 category 별로 분석\n        (web, DB, auth, monitoring, kernel thread (journald, kworker 등), ...)\n\n<!--\n  - State machine 기반의 macro IO pattern analsysis\n    : 유의미한 parameter 몇 가지로 정의되는 state machine을 정의하고,\n      이 state 간의 transition 확률을 이용하여 다가올 macro IO 패턴을 예측\n-->\n\n  특허 출원에 대해 말씀드리자면, 위의 기법 각각에 대한 특허에 대해서는\n  일부 시스템 등록, 혹은 현재 직무발명서 작성 중이었습니다만,\n  말씀 주신 바처럼 현재 저희가 진행하고 있는\n  IOWA 기반의 proactive data placement 자체에 대한\n  특허 가능성도 검토하도록 하겠습니다.\n\n  참고로, 2/12일 (오늘) 현재, 삼성 특허 검색 사이트에서, 검색식\n  ((proactive data placement) OR (data preplacement))\n  (cach* OR tiering)\n  ((IO OR I/O OR workload) analy*)\n  으로 찾아본 결과, 검색된 관련 특허는 없었습니다만,\n  검색 범위를 더 넓혀서 추가로 더 찾아보겠습니다.\n\n</pre>\n\n----\n\n==== Realistic sample trace log collection ====\n<pre>\n이렇게 다양한 데이터를 이용해서 데이터 I/O 패턴을 예측하려면 많은 sample log들을 모아서\n예측 함수를 만들어야 합니다. 분산 스토리지가 특정 용도에 이용되고 있다면\n(예로, 스트리밍 서버, 웹 서버, VDI 서버 등), 그것별로 sample log들을 많이 모아야 할 것 같습니다.\n우리가 어떻게 이런 다양한 로그들을 모을 수 있을까요?\n\n> 네, 전적으로 동감합니다. 수집 현황/계획은 아래와 같습니다.\n\n  1. Web Server IO Log\n    : 메모리사에서도 SAVL 타겟으로 우선적으로 관심을 가졌던 workload로서\n      SPECweb benchmark을 통해 realistic workload를 얻을 수 있습니다.\n      (메모리 사 응용기술팀으로부터 시스템 구축에 필요한 파일 받아옴)\n\n  2. DB IO Log\n    : TPC-C benchmark을 통해 realistic workload를 얻을 수 있습니다.\n      (이미 환경 구축 완료 및 사용 중)\n\n  3. Supercomputing IO Log\n    : 현재 기술원 3층의 수퍼컴퓨터 관리팀과 접촉 중\n      (Matlab, R, MapReduce-like RSP framework에 대한 workload 존재)\n\n  4. Streaming IO Log\n    : 아직 확보 방안 마련되지 않음\n		\n  5. VDI IO Log\n    : 기술원 정보전략팀과 의논 가능\n      (\'12년에 1차로 trace log 확보를 시도해본 적 있으나\n       정보전략팀과 SDS와의 관계가 어려워져 실패)\n\n</pre>\n\n----\n\n==== problem formulation considering on-line learning ====\n<pre>\n\n[ 지시 1 ]\n\n앞 경우는 sample log들을 모아서 예측 함수를 만드는 것이고,\n실제 deploy되었을 때는 dynamic 하게 예측 함수가 adaption할 수 있어야 하는데,\n그런 adaptation을 on-line learning으로 볼 수 있습니다.\n최희열 전문과 협력해서 위 문제들을 formulation 하기 바랍니다.\n\n> 네, 위의 내용들을 정리하여 최희열 전문과 함께 위의 문제들을 명확히 정리해보겠습니다.\n\n</pre>\n\n----\n\n== ## bNote-2013-02-08 ==\n\n=== Git Server on My Local Machine ===\n\n[[Git server on my local machine]]\n\n== ## bNote-2013-02-07 ==\n\n=== World Largest Data Centers ===\n\n* [http://wikibon.org/blog/inside-ten-of-the-worlds-largest-data-centers/ Inside Ten of the World’s Largest Data Centers, on March 25, 2010]\n\n\n=== Enterprise Storage Related Articles ===\n\n----\n==== The future of enterprise storage? ====\n: Henry Baltazar / Senior Analyst / Storage & Systems / The 451 Group\n: [http://www.snia.org/sites/default/files2/sdc_archives/2010_presentations/general_session/HenryBaltazar_Cloud_Convergence_Consolidation.pdf Clouds, Convergence, and Consolidation - SNIA SDC 2010]\n\n----\n==== Tiering vs. Caching ====\n; Automated Storage Tiering\n: Has become a popular technology in the past two years with multiple vendors embedding this feature within their storage systems.\n: Allows storage systems to migrate \'hot\' data to high speed flash-based storage tiers, while simultaneously moving stale data to inexpensive SATA hard drives.\n; Storage Caching\n: Primarily being driven by NetApp (i.e. Flash Cache) and a few startups, storage systems with caching use solid state storage as a memory extension technology.\n: Caching proponents claim that the migration required to do tiering is inefficient and reacts too slowly to eliminate hotspots.\n----\n\n=== Comparison of NAS vs. SAN // Pain Points of Storage System ===\n\n어제 회의의 Action Items 중 하나로, 제가 맡았던 부분인\n\"현재 Arch.가 야기하는 기술적 Poin Points 추가 정리\" 관련 메일입니다.\n\n내용은 다음과 같이 크게 두 가지로 구분됩니다.\n\n1. NAS vs. SAN 비교\n\n2. 기존 Storage System의 Pain Points (HP StorageWorks 제품 Feature를 통해 바라본)\n  -> 여기에는 이전문님께서 말씀하시곤 했던 ILM(Info. Lifecycle Mgmt.)에 \n     대한 언급도 나오네요. (더 자세한 내용은 첨부파일에 있습니다)\n\n// 아래 내용들은  HP에서 만든 \"Storage Overview and Architecture (HP, Arvind Shrivastava)\" 자료를 참고하였습니다. (첨부 1)  다소 NAS에 불리한 점들이 많이 부각되어 보입니다만, 다르게 보면, \"기존 NAS의 구조적 한계점이 이러했으니 Performance Drop 없이도 Scale-out 되는 NAS로 가기 위해서는 이러한 단점들이 극복되어야하겠다\" 라는 내용으로 받아들일 수도 있겠습니다.\n\n\n==== Comparisons of SAN, NAS (and DAS) ====\n\n{| border=1\n|+ \'\'\'DAS vs. NAS vs. SAN\'\'\'\n| Direct Attached Storage (DAS)\n| Network Attached Storage (NAS)\n| Storage Area Network (SAN)\n|-\n|\n*+ Low cost solution\n*+ Simple to configure\n*- De-centralized storage management\n*- No storage consolidation\n*- No high availability\n*- Low performance\n|\n*+ Heterogeneous environment\n*+ Centralized storage management\n*- Low performance\n*- Limited scalability\n*- Network congestion during backups & restore\n*- Ethernet limitations\n|\n*+ Heterogeneous environment\n*+ Centralized storage management\n*+ High degree of fault tolerance\n*+ Best and superior performance\n*+ Storage consolidation\n*+ Fast and efficient backups and restores\n*+ Dynamic scalability\n*- Expensive solution for small setups\n|-\n|}\n\n\n\n{| border=1\n|+ \'\'\'NAS vs. SAN\'\'\'\n| Items\n| style=\"width:40%\" | Network Attached Storage (NAS)\n| style=\"width:40%\" | Storage Area Network (SAN)\n|-\n| Network\n| IP\n| Fibre Channel\n|-\n| Reliability\n| Unreliable\n: >> FCoE, iSCSI 도 있는데 정말 그렇게 Unreliable인걸까요? FC에 비해 상대적으로 그렇다는 의미로 받아들이면 될까요?\n| Reliable\n|-\n| CPU overhead\n| Extremely High\n: >> \"Extremely High\" 까지일 줄은 몰랐네요. Block IO에 비해 File System Operation까지 해야 하는 NAS의 경우 CPU overhead가 더 있을 수 있겠다는 생각은 했었지만, 생각보다 overhead가 꽤 있는지도 모르겠습니다. (NetApp NAS, EMC Isilon 확인 필요).\n| Extremely Low\n: >> 반대로, SAN이 상대적으로 low-overhead라고 해서 \"Extremely\" 까지일 줄은...\n|-\n| Blocks\n| Large number of small blocks\n| Large blocks of data\n|-\n| Backup\n| LAN backup\n| LAN-free backup\n|-\n| Access and control by application\n| Applications driven by universal access to files\n| Applications managing own data specs\n|-\n| Typical usage\n|\n* Office applications file serving\n* File sharing between office users and CAD users\n* Content management for web servers\n* Consolidation of \'home directories\' for controlled and centralized backup\n* Boot device for diskless server farms\n* Rich media such as audio or video streaming\n|\n* Physical storage consolidation of any application mix\n* Physical storage sharing for cluster configurations\n* Sharing of backup devices (libraries)\n* Long distance data mirroring and/or data replication on storage device level\n* Redundant, high speed disk access for large database servers\n|}\n\n<br/>\n----\n\n==== HP Storage Product - StorageWorks ====\n\n\'\'\'Pain Points\'\'\'\n* HP의 storage 제품인 StorageWorks에서 언급된 pain points를 요약해보면 다음과 같습니다.\n\n# 네트워크 트래픽 증가로 인한 스토리지 시스템 성능 저하 (여기서 WAN을 언급하는 것으로 보아 물리적으로 멀리 떨어진 스토리지 간의 트래픽을 언급하는 것으로 보입니다)\n# 저장된 data의 대부분을 차지하는 (operational data에 비해 3~4배 이상 크기) reference data의 지속적 증가 (여기서 reference data라는 것은, 당장 operation되고 있지는 않지만, 이전에 생성되어 사용된 적은 있으나 폐기되지 않고 저장되어 있는, 그렇게 계속 쌓여가는 data들을 의미하는 것으로 보여집니다. 한마디로, operational data가 hot-data라면, reference data는 cold-data라고 볼 수 있을 것 같습니다)\n# 관리되고 저장된 data의 증가에 따라 속도 저하 및 TCO 증가\n# hetero한 사양 및 상태로 여러 곳에 흩어져있는 storage system의 관리\n\n\n\n\'\'\'HP StorageWorks - Reference Information Storage System\'\'\'\n:* 일종의 ILM (Information Lifecycle Management)\n:* Cost 절감 위해 application server/storage로부터 잘 사용되지 않는 static data 및 중복 data 감소.\n:* data의 integrity 보장.\n:* Power indexing 및 Grid computing을 통해 훨씬 빠른 search/retrieval을 달성하겠다.\n\n[[File:20130207 HP Storage Product Slides Scale is the problem .png|500px]]\n\n\n* Changing digital storage demands \n:* Majority is reference information (operational data 보다는 reference 정보가 저장된 data의 대부분 차지)\n\n[[File:20130207 HP Storage Product Slides - Changing digital storage demands .png|500px]] \n\n\n* WAN acceleration\n:* WAN을 통해 연결되는 스토리지 트래픽 감소 기술\n(100배 이상 빠른 throughput 달성 목표)\n{| border=1\n| Problems\n| style=\"width:40%\" | HP\'s solution\n| style=\"width:40%\" | What it does\n|-\n| Not enough bandwidth\n|\n* Scalable data referencing (SDR)\n|\n* Remove repeated bytes from the WAN for all TCP traffic\n|-\n| TCP chattiness and inefficiency\n|\n* Virtual Window Expansion\n* High speed TCP optimizations\n|\n* Optimizes performance of TCP\n* Enables much higher throughput on \"LFNs\"\n|-\n| Application chattiness and inefficiency\n| Transaction prediction (CIFS, MAPI)\n| Optimizes the performance of specific applications\n|}\n\n== ## bNote-2013-02-06 ==\n\n\n=== Defining: Target Segment, Product, Architecture ===\n금일 회의때 논의한 내용을 정리합니다.\n\n\n\'\'\'Consensus\'\'\'\n\n* Big data 시장만 보면 현재로서는 시장이 크지 않다. \n\n* HW Box를 같이 팔아야 한다. 그렇지 않으면 돈이 안된다.\n\n* Google, NHN등 big data center player에게 팔것은 아니다.\n\n* 삼성이 Enterprise Storage 사업 진입한다고 가정 (사업 진입유무를 우리가 고민하지 말자)\n\n* Large-scale system에서 data/node scale이 증가할수록 manageability issue가 커진다.\n\n* storage 신사업 초기에는 기존 infra에 대한 당사 제품의 compatibility가 매우 중요할 것이다. 이 부분은 신사업추진단이 고민할 것이고 우리는 2nd/3rd generation에 해당하는 innovative architecture에 집중한다.\n\n* new architecture는 고객의 현재/미래의 pain point에 기반해야 한다.\n\n \n\'\'\'Pain Point (고객입장)\'\'\'\n\n* \"Google같은 flexible/efficient data center 를 우리 회사 scale에 맞게 내부에서 소유하고 관리하고 싶습니다.\" (Google-in-a-box)\n\n* \"여러 인프라에 흩어진 기업데이터를 통합 관리하고 싶습니다.\" (for bigdata analytics?)\n\n* \"어떠한 경우에도 data loss가 없고 available한 불멸의 스토리지를 소유하고 싶습니다.\"\n\n* \"우리가 소유한 모든 infra를 통합관리하는 극강의 manageability가 필요합니다. 충분히 자동화되고 self-healing되어 관리자 1명당 관리노드의 갯수를 혁신적으로 올릴 수 있으면 좋겠습니다.\" (ML, Brain-inspired computing 적용가능성 있음)\n\n* replication 기술 (InformationWeek 자료에서 니즈 언급)\n\n\n\'\'\'Big Question\'\'\'\n\n* SAN ? NAS ? SAN+NAS ?\n\n* Server+Storage ?\n\n* up to which scale does the distribute architecture need to be expanded?\n\n* 고객이 별도의 big data platform (HW+SW) 을 구매할까? (in addition to already owned infra)\n\n* SW-defined storage 시대가 올까?\n\n* 왜 Google/NHN은 직접 data center를 buildup 했을까? 왜 EMC를 안쓸까?\n\n* InformationWeek 자료의 기술선호도가 2018년에는 어떻게 바뀔까?\n\n* Replication 기술이 중요한만큼 매우 어려울까?\n\n* 2018년에는 Cloud Storage 의존성이 어떻게 될까?\n\n* Data center model이 어떻게 다를까? : Google vs. Amazon (Fusion-IO구매) vs. Apple (Isilon구매)\n\n* service fee 로 먹고사는 biz model이 나올 수 있나?\n\n* Hadoop + Dedup? (data가 있는곳에 processing을! 이라는 hadoop philosophy가 dedup에 의해 어떤 영향을 받는가?)\n\n* block I/F, file I/F 이후 new interface가 어떻게 확산될까? 왜 object I/F는 확산되지 못했을까? \n\n* brain-like한 I/F는 어떤 형태가 될까?\n\n\n\'\'\'Action Items\'\'\'\n\n* 이: Google의 data center evolution history\n\n* 서: SAN/NAS, Converged/Storage, 분산 scale(100s? 1000? 10000?)\n\n* 신+구: 분산-scale storage에 분산-scale dedup이 적용되면 architecture가 어떻게 바뀌나? dedup manager의 위치가 어디가 되어야 하나?\n\n* 융: 현 architecture가 야기하는 기술적인 Pain Point 추가정리\n\n\n\'\'\'Check this out\'\'\'\n* 451 report provided by Seo-C: Automatic Tiering is the very hot technology\n\n== ## bNote-2013-02-05 ==\n\n\n=== IOWA:: machine learning ===\n\nk-addr-group\n\ndistance calculation and clustering\n\nhamming distance\n\n\n[[ bsc.iowa.lsp.addr.access_ptrn ]]\n\n\n[http://en.wikipedia.org/wiki/Levenshtein_distance Levenshtein distance]\n\n\n <pre>\n__k_addr_group__ 0 : [720616224, 3282792808, 3282793000, 3282793096]\n__k_addr_group__ 1 : [728218248, 730235992, 817521776, 3765476016]\n__k_addr_group__ 2 : [721421456, 724475672, 724475864, 3712136296]\n__k_addr_group__ 3 : [724475960, 828267560, 895981360, 3277437312]\n__k_addr_group__ 4 : [725469848, 726381104, 726381496, 726793728]\n__k_addr_group__ 5 : [723112360, 730730224, 3677713128, 3773683448]\n__k_addr_group__ 6 : [725235304, 828274704, 896778072, 3277440104]\n__k_addr_group__ 7 : [720987560, 724481496, 724727104, 725008864]\n__k_addr_group__ 8 : [545503608, 896780128, 3069345832, 3076725824]\n__k_addr_group__ 9 : [721386480, 721386960, 3752324048, 3767545408]\n__k_addr_group__ 10 : [725420616, 726671672, 727735192, 3066446120]\n__k_addr_group__ 11 : [721030800, 721030896, 721411176, 827916488]\n...\n__k_addr_group__ 245 : [725218552, 822170744, 870855200, 3642881480]\n__k_addr_group__ 246 : [457025608, 722055120, 3685634288, 3779707504]\n__k_addr_group__ 247 : [722055216, 722055312, 722055376, 725157784]\n__k_addr_group__ 248 : [721285200, 721285296, 728117992, 728118184]\n__k_addr_group__ 249 : [725218680, 725218808, 725218936, 725219032]\n</pre>\n\n\n\n=== Python:: numerical sort for string list ===\n\n <pre>\n\n>>> typeof(mlist)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name \'typeof\' is not defined\n>>> help(mlist)\n\n>>> mlist = {}\n>>> mlist[0] = []\n>>> mlist[1] = []\n>>> mlist[0] = [\"11243\", \"92\", \"2132\", \"42\"]\n>>> mlist[1] = [\"243\", \"892\", \"19132\", \"3242\"]\n>>> mlist[0] = [ int(x) for x in mlist[0] ]\n>>> mlist[0]\n[11243, 92, 2132, 42]\n>>> mlist[1]\n[\'243\', \'892\', \'19132\', \'3242\']\n>>> mlist[1] = [ int(x) for x in mlist[1] ]\n>>> mlist[1]\n[243, 892, 19132, 3242]\n>>> mlist   \n{0: [11243, 92, 2132, 42], 1: [243, 892, 19132, 3242]}\n>>> mlist[0].sort()\n>>> mlist\n{0: [42, 92, 2132, 11243], 1: [243, 892, 19132, 3242]}\n>>> mlist[1].sort()\n>>> mlist\n{0: [42, 92, 2132, 11243], 1: [243, 892, 3242, 19132]}\n>>> \n\n</pre>\n\n== ## bNote-2013-02-04 ==\n\n\n\n=== CWD ===\n\n <pre>\nb@ub04:[anal] $ pwd\n/x/iowa/now/tracelog_analysis.v02.Q_phase.2_mysqld/anal\nb@ub04:[anal] $ l\ntotal 36\ndrwxrwxr-x 8 b b 4096 Feb  4 19:23 ./\ndrwxrwxr-x 5 b b 4096 Feb  4 19:23 ../\n-rw-rw-r-- 1 b b  127 Jan 30 19:42 .bdx.0100.y.exec_iowa_for_all_cases.sh\ndrwxrwxr-x 2 b b 4096 Jan 30 19:38 .tools/\nlrwxrwxrwx 1 b b   27 Jan 30 15:55 .tracelog.A.addr -> ../preproc/.tracelog.A.addr\nlrwxrwxrwx 1 b b   27 Jan 30 18:48 .tracelog.R.addr -> ../preproc/.tracelog.R.addr\nlrwxrwxrwx 1 b b   27 Jan 30 18:48 .tracelog.W.addr -> ../preproc/.tracelog.W.addr\ndrwxrwxr-x 2 b b 4096 Jan 30 19:39 t00.R.xLBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:43 t01.R.1LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t02.R.100LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t03.R.10000LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t04.R.20000LBA/\n</pre>\n\n=== IOWA.data.tpcc_250gb_48h.mysql.R.addr:: Offset 100K, Len 1K ===\n\n* [[IOWA.data.tpcc_250gb_48h.mysql.R.addr:: Offset 100K, Len 1K]]\n\n=== Frequent Item Detection/Discovery ===\n\n==== Stanford Open Book - Mining of Massive Datasets ((B.GOOD)) ====\n\n* MMD Book [http://i.stanford.edu/~ullman/mmds.html]\n\n: The book has now been published by Cambridge University Press. The publisher is offering a 20% discount to anyone who buys the hardcopy Here. By agreement with the publisher, you can still download it free from this page. Cambridge Press does, however, retain copyright on the work, and we expect that you will obtain their permission and acknowledge our authorship if you republish parts or all of it. We are sorry to have to mention this point, but we have evidence that other items we have published on the Web have been appropriated and republished under other names. It is easy to detect such misuse, by the way, as you will learn in Chapter 3.\n: Anand Rajaraman (@anand_raj) and Jeff Ullman\n\n: Ch 3. Finding Similar Items [http://i.stanford.edu/~ullman/mmds/ch3.pdf]\n:: 3.1 Applications of Near-Neighbor Search\n::: 3.1.1 Jaccard Similarity of Sets\n:: 3.2 Shingling of Documents\n:: 3.3 Similarity-preserving Summaries of Sets\n:: 3.4 Locality-sensitive Hashing for Documents\n:: 3.5 Distance Measures\n:: 3.6 The Theory of Locality-sensitive Functions\n:: 3.7 LSH Families for Other Distance Measures\n:: 3.8 Applications of Locality-sensitive Hashing\n:: 3.9 Methods for High Degrees of Similarity\n\n: Ch 6. Frequent Itemsets [http://i.stanford.edu/~ullman/mmds/ch6.pdf]\n:: 6.1 The Market-Basket Model\n:: 6.2 Market Baskets and the A-Priori Algorithm\n:: 6.3 Handling Larger Datasets in Main Memory\n:: 6.4 Limited-pass Algorithms\n:: 6.5 Counting Frequent Items in a Stream\n\n==== Orange - open source data visualization and analysis tool ====\n\nOpen source data visualization and analysis for novice and experts. Data mining through visual programming or Python scripting. Components for machine learning. Add-ons for bioinformatics and text mining. Packed with features for data analytics.\n\n\n==== SAX - Symbolic Aggregate Approximation ====\n\nSAX (Symbolic Aggregate approXimation) Homepage [http://www.cs.ucr.edu/~eamonn/SAX.htm]\n\nSAX is the first symbolic representation for time series that allows for dimensionality reduction and indexing with a lower-bounding distance measure. In classic data mining tasks such as clustering, classification, index, etc., SAX is as good as well-known representations such as Discrete Wavelet Transform (DWT) and Discrete Fourier Transform (DFT), while requiring less storage space. In addition, the representation allows researchers to avail of the wealth of data structures and algorithms in bioinformatics or text mining, and also provides solutions to many challenges associated with current data mining tasks. One example is motif discovery, a problem which we defined for time series data. There is great potential for extending and applying the discrete representation on a wide class of data mining tasks.\n\n\n==== The Amadeus motif discovery platform ====\n\nAmadeus Home [http://acgt.cs.tau.ac.il/amadeus/]\n\nAmadeus is a user-friendly software platform for genome-scale detection of known and novel motifs (recurring patterns) in DNA sequences, applicable to a wide range of motif discovery tasks. Amadeus outperforms extant tools and sets a new standard for motif discovery software in terms of accuracy, running time, range of application, wealth of output information and ease of use.\n\n==== Motif Detection ====\n\n* Python for Bioinformatics [http://telliott99.blogspot.kr/2009/06/motif-discovery.html]\n* MOODS: Motif Occurrence Detection Suite - ALGOrithmic Data ANalysis [http://www.cs.helsinki.fi/group/pssmfind/]\n* CMU CS Lecture -- Motif Detection [http://www.cs.cmu.edu/~epxing/Class/10810-05/Lecture6.pdf]\n\n==== Biopython ====\n\nBiopython is a set of freely available tools for biological computation written in Python by an international team of developers. [http://biopython.org/wiki/Main_Page Biopython]\n\n* Required Python Modules [http://www.scipy.org/more_about_SciPy NumPy / SciPy (SciPy.org)]\n** NumPy: N-dimensional Array Manipulations\n** SciPy: Scientific tools for Python\n\n* References\n# [http://biopython.org/DIST/docs/tutorial/Tutorial.pdf Biopython Tutorial]\n\n<br/>\n----\n\n=== RAM-SSD 하이브리드 Page Cache 아키텍쳐 :: WIPS 의견 대응 ===\n <pre>\n접수번호 : RN-201301-004-1\n\n명칭 : RAM-SSD 하이브리드 Page Cache 아키텍쳐\n\n기술 핵심 : RAM과 SSD를 이용하여 디스크 data를 caching하는 구조가\npage cache layer에서 동작하도록 함으로써 hot access를 check하고\naccess 정도에 따라 RAM 또는 SSD에 저장(tier-1~2)하거나 disk에서 access되도록 하는 것\n\n구성 : A) page cache에서 동작하는 RAN-SSD hybrid cache architecture\n      B) tier-1 page cache(RAM), tier-2 page cache(SSD)\n      C) page에 대한 hit pattern으로 page들을 tier-1,2로 class 지정\n\n\n--\n안녕하세요? 정명준입니다.\n다루는 내용이 많았음에도 깔끔하게 잘 정리해주시느라 고생이 많으셨습니다. ^_^\n꼭 추가되었으면 하는 내용과, 서술의 톤이 살짝 바뀌면 좋을만한 부분이 있어\n아래 보내주신 메일 본문에 추가하였습니다.\n(제 의견은 파란색으로 표시되어있습니다)\n\n1) \'기술 핵심\' 부분\n\n	\"RAM과 SSD를 이용하여 디스크 data를 caching하는 구조가 page cache layer에서 동작하도록 함으로써\" \n		-> 본 발명의 핵심 부분인 부분을 잘 적어주셨습니다.\n\n	\"hot access를 check하고 access 정도에 따라\"\n		-> 이 부분은 \"access pattern을 점검하여 rehit-potential에 따라\" 라는 표현으로 바뀌면 더욱 좋겠습니다.\n		기존에는 recently hot (recency) 위주로 cache될지 evict될지를 판단하였는데,\n		본 발명에서는 rehit potential을 보고 판단하겠다는 점이 다릅니다.\n		(once-hot data 라든지, periodicity 등을 파악)\n\n	\"RAM 또는 SSD에 저장(tier-1~2)하거나 disk에서 access되도록 하는 것\"\n		-> 여기에 \"page chunk 단위로 page tiering을 한다\"는 언급이 되었으면 좋겠습니다. (청구의 범위를 너무 제한하는 것이 아니라면요)\n\n\n2) \'구성\' 부분\n\n	아래 \'가~바\' 부분이 본 발명에서 청구하고자 하는 주요 내용들입니다.\n	이 중에서 \'가\' 부분이 보내주신 메일에서 A, B, C로 적어주신 \'구성\' 부분인데요,\n	대표 청구항을 적자면 아래 \'가~바\' 부분이 모두 필요할 것 같습니다.\n\n	가) RAM-SSD hybrid page cache architecture\n		page cache layer에서 동작하고;\n		tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는;\n		page tiering 기반의 hybrid page cache 아키텍쳐.\n\n	나) rehit-potential aware page classification, and page class based cache data placement\n\n	다) page tiering operation in host side\n\n	라) page tiering operation in SSD side\n		chunk I/O in SSD in {log-structured and/or parallelism-maximized} manner\n\n	마) TRIM ahead 메커니즘 (page chunk writes latency를 줄이기 위해 미리 공간 확보하는)\n\n	바) victim page chunk selection in SSD\n		oldest-first\n		most-mark-first\n		with upper-tier-backup\n		\n\n오후에 전화로 말씀 나누면서 설명 드리겠습니다.		\n감사합니다.\n정명준 드림.\n</pre>\n\n\n=== 서울대 2013-02-01 Meeting Minutes ===\n\n1. 실험 관련\n: HDD 개수와 map slot의 개수를 align하여 실험 진행 후 기존 결과와 함께 분석\n: <span style=\"color:blue\">>>네, 본 실험에서 나타난 \'재미있는\' 현상이 Hadoop 권장 환경 구성과 맞지 않아서 발생한 것이 아니라 Hadoop 시스템 근원적으로 가지고 있는 \'한계\' 때문이라는 것으로 주장이 되려면, HDD 수와 execution slot 간 alignment는 필수적일 것 같습니다. 잘 부탁드리겠습니다.[1]</span>\n\n: In-memory buffer 크기 변화를 통한 spill file 개수 및 phase elapsed time의 관계를 exponential expression 으로 분석\n: <span style=\"color:blue\">>> 아, 제 발음이 좋지 않았던 것 같습니다. ^^;; exponential expression이라기 보다는 extrapolation이 가능한 equation으로 나오면 좋겠다는 것으로 이해해주시면 좋겠습니다.  아마 이게 되려면 충분한 경우에 대해서 실험이 되어야 할 것인데, equation이 나오기만 한다면 무척 중요한 의미를 갖게 될 것 같습니다. 전체적인 phase elapsed time 뿐만 아니라 p.15에 색깔로 구분된 각 영역들(map - map/shuffle - shuffle - reduce)의 변화 양상도 잘 설명이 될 수 있다면 좋겠습니다.[2]</span>\n\n2. SSD 관련\n: <span style=\"color:blue\">>> 특히 보내주신 발표 슬라이드의 p.18에 언급된 SSD 사용 시 9.5% 감소 부분은 충격이었습니다. 이 현상을 유발한 원인이 명확하게 규명되고 (1), 그 규명된 원인이 기존에 알려지지 않았던 것이며 (2), 그것을 해소할 수 있는 방법도 같이 제시될 수 있다면 (3), 본 산학 과제가 더욱 큰 의미를 갖게 될 것 같습니다.[3]</span>\n\n: random write는 피하는 방향으로 SSD을 통한 하둡 수행 성능 향상이 나올 수 있도록 I/O contention의 오버헤드가 SSD 성능과 상쇄될 수 있도록 detail하게 실험.\n: network i/o와 disk i/o의 trade-off\n: <span style=\"color:blue\">>> 네, combine 등을 통해서 network overhead를 local overhead (local computation 및 local I/O)로 trade-off 시키게 되지만, 그 local overhead를 SSD로 상쇄시킬 수 있는 best 방법 및 예상 효과가 나온다면 매우 의미가 있을 것 같습니다.[4]</span>','utf-8');
INSERT INTO `radiohead_text` VALUES (130,'== ## bNote-2013-02-28 ==\n\n=== 수원임직원주차장 방문예약 ===\n[https://www.sdigitalcity.com/security/visit/login.do 수원임직원주차장 방문예약]\n\n== ## bNote-2013-02-26 ==\n\n=== iowa.sidewinder ===\n\n* construction: filebench workload generator\n\n== ## bNote-2013-02-21 ==\n\n\n=== Daily.Kickoff ===\n\n* acsptrn outlook program\n:- kag (K-address-group)\n::- can \'K\' be a signature of a IO pattern?\n:::- what kind of implication can be extracted from \'KAG\'?\n::: : what size of chunk can have a meaning?\n::: : unit of tiering IO? unit of proactive caching? unit of IO dump size to the SSD?\n::- hamming distance (proper choice! why? difference value means the # of items to be added in cache) (older elements identified as a different one can be evicted or not, according to the cache space condition)\n:- outlook seek distance stat-dist\n:: - can \'seek distance\' be a metric for identifying hot zone?\n\n* cache simulation program\n:- hit ratio calculation, performance gain calculation\n\n* ML approach planning\n:- what to do for proactive data placement?\n::- what to do for static(?) macro insight?\n::- what to do for dynamic(?) micro prediction? (periodicity?)\n\n== ## bNote-2013-02-20 ==\n\n=== IOWA planning with Yanpei Chen Paper (SOSP 2011) ===\n\n\n\n\n\n\n\n=== Intel releases SSD cache acceleration software for Linux servers ===\n\n인텔이 Linux Server를 위한 CAS (Cache Acceleration Software) 기술을 Release했다는 소식입니다.\n\n이 CAS라는 기술은 작년에 인수한 Nevex사의 CacheWorks 기술에 기반하고 있습니다.\n\n특징으로 볼 수 있는 것은 기존 block cache의 문제점으로 지적되었던 DRAM에도 올라간 data가 SSD cache에도 올라가는 현상을 피하기 위해 초보적인 수준의 intelligence를 도입했다는 점입니다.\n\n(very hot data는 DRAM에 보내되, 덜 hot하지만 still I/O active data는 SSD로 나누어 보낸다는 것이 핵심입니다) \n\n\'\'\'Intel releases SSD cache acceleration software for Linux servers\'\'\'\n\nSSD-based CAS for Linux servers can offer up to 18 times the performance for read-intensive applications, Intel says. \n\n[http://www.infoworld.com/d/open-source-software/intel-releases-ssd-cache-acceleration-software-linux-servers-212673?page=0,0 InfoWorld News FEBRUARY 12, 2013]\n\n\n발췌 내용:\n...\nTo ensure it does not duplicate work already being performed in DRAM, the CAS software takes control of all data access. The hottest data is placed on DRAM, while less hot, but still I/O active data, is placed on SSD.\n\nIntel\'s CAS software provides cooperative integration between the standard server DRAM cache and the Intel SSD cache, creating a multi-level cache that optimizes the use of system memory and automatically determines the best cache level for active data.\n...\n\n== ## bNote-2013-02-19 ==\n\n\n=== Workload generation and tracing (with filebench, blktrace) ===\n\n* target workloads\n:- fileserver.f\n:- mongo.f\n:- netsfs.f\n:- networkfs.f\n:- oltp.f\n:- tpcso.f\n:- varmail.f\n:- videoserver.f\n:- webserver.f\n\n\n* video server workload (running time: 3600 seconds) (20130219_143400)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\n#>> cleaning up \'30.tmp/\' directory\n#>> ready for generating workload specified in \'20.conf/.tconf.sh\'\n#>> please run iotrace (blktrace) in \'34.iotrace/\'\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\n#>> [20130219_143400] waiting for iotrace getting started \n.................................\n----------------------------------------------------\n#>> Ok, now start workload generation\nstarted: 20130219_143433\n----------------------------------------------------\nFilebench Version 1.4.9.1\n 3599: 0.000: Allocated 170MB of shared memory\n 3599: 0.001: Eventgen rate taken from variable\n 3599: 0.001: Video Server Version 3.0 personality successfully loaded\n 3599: 0.001: Creating/pre-allocating files and filesets\n 3599: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n 3599: 0.002: Re-using fileset passivevids.\n 3599: 0.002: Creating fileset passivevids...\n 3599: 2227.782: Preallocated 104 of 194 of fileset passivevids in 2228 seconds\n 3599: 2227.800: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n 3599: 2227.800: Re-using fileset activevids.\n 3599: 2227.800: Creating fileset activevids...\n 3599: 2227.805: Preallocated 32 of 32 of fileset activevids in 1 seconds\n 3599: 2227.805: waiting for fileset pre-allocation to finish\n 3974: 4324.570: Starting 1 vidreaders instances\n 3974: 4324.599: Starting 1 vidwriter instances\n 3976: 4324.780: Starting 1 vidwriter threads\n 3975: 4324.790: Starting 48 vidreaders threads\n 3599: 4325.877: Running...\n 3599: 7926.189: Run took 3600 seconds...\n 3599: 7926.234: Per-Operation Breakdown\nserverlimit          423882ops      118ops/s   0.0mb/s    305.7ms/op    60431us/op-cpu [0ms - 2292ms]\nvidreader            423983ops      118ops/s  29.4mb/s    407.1ms/op    39674us/op-cpu [0ms - 2292ms]\nreplaceinterval      17ops        0ops/s   0.0mb/s  10000.1ms/op        0us/op-cpu [10000ms - 10000ms]\nwrtclose             17ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               17ops        0ops/s  47.2mb/s 134156.2ms/op 17297647us/op-cpu [3919ms - 421911ms]\nwrtopen              18ops        0ops/s   0.0mb/s     26.3ms/op     1111us/op-cpu [0ms - 247ms]\nvidremover           18ops        0ops/s   0.0mb/s  60936.6ms/op   281667us/op-cpu [69ms - 181905ms]\n 3599: 7926.234: IO Summary: 424053 ops, 117.782 ops/s, (118/0 r/w),  76.6mb/s,      0us cpu/op, 415.0ms latency\n 3599: 7926.234: Shutting down processes\n----------------------------------------------------\nfinished: 20130219_164641\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n\n* video server workload (running time: 60 seconds) (20130219_111030)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\n#>> cleaning up \'30.tmp/\' directory\n#>> ready for generating workload specified in \'20.conf/.tconf.sh\'\n#>> please run iotrace (blktrace) in \'34.iotrace/\'\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\n#>> [20130219_111030] waiting for iotrace getting started \n...........\n----------------------------------------------------\n#>> Ok, now start workload generation\nstarted: 20130219_111041\n----------------------------------------------------\nFilebench Version 1.4.9.1\n 2322: 0.000: Allocated 170MB of shared memory\n 2322: 0.001: Eventgen rate taken from variable\n 2322: 0.001: Video Server Version 3.0 personality successfully loaded\n 2322: 0.001: Creating/pre-allocating files and filesets\n 2322: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n 2322: 0.720: Removed any existing fileset passivevids in 1 seconds\n 2322: 0.720: making tree for filset /mnt/hdd_4/filebench/passivevids\n 2322: 0.721: Creating fileset passivevids...\n 2322: 5398.571: Preallocated 104 of 194 of fileset passivevids in 5398 seconds\n 2322: 5398.580: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n 2322: 5441.697: Removed any existing fileset activevids in 44 seconds\n 2322: 5441.698: making tree for filset /mnt/hdd_4/filebench/activevids\n 2322: 5441.698: Creating fileset activevids...\n 2322: 7548.330: Preallocated 32 of 32 of fileset activevids in 2107 seconds\n 2322: 7548.330: waiting for fileset pre-allocation to finish\n 2939: 9496.045: Starting 1 vidreaders instances\n 2939: 9496.077: Starting 1 vidwriter instances\n 2941: 9496.275: Starting 1 vidwriter threads\n 2940: 9496.294: Starting 48 vidreaders threads\n 2322: 9497.515: Running...\n 2322: 9557.521: Run took 60 seconds...\n 2322: 9557.540: Per-Operation Breakdown\nserverlimit          22911ops      382ops/s   0.0mb/s    116.4ms/op   129288us/op-cpu [0ms - 5662ms]\nvidreader            23044ops      384ops/s  95.8mb/s     11.3ms/op     3815us/op-cpu [0ms - 2135ms]\nreplaceinterval      0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtclose             0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtopen              1ops        0ops/s   0.0mb/s      0.1ms/op        0us/op-cpu [0ms - 0ms]\nvidremover           1ops        0ops/s   0.0mb/s   5112.0ms/op   110000us/op-cpu [5111ms - 5111ms]\n 2322: 9557.540: IO Summary: 23046 ops, 384.062 ops/s, (384/0 r/w),  95.8mb/s,      0us cpu/op,  11.6ms latency\n 2322: 9557.540: Shutting down processes\n----------------------------------------------------\nfinished: 20130219_135000\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n\n* video server workload (running time: 3600 seconds) (20130218_161548)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\nrm: cannot remove `wloadsrc.f\': No such file or directory\n#>> [20130218_161548] waiting for iotrace getting started \n........\n----------------------------------------------------\n#>> Ok, now start workload generation\n----------------------------------------------------\n20130218_161556\nFilebench Version 1.4.9.1\n30523: 0.000: Allocated 170MB of shared memory\n30523: 0.001: Eventgen rate taken from variable\n30523: 0.001: Video Server Version 3.0 personality successfully loaded\n30523: 0.001: Creating/pre-allocating files and filesets\n30523: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n30523: 5.754: Removed any existing fileset passivevids in 6 seconds\n30523: 5.754: making tree for filset /mnt/hdd_4/filebench/passivevids\n30523: 5.755: Creating fileset passivevids...\n30523: 5296.248: Preallocated 104 of 194 of fileset passivevids in 5291 seconds\n30523: 5296.261: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n30523: 5296.331: Removed any existing fileset activevids in 1 seconds\n30523: 5296.331: making tree for filset /mnt/hdd_4/filebench/activevids\n30523: 5296.331: Creating fileset activevids...\n30523: 8052.228: Preallocated 32 of 32 of fileset activevids in 2756 seconds\n30523: 8052.228: waiting for fileset pre-allocation to finish\n31345: 10248.763: Starting 1 vidreaders instances\n31345: 10248.804: Starting 1 vidwriter instances\n31346: 10248.868: Starting 48 vidreaders threads\n31347: 10249.084: Starting 1 vidwriter threads\n30523: 10250.088: Running...\n30523: 13850.427: Run took 3600 seconds...\n30523: 13850.506: Per-Operation Breakdown\nserverlimit          1228002ops      341ops/s   0.0mb/s    105.3ms/op    71013us/op-cpu [0ms - 2410ms]\nvidreader            1228102ops      341ops/s  85.3mb/s    140.0ms/op    46691us/op-cpu [0ms - 1919ms]\nreplaceinterval      0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtclose             0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtopen              1ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nvidremover           1ops        0ops/s   0.0mb/s     68.8ms/op    70000us/op-cpu [68ms - 68ms]\n30523: 13850.506: IO Summary: 1228104 ops, 341.104 ops/s, (341/0 r/w),  85.3mb/s,      0us cpu/op, 140.0ms latency\n30523: 13850.506: Shutting down processes\n20130218_200648\n----------------------------------------------------\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n== ## bNote-2013-02-18 ==\n\n=== Misc. ===\n\n* [https://h30613.www3.hp.com/media/files/downloads/Non-FilmedSessions/TB2216_Becoming.pdf Become a Flash expert in minutes! SAMSUNG]\n\n* [http://static.usenix.org/event/lsf08/tech/FS_shepler.pdf Filebench Spencer Shepler]\n\n\n=== blktrace knowledge ===\n\n\n <pre>\n  8,48   4     1657     0.426331441  3443  A   W 2593896448 + 1024 <- (8,49) 2593894400\n  8,48   4     1658     0.426332295  3443  Q   W 2593896448 + 1024 [flush-8:48]\n  8,48   4     1659     0.426334674  3443  G   W 2593896448 + 1024 [flush-8:48]\n  8,48   4     1660     0.426503502  3443  A   W 2593897472 + 1024 <- (8,49) 2593895424\n  8,48   4     1661     0.426503961  3443  Q   W 2593897472 + 1024 [flush-8:48]\n  8,48   4     1662     0.426505840  3443  G   W 2593897472 + 1024 [flush-8:48]\n  8,48   4     1663     0.426666260  3443  A   W 2593898496 + 1024 <- (8,49) 2593896448\n  8,48   4     1664     0.426666707  3443  Q   W 2593898496 + 1024 [flush-8:48]\n  8,48   4     1665     0.426668036  3443  G   W 2593898496 + 1024 [flush-8:48]\n  8,48   4     1666     0.426829266  3443  A   W 2593899520 + 1024 <- (8,49) 2593897472\n  8,48   4     1667     0.426829714  3443  Q   W 2593899520 + 1024 [flush-8:48]\n  8,48   4     1668     0.426831154  3443  G   W 2593899520 + 1024 [flush-8:48]\n</pre>\n\n <pre>\n2593896448 + 1024 <- (8,49) 2593894400\n2593896448 - 2593894400 = 2048\n\n2593897472 + 1024 <- (8,49) 2593895424\n2593897472 - 2593895424 = 2048\n\n2593898496 + 1024 <- (8,49) 2593896448\n2593898496 - 2593896448 = 2048\n\n2593899520 + 1024 <- (8,49) 2593897472\n2593899520 - 2593897472 = 2048\n</pre>\n\n\'\'\'2048\'\'\' is the offset of the partition (/dev/sdd1) from the start point of the block device (/dev/sdd)\n\n <pre>\nblusjune@radiohead:[20130219_143400--videoserver] $ sudo fdisk -l /dev/sdd\n[sudo] password for blusjune: \n\nDisk /dev/sdd: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x000c526d\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdd1            2048  3907026943  1953512448   83  Linux\n</pre>\n\n== ## bNote-2013-02-15 ==\n\n\n\n\n=== Hierarchical Temporal Memory ===\n\n* Numenta Hierarchical Temporal Memory - including HTM Cortical Learning Algorithms Version 0.2.1, September 12, 2011 [https://www.numenta.com/htm-overview/education/HTM_CorticalLearningAlgorithms.pdf Numenta Hierarchical Temporal Memory]\n\n# HTM Overview\n# HTM Cortical Learning Algorithms\n# Spatial Pooling Implementation and Pseudocode\n# Temporal Pooling Implementation and Pseudocode\n# Appendix A: A Comparison between Biological Neurons and HTM Cells\n# Appendix B: A Comparison of Layers in the Neocortex and an HTM Region\n\n==== About Numenta ====\n\n: Numenta, Inc. (www.numenta.com) was formed in 2005 // to develop HTM technology for both commercial and scientific use\n: Use of Numenta\'s software and intellectual property is free for research purposes.\n: Numenta will generate revenue by selling support, licensing software, and licensing intellectual property for commercial deployments.\n: Numenta is based in Redwood City, California. (Privately funded)\n\n\n==== Chapter 1. HTM Overview ====\n\n: Hierarchical Temporal Memory (HTM) is a machine learning technology that aims to capture the structural and algorithmic properties of the neocortex.\n\n: The neocortex is the seat of intelligent thought in the mammalian brain.\n\n: HTM\n\n=== SSD dirty script (iometer) ===\n\n[http://www.iometer.org/doc/downloads.html IOmeter dirty SSD script]\n\n\n=== BDP 근거 ===\n\n* 48시간 TPC-C 벤치마크한 blktrace log: 54.4GB\n* 1차 pre-processing: 52.2 GB\n\n== ## bNote-2013-02-14 ==\n\n\n=== 특허화 ===\n\n==== Patent #3 ====\n\n* (data + io insight) packaging 관련 특허\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n==== Patent #4 ====\n\n* IOWA based Proactive Data Placement 자체에 대한 특허\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 clue를 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보이는 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# clue, as simple as possible\n::::# clue, as specific as possible\n::::# clue, efficiently traversable - corresponding clue case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# clue, easily extensible - clue 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n==== Patent #5 ====\n\n* SSD Retention Time Controlling for Caching/Tiering 관련 특허\n\n\n==== Patent #6 ====\n\n* HML (Hierarchical ML)에서 FPGA, ASIC 등을 이용해서 Acceleration할 수 있는 방법은 없을까?\n\n== ## bNote-2013-02-12 ==\n\n\n\n=== IBdM 팀 회의 (15:40 ~ 17:00, 실험실) ===\n\n* 과제목표 (현재안): Intelligent Big Data Management 문제를 “H-ML을 통해” 남들보다 (EMC보다) 훨씬 잘 풀겠다!\n:- 요구사항\n::- 성공적으로 연구를 진행했을 경우의 최종 “기대치” 가 order-of-magnitude 높아야 함, 해결하는 문제가 수년간의 심도깊은 연구를 요구하여야 함\n::- I/O 성능 혁신 <- Data Placement 문제 : H-ML을 통한 Proactive data placement [70x improvement]\n::- 저장 효율 혁신 <- Data Reduction 문제 : H-ML을 통한 Clustered Deduplication [XXXX improvement] “Big Data를 Small Data로 만드는 기법”\n\n* Big Data Era에는 기존 Data Management 문제가 어떻게 달라지나?\n:- data양 폭증, IT budget증가 제한적 -> data reduction을 통한 IT 비용 절감 -> Data Deduplication 기술\n:- data capacity planning이 더 어려워짐 -> 필요에 따라 on-demand로 용량/성능 증설 요구 -> Scale-out 기술\n:- 비정형 data의 증가 -> NoSQL (?), Stream Processing (?)\n:- 비정형 data를 표현하는 데이터의 요구 증가 -> tagged data(metadata) 관리기술\n:: (data에 tag되어 data의 과거 access이력, access pattern, 복제/분열 history 등 intelligence정보를 저장)\n:- decentralized data(?) -> 자동화된 데이터 관리 중요 -> self-managed architecture 기술\n\n* 우리의 차별화\n:- Tool : hierarchical machine learning\n:- Input Data : 풍부한 tagged data (I/O workload, File I/O log, Application-level log, …)\n:- Motto\n::- Big Data의 근본적 이해를 통한 Big Data 관리기법 지능화\n::- Big Data의 생성, 확장, 분열, 복제, 소멸 등 full-cycle에 대한 근본적 이해\n\n* Action Item\n:- H-ML 기법 적용을 위한 data placement 문제 formulate, 기대치 estimation [융]\n:- H-ML 기법 적용을 위한 deduplication 문제 formulate, 기대치 estimation [신,구]\n:- “big data era에는 기존 data 관리 문제가 어떻게 달라지나?” [all]\n\n=== IOWA based Proactive Data Placement ===\n\n심상무님 지시.\n\n---- \n <pre>\n\nTM-20130212-IOWAPDP\n\n[ IO Workload Analysis 기반의\nProactive Data Placement ]\n\n\n\n2013-02-12\n\n\n\nData-intensive Storage\nIntelligence Group\nIntelligent Computing Lab\n</pre>\n\n----\n==== expected benefit ====\n\n <pre>\n이전에 얘기한 바가 있는데, proactive data placement와\nreactive data placement의 논리적인 비교를 통해\n원리상 기대되는 benefit의 크기 규모를 얘기할 수 있을까요?\n\n> 네, IO 성능 Penalty 최소화 측면과 SSD cache media 효율성 향상 측면에서\n  가늠해볼 수 있습니다. \n  \n  가) IO penalty 최소화\n    : 기존 방식은 미리 cache (혹은 fast-tier)에 proactive하게 올린다는 개념이 없었기 때문에\n      일단 최초 한 번은 HDD로부터 access되는 것이 필요하며,\n      그만큼 성능 penalty를 겪을 수 밖에 없습니다.\n      문제는 이렇게 성능 penalty를 겪는 경우가 얼마나 많이 존재하느냐입니다.\n\n      특히, \n      many-times-of-rehit 되는 small-number-of-data와 ................... (1)\n      small-times-of-rehit 되는 large-number-of-data 의 ................... (2)\n      분포 양상이 중요한데,\n      (1), (2) 각각이 전체 IO 에 얼만큼의 contribution을 해내는지가\n      proactive 방식과 reactive 방식과의 성능 차이에 영향을 미칩니다.\n\n      7200rpm의 HDD의 Read latency는 8500us,\n      SLC 기반의 NAND의 Read latency는 25us 라고 하고,\n      총 1,000 번의 IO access가 있었다고 했을 때,\n      A, B, C 각 workload 경우에 따른 IO 처리 소요 시간을 계산함으로써\n      reactive data placement 대비 proactive data placement의\n      성능 향상 정도를 가늠해보겠습니다.\n\n      (참고로, 아래 성능 비교 계산은 reactive Vs. proactive의 \n       핵심 logic에만 집중한 highly-abstracted 계산으로서,\n       블럭 레이어에서의 IO scheduling, HDD의 read-ahead, SATA/PCIe 버스 구조,\n       스토리지 디바이스 내부 구조 등 세부 시스템 적인 고려는 전혀 되어 있지 않음.\n       또한 cache media로 사용되는 SSD의 용량은 충분히 크다고 가정하였음)\n\n      A) 1개의 addr가 1,000번의 access를 다 받아낸 경우 (active range: 1) // 극단적 (1)의 경우\n         reactive 방식: 1 x 8500us + 999 x 25us = 33,475us\n         proactive 방식 (예측 적중률 100%): 1000 x 25us = 25,000us // IO 시간 1.3배 단축\n         proactive 방식 (예측 적중률 100% 미만): 1 x 8500us + 999 x 25us = 33,475us // IO 시간 단축 없음\n\n      B) 50개 addr가 400번의 access를, 150개 addr가 600번의 access를 받은 경우 (active range: 200)\n         reactive 방식: (50 x 8500us + 350 x 25us) + (150 x 8500us + 450 x 25us) = 1,720,000us\n         proactive 방식 (예측 적중률 100%): 1000 x 25us = 25,000us // IO 시간 68배 단축\n         proactive 방식 (예측 적중률 90%): 900 x 25us + 100 x 8500us = 872,500us // IO 시간 1.97배 단축\n\n      C) 1,000개 addr가 1번씩 다 access된 경우 (active range: 1,000) // 극단적 (2)의 경우\n         reactive 방식: 1000 x 8500us = 8,500,000us\n         proactive 방식 (100% 예측 적중시): 1000 x 25us = 25,000us // IO 시간 340배 단축\n   \n  나) SSD cache 미디어 효율성 향상\n    : 기존에는 미래에 다가올 IO에 대한 예측을 가지고 있지 않기 때문에\n      무조건 hit된 data들을 cache에 올리게 되며, 이후 자주 access될 data들과\n      그렇지 않은 data들이 모두 cache되므로, cache 공간의 낭비가 커집니다.\n      특히, data 규모가 커져서 SSD cache를 쓸 필요가 있는 경우,\n      SSD에 대한 write 및 erase를 많이 유발하게 되는 기존 방식은\n      SSD cache의 성능을 저하 시키는 중요한 요인이 될 수 있습니다.\n      이 측면에 대한 수치적 검증은, SSDsim 등을 이용하거나,\n      실제 SSD에 주어진 workload를 인가하여 IO 성능을 측정함으로써\n      실험적으로 증명해볼 수 있을 것 같습니다.\n</pre>\n\n----\n\n==== what is the heart of proactive data placement? ====\n <pre>\n대개 caching 알고리즘들은 데이터 I/O 패턴만 이용해서 cache에 남겨둘 데이터를 고르잖아요.\nproactive data placement 기술의 핵심은 미래의 데이터 I/O 패턴을 예측하는 것이라고 할 수 있나요?\n\n> 네, 맞습니다.\n  proactive data placement 기술의 핵심은 다가올 data IO 패턴 예측입니다.\n\n  상무님 말씀하신 것처럼,\n  기존의 cache 알고리즘들은 이미 cache된 data들에 대한\n  replacement (즉 eviction) 알고리즘이라고 보시면 됩니다.\n  즉, 일단은 access된 data들을 모두 cache에 올려두었다가,\n  그 cahced data들 중에서 \'미래에 덜 필요할 것\'같은 데이터를 골라내는 것입니다.\n  그러나 cache되어야 할 data를 예측하여 \'proactive 하게 미리\'\n  cache에 가져다놓는 것은 고려되어 있지 않습니다.\n  (이렇게 하지 못한 이유는, CPU L2/L3 cache와 DRAM 간의 cache이다보니\n   복잡한 알고리즘을 돌릴 시간 및 공간이 절대적으로 부족하기 때문이었습니다)\n	\n  기존 cache 알고리즘의 장점은 cache라는 공간에 data를 placement하는 알고리즘은\n  극도로 심플하지만, (별도의 알고리즘 없이 access된 data는 무조건 cache에 두므로)\n  일단 한 번이라도 data가 access되어야만 caching이 될 수 있다는 한계가 있습니다.\n\n  앞에서 이미 잠시 언급하였습니다만,\n  small-number-of-data가 many-times-of-rehit 이 된다면,\n  reactive data placement 방식과 proactive data placement 방식 간에\n  성능 차이가 크지 않을 수 있습니다.\n  그러나, active data range가 매우 넓은 경우에는\n  small-number-of-data가 many-times-of-rehit 되는 경우 뿐만 아니라,\n  large-number-of-data들이 small-times-of-rehit되는 경향 (long-tail)이\n  보여질 수 있기 때문에 proactive data placement에 의한 이득이 커질 수 있습니다.\n  (Active range가 매우 커지는 Big data 상황에서는\n   이렇게 될 가능성이 더 커질 것으로 보입니다)\n\n</pre>\n\n----\n\n==== Patent: IOWA based PDP ====\n <pre>\n미래의 데이터 I/O 패턴을 예측하는데 과거의 데이터 I/O 패턴만 볼 것이 아니라,\n데이터 자체의 특성, 파일 단위의 특성 (예를 들면, 이미지인지, 서버 로그인지, conf 파일인지),\nblock 자체의 특성 (뭐가 있을까요?), creator (user, process)의 종류,\nlast change/update time, 현재 active한 process들의 종류 등,\n다양한 데이터를 이용하는 것이 어떨까요?\n이런 방식이 새로운 아이디어라면 특허 출원을 검토해 주세요.\n\n> 네, 상무님, 다양한 데이터를 이용하는 것이 필요합니다.\n  이에, trace 기반의 access 패턴 뿐만 아니라 호스트 시스템에서 얻을 수 있는\n  정보들을 최대한 활용하고자 하고 있습니다.\n  그리고 이 부분에서 ML 기술의 적극적인 도입이 필요합니다.\n\n  (이 정보들이 IO 패턴 발굴 및 예측에 도움을 줄 수 있는지 유효성을 검증하고자\n   일부 정보들을 활용하여 이미 실험을 시작하고 있습니다.\n   현재는 IO 유발자 정보를 고려하여 실험 중입니다.)\n\n  고려중인 정보/접근 방식들은 다음과 같습니다.\n\n  - IO 액세스 패턴\n    - trace 기반의 address 별 access pattern\n      : periodicity 뿐만 아니라 recency, 누적 access count,\n        read/write intensiveness, access randomness, data usage-cycle 패턴\n        (read-modify-write(update)-read 등) 등\n      : 수집 시 blktrace 등 IO trace 툴 활용\n    - 시스템 meta 정보 기반 access pattern\n      : inode 등에서 얻어낼 수 있는 last create/update/access time 등\n      : 수집 시 inode 등 file, filesystem 정보 활용\n\n  - IO 유발자 정보\n    - user ID, group ID, process ID 등 creator 정보 및 현재 해당 file을 이용하는 \n      process / class 등\n      : 수집 시 debugfs, proc, sysfs 등의 메커니즘 활용\n      : IO 유발자가 어느 category에 속하는지 category 별로 분석\n        (web, DB, auth, monitoring, kernel thread (journald, kworker 등), ...)\n\n<!--\n  - State machine 기반의 macro IO pattern analsysis\n    : 유의미한 parameter 몇 가지로 정의되는 state machine을 정의하고,\n      이 state 간의 transition 확률을 이용하여 다가올 macro IO 패턴을 예측\n-->\n\n  특허 출원에 대해 말씀드리자면, 위의 기법 각각에 대한 특허에 대해서는\n  일부 시스템 등록, 혹은 현재 직무발명서 작성 중이었습니다만,\n  말씀 주신 바처럼 현재 저희가 진행하고 있는\n  IOWA 기반의 proactive data placement 자체에 대한\n  특허 가능성도 검토하도록 하겠습니다.\n\n  참고로, 2/12일 (오늘) 현재, 삼성 특허 검색 사이트에서, 검색식\n  ((proactive data placement) OR (data preplacement))\n  (cach* OR tiering)\n  ((IO OR I/O OR workload) analy*)\n  으로 찾아본 결과, 검색된 관련 특허는 없었습니다만,\n  검색 범위를 더 넓혀서 추가로 더 찾아보겠습니다.\n\n</pre>\n\n----\n\n==== Realistic sample trace log collection ====\n<pre>\n이렇게 다양한 데이터를 이용해서 데이터 I/O 패턴을 예측하려면 많은 sample log들을 모아서\n예측 함수를 만들어야 합니다. 분산 스토리지가 특정 용도에 이용되고 있다면\n(예로, 스트리밍 서버, 웹 서버, VDI 서버 등), 그것별로 sample log들을 많이 모아야 할 것 같습니다.\n우리가 어떻게 이런 다양한 로그들을 모을 수 있을까요?\n\n> 네, 전적으로 동감합니다. 수집 현황/계획은 아래와 같습니다.\n\n  1. Web Server IO Log\n    : 메모리사에서도 SAVL 타겟으로 우선적으로 관심을 가졌던 workload로서\n      SPECweb benchmark을 통해 realistic workload를 얻을 수 있습니다.\n      (메모리 사 응용기술팀으로부터 시스템 구축에 필요한 파일 받아옴)\n\n  2. DB IO Log\n    : TPC-C benchmark을 통해 realistic workload를 얻을 수 있습니다.\n      (이미 환경 구축 완료 및 사용 중)\n\n  3. Supercomputing IO Log\n    : 현재 기술원 3층의 수퍼컴퓨터 관리팀과 접촉 중\n      (Matlab, R, MapReduce-like RSP framework에 대한 workload 존재)\n\n  4. Streaming IO Log\n    : 아직 확보 방안 마련되지 않음\n		\n  5. VDI IO Log\n    : 기술원 정보전략팀과 의논 가능\n      (\'12년에 1차로 trace log 확보를 시도해본 적 있으나\n       정보전략팀과 SDS와의 관계가 어려워져 실패)\n\n</pre>\n\n----\n\n==== problem formulation considering on-line learning ====\n<pre>\n\n[ 지시 1 ]\n\n앞 경우는 sample log들을 모아서 예측 함수를 만드는 것이고,\n실제 deploy되었을 때는 dynamic 하게 예측 함수가 adaption할 수 있어야 하는데,\n그런 adaptation을 on-line learning으로 볼 수 있습니다.\n최희열 전문과 협력해서 위 문제들을 formulation 하기 바랍니다.\n\n> 네, 위의 내용들을 정리하여 최희열 전문과 함께 위의 문제들을 명확히 정리해보겠습니다.\n\n</pre>\n\n----\n\n== ## bNote-2013-02-08 ==\n\n=== Git Server on My Local Machine ===\n\nPlease check this link: [[Git server on my local machine]]\n\n== ## bNote-2013-02-07 ==\n\n=== World Largest Data Centers ===\n\n* [http://wikibon.org/blog/inside-ten-of-the-worlds-largest-data-centers/ Inside Ten of the World’s Largest Data Centers, on March 25, 2010]\n\n\n=== Enterprise Storage Related Articles ===\n\n----\n==== The future of enterprise storage? ====\n: Henry Baltazar / Senior Analyst / Storage & Systems / The 451 Group\n: [http://www.snia.org/sites/default/files2/sdc_archives/2010_presentations/general_session/HenryBaltazar_Cloud_Convergence_Consolidation.pdf Clouds, Convergence, and Consolidation - SNIA SDC 2010]\n\n----\n==== Tiering vs. Caching ====\n; Automated Storage Tiering\n: Has become a popular technology in the past two years with multiple vendors embedding this feature within their storage systems.\n: Allows storage systems to migrate \'hot\' data to high speed flash-based storage tiers, while simultaneously moving stale data to inexpensive SATA hard drives.\n; Storage Caching\n: Primarily being driven by NetApp (i.e. Flash Cache) and a few startups, storage systems with caching use solid state storage as a memory extension technology.\n: Caching proponents claim that the migration required to do tiering is inefficient and reacts too slowly to eliminate hotspots.\n----\n\n=== Comparison of NAS vs. SAN // Pain Points of Storage System ===\n\n어제 회의의 Action Items 중 하나로, 제가 맡았던 부분인\n\"현재 Arch.가 야기하는 기술적 Poin Points 추가 정리\" 관련 메일입니다.\n\n내용은 다음과 같이 크게 두 가지로 구분됩니다.\n\n1. NAS vs. SAN 비교\n\n2. 기존 Storage System의 Pain Points (HP StorageWorks 제품 Feature를 통해 바라본)\n  -> 여기에는 이전문님께서 말씀하시곤 했던 ILM(Info. Lifecycle Mgmt.)에 \n     대한 언급도 나오네요. (더 자세한 내용은 첨부파일에 있습니다)\n\n// 아래 내용들은  HP에서 만든 \"Storage Overview and Architecture (HP, Arvind Shrivastava)\" 자료를 참고하였습니다. (첨부 1)  다소 NAS에 불리한 점들이 많이 부각되어 보입니다만, 다르게 보면, \"기존 NAS의 구조적 한계점이 이러했으니 Performance Drop 없이도 Scale-out 되는 NAS로 가기 위해서는 이러한 단점들이 극복되어야하겠다\" 라는 내용으로 받아들일 수도 있겠습니다.\n\n\n==== Comparisons of SAN, NAS (and DAS) ====\n\n{| border=1\n|+ \'\'\'DAS vs. NAS vs. SAN\'\'\'\n| Direct Attached Storage (DAS)\n| Network Attached Storage (NAS)\n| Storage Area Network (SAN)\n|-\n|\n*+ Low cost solution\n*+ Simple to configure\n*- De-centralized storage management\n*- No storage consolidation\n*- No high availability\n*- Low performance\n|\n*+ Heterogeneous environment\n*+ Centralized storage management\n*- Low performance\n*- Limited scalability\n*- Network congestion during backups & restore\n*- Ethernet limitations\n|\n*+ Heterogeneous environment\n*+ Centralized storage management\n*+ High degree of fault tolerance\n*+ Best and superior performance\n*+ Storage consolidation\n*+ Fast and efficient backups and restores\n*+ Dynamic scalability\n*- Expensive solution for small setups\n|-\n|}\n\n\n\n{| border=1\n|+ \'\'\'NAS vs. SAN\'\'\'\n| Items\n| style=\"width:40%\" | Network Attached Storage (NAS)\n| style=\"width:40%\" | Storage Area Network (SAN)\n|-\n| Network\n| IP\n| Fibre Channel\n|-\n| Reliability\n| Unreliable\n: >> FCoE, iSCSI 도 있는데 정말 그렇게 Unreliable인걸까요? FC에 비해 상대적으로 그렇다는 의미로 받아들이면 될까요?\n| Reliable\n|-\n| CPU overhead\n| Extremely High\n: >> \"Extremely High\" 까지일 줄은 몰랐네요. Block IO에 비해 File System Operation까지 해야 하는 NAS의 경우 CPU overhead가 더 있을 수 있겠다는 생각은 했었지만, 생각보다 overhead가 꽤 있는지도 모르겠습니다. (NetApp NAS, EMC Isilon 확인 필요).\n| Extremely Low\n: >> 반대로, SAN이 상대적으로 low-overhead라고 해서 \"Extremely\" 까지일 줄은...\n|-\n| Blocks\n| Large number of small blocks\n| Large blocks of data\n|-\n| Backup\n| LAN backup\n| LAN-free backup\n|-\n| Access and control by application\n| Applications driven by universal access to files\n| Applications managing own data specs\n|-\n| Typical usage\n|\n* Office applications file serving\n* File sharing between office users and CAD users\n* Content management for web servers\n* Consolidation of \'home directories\' for controlled and centralized backup\n* Boot device for diskless server farms\n* Rich media such as audio or video streaming\n|\n* Physical storage consolidation of any application mix\n* Physical storage sharing for cluster configurations\n* Sharing of backup devices (libraries)\n* Long distance data mirroring and/or data replication on storage device level\n* Redundant, high speed disk access for large database servers\n|}\n\n<br/>\n----\n\n==== HP Storage Product - StorageWorks ====\n\n\'\'\'Pain Points\'\'\'\n* HP의 storage 제품인 StorageWorks에서 언급된 pain points를 요약해보면 다음과 같습니다.\n\n# 네트워크 트래픽 증가로 인한 스토리지 시스템 성능 저하 (여기서 WAN을 언급하는 것으로 보아 물리적으로 멀리 떨어진 스토리지 간의 트래픽을 언급하는 것으로 보입니다)\n# 저장된 data의 대부분을 차지하는 (operational data에 비해 3~4배 이상 크기) reference data의 지속적 증가 (여기서 reference data라는 것은, 당장 operation되고 있지는 않지만, 이전에 생성되어 사용된 적은 있으나 폐기되지 않고 저장되어 있는, 그렇게 계속 쌓여가는 data들을 의미하는 것으로 보여집니다. 한마디로, operational data가 hot-data라면, reference data는 cold-data라고 볼 수 있을 것 같습니다)\n# 관리되고 저장된 data의 증가에 따라 속도 저하 및 TCO 증가\n# hetero한 사양 및 상태로 여러 곳에 흩어져있는 storage system의 관리\n\n\n\n\'\'\'HP StorageWorks - Reference Information Storage System\'\'\'\n:* 일종의 ILM (Information Lifecycle Management)\n:* Cost 절감 위해 application server/storage로부터 잘 사용되지 않는 static data 및 중복 data 감소.\n:* data의 integrity 보장.\n:* Power indexing 및 Grid computing을 통해 훨씬 빠른 search/retrieval을 달성하겠다.\n\n[[File:20130207 HP Storage Product Slides Scale is the problem .png|500px]]\n\n\n* Changing digital storage demands \n:* Majority is reference information (operational data 보다는 reference 정보가 저장된 data의 대부분 차지)\n\n[[File:20130207 HP Storage Product Slides - Changing digital storage demands .png|500px]] \n\n\n* WAN acceleration\n:* WAN을 통해 연결되는 스토리지 트래픽 감소 기술\n(100배 이상 빠른 throughput 달성 목표)\n{| border=1\n| Problems\n| style=\"width:40%\" | HP\'s solution\n| style=\"width:40%\" | What it does\n|-\n| Not enough bandwidth\n|\n* Scalable data referencing (SDR)\n|\n* Remove repeated bytes from the WAN for all TCP traffic\n|-\n| TCP chattiness and inefficiency\n|\n* Virtual Window Expansion\n* High speed TCP optimizations\n|\n* Optimizes performance of TCP\n* Enables much higher throughput on \"LFNs\"\n|-\n| Application chattiness and inefficiency\n| Transaction prediction (CIFS, MAPI)\n| Optimizes the performance of specific applications\n|}\n\n== ## bNote-2013-02-06 ==\n\n\n=== Defining: Target Segment, Product, Architecture ===\n금일 회의때 논의한 내용을 정리합니다.\n\n\n\'\'\'Consensus\'\'\'\n\n* Big data 시장만 보면 현재로서는 시장이 크지 않다. \n\n* HW Box를 같이 팔아야 한다. 그렇지 않으면 돈이 안된다.\n\n* Google, NHN등 big data center player에게 팔것은 아니다.\n\n* 삼성이 Enterprise Storage 사업 진입한다고 가정 (사업 진입유무를 우리가 고민하지 말자)\n\n* Large-scale system에서 data/node scale이 증가할수록 manageability issue가 커진다.\n\n* storage 신사업 초기에는 기존 infra에 대한 당사 제품의 compatibility가 매우 중요할 것이다. 이 부분은 신사업추진단이 고민할 것이고 우리는 2nd/3rd generation에 해당하는 innovative architecture에 집중한다.\n\n* new architecture는 고객의 현재/미래의 pain point에 기반해야 한다.\n\n \n\'\'\'Pain Point (고객입장)\'\'\'\n\n* \"Google같은 flexible/efficient data center 를 우리 회사 scale에 맞게 내부에서 소유하고 관리하고 싶습니다.\" (Google-in-a-box)\n\n* \"여러 인프라에 흩어진 기업데이터를 통합 관리하고 싶습니다.\" (for bigdata analytics?)\n\n* \"어떠한 경우에도 data loss가 없고 available한 불멸의 스토리지를 소유하고 싶습니다.\"\n\n* \"우리가 소유한 모든 infra를 통합관리하는 극강의 manageability가 필요합니다. 충분히 자동화되고 self-healing되어 관리자 1명당 관리노드의 갯수를 혁신적으로 올릴 수 있으면 좋겠습니다.\" (ML, Brain-inspired computing 적용가능성 있음)\n\n* replication 기술 (InformationWeek 자료에서 니즈 언급)\n\n\n\'\'\'Big Question\'\'\'\n\n* SAN ? NAS ? SAN+NAS ?\n\n* Server+Storage ?\n\n* up to which scale does the distribute architecture need to be expanded?\n\n* 고객이 별도의 big data platform (HW+SW) 을 구매할까? (in addition to already owned infra)\n\n* SW-defined storage 시대가 올까?\n\n* 왜 Google/NHN은 직접 data center를 buildup 했을까? 왜 EMC를 안쓸까?\n\n* InformationWeek 자료의 기술선호도가 2018년에는 어떻게 바뀔까?\n\n* Replication 기술이 중요한만큼 매우 어려울까?\n\n* 2018년에는 Cloud Storage 의존성이 어떻게 될까?\n\n* Data center model이 어떻게 다를까? : Google vs. Amazon (Fusion-IO구매) vs. Apple (Isilon구매)\n\n* service fee 로 먹고사는 biz model이 나올 수 있나?\n\n* Hadoop + Dedup? (data가 있는곳에 processing을! 이라는 hadoop philosophy가 dedup에 의해 어떤 영향을 받는가?)\n\n* block I/F, file I/F 이후 new interface가 어떻게 확산될까? 왜 object I/F는 확산되지 못했을까? \n\n* brain-like한 I/F는 어떤 형태가 될까?\n\n\n\'\'\'Action Items\'\'\'\n\n* 이: Google의 data center evolution history\n\n* 서: SAN/NAS, Converged/Storage, 분산 scale(100s? 1000? 10000?)\n\n* 신+구: 분산-scale storage에 분산-scale dedup이 적용되면 architecture가 어떻게 바뀌나? dedup manager의 위치가 어디가 되어야 하나?\n\n* 융: 현 architecture가 야기하는 기술적인 Pain Point 추가정리\n\n\n\'\'\'Check this out\'\'\'\n* 451 report provided by Seo-C: Automatic Tiering is the very hot technology\n\n== ## bNote-2013-02-05 ==\n\n\n=== IOWA:: machine learning ===\n\nk-addr-group\n\ndistance calculation and clustering\n\nhamming distance\n\n\n[[ bsc.iowa.lsp.addr.access_ptrn ]]\n\n\n[http://en.wikipedia.org/wiki/Levenshtein_distance Levenshtein distance]\n\n\n <pre>\n__k_addr_group__ 0 : [720616224, 3282792808, 3282793000, 3282793096]\n__k_addr_group__ 1 : [728218248, 730235992, 817521776, 3765476016]\n__k_addr_group__ 2 : [721421456, 724475672, 724475864, 3712136296]\n__k_addr_group__ 3 : [724475960, 828267560, 895981360, 3277437312]\n__k_addr_group__ 4 : [725469848, 726381104, 726381496, 726793728]\n__k_addr_group__ 5 : [723112360, 730730224, 3677713128, 3773683448]\n__k_addr_group__ 6 : [725235304, 828274704, 896778072, 3277440104]\n__k_addr_group__ 7 : [720987560, 724481496, 724727104, 725008864]\n__k_addr_group__ 8 : [545503608, 896780128, 3069345832, 3076725824]\n__k_addr_group__ 9 : [721386480, 721386960, 3752324048, 3767545408]\n__k_addr_group__ 10 : [725420616, 726671672, 727735192, 3066446120]\n__k_addr_group__ 11 : [721030800, 721030896, 721411176, 827916488]\n...\n__k_addr_group__ 245 : [725218552, 822170744, 870855200, 3642881480]\n__k_addr_group__ 246 : [457025608, 722055120, 3685634288, 3779707504]\n__k_addr_group__ 247 : [722055216, 722055312, 722055376, 725157784]\n__k_addr_group__ 248 : [721285200, 721285296, 728117992, 728118184]\n__k_addr_group__ 249 : [725218680, 725218808, 725218936, 725219032]\n</pre>\n\n\n\n=== Python:: numerical sort for string list ===\n\n <pre>\n\n>>> typeof(mlist)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name \'typeof\' is not defined\n>>> help(mlist)\n\n>>> mlist = {}\n>>> mlist[0] = []\n>>> mlist[1] = []\n>>> mlist[0] = [\"11243\", \"92\", \"2132\", \"42\"]\n>>> mlist[1] = [\"243\", \"892\", \"19132\", \"3242\"]\n>>> mlist[0] = [ int(x) for x in mlist[0] ]\n>>> mlist[0]\n[11243, 92, 2132, 42]\n>>> mlist[1]\n[\'243\', \'892\', \'19132\', \'3242\']\n>>> mlist[1] = [ int(x) for x in mlist[1] ]\n>>> mlist[1]\n[243, 892, 19132, 3242]\n>>> mlist   \n{0: [11243, 92, 2132, 42], 1: [243, 892, 19132, 3242]}\n>>> mlist[0].sort()\n>>> mlist\n{0: [42, 92, 2132, 11243], 1: [243, 892, 19132, 3242]}\n>>> mlist[1].sort()\n>>> mlist\n{0: [42, 92, 2132, 11243], 1: [243, 892, 3242, 19132]}\n>>> \n\n</pre>\n\n== ## bNote-2013-02-04 ==\n\n\n\n=== CWD ===\n\n <pre>\nb@ub04:[anal] $ pwd\n/x/iowa/now/tracelog_analysis.v02.Q_phase.2_mysqld/anal\nb@ub04:[anal] $ l\ntotal 36\ndrwxrwxr-x 8 b b 4096 Feb  4 19:23 ./\ndrwxrwxr-x 5 b b 4096 Feb  4 19:23 ../\n-rw-rw-r-- 1 b b  127 Jan 30 19:42 .bdx.0100.y.exec_iowa_for_all_cases.sh\ndrwxrwxr-x 2 b b 4096 Jan 30 19:38 .tools/\nlrwxrwxrwx 1 b b   27 Jan 30 15:55 .tracelog.A.addr -> ../preproc/.tracelog.A.addr\nlrwxrwxrwx 1 b b   27 Jan 30 18:48 .tracelog.R.addr -> ../preproc/.tracelog.R.addr\nlrwxrwxrwx 1 b b   27 Jan 30 18:48 .tracelog.W.addr -> ../preproc/.tracelog.W.addr\ndrwxrwxr-x 2 b b 4096 Jan 30 19:39 t00.R.xLBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:43 t01.R.1LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t02.R.100LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t03.R.10000LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t04.R.20000LBA/\n</pre>\n\n=== IOWA.data.tpcc_250gb_48h.mysql.R.addr:: Offset 100K, Len 1K ===\n\n* [[IOWA.data.tpcc_250gb_48h.mysql.R.addr:: Offset 100K, Len 1K]]\n\n=== Frequent Item Detection/Discovery ===\n\n==== Stanford Open Book - Mining of Massive Datasets ((B.GOOD)) ====\n\n* MMD Book [http://i.stanford.edu/~ullman/mmds.html]\n\n: The book has now been published by Cambridge University Press. The publisher is offering a 20% discount to anyone who buys the hardcopy Here. By agreement with the publisher, you can still download it free from this page. Cambridge Press does, however, retain copyright on the work, and we expect that you will obtain their permission and acknowledge our authorship if you republish parts or all of it. We are sorry to have to mention this point, but we have evidence that other items we have published on the Web have been appropriated and republished under other names. It is easy to detect such misuse, by the way, as you will learn in Chapter 3.\n: Anand Rajaraman (@anand_raj) and Jeff Ullman\n\n: Ch 3. Finding Similar Items [http://i.stanford.edu/~ullman/mmds/ch3.pdf]\n:: 3.1 Applications of Near-Neighbor Search\n::: 3.1.1 Jaccard Similarity of Sets\n:: 3.2 Shingling of Documents\n:: 3.3 Similarity-preserving Summaries of Sets\n:: 3.4 Locality-sensitive Hashing for Documents\n:: 3.5 Distance Measures\n:: 3.6 The Theory of Locality-sensitive Functions\n:: 3.7 LSH Families for Other Distance Measures\n:: 3.8 Applications of Locality-sensitive Hashing\n:: 3.9 Methods for High Degrees of Similarity\n\n: Ch 6. Frequent Itemsets [http://i.stanford.edu/~ullman/mmds/ch6.pdf]\n:: 6.1 The Market-Basket Model\n:: 6.2 Market Baskets and the A-Priori Algorithm\n:: 6.3 Handling Larger Datasets in Main Memory\n:: 6.4 Limited-pass Algorithms\n:: 6.5 Counting Frequent Items in a Stream\n\n==== Orange - open source data visualization and analysis tool ====\n\nOpen source data visualization and analysis for novice and experts. Data mining through visual programming or Python scripting. Components for machine learning. Add-ons for bioinformatics and text mining. Packed with features for data analytics.\n\n\n==== SAX - Symbolic Aggregate Approximation ====\n\nSAX (Symbolic Aggregate approXimation) Homepage [http://www.cs.ucr.edu/~eamonn/SAX.htm]\n\nSAX is the first symbolic representation for time series that allows for dimensionality reduction and indexing with a lower-bounding distance measure. In classic data mining tasks such as clustering, classification, index, etc., SAX is as good as well-known representations such as Discrete Wavelet Transform (DWT) and Discrete Fourier Transform (DFT), while requiring less storage space. In addition, the representation allows researchers to avail of the wealth of data structures and algorithms in bioinformatics or text mining, and also provides solutions to many challenges associated with current data mining tasks. One example is motif discovery, a problem which we defined for time series data. There is great potential for extending and applying the discrete representation on a wide class of data mining tasks.\n\n\n==== The Amadeus motif discovery platform ====\n\nAmadeus Home [http://acgt.cs.tau.ac.il/amadeus/]\n\nAmadeus is a user-friendly software platform for genome-scale detection of known and novel motifs (recurring patterns) in DNA sequences, applicable to a wide range of motif discovery tasks. Amadeus outperforms extant tools and sets a new standard for motif discovery software in terms of accuracy, running time, range of application, wealth of output information and ease of use.\n\n==== Motif Detection ====\n\n* Python for Bioinformatics [http://telliott99.blogspot.kr/2009/06/motif-discovery.html]\n* MOODS: Motif Occurrence Detection Suite - ALGOrithmic Data ANalysis [http://www.cs.helsinki.fi/group/pssmfind/]\n* CMU CS Lecture -- Motif Detection [http://www.cs.cmu.edu/~epxing/Class/10810-05/Lecture6.pdf]\n\n==== Biopython ====\n\nBiopython is a set of freely available tools for biological computation written in Python by an international team of developers. [http://biopython.org/wiki/Main_Page Biopython]\n\n* Required Python Modules [http://www.scipy.org/more_about_SciPy NumPy / SciPy (SciPy.org)]\n** NumPy: N-dimensional Array Manipulations\n** SciPy: Scientific tools for Python\n\n* References\n# [http://biopython.org/DIST/docs/tutorial/Tutorial.pdf Biopython Tutorial]\n\n<br/>\n----\n\n=== RAM-SSD 하이브리드 Page Cache 아키텍쳐 :: WIPS 의견 대응 ===\n <pre>\n접수번호 : RN-201301-004-1\n\n명칭 : RAM-SSD 하이브리드 Page Cache 아키텍쳐\n\n기술 핵심 : RAM과 SSD를 이용하여 디스크 data를 caching하는 구조가\npage cache layer에서 동작하도록 함으로써 hot access를 check하고\naccess 정도에 따라 RAM 또는 SSD에 저장(tier-1~2)하거나 disk에서 access되도록 하는 것\n\n구성 : A) page cache에서 동작하는 RAN-SSD hybrid cache architecture\n      B) tier-1 page cache(RAM), tier-2 page cache(SSD)\n      C) page에 대한 hit pattern으로 page들을 tier-1,2로 class 지정\n\n\n--\n안녕하세요? 정명준입니다.\n다루는 내용이 많았음에도 깔끔하게 잘 정리해주시느라 고생이 많으셨습니다. ^_^\n꼭 추가되었으면 하는 내용과, 서술의 톤이 살짝 바뀌면 좋을만한 부분이 있어\n아래 보내주신 메일 본문에 추가하였습니다.\n(제 의견은 파란색으로 표시되어있습니다)\n\n1) \'기술 핵심\' 부분\n\n	\"RAM과 SSD를 이용하여 디스크 data를 caching하는 구조가 page cache layer에서 동작하도록 함으로써\" \n		-> 본 발명의 핵심 부분인 부분을 잘 적어주셨습니다.\n\n	\"hot access를 check하고 access 정도에 따라\"\n		-> 이 부분은 \"access pattern을 점검하여 rehit-potential에 따라\" 라는 표현으로 바뀌면 더욱 좋겠습니다.\n		기존에는 recently hot (recency) 위주로 cache될지 evict될지를 판단하였는데,\n		본 발명에서는 rehit potential을 보고 판단하겠다는 점이 다릅니다.\n		(once-hot data 라든지, periodicity 등을 파악)\n\n	\"RAM 또는 SSD에 저장(tier-1~2)하거나 disk에서 access되도록 하는 것\"\n		-> 여기에 \"page chunk 단위로 page tiering을 한다\"는 언급이 되었으면 좋겠습니다. (청구의 범위를 너무 제한하는 것이 아니라면요)\n\n\n2) \'구성\' 부분\n\n	아래 \'가~바\' 부분이 본 발명에서 청구하고자 하는 주요 내용들입니다.\n	이 중에서 \'가\' 부분이 보내주신 메일에서 A, B, C로 적어주신 \'구성\' 부분인데요,\n	대표 청구항을 적자면 아래 \'가~바\' 부분이 모두 필요할 것 같습니다.\n\n	가) RAM-SSD hybrid page cache architecture\n		page cache layer에서 동작하고;\n		tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는;\n		page tiering 기반의 hybrid page cache 아키텍쳐.\n\n	나) rehit-potential aware page classification, and page class based cache data placement\n\n	다) page tiering operation in host side\n\n	라) page tiering operation in SSD side\n		chunk I/O in SSD in {log-structured and/or parallelism-maximized} manner\n\n	마) TRIM ahead 메커니즘 (page chunk writes latency를 줄이기 위해 미리 공간 확보하는)\n\n	바) victim page chunk selection in SSD\n		oldest-first\n		most-mark-first\n		with upper-tier-backup\n		\n\n오후에 전화로 말씀 나누면서 설명 드리겠습니다.		\n감사합니다.\n정명준 드림.\n</pre>\n\n\n=== 서울대 2013-02-01 Meeting Minutes ===\n\n1. 실험 관련\n: HDD 개수와 map slot의 개수를 align하여 실험 진행 후 기존 결과와 함께 분석\n: <span style=\"color:blue\">>>네, 본 실험에서 나타난 \'재미있는\' 현상이 Hadoop 권장 환경 구성과 맞지 않아서 발생한 것이 아니라 Hadoop 시스템 근원적으로 가지고 있는 \'한계\' 때문이라는 것으로 주장이 되려면, HDD 수와 execution slot 간 alignment는 필수적일 것 같습니다. 잘 부탁드리겠습니다.[1]</span>\n\n: In-memory buffer 크기 변화를 통한 spill file 개수 및 phase elapsed time의 관계를 exponential expression 으로 분석\n: <span style=\"color:blue\">>> 아, 제 발음이 좋지 않았던 것 같습니다. ^^;; exponential expression이라기 보다는 extrapolation이 가능한 equation으로 나오면 좋겠다는 것으로 이해해주시면 좋겠습니다.  아마 이게 되려면 충분한 경우에 대해서 실험이 되어야 할 것인데, equation이 나오기만 한다면 무척 중요한 의미를 갖게 될 것 같습니다. 전체적인 phase elapsed time 뿐만 아니라 p.15에 색깔로 구분된 각 영역들(map - map/shuffle - shuffle - reduce)의 변화 양상도 잘 설명이 될 수 있다면 좋겠습니다.[2]</span>\n\n2. SSD 관련\n: <span style=\"color:blue\">>> 특히 보내주신 발표 슬라이드의 p.18에 언급된 SSD 사용 시 9.5% 감소 부분은 충격이었습니다. 이 현상을 유발한 원인이 명확하게 규명되고 (1), 그 규명된 원인이 기존에 알려지지 않았던 것이며 (2), 그것을 해소할 수 있는 방법도 같이 제시될 수 있다면 (3), 본 산학 과제가 더욱 큰 의미를 갖게 될 것 같습니다.[3]</span>\n\n: random write는 피하는 방향으로 SSD을 통한 하둡 수행 성능 향상이 나올 수 있도록 I/O contention의 오버헤드가 SSD 성능과 상쇄될 수 있도록 detail하게 실험.\n: network i/o와 disk i/o의 trade-off\n: <span style=\"color:blue\">>> 네, combine 등을 통해서 network overhead를 local overhead (local computation 및 local I/O)로 trade-off 시키게 되지만, 그 local overhead를 SSD로 상쇄시킬 수 있는 best 방법 및 예상 효과가 나온다면 매우 의미가 있을 것 같습니다.[4]</span>','utf-8'),(131,'== ## bNote-2013-02-28 ==\n\n=== 수원임직원주차장 방문예약 ===\n[https://www.sdigitalcity.com/security/visit/login.do 수원임직원주차장 방문예약]\n\n== ## bNote-2013-02-26 ==\n\n=== iowa.sidewinder ===\n\n* construction: filebench workload generator\n\n== ## bNote-2013-02-21 ==\n\n\n=== Daily.Kickoff ===\n\n* acsptrn outlook program\n:- kag (K-address-group)\n::- can \'K\' be a signature of a IO pattern?\n:::- what kind of implication can be extracted from \'KAG\'?\n::: : what size of chunk can have a meaning?\n::: : unit of tiering IO? unit of proactive caching? unit of IO dump size to the SSD?\n::- hamming distance (proper choice! why? difference value means the # of items to be added in cache) (older elements identified as a different one can be evicted or not, according to the cache space condition)\n:- outlook seek distance stat-dist\n:: - can \'seek distance\' be a metric for identifying hot zone?\n\n* cache simulation program\n:- hit ratio calculation, performance gain calculation\n\n* ML approach planning\n:- what to do for proactive data placement?\n::- what to do for static(?) macro insight?\n::- what to do for dynamic(?) micro prediction? (periodicity?)\n\n== ## bNote-2013-02-20 ==\n\n=== IOWA planning with Yanpei Chen Paper (SOSP 2011) ===\n\n\n\n\n\n\n\n=== Intel releases SSD cache acceleration software for Linux servers ===\n\n인텔이 Linux Server를 위한 CAS (Cache Acceleration Software) 기술을 Release했다는 소식입니다.\n\n이 CAS라는 기술은 작년에 인수한 Nevex사의 CacheWorks 기술에 기반하고 있습니다.\n\n특징으로 볼 수 있는 것은 기존 block cache의 문제점으로 지적되었던 DRAM에도 올라간 data가 SSD cache에도 올라가는 현상을 피하기 위해 초보적인 수준의 intelligence를 도입했다는 점입니다.\n\n(very hot data는 DRAM에 보내되, 덜 hot하지만 still I/O active data는 SSD로 나누어 보낸다는 것이 핵심입니다) \n\n\'\'\'Intel releases SSD cache acceleration software for Linux servers\'\'\'\n\nSSD-based CAS for Linux servers can offer up to 18 times the performance for read-intensive applications, Intel says. \n\n[http://www.infoworld.com/d/open-source-software/intel-releases-ssd-cache-acceleration-software-linux-servers-212673?page=0,0 InfoWorld News FEBRUARY 12, 2013]\n\n\n발췌 내용:\n...\nTo ensure it does not duplicate work already being performed in DRAM, the CAS software takes control of all data access. The hottest data is placed on DRAM, while less hot, but still I/O active data, is placed on SSD.\n\nIntel\'s CAS software provides cooperative integration between the standard server DRAM cache and the Intel SSD cache, creating a multi-level cache that optimizes the use of system memory and automatically determines the best cache level for active data.\n...\n\n== ## bNote-2013-02-19 ==\n\n\n=== Workload generation and tracing (with filebench, blktrace) ===\n\n* target workloads\n:- fileserver.f\n:- mongo.f\n:- netsfs.f\n:- networkfs.f\n:- oltp.f\n:- tpcso.f\n:- varmail.f\n:- videoserver.f\n:- webserver.f\n\n\n* video server workload (running time: 3600 seconds) (20130219_143400)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\n#>> cleaning up \'30.tmp/\' directory\n#>> ready for generating workload specified in \'20.conf/.tconf.sh\'\n#>> please run iotrace (blktrace) in \'34.iotrace/\'\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\n#>> [20130219_143400] waiting for iotrace getting started \n.................................\n----------------------------------------------------\n#>> Ok, now start workload generation\nstarted: 20130219_143433\n----------------------------------------------------\nFilebench Version 1.4.9.1\n 3599: 0.000: Allocated 170MB of shared memory\n 3599: 0.001: Eventgen rate taken from variable\n 3599: 0.001: Video Server Version 3.0 personality successfully loaded\n 3599: 0.001: Creating/pre-allocating files and filesets\n 3599: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n 3599: 0.002: Re-using fileset passivevids.\n 3599: 0.002: Creating fileset passivevids...\n 3599: 2227.782: Preallocated 104 of 194 of fileset passivevids in 2228 seconds\n 3599: 2227.800: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n 3599: 2227.800: Re-using fileset activevids.\n 3599: 2227.800: Creating fileset activevids...\n 3599: 2227.805: Preallocated 32 of 32 of fileset activevids in 1 seconds\n 3599: 2227.805: waiting for fileset pre-allocation to finish\n 3974: 4324.570: Starting 1 vidreaders instances\n 3974: 4324.599: Starting 1 vidwriter instances\n 3976: 4324.780: Starting 1 vidwriter threads\n 3975: 4324.790: Starting 48 vidreaders threads\n 3599: 4325.877: Running...\n 3599: 7926.189: Run took 3600 seconds...\n 3599: 7926.234: Per-Operation Breakdown\nserverlimit          423882ops      118ops/s   0.0mb/s    305.7ms/op    60431us/op-cpu [0ms - 2292ms]\nvidreader            423983ops      118ops/s  29.4mb/s    407.1ms/op    39674us/op-cpu [0ms - 2292ms]\nreplaceinterval      17ops        0ops/s   0.0mb/s  10000.1ms/op        0us/op-cpu [10000ms - 10000ms]\nwrtclose             17ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               17ops        0ops/s  47.2mb/s 134156.2ms/op 17297647us/op-cpu [3919ms - 421911ms]\nwrtopen              18ops        0ops/s   0.0mb/s     26.3ms/op     1111us/op-cpu [0ms - 247ms]\nvidremover           18ops        0ops/s   0.0mb/s  60936.6ms/op   281667us/op-cpu [69ms - 181905ms]\n 3599: 7926.234: IO Summary: 424053 ops, 117.782 ops/s, (118/0 r/w),  76.6mb/s,      0us cpu/op, 415.0ms latency\n 3599: 7926.234: Shutting down processes\n----------------------------------------------------\nfinished: 20130219_164641\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n\n* video server workload (running time: 60 seconds) (20130219_111030)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\n#>> cleaning up \'30.tmp/\' directory\n#>> ready for generating workload specified in \'20.conf/.tconf.sh\'\n#>> please run iotrace (blktrace) in \'34.iotrace/\'\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\n#>> [20130219_111030] waiting for iotrace getting started \n...........\n----------------------------------------------------\n#>> Ok, now start workload generation\nstarted: 20130219_111041\n----------------------------------------------------\nFilebench Version 1.4.9.1\n 2322: 0.000: Allocated 170MB of shared memory\n 2322: 0.001: Eventgen rate taken from variable\n 2322: 0.001: Video Server Version 3.0 personality successfully loaded\n 2322: 0.001: Creating/pre-allocating files and filesets\n 2322: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n 2322: 0.720: Removed any existing fileset passivevids in 1 seconds\n 2322: 0.720: making tree for filset /mnt/hdd_4/filebench/passivevids\n 2322: 0.721: Creating fileset passivevids...\n 2322: 5398.571: Preallocated 104 of 194 of fileset passivevids in 5398 seconds\n 2322: 5398.580: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n 2322: 5441.697: Removed any existing fileset activevids in 44 seconds\n 2322: 5441.698: making tree for filset /mnt/hdd_4/filebench/activevids\n 2322: 5441.698: Creating fileset activevids...\n 2322: 7548.330: Preallocated 32 of 32 of fileset activevids in 2107 seconds\n 2322: 7548.330: waiting for fileset pre-allocation to finish\n 2939: 9496.045: Starting 1 vidreaders instances\n 2939: 9496.077: Starting 1 vidwriter instances\n 2941: 9496.275: Starting 1 vidwriter threads\n 2940: 9496.294: Starting 48 vidreaders threads\n 2322: 9497.515: Running...\n 2322: 9557.521: Run took 60 seconds...\n 2322: 9557.540: Per-Operation Breakdown\nserverlimit          22911ops      382ops/s   0.0mb/s    116.4ms/op   129288us/op-cpu [0ms - 5662ms]\nvidreader            23044ops      384ops/s  95.8mb/s     11.3ms/op     3815us/op-cpu [0ms - 2135ms]\nreplaceinterval      0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtclose             0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtopen              1ops        0ops/s   0.0mb/s      0.1ms/op        0us/op-cpu [0ms - 0ms]\nvidremover           1ops        0ops/s   0.0mb/s   5112.0ms/op   110000us/op-cpu [5111ms - 5111ms]\n 2322: 9557.540: IO Summary: 23046 ops, 384.062 ops/s, (384/0 r/w),  95.8mb/s,      0us cpu/op,  11.6ms latency\n 2322: 9557.540: Shutting down processes\n----------------------------------------------------\nfinished: 20130219_135000\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n\n* video server workload (running time: 3600 seconds) (20130218_161548)\n <pre>\nroot@radiohead:[iowa_filebench_trace] # _BDX \nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace ]# 0100 : test_main\nBDX[ /xd/bx_data/tsk/2013/01/07/t01.iowa_mixed_wkld/iowa_filebench_trace/32.wloadgen ]# 0100 : run_wloadgen_filebench\nrm: cannot remove `wloadsrc.f\': No such file or directory\n#>> [20130218_161548] waiting for iotrace getting started \n........\n----------------------------------------------------\n#>> Ok, now start workload generation\n----------------------------------------------------\n20130218_161556\nFilebench Version 1.4.9.1\n30523: 0.000: Allocated 170MB of shared memory\n30523: 0.001: Eventgen rate taken from variable\n30523: 0.001: Video Server Version 3.0 personality successfully loaded\n30523: 0.001: Creating/pre-allocating files and filesets\n30523: 0.002: Fileset passivevids: 194 files, 0 leafdirs, avg dir width = 20, avg dir depth = 1.8, 1873608.045MB\n30523: 5.754: Removed any existing fileset passivevids in 6 seconds\n30523: 5.754: making tree for filset /mnt/hdd_4/filebench/passivevids\n30523: 5.755: Creating fileset passivevids...\n30523: 5296.248: Preallocated 104 of 194 of fileset passivevids in 5291 seconds\n30523: 5296.261: Fileset activevids: 32 files, 0 leafdirs, avg dir width = 4, avg dir depth = 2.5, 357480.206MB\n30523: 5296.331: Removed any existing fileset activevids in 1 seconds\n30523: 5296.331: making tree for filset /mnt/hdd_4/filebench/activevids\n30523: 5296.331: Creating fileset activevids...\n30523: 8052.228: Preallocated 32 of 32 of fileset activevids in 2756 seconds\n30523: 8052.228: waiting for fileset pre-allocation to finish\n31345: 10248.763: Starting 1 vidreaders instances\n31345: 10248.804: Starting 1 vidwriter instances\n31346: 10248.868: Starting 48 vidreaders threads\n31347: 10249.084: Starting 1 vidwriter threads\n30523: 10250.088: Running...\n30523: 13850.427: Run took 3600 seconds...\n30523: 13850.506: Per-Operation Breakdown\nserverlimit          1228002ops      341ops/s   0.0mb/s    105.3ms/op    71013us/op-cpu [0ms - 2410ms]\nvidreader            1228102ops      341ops/s  85.3mb/s    140.0ms/op    46691us/op-cpu [0ms - 1919ms]\nreplaceinterval      0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtclose             0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nnewvid               0ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nwrtopen              1ops        0ops/s   0.0mb/s      0.0ms/op        0us/op-cpu [0ms - 0ms]\nvidremover           1ops        0ops/s   0.0mb/s     68.8ms/op    70000us/op-cpu [68ms - 68ms]\n30523: 13850.506: IO Summary: 1228104 ops, 341.104 ops/s, (341/0 r/w),  85.3mb/s,      0us cpu/op, 140.0ms latency\n30523: 13850.506: Shutting down processes\n20130218_200648\n----------------------------------------------------\n#>> workload generation completed (STOP iotrace now)\n----------------------------------------------------\nroot@radiohead:[iowa_filebench_trace] # \n\n</pre>\n\n== ## bNote-2013-02-18 ==\n\n=== Misc. ===\n\n* [https://h30613.www3.hp.com/media/files/downloads/Non-FilmedSessions/TB2216_Becoming.pdf Become a Flash expert in minutes! SAMSUNG]\n\n* [http://static.usenix.org/event/lsf08/tech/FS_shepler.pdf Filebench Spencer Shepler]\n\n\n=== blktrace knowledge ===\n\n\n <pre>\n  8,48   4     1657     0.426331441  3443  A   W 2593896448 + 1024 <- (8,49) 2593894400\n  8,48   4     1658     0.426332295  3443  Q   W 2593896448 + 1024 [flush-8:48]\n  8,48   4     1659     0.426334674  3443  G   W 2593896448 + 1024 [flush-8:48]\n  8,48   4     1660     0.426503502  3443  A   W 2593897472 + 1024 <- (8,49) 2593895424\n  8,48   4     1661     0.426503961  3443  Q   W 2593897472 + 1024 [flush-8:48]\n  8,48   4     1662     0.426505840  3443  G   W 2593897472 + 1024 [flush-8:48]\n  8,48   4     1663     0.426666260  3443  A   W 2593898496 + 1024 <- (8,49) 2593896448\n  8,48   4     1664     0.426666707  3443  Q   W 2593898496 + 1024 [flush-8:48]\n  8,48   4     1665     0.426668036  3443  G   W 2593898496 + 1024 [flush-8:48]\n  8,48   4     1666     0.426829266  3443  A   W 2593899520 + 1024 <- (8,49) 2593897472\n  8,48   4     1667     0.426829714  3443  Q   W 2593899520 + 1024 [flush-8:48]\n  8,48   4     1668     0.426831154  3443  G   W 2593899520 + 1024 [flush-8:48]\n</pre>\n\n <pre>\n2593896448 + 1024 <- (8,49) 2593894400\n2593896448 - 2593894400 = 2048\n\n2593897472 + 1024 <- (8,49) 2593895424\n2593897472 - 2593895424 = 2048\n\n2593898496 + 1024 <- (8,49) 2593896448\n2593898496 - 2593896448 = 2048\n\n2593899520 + 1024 <- (8,49) 2593897472\n2593899520 - 2593897472 = 2048\n</pre>\n\n\'\'\'2048\'\'\' is the offset of the partition (/dev/sdd1) from the start point of the block device (/dev/sdd)\n\n <pre>\nblusjune@radiohead:[20130219_143400--videoserver] $ sudo fdisk -l /dev/sdd\n[sudo] password for blusjune: \n\nDisk /dev/sdd: 2000.4 GB, 2000398934016 bytes\n255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors\nUnits = sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x000c526d\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdd1            2048  3907026943  1953512448   83  Linux\n</pre>\n\n== ## bNote-2013-02-15 ==\n\n\n\n\n=== Hierarchical Temporal Memory ===\n\n* Numenta Hierarchical Temporal Memory - including HTM Cortical Learning Algorithms Version 0.2.1, September 12, 2011 [https://www.numenta.com/htm-overview/education/HTM_CorticalLearningAlgorithms.pdf Numenta Hierarchical Temporal Memory]\n\n# HTM Overview\n# HTM Cortical Learning Algorithms\n# Spatial Pooling Implementation and Pseudocode\n# Temporal Pooling Implementation and Pseudocode\n# Appendix A: A Comparison between Biological Neurons and HTM Cells\n# Appendix B: A Comparison of Layers in the Neocortex and an HTM Region\n\n==== About Numenta ====\n\n: Numenta, Inc. (www.numenta.com) was formed in 2005 // to develop HTM technology for both commercial and scientific use\n: Use of Numenta\'s software and intellectual property is free for research purposes.\n: Numenta will generate revenue by selling support, licensing software, and licensing intellectual property for commercial deployments.\n: Numenta is based in Redwood City, California. (Privately funded)\n\n\n==== Chapter 1. HTM Overview ====\n\n: Hierarchical Temporal Memory (HTM) is a machine learning technology that aims to capture the structural and algorithmic properties of the neocortex.\n\n: The neocortex is the seat of intelligent thought in the mammalian brain.\n\n: HTM\n\n=== SSD dirty script (iometer) ===\n\n[http://www.iometer.org/doc/downloads.html IOmeter dirty SSD script]\n\n\n=== BDP 근거 ===\n\n* 48시간 TPC-C 벤치마크한 blktrace log: 54.4GB\n* 1차 pre-processing: 52.2 GB\n\n== ## bNote-2013-02-14 ==\n\n\n=== 특허화 ===\n\n==== Patent #3 ====\n\n* (data + io insight) packaging 관련 특허\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n==== Patent #4 ====\n\n* IOWA based Proactive Data Placement 자체에 대한 특허\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 clue를 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보이는 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# clue, as simple as possible\n::::# clue, as specific as possible\n::::# clue, efficiently traversable - corresponding clue case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# clue, easily extensible - clue 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n==== Patent #5 ====\n\n* SSD Retention Time Controlling for Caching/Tiering 관련 특허\n\n\n==== Patent #6 ====\n\n* HML (Hierarchical ML)에서 FPGA, ASIC 등을 이용해서 Acceleration할 수 있는 방법은 없을까?\n\n== ## bNote-2013-02-12 ==\n\n\n\n=== IBdM 팀 회의 (15:40 ~ 17:00, 실험실) ===\n\n* 과제목표 (현재안): Intelligent Big Data Management 문제를 “H-ML을 통해” 남들보다 (EMC보다) 훨씬 잘 풀겠다!\n:- 요구사항\n::- 성공적으로 연구를 진행했을 경우의 최종 “기대치” 가 order-of-magnitude 높아야 함, 해결하는 문제가 수년간의 심도깊은 연구를 요구하여야 함\n::- I/O 성능 혁신 <- Data Placement 문제 : H-ML을 통한 Proactive data placement [70x improvement]\n::- 저장 효율 혁신 <- Data Reduction 문제 : H-ML을 통한 Clustered Deduplication [XXXX improvement] “Big Data를 Small Data로 만드는 기법”\n\n* Big Data Era에는 기존 Data Management 문제가 어떻게 달라지나?\n:- data양 폭증, IT budget증가 제한적 -> data reduction을 통한 IT 비용 절감 -> Data Deduplication 기술\n:- data capacity planning이 더 어려워짐 -> 필요에 따라 on-demand로 용량/성능 증설 요구 -> Scale-out 기술\n:- 비정형 data의 증가 -> NoSQL (?), Stream Processing (?)\n:- 비정형 data를 표현하는 데이터의 요구 증가 -> tagged data(metadata) 관리기술\n:: (data에 tag되어 data의 과거 access이력, access pattern, 복제/분열 history 등 intelligence정보를 저장)\n:- decentralized data(?) -> 자동화된 데이터 관리 중요 -> self-managed architecture 기술\n\n* 우리의 차별화\n:- Tool : hierarchical machine learning\n:- Input Data : 풍부한 tagged data (I/O workload, File I/O log, Application-level log, …)\n:- Motto\n::- Big Data의 근본적 이해를 통한 Big Data 관리기법 지능화\n::- Big Data의 생성, 확장, 분열, 복제, 소멸 등 full-cycle에 대한 근본적 이해\n\n* Action Item\n:- H-ML 기법 적용을 위한 data placement 문제 formulate, 기대치 estimation [융]\n:- H-ML 기법 적용을 위한 deduplication 문제 formulate, 기대치 estimation [신,구]\n:- “big data era에는 기존 data 관리 문제가 어떻게 달라지나?” [all]\n\n=== IOWA based Proactive Data Placement ===\n\n심상무님 지시.\n\n---- \n <pre>\n\nTM-20130212-IOWAPDP\n\n[ IO Workload Analysis 기반의\nProactive Data Placement ]\n\n\n\n2013-02-12\n\n\n\nData-intensive Storage\nIntelligence Group\nIntelligent Computing Lab\n</pre>\n\n----\n==== expected benefit ====\n\n <pre>\n이전에 얘기한 바가 있는데, proactive data placement와\nreactive data placement의 논리적인 비교를 통해\n원리상 기대되는 benefit의 크기 규모를 얘기할 수 있을까요?\n\n> 네, IO 성능 Penalty 최소화 측면과 SSD cache media 효율성 향상 측면에서\n  가늠해볼 수 있습니다. \n  \n  가) IO penalty 최소화\n    : 기존 방식은 미리 cache (혹은 fast-tier)에 proactive하게 올린다는 개념이 없었기 때문에\n      일단 최초 한 번은 HDD로부터 access되는 것이 필요하며,\n      그만큼 성능 penalty를 겪을 수 밖에 없습니다.\n      문제는 이렇게 성능 penalty를 겪는 경우가 얼마나 많이 존재하느냐입니다.\n\n      특히, \n      many-times-of-rehit 되는 small-number-of-data와 ................... (1)\n      small-times-of-rehit 되는 large-number-of-data 의 ................... (2)\n      분포 양상이 중요한데,\n      (1), (2) 각각이 전체 IO 에 얼만큼의 contribution을 해내는지가\n      proactive 방식과 reactive 방식과의 성능 차이에 영향을 미칩니다.\n\n      7200rpm의 HDD의 Read latency는 8500us,\n      SLC 기반의 NAND의 Read latency는 25us 라고 하고,\n      총 1,000 번의 IO access가 있었다고 했을 때,\n      A, B, C 각 workload 경우에 따른 IO 처리 소요 시간을 계산함으로써\n      reactive data placement 대비 proactive data placement의\n      성능 향상 정도를 가늠해보겠습니다.\n\n      (참고로, 아래 성능 비교 계산은 reactive Vs. proactive의 \n       핵심 logic에만 집중한 highly-abstracted 계산으로서,\n       블럭 레이어에서의 IO scheduling, HDD의 read-ahead, SATA/PCIe 버스 구조,\n       스토리지 디바이스 내부 구조 등 세부 시스템 적인 고려는 전혀 되어 있지 않음.\n       또한 cache media로 사용되는 SSD의 용량은 충분히 크다고 가정하였음)\n\n      A) 1개의 addr가 1,000번의 access를 다 받아낸 경우 (active range: 1) // 극단적 (1)의 경우\n         reactive 방식: 1 x 8500us + 999 x 25us = 33,475us\n         proactive 방식 (예측 적중률 100%): 1000 x 25us = 25,000us // IO 시간 1.3배 단축\n         proactive 방식 (예측 적중률 100% 미만): 1 x 8500us + 999 x 25us = 33,475us // IO 시간 단축 없음\n\n      B) 50개 addr가 400번의 access를, 150개 addr가 600번의 access를 받은 경우 (active range: 200)\n         reactive 방식: (50 x 8500us + 350 x 25us) + (150 x 8500us + 450 x 25us) = 1,720,000us\n         proactive 방식 (예측 적중률 100%): 1000 x 25us = 25,000us // IO 시간 68배 단축\n         proactive 방식 (예측 적중률 90%): 900 x 25us + 100 x 8500us = 872,500us // IO 시간 1.97배 단축\n\n      C) 1,000개 addr가 1번씩 다 access된 경우 (active range: 1,000) // 극단적 (2)의 경우\n         reactive 방식: 1000 x 8500us = 8,500,000us\n         proactive 방식 (100% 예측 적중시): 1000 x 25us = 25,000us // IO 시간 340배 단축\n   \n  나) SSD cache 미디어 효율성 향상\n    : 기존에는 미래에 다가올 IO에 대한 예측을 가지고 있지 않기 때문에\n      무조건 hit된 data들을 cache에 올리게 되며, 이후 자주 access될 data들과\n      그렇지 않은 data들이 모두 cache되므로, cache 공간의 낭비가 커집니다.\n      특히, data 규모가 커져서 SSD cache를 쓸 필요가 있는 경우,\n      SSD에 대한 write 및 erase를 많이 유발하게 되는 기존 방식은\n      SSD cache의 성능을 저하 시키는 중요한 요인이 될 수 있습니다.\n      이 측면에 대한 수치적 검증은, SSDsim 등을 이용하거나,\n      실제 SSD에 주어진 workload를 인가하여 IO 성능을 측정함으로써\n      실험적으로 증명해볼 수 있을 것 같습니다.\n</pre>\n\n----\n\n==== what is the heart of proactive data placement? ====\n <pre>\n대개 caching 알고리즘들은 데이터 I/O 패턴만 이용해서 cache에 남겨둘 데이터를 고르잖아요.\nproactive data placement 기술의 핵심은 미래의 데이터 I/O 패턴을 예측하는 것이라고 할 수 있나요?\n\n> 네, 맞습니다.\n  proactive data placement 기술의 핵심은 다가올 data IO 패턴 예측입니다.\n\n  상무님 말씀하신 것처럼,\n  기존의 cache 알고리즘들은 이미 cache된 data들에 대한\n  replacement (즉 eviction) 알고리즘이라고 보시면 됩니다.\n  즉, 일단은 access된 data들을 모두 cache에 올려두었다가,\n  그 cahced data들 중에서 \'미래에 덜 필요할 것\'같은 데이터를 골라내는 것입니다.\n  그러나 cache되어야 할 data를 예측하여 \'proactive 하게 미리\'\n  cache에 가져다놓는 것은 고려되어 있지 않습니다.\n  (이렇게 하지 못한 이유는, CPU L2/L3 cache와 DRAM 간의 cache이다보니\n   복잡한 알고리즘을 돌릴 시간 및 공간이 절대적으로 부족하기 때문이었습니다)\n	\n  기존 cache 알고리즘의 장점은 cache라는 공간에 data를 placement하는 알고리즘은\n  극도로 심플하지만, (별도의 알고리즘 없이 access된 data는 무조건 cache에 두므로)\n  일단 한 번이라도 data가 access되어야만 caching이 될 수 있다는 한계가 있습니다.\n\n  앞에서 이미 잠시 언급하였습니다만,\n  small-number-of-data가 many-times-of-rehit 이 된다면,\n  reactive data placement 방식과 proactive data placement 방식 간에\n  성능 차이가 크지 않을 수 있습니다.\n  그러나, active data range가 매우 넓은 경우에는\n  small-number-of-data가 many-times-of-rehit 되는 경우 뿐만 아니라,\n  large-number-of-data들이 small-times-of-rehit되는 경향 (long-tail)이\n  보여질 수 있기 때문에 proactive data placement에 의한 이득이 커질 수 있습니다.\n  (Active range가 매우 커지는 Big data 상황에서는\n   이렇게 될 가능성이 더 커질 것으로 보입니다)\n\n</pre>\n\n----\n\n==== Patent: IOWA based PDP ====\n <pre>\n미래의 데이터 I/O 패턴을 예측하는데 과거의 데이터 I/O 패턴만 볼 것이 아니라,\n데이터 자체의 특성, 파일 단위의 특성 (예를 들면, 이미지인지, 서버 로그인지, conf 파일인지),\nblock 자체의 특성 (뭐가 있을까요?), creator (user, process)의 종류,\nlast change/update time, 현재 active한 process들의 종류 등,\n다양한 데이터를 이용하는 것이 어떨까요?\n이런 방식이 새로운 아이디어라면 특허 출원을 검토해 주세요.\n\n> 네, 상무님, 다양한 데이터를 이용하는 것이 필요합니다.\n  이에, trace 기반의 access 패턴 뿐만 아니라 호스트 시스템에서 얻을 수 있는\n  정보들을 최대한 활용하고자 하고 있습니다.\n  그리고 이 부분에서 ML 기술의 적극적인 도입이 필요합니다.\n\n  (이 정보들이 IO 패턴 발굴 및 예측에 도움을 줄 수 있는지 유효성을 검증하고자\n   일부 정보들을 활용하여 이미 실험을 시작하고 있습니다.\n   현재는 IO 유발자 정보를 고려하여 실험 중입니다.)\n\n  고려중인 정보/접근 방식들은 다음과 같습니다.\n\n  - IO 액세스 패턴\n    - trace 기반의 address 별 access pattern\n      : periodicity 뿐만 아니라 recency, 누적 access count,\n        read/write intensiveness, access randomness, data usage-cycle 패턴\n        (read-modify-write(update)-read 등) 등\n      : 수집 시 blktrace 등 IO trace 툴 활용\n    - 시스템 meta 정보 기반 access pattern\n      : inode 등에서 얻어낼 수 있는 last create/update/access time 등\n      : 수집 시 inode 등 file, filesystem 정보 활용\n\n  - IO 유발자 정보\n    - user ID, group ID, process ID 등 creator 정보 및 현재 해당 file을 이용하는 \n      process / class 등\n      : 수집 시 debugfs, proc, sysfs 등의 메커니즘 활용\n      : IO 유발자가 어느 category에 속하는지 category 별로 분석\n        (web, DB, auth, monitoring, kernel thread (journald, kworker 등), ...)\n\n<!--\n  - State machine 기반의 macro IO pattern analsysis\n    : 유의미한 parameter 몇 가지로 정의되는 state machine을 정의하고,\n      이 state 간의 transition 확률을 이용하여 다가올 macro IO 패턴을 예측\n-->\n\n  특허 출원에 대해 말씀드리자면, 위의 기법 각각에 대한 특허에 대해서는\n  일부 시스템 등록, 혹은 현재 직무발명서 작성 중이었습니다만,\n  말씀 주신 바처럼 현재 저희가 진행하고 있는\n  IOWA 기반의 proactive data placement 자체에 대한\n  특허 가능성도 검토하도록 하겠습니다.\n\n  참고로, 2/12일 (오늘) 현재, 삼성 특허 검색 사이트에서, 검색식\n  ((proactive data placement) OR (data preplacement))\n  (cach* OR tiering)\n  ((IO OR I/O OR workload) analy*)\n  으로 찾아본 결과, 검색된 관련 특허는 없었습니다만,\n  검색 범위를 더 넓혀서 추가로 더 찾아보겠습니다.\n\n</pre>\n\n----\n\n==== Realistic sample trace log collection ====\n<pre>\n이렇게 다양한 데이터를 이용해서 데이터 I/O 패턴을 예측하려면 많은 sample log들을 모아서\n예측 함수를 만들어야 합니다. 분산 스토리지가 특정 용도에 이용되고 있다면\n(예로, 스트리밍 서버, 웹 서버, VDI 서버 등), 그것별로 sample log들을 많이 모아야 할 것 같습니다.\n우리가 어떻게 이런 다양한 로그들을 모을 수 있을까요?\n\n> 네, 전적으로 동감합니다. 수집 현황/계획은 아래와 같습니다.\n\n  1. Web Server IO Log\n    : 메모리사에서도 SAVL 타겟으로 우선적으로 관심을 가졌던 workload로서\n      SPECweb benchmark을 통해 realistic workload를 얻을 수 있습니다.\n      (메모리 사 응용기술팀으로부터 시스템 구축에 필요한 파일 받아옴)\n\n  2. DB IO Log\n    : TPC-C benchmark을 통해 realistic workload를 얻을 수 있습니다.\n      (이미 환경 구축 완료 및 사용 중)\n\n  3. Supercomputing IO Log\n    : 현재 기술원 3층의 수퍼컴퓨터 관리팀과 접촉 중\n      (Matlab, R, MapReduce-like RSP framework에 대한 workload 존재)\n\n  4. Streaming IO Log\n    : 아직 확보 방안 마련되지 않음\n		\n  5. VDI IO Log\n    : 기술원 정보전략팀과 의논 가능\n      (\'12년에 1차로 trace log 확보를 시도해본 적 있으나\n       정보전략팀과 SDS와의 관계가 어려워져 실패)\n\n</pre>\n\n----\n\n==== problem formulation considering on-line learning ====\n<pre>\n\n[ 지시 1 ]\n\n앞 경우는 sample log들을 모아서 예측 함수를 만드는 것이고,\n실제 deploy되었을 때는 dynamic 하게 예측 함수가 adaption할 수 있어야 하는데,\n그런 adaptation을 on-line learning으로 볼 수 있습니다.\n최희열 전문과 협력해서 위 문제들을 formulation 하기 바랍니다.\n\n> 네, 위의 내용들을 정리하여 최희열 전문과 함께 위의 문제들을 명확히 정리해보겠습니다.\n\n</pre>\n\n----\n\n== ## bNote-2013-02-08 ==\n\n=== Git Server on My Local Machine ===\n\nPlease check this wiki page: [[Git server on my local machine]]\n\n== ## bNote-2013-02-07 ==\n\n=== World Largest Data Centers ===\n\n* [http://wikibon.org/blog/inside-ten-of-the-worlds-largest-data-centers/ Inside Ten of the World’s Largest Data Centers, on March 25, 2010]\n\n\n=== Enterprise Storage Related Articles ===\n\n----\n==== The future of enterprise storage? ====\n: Henry Baltazar / Senior Analyst / Storage & Systems / The 451 Group\n: [http://www.snia.org/sites/default/files2/sdc_archives/2010_presentations/general_session/HenryBaltazar_Cloud_Convergence_Consolidation.pdf Clouds, Convergence, and Consolidation - SNIA SDC 2010]\n\n----\n==== Tiering vs. Caching ====\n; Automated Storage Tiering\n: Has become a popular technology in the past two years with multiple vendors embedding this feature within their storage systems.\n: Allows storage systems to migrate \'hot\' data to high speed flash-based storage tiers, while simultaneously moving stale data to inexpensive SATA hard drives.\n; Storage Caching\n: Primarily being driven by NetApp (i.e. Flash Cache) and a few startups, storage systems with caching use solid state storage as a memory extension technology.\n: Caching proponents claim that the migration required to do tiering is inefficient and reacts too slowly to eliminate hotspots.\n----\n\n=== Comparison of NAS vs. SAN // Pain Points of Storage System ===\n\n어제 회의의 Action Items 중 하나로, 제가 맡았던 부분인\n\"현재 Arch.가 야기하는 기술적 Poin Points 추가 정리\" 관련 메일입니다.\n\n내용은 다음과 같이 크게 두 가지로 구분됩니다.\n\n1. NAS vs. SAN 비교\n\n2. 기존 Storage System의 Pain Points (HP StorageWorks 제품 Feature를 통해 바라본)\n  -> 여기에는 이전문님께서 말씀하시곤 했던 ILM(Info. Lifecycle Mgmt.)에 \n     대한 언급도 나오네요. (더 자세한 내용은 첨부파일에 있습니다)\n\n// 아래 내용들은  HP에서 만든 \"Storage Overview and Architecture (HP, Arvind Shrivastava)\" 자료를 참고하였습니다. (첨부 1)  다소 NAS에 불리한 점들이 많이 부각되어 보입니다만, 다르게 보면, \"기존 NAS의 구조적 한계점이 이러했으니 Performance Drop 없이도 Scale-out 되는 NAS로 가기 위해서는 이러한 단점들이 극복되어야하겠다\" 라는 내용으로 받아들일 수도 있겠습니다.\n\n\n==== Comparisons of SAN, NAS (and DAS) ====\n\n{| border=1\n|+ \'\'\'DAS vs. NAS vs. SAN\'\'\'\n| Direct Attached Storage (DAS)\n| Network Attached Storage (NAS)\n| Storage Area Network (SAN)\n|-\n|\n*+ Low cost solution\n*+ Simple to configure\n*- De-centralized storage management\n*- No storage consolidation\n*- No high availability\n*- Low performance\n|\n*+ Heterogeneous environment\n*+ Centralized storage management\n*- Low performance\n*- Limited scalability\n*- Network congestion during backups & restore\n*- Ethernet limitations\n|\n*+ Heterogeneous environment\n*+ Centralized storage management\n*+ High degree of fault tolerance\n*+ Best and superior performance\n*+ Storage consolidation\n*+ Fast and efficient backups and restores\n*+ Dynamic scalability\n*- Expensive solution for small setups\n|-\n|}\n\n\n\n{| border=1\n|+ \'\'\'NAS vs. SAN\'\'\'\n| Items\n| style=\"width:40%\" | Network Attached Storage (NAS)\n| style=\"width:40%\" | Storage Area Network (SAN)\n|-\n| Network\n| IP\n| Fibre Channel\n|-\n| Reliability\n| Unreliable\n: >> FCoE, iSCSI 도 있는데 정말 그렇게 Unreliable인걸까요? FC에 비해 상대적으로 그렇다는 의미로 받아들이면 될까요?\n| Reliable\n|-\n| CPU overhead\n| Extremely High\n: >> \"Extremely High\" 까지일 줄은 몰랐네요. Block IO에 비해 File System Operation까지 해야 하는 NAS의 경우 CPU overhead가 더 있을 수 있겠다는 생각은 했었지만, 생각보다 overhead가 꽤 있는지도 모르겠습니다. (NetApp NAS, EMC Isilon 확인 필요).\n| Extremely Low\n: >> 반대로, SAN이 상대적으로 low-overhead라고 해서 \"Extremely\" 까지일 줄은...\n|-\n| Blocks\n| Large number of small blocks\n| Large blocks of data\n|-\n| Backup\n| LAN backup\n| LAN-free backup\n|-\n| Access and control by application\n| Applications driven by universal access to files\n| Applications managing own data specs\n|-\n| Typical usage\n|\n* Office applications file serving\n* File sharing between office users and CAD users\n* Content management for web servers\n* Consolidation of \'home directories\' for controlled and centralized backup\n* Boot device for diskless server farms\n* Rich media such as audio or video streaming\n|\n* Physical storage consolidation of any application mix\n* Physical storage sharing for cluster configurations\n* Sharing of backup devices (libraries)\n* Long distance data mirroring and/or data replication on storage device level\n* Redundant, high speed disk access for large database servers\n|}\n\n<br/>\n----\n\n==== HP Storage Product - StorageWorks ====\n\n\'\'\'Pain Points\'\'\'\n* HP의 storage 제품인 StorageWorks에서 언급된 pain points를 요약해보면 다음과 같습니다.\n\n# 네트워크 트래픽 증가로 인한 스토리지 시스템 성능 저하 (여기서 WAN을 언급하는 것으로 보아 물리적으로 멀리 떨어진 스토리지 간의 트래픽을 언급하는 것으로 보입니다)\n# 저장된 data의 대부분을 차지하는 (operational data에 비해 3~4배 이상 크기) reference data의 지속적 증가 (여기서 reference data라는 것은, 당장 operation되고 있지는 않지만, 이전에 생성되어 사용된 적은 있으나 폐기되지 않고 저장되어 있는, 그렇게 계속 쌓여가는 data들을 의미하는 것으로 보여집니다. 한마디로, operational data가 hot-data라면, reference data는 cold-data라고 볼 수 있을 것 같습니다)\n# 관리되고 저장된 data의 증가에 따라 속도 저하 및 TCO 증가\n# hetero한 사양 및 상태로 여러 곳에 흩어져있는 storage system의 관리\n\n\n\n\'\'\'HP StorageWorks - Reference Information Storage System\'\'\'\n:* 일종의 ILM (Information Lifecycle Management)\n:* Cost 절감 위해 application server/storage로부터 잘 사용되지 않는 static data 및 중복 data 감소.\n:* data의 integrity 보장.\n:* Power indexing 및 Grid computing을 통해 훨씬 빠른 search/retrieval을 달성하겠다.\n\n[[File:20130207 HP Storage Product Slides Scale is the problem .png|500px]]\n\n\n* Changing digital storage demands \n:* Majority is reference information (operational data 보다는 reference 정보가 저장된 data의 대부분 차지)\n\n[[File:20130207 HP Storage Product Slides - Changing digital storage demands .png|500px]] \n\n\n* WAN acceleration\n:* WAN을 통해 연결되는 스토리지 트래픽 감소 기술\n(100배 이상 빠른 throughput 달성 목표)\n{| border=1\n| Problems\n| style=\"width:40%\" | HP\'s solution\n| style=\"width:40%\" | What it does\n|-\n| Not enough bandwidth\n|\n* Scalable data referencing (SDR)\n|\n* Remove repeated bytes from the WAN for all TCP traffic\n|-\n| TCP chattiness and inefficiency\n|\n* Virtual Window Expansion\n* High speed TCP optimizations\n|\n* Optimizes performance of TCP\n* Enables much higher throughput on \"LFNs\"\n|-\n| Application chattiness and inefficiency\n| Transaction prediction (CIFS, MAPI)\n| Optimizes the performance of specific applications\n|}\n\n== ## bNote-2013-02-06 ==\n\n\n=== Defining: Target Segment, Product, Architecture ===\n금일 회의때 논의한 내용을 정리합니다.\n\n\n\'\'\'Consensus\'\'\'\n\n* Big data 시장만 보면 현재로서는 시장이 크지 않다. \n\n* HW Box를 같이 팔아야 한다. 그렇지 않으면 돈이 안된다.\n\n* Google, NHN등 big data center player에게 팔것은 아니다.\n\n* 삼성이 Enterprise Storage 사업 진입한다고 가정 (사업 진입유무를 우리가 고민하지 말자)\n\n* Large-scale system에서 data/node scale이 증가할수록 manageability issue가 커진다.\n\n* storage 신사업 초기에는 기존 infra에 대한 당사 제품의 compatibility가 매우 중요할 것이다. 이 부분은 신사업추진단이 고민할 것이고 우리는 2nd/3rd generation에 해당하는 innovative architecture에 집중한다.\n\n* new architecture는 고객의 현재/미래의 pain point에 기반해야 한다.\n\n \n\'\'\'Pain Point (고객입장)\'\'\'\n\n* \"Google같은 flexible/efficient data center 를 우리 회사 scale에 맞게 내부에서 소유하고 관리하고 싶습니다.\" (Google-in-a-box)\n\n* \"여러 인프라에 흩어진 기업데이터를 통합 관리하고 싶습니다.\" (for bigdata analytics?)\n\n* \"어떠한 경우에도 data loss가 없고 available한 불멸의 스토리지를 소유하고 싶습니다.\"\n\n* \"우리가 소유한 모든 infra를 통합관리하는 극강의 manageability가 필요합니다. 충분히 자동화되고 self-healing되어 관리자 1명당 관리노드의 갯수를 혁신적으로 올릴 수 있으면 좋겠습니다.\" (ML, Brain-inspired computing 적용가능성 있음)\n\n* replication 기술 (InformationWeek 자료에서 니즈 언급)\n\n\n\'\'\'Big Question\'\'\'\n\n* SAN ? NAS ? SAN+NAS ?\n\n* Server+Storage ?\n\n* up to which scale does the distribute architecture need to be expanded?\n\n* 고객이 별도의 big data platform (HW+SW) 을 구매할까? (in addition to already owned infra)\n\n* SW-defined storage 시대가 올까?\n\n* 왜 Google/NHN은 직접 data center를 buildup 했을까? 왜 EMC를 안쓸까?\n\n* InformationWeek 자료의 기술선호도가 2018년에는 어떻게 바뀔까?\n\n* Replication 기술이 중요한만큼 매우 어려울까?\n\n* 2018년에는 Cloud Storage 의존성이 어떻게 될까?\n\n* Data center model이 어떻게 다를까? : Google vs. Amazon (Fusion-IO구매) vs. Apple (Isilon구매)\n\n* service fee 로 먹고사는 biz model이 나올 수 있나?\n\n* Hadoop + Dedup? (data가 있는곳에 processing을! 이라는 hadoop philosophy가 dedup에 의해 어떤 영향을 받는가?)\n\n* block I/F, file I/F 이후 new interface가 어떻게 확산될까? 왜 object I/F는 확산되지 못했을까? \n\n* brain-like한 I/F는 어떤 형태가 될까?\n\n\n\'\'\'Action Items\'\'\'\n\n* 이: Google의 data center evolution history\n\n* 서: SAN/NAS, Converged/Storage, 분산 scale(100s? 1000? 10000?)\n\n* 신+구: 분산-scale storage에 분산-scale dedup이 적용되면 architecture가 어떻게 바뀌나? dedup manager의 위치가 어디가 되어야 하나?\n\n* 융: 현 architecture가 야기하는 기술적인 Pain Point 추가정리\n\n\n\'\'\'Check this out\'\'\'\n* 451 report provided by Seo-C: Automatic Tiering is the very hot technology\n\n== ## bNote-2013-02-05 ==\n\n\n=== IOWA:: machine learning ===\n\nk-addr-group\n\ndistance calculation and clustering\n\nhamming distance\n\n\n[[ bsc.iowa.lsp.addr.access_ptrn ]]\n\n\n[http://en.wikipedia.org/wiki/Levenshtein_distance Levenshtein distance]\n\n\n <pre>\n__k_addr_group__ 0 : [720616224, 3282792808, 3282793000, 3282793096]\n__k_addr_group__ 1 : [728218248, 730235992, 817521776, 3765476016]\n__k_addr_group__ 2 : [721421456, 724475672, 724475864, 3712136296]\n__k_addr_group__ 3 : [724475960, 828267560, 895981360, 3277437312]\n__k_addr_group__ 4 : [725469848, 726381104, 726381496, 726793728]\n__k_addr_group__ 5 : [723112360, 730730224, 3677713128, 3773683448]\n__k_addr_group__ 6 : [725235304, 828274704, 896778072, 3277440104]\n__k_addr_group__ 7 : [720987560, 724481496, 724727104, 725008864]\n__k_addr_group__ 8 : [545503608, 896780128, 3069345832, 3076725824]\n__k_addr_group__ 9 : [721386480, 721386960, 3752324048, 3767545408]\n__k_addr_group__ 10 : [725420616, 726671672, 727735192, 3066446120]\n__k_addr_group__ 11 : [721030800, 721030896, 721411176, 827916488]\n...\n__k_addr_group__ 245 : [725218552, 822170744, 870855200, 3642881480]\n__k_addr_group__ 246 : [457025608, 722055120, 3685634288, 3779707504]\n__k_addr_group__ 247 : [722055216, 722055312, 722055376, 725157784]\n__k_addr_group__ 248 : [721285200, 721285296, 728117992, 728118184]\n__k_addr_group__ 249 : [725218680, 725218808, 725218936, 725219032]\n</pre>\n\n\n\n=== Python:: numerical sort for string list ===\n\n <pre>\n\n>>> typeof(mlist)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name \'typeof\' is not defined\n>>> help(mlist)\n\n>>> mlist = {}\n>>> mlist[0] = []\n>>> mlist[1] = []\n>>> mlist[0] = [\"11243\", \"92\", \"2132\", \"42\"]\n>>> mlist[1] = [\"243\", \"892\", \"19132\", \"3242\"]\n>>> mlist[0] = [ int(x) for x in mlist[0] ]\n>>> mlist[0]\n[11243, 92, 2132, 42]\n>>> mlist[1]\n[\'243\', \'892\', \'19132\', \'3242\']\n>>> mlist[1] = [ int(x) for x in mlist[1] ]\n>>> mlist[1]\n[243, 892, 19132, 3242]\n>>> mlist   \n{0: [11243, 92, 2132, 42], 1: [243, 892, 19132, 3242]}\n>>> mlist[0].sort()\n>>> mlist\n{0: [42, 92, 2132, 11243], 1: [243, 892, 19132, 3242]}\n>>> mlist[1].sort()\n>>> mlist\n{0: [42, 92, 2132, 11243], 1: [243, 892, 3242, 19132]}\n>>> \n\n</pre>\n\n== ## bNote-2013-02-04 ==\n\n\n\n=== CWD ===\n\n <pre>\nb@ub04:[anal] $ pwd\n/x/iowa/now/tracelog_analysis.v02.Q_phase.2_mysqld/anal\nb@ub04:[anal] $ l\ntotal 36\ndrwxrwxr-x 8 b b 4096 Feb  4 19:23 ./\ndrwxrwxr-x 5 b b 4096 Feb  4 19:23 ../\n-rw-rw-r-- 1 b b  127 Jan 30 19:42 .bdx.0100.y.exec_iowa_for_all_cases.sh\ndrwxrwxr-x 2 b b 4096 Jan 30 19:38 .tools/\nlrwxrwxrwx 1 b b   27 Jan 30 15:55 .tracelog.A.addr -> ../preproc/.tracelog.A.addr\nlrwxrwxrwx 1 b b   27 Jan 30 18:48 .tracelog.R.addr -> ../preproc/.tracelog.R.addr\nlrwxrwxrwx 1 b b   27 Jan 30 18:48 .tracelog.W.addr -> ../preproc/.tracelog.W.addr\ndrwxrwxr-x 2 b b 4096 Jan 30 19:39 t00.R.xLBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:43 t01.R.1LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t02.R.100LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t03.R.10000LBA/\ndrwxrwxr-x 3 b b 4096 Jan 30 19:44 t04.R.20000LBA/\n</pre>\n\n=== IOWA.data.tpcc_250gb_48h.mysql.R.addr:: Offset 100K, Len 1K ===\n\n* [[IOWA.data.tpcc_250gb_48h.mysql.R.addr:: Offset 100K, Len 1K]]\n\n=== Frequent Item Detection/Discovery ===\n\n==== Stanford Open Book - Mining of Massive Datasets ((B.GOOD)) ====\n\n* MMD Book [http://i.stanford.edu/~ullman/mmds.html]\n\n: The book has now been published by Cambridge University Press. The publisher is offering a 20% discount to anyone who buys the hardcopy Here. By agreement with the publisher, you can still download it free from this page. Cambridge Press does, however, retain copyright on the work, and we expect that you will obtain their permission and acknowledge our authorship if you republish parts or all of it. We are sorry to have to mention this point, but we have evidence that other items we have published on the Web have been appropriated and republished under other names. It is easy to detect such misuse, by the way, as you will learn in Chapter 3.\n: Anand Rajaraman (@anand_raj) and Jeff Ullman\n\n: Ch 3. Finding Similar Items [http://i.stanford.edu/~ullman/mmds/ch3.pdf]\n:: 3.1 Applications of Near-Neighbor Search\n::: 3.1.1 Jaccard Similarity of Sets\n:: 3.2 Shingling of Documents\n:: 3.3 Similarity-preserving Summaries of Sets\n:: 3.4 Locality-sensitive Hashing for Documents\n:: 3.5 Distance Measures\n:: 3.6 The Theory of Locality-sensitive Functions\n:: 3.7 LSH Families for Other Distance Measures\n:: 3.8 Applications of Locality-sensitive Hashing\n:: 3.9 Methods for High Degrees of Similarity\n\n: Ch 6. Frequent Itemsets [http://i.stanford.edu/~ullman/mmds/ch6.pdf]\n:: 6.1 The Market-Basket Model\n:: 6.2 Market Baskets and the A-Priori Algorithm\n:: 6.3 Handling Larger Datasets in Main Memory\n:: 6.4 Limited-pass Algorithms\n:: 6.5 Counting Frequent Items in a Stream\n\n==== Orange - open source data visualization and analysis tool ====\n\nOpen source data visualization and analysis for novice and experts. Data mining through visual programming or Python scripting. Components for machine learning. Add-ons for bioinformatics and text mining. Packed with features for data analytics.\n\n\n==== SAX - Symbolic Aggregate Approximation ====\n\nSAX (Symbolic Aggregate approXimation) Homepage [http://www.cs.ucr.edu/~eamonn/SAX.htm]\n\nSAX is the first symbolic representation for time series that allows for dimensionality reduction and indexing with a lower-bounding distance measure. In classic data mining tasks such as clustering, classification, index, etc., SAX is as good as well-known representations such as Discrete Wavelet Transform (DWT) and Discrete Fourier Transform (DFT), while requiring less storage space. In addition, the representation allows researchers to avail of the wealth of data structures and algorithms in bioinformatics or text mining, and also provides solutions to many challenges associated with current data mining tasks. One example is motif discovery, a problem which we defined for time series data. There is great potential for extending and applying the discrete representation on a wide class of data mining tasks.\n\n\n==== The Amadeus motif discovery platform ====\n\nAmadeus Home [http://acgt.cs.tau.ac.il/amadeus/]\n\nAmadeus is a user-friendly software platform for genome-scale detection of known and novel motifs (recurring patterns) in DNA sequences, applicable to a wide range of motif discovery tasks. Amadeus outperforms extant tools and sets a new standard for motif discovery software in terms of accuracy, running time, range of application, wealth of output information and ease of use.\n\n==== Motif Detection ====\n\n* Python for Bioinformatics [http://telliott99.blogspot.kr/2009/06/motif-discovery.html]\n* MOODS: Motif Occurrence Detection Suite - ALGOrithmic Data ANalysis [http://www.cs.helsinki.fi/group/pssmfind/]\n* CMU CS Lecture -- Motif Detection [http://www.cs.cmu.edu/~epxing/Class/10810-05/Lecture6.pdf]\n\n==== Biopython ====\n\nBiopython is a set of freely available tools for biological computation written in Python by an international team of developers. [http://biopython.org/wiki/Main_Page Biopython]\n\n* Required Python Modules [http://www.scipy.org/more_about_SciPy NumPy / SciPy (SciPy.org)]\n** NumPy: N-dimensional Array Manipulations\n** SciPy: Scientific tools for Python\n\n* References\n# [http://biopython.org/DIST/docs/tutorial/Tutorial.pdf Biopython Tutorial]\n\n<br/>\n----\n\n=== RAM-SSD 하이브리드 Page Cache 아키텍쳐 :: WIPS 의견 대응 ===\n <pre>\n접수번호 : RN-201301-004-1\n\n명칭 : RAM-SSD 하이브리드 Page Cache 아키텍쳐\n\n기술 핵심 : RAM과 SSD를 이용하여 디스크 data를 caching하는 구조가\npage cache layer에서 동작하도록 함으로써 hot access를 check하고\naccess 정도에 따라 RAM 또는 SSD에 저장(tier-1~2)하거나 disk에서 access되도록 하는 것\n\n구성 : A) page cache에서 동작하는 RAN-SSD hybrid cache architecture\n      B) tier-1 page cache(RAM), tier-2 page cache(SSD)\n      C) page에 대한 hit pattern으로 page들을 tier-1,2로 class 지정\n\n\n--\n안녕하세요? 정명준입니다.\n다루는 내용이 많았음에도 깔끔하게 잘 정리해주시느라 고생이 많으셨습니다. ^_^\n꼭 추가되었으면 하는 내용과, 서술의 톤이 살짝 바뀌면 좋을만한 부분이 있어\n아래 보내주신 메일 본문에 추가하였습니다.\n(제 의견은 파란색으로 표시되어있습니다)\n\n1) \'기술 핵심\' 부분\n\n	\"RAM과 SSD를 이용하여 디스크 data를 caching하는 구조가 page cache layer에서 동작하도록 함으로써\" \n		-> 본 발명의 핵심 부분인 부분을 잘 적어주셨습니다.\n\n	\"hot access를 check하고 access 정도에 따라\"\n		-> 이 부분은 \"access pattern을 점검하여 rehit-potential에 따라\" 라는 표현으로 바뀌면 더욱 좋겠습니다.\n		기존에는 recently hot (recency) 위주로 cache될지 evict될지를 판단하였는데,\n		본 발명에서는 rehit potential을 보고 판단하겠다는 점이 다릅니다.\n		(once-hot data 라든지, periodicity 등을 파악)\n\n	\"RAM 또는 SSD에 저장(tier-1~2)하거나 disk에서 access되도록 하는 것\"\n		-> 여기에 \"page chunk 단위로 page tiering을 한다\"는 언급이 되었으면 좋겠습니다. (청구의 범위를 너무 제한하는 것이 아니라면요)\n\n\n2) \'구성\' 부분\n\n	아래 \'가~바\' 부분이 본 발명에서 청구하고자 하는 주요 내용들입니다.\n	이 중에서 \'가\' 부분이 보내주신 메일에서 A, B, C로 적어주신 \'구성\' 부분인데요,\n	대표 청구항을 적자면 아래 \'가~바\' 부분이 모두 필요할 것 같습니다.\n\n	가) RAM-SSD hybrid page cache architecture\n		page cache layer에서 동작하고;\n		tier-1 page cache (RAM), tier-2 page cache (SSD), page cache tiering manager를 포함하는;\n		page tiering 기반의 hybrid page cache 아키텍쳐.\n\n	나) rehit-potential aware page classification, and page class based cache data placement\n\n	다) page tiering operation in host side\n\n	라) page tiering operation in SSD side\n		chunk I/O in SSD in {log-structured and/or parallelism-maximized} manner\n\n	마) TRIM ahead 메커니즘 (page chunk writes latency를 줄이기 위해 미리 공간 확보하는)\n\n	바) victim page chunk selection in SSD\n		oldest-first\n		most-mark-first\n		with upper-tier-backup\n		\n\n오후에 전화로 말씀 나누면서 설명 드리겠습니다.		\n감사합니다.\n정명준 드림.\n</pre>\n\n\n=== 서울대 2013-02-01 Meeting Minutes ===\n\n1. 실험 관련\n: HDD 개수와 map slot의 개수를 align하여 실험 진행 후 기존 결과와 함께 분석\n: <span style=\"color:blue\">>>네, 본 실험에서 나타난 \'재미있는\' 현상이 Hadoop 권장 환경 구성과 맞지 않아서 발생한 것이 아니라 Hadoop 시스템 근원적으로 가지고 있는 \'한계\' 때문이라는 것으로 주장이 되려면, HDD 수와 execution slot 간 alignment는 필수적일 것 같습니다. 잘 부탁드리겠습니다.[1]</span>\n\n: In-memory buffer 크기 변화를 통한 spill file 개수 및 phase elapsed time의 관계를 exponential expression 으로 분석\n: <span style=\"color:blue\">>> 아, 제 발음이 좋지 않았던 것 같습니다. ^^;; exponential expression이라기 보다는 extrapolation이 가능한 equation으로 나오면 좋겠다는 것으로 이해해주시면 좋겠습니다.  아마 이게 되려면 충분한 경우에 대해서 실험이 되어야 할 것인데, equation이 나오기만 한다면 무척 중요한 의미를 갖게 될 것 같습니다. 전체적인 phase elapsed time 뿐만 아니라 p.15에 색깔로 구분된 각 영역들(map - map/shuffle - shuffle - reduce)의 변화 양상도 잘 설명이 될 수 있다면 좋겠습니다.[2]</span>\n\n2. SSD 관련\n: <span style=\"color:blue\">>> 특히 보내주신 발표 슬라이드의 p.18에 언급된 SSD 사용 시 9.5% 감소 부분은 충격이었습니다. 이 현상을 유발한 원인이 명확하게 규명되고 (1), 그 규명된 원인이 기존에 알려지지 않았던 것이며 (2), 그것을 해소할 수 있는 방법도 같이 제시될 수 있다면 (3), 본 산학 과제가 더욱 큰 의미를 갖게 될 것 같습니다.[3]</span>\n\n: random write는 피하는 방향으로 SSD을 통한 하둡 수행 성능 향상이 나올 수 있도록 I/O contention의 오버헤드가 SSD 성능과 상쇄될 수 있도록 detail하게 실험.\n: network i/o와 disk i/o의 trade-off\n: <span style=\"color:blue\">>> 네, combine 등을 통해서 network overhead를 local overhead (local computation 및 local I/O)로 trade-off 시키게 되지만, 그 local overhead를 SSD로 상쇄시킬 수 있는 best 방법 및 예상 효과가 나온다면 매우 의미가 있을 것 같습니다.[4]</span>','utf-8'),(132,'== ## bNote-2013-04-29 ==\n\n=== 경쟁사분석 ===\n\n* [http://www.computerweekly.com/news/2240182642/Fusion-io-buys-NexGen-to-join-hybrid-flash-array-fray Fusion IO Buys NexGen to Join Hybrid Flash Array // 2013-04-24]\n\n* [http://www.storagereview.com/fusionio_announces_nexgen_storage_acquisition Fusion-io Announces NexGen Storage Acquisition // 2013-04-24]\n\n* [http://www.theregister.co.uk/2013/04/24/fusion_io_nexgen/ Fusion-io buys NexGen - Gets hybrid flash/disk array startup // 2013-04-24]\n\n* [http://venturebeat.com/2013/04/24/fusion-io-acquires-hybrid-storage-appliance-vendor-nexgen-storage-for-114m/ Fusion-io acquires hybrid storage appliance vendor NexGen Storage for $114M // 2013-04-24]\n\n* [http://www.computerweekly.com/news/2240179705/ID7-buy-takes-Fusion-io-deeper-into-software-defined-storage ID7 buy takes Fusion-IO deeper into software-defined storage]\n\n* [http://searchstorage.techtarget.com/opinion/Software-defined-storage-Is-hardware-obsolete Software-defined Storage: Is Hardware Obsolete? // SearchStorage.techtarget.com]\n\n* [http://www.storagereview.com/fusionio_acquires_id7_developers_of_scst Fusion-IO Acquires ID7, Developers of SCST (SCSI Target Subsystem)]\n\n* [http://www.theregister.co.uk/2013/04/09/blind_spot/ Mutant Array Upstarts Feast on EMC, NetApp\'s Leavings -- Nimble, Tegile, Tintri -- SSD/HDD Hybrid Array Startups re-inventing the hybrid array with new software]\n\n== ## bNote-2013-04-26 ==\n\n=== Consistent Hashing ===\n\n* [http://www.tom-e-white.com/2007/11/consistent-hashing.html Consistent Hashing Illustrated ((B.GOOD))]\n\n* [http://amix.dk/blog/post/19367 Consistent Hashing Simply in Python]\n\n=== SDN ===\n\n==== SDN의 정의 및 발생 배경 ====\n\n* [http://katesfam.blogspot.kr/2012/01/sdn.html SDN은 무엇인가? 그리고 왜 대두되었는가?]\n \n <pre>\n\nSDN에 대해 잘 정리한 글이 있어서 보내드립니다.\n\nhttp://katesfam.blogspot.kr/2012/01/sdn.html\n\n \n\n\n\n----\n\n \n\nSDN은 무엇인가? 그리고 왜 대두되었는가?\n\n \n\n \n\n1. SDN은 무엇인가?\n\n\n수년 동안 컴퓨터 과학자들은 네트워크의 속도와 안정성, 에너지 효율, 보안 등을 획기적으로 개선시킬 수 있는 방법을 꿈꿔왔다. 그러나 그 방법을 설계하거나 고안하더라도, 실제로 대규모(large-scale)로 실험하거나 검증하는 것은 불가능했다. 인터넷의 코어(core)를 구성하는 라우터나 스위치들이 이른바 완전히 닫혀 있어서 그 위에서 새로운 소프트웨어나 프로그램을 실험하는 것이 원천적으로 봉쇄되었기 때문이다.\n\n \n\n이러한 연유로 연구되어온 많은 기술 중 SDN은 Software Defined Networking을 의미하며 우리말로 소프트웨어 정의 네트워킹이라 부른다. SDN은 OpenFlow라는 기술 혹은 소프트웨어를 통하여 널리 알려졌다. OpenFlow와 SDN은 뗄레야 뗄 수 없는 관계이다. SDN이 물론 더 큰 개념으로 네트워크 구조 혹은 새로운 패러다임이며, OpenFlow는 SDN을 위한 “인터페이스 표준 기술”로 정의된다. SDN을 지원하는 기술 중에서 학교, 연구소, 기업 등으로부터 가장 관심을 받는  OpenFlow는 별도로 설명하기로 하고 (이미 많은 참고자료가 나와 있기도 하다), 여기서는 먼저 SDN이 무엇인지, 그리고 왜 필요성이 부각되었는지에 대하여 몇 가지 레퍼런스를 바탕으로 다루어 보려 한다.\n\n \n\n먼저 위키피디어의 정의를 살펴보자. Kate Greene이 2009년도3/4월 판 MIT 테크니컬 리뷰에서 소개한 용어로 알려져 있는 SDN은 네트워크 제어 기능(control plane)이 물리적 네트워크와 분리되어 있는 “네트워크 구조”를 말한다. 위키피디어에 따르면  SDN을 특징짓는 두 가지 중요한 포인트는 다음과 같다. 첫째, 네트워크 제어 기능을 데이터 전달 기능(data plane)과 분리하여 구현해야 한다. 둘째, 네트워크 제어 기능이 개발되고 실행될 수 있는 환경을 분리하여 전형적인 낮은 성능의 CPU가 장착된 하드웨어 스위치에 더 이상 위치시키지 않는다. 다시 말해서 SDN이라면 기본적으로 네트워크 제어 기능이 기존의 스위치나 라우터 등의 하드웨어와 별도로 분리되어야 하고, 데이터 전달 기능과도 역시 분리되어 개발 및 실행될 수 있는 네트워크 구조를 가져야 한다.\n\n \n\n분리된 SDN의 제어 기능은 필연적으로 네트워크 스위치(하드웨어) 상의 데이터 경로와 상호작용할 수 있는 기능을 가져야만 한다. 이러한 상호작용 혹은 통신 메커니즘 중의 하나가 바로 OpenFlow 기술이다. OpenFlow는 흔히 SDN과 동일한 것으로 혼동되기도 하지만, 사실 SDN을 구성하는 하나의 요소로 제어 기능을 가진 머쉰과 네트워킹 스위치간의 통신을 담당하는 표준 인터페이스이다. 그리고, SDN의 범주 안에서 OpenFlow를 반드시 사용해야 한다는 아무런 제약이나 요구사항도 없다. 현재 SDN과 OpenFlow의 정의, 마켓팅 등의 이슈는 개방형 네트워킹 재단(Open Networking Foundation; ONF)에서 관리되고 있다. \n\n \n\n그렇다면 ONF에서는 SDN을 어떻게 바라보고 있을까? 일단 ONF가 무엇인지부터 살펴보자. ONF는 (미연방세법을 따르며) 비영리, 상호 이익을 바탕으로 하는 국제 기구로 SDN의 개발과 활용을 촉진하는 것을 목표로 삼고 있다. ONF의 이사회는 여덟 명의 멤버로 구성되는데 여섯 개의 설립 회사가 각각 한 명씩 지정한 여섯 명의 이사와 두 명의 창립자이다. 여섯 개의 설립 회사는 대규모 네트워크 운영자 및 (잠재) 사용자 그룹을 대표하는 도이치 텔레콤(Deutsche Telecom), 페이스북(Facebook), 구글(Google), 마이크로소프트(Microsoft), 버라이즌(Verizon)과 야후(Yahoo)이며, 두 명의 창립자는 UC 버클리의 Scott Shenker와 스탠포드 대학의 Nick Mckeown이다. 그리고 이외에 사무총장(Executive Director) Dan Pitt이 ONF를 총괄 관리한다.\n\n \n\nONF가 SDN을 바라보는 관점은 크게 두 가지의 기본적인 원칙을 바탕으로 하고 있다.\n\n \n\n먼저 SDN은 소프트웨어 정의 포워딩(Software Defined Forwarding)을 해야 한다. 이것은 스위치와 같은 하드웨어가 수행하는 데이터 포워딩 기능이 반드시 개방형 인터페이스와 소프트웨어를 통해서 제어되어야만 한다는 것을 의미한다. 하드웨어는 소프트웨어로부터 [헤더 템플릿, 포워딩 액션] 셋을 받아 특정한 액션(action)을 실행한다. 예를 들면 어떤 네트워크 포트로 패킷을 “전달(forwarding)”하거나 혹은 “폐기(drop)”할 수 있다. 다만 해당 특정 액션은 [헤더 템플릿, 포워딩 액션]의 “헤더 템플릿”에 상응하는 패킷에 대해서만 실행된다. 여기서 헤더 템플릿은 “모든 패킷” 혹은 “어떤 패킷의 그룹”등을 의미하는 와일드 카드를 포함할 수 도 있다. 앞서 언급되었듯이 SDN의 소프트웨어 정의 포워딩은 반드시 개방형 인터페이스와 소프트웨어를 포함하는데, OpenFlow 기술이 “개방형 인터페이스”에 해당된다.\n\n \n\n그리고, 두 번째 원칙은 SDN이 추상화된 글로벌 관리 혹은 글로벌 관리 추상화(Global Management Abstraction)를 목표로 한다는 것이다. SDN은 기본적인 글로벌 관리 추상화를 지원함으로서 보다 선도적인 네트워크 관리 툴이 개발될 수 있도록 해야 한다. 예를 들면 이런 추상화 도구들은 네트워크의 글로벌 뷰, 네트워크 이벤트(토폴로지 변화나 새로운 플로우 생성 등)에 따른 반응, 그리고 네트워크 요소를 제어할 수 있는 기능 등을 포함할 수 있다. (네트워크 요소 제어는 해당 엔트리를 하드웨어의 포워딩 테이블에 넣는 방법을 사용한다.)\n\n \n\n따라서, ONF 가 바라보는 SDN은 두 가지, 즉,  소프트웨어 정의 포워딩과 글로벌 관리 추상화가 핵심이다. 그리고, 이를 위해서 개방형 인터페이스(예: OpenFlow), 제어 소프트웨어, 글로벌 네트워크 관리 툴 등의 세부적인 기능이 언급되었다.\n\n \n\n여기서 잠시 위키피디어로 돌아가보자. 위키피디어에서 언급한 Kate Greene (과학기술 저널리스트)이 작성한 테크니컬 리뷰의 내용을 보면 SDN이 무엇인지 개괄적으로 잘 정리되어 있다. 이 기사에 따르면 ONF의 창립자 중 하나이자 이사회 멤버이며, OpenFlow 기술과 표준을 개발한 Nick McKeown이 다음과 같이 말한다. “오늘날 보안, 라우팅, 에너지 효율 관리 등은 단지 기계덩어리인 네트워크 장비에 의해 좌지우지됩니다. 그건 정말 바꾸기 힘들지요. 이것이 바로 인터넷 인프라가 40년 동안이나 변하지 않은 이유입니다.” 일반적으로 데이터 패킷이 스위치 (혹은 라우터)에 도착하면 스위치의 펌웨어가 해당 패킷의 목적지 주소를 보고 그 패킷을 이미 정해진 규칙에 따라 포워딩(forwarding)한다. 정해진 규칙은 네트워크 운영자도 제어하기 어렵다. 같은 목적지를 갖는 모든 패킷은 같은 경로를 이용하고 언제나 같은 방식으로 다루어진다. 이것이 현대 인터넷의 일반적인 패킷 포워딩 방식이다.\n\nSDN은 무엇인가? 라는 질문의 답은 바로 현재 인터넷이 가지고 있는 “항상 같은 방식이며 제어가 어려운” 패킷 포워딩 방식을 바꾸는 것으로 부터 시작한다. OpenFlow의 예를 들면, 그 전에는 거의 아무도 손대기 어려웠던 종단간 네트워크 경로를 컴퓨터 과학자들이 쉽게 변경할 수 있도록 지원하여 e-mail보다 비디오 어플리케이션이 우선 데이터를 받을 수 있도록 하거나 다양한 트래픽을 각자 다른 경로로 보낼 수도 있고, 어떤 트래픽은 보안 목적으로 격리할 수 있도록 해준다.\n\n \n\n그렇다면 패킷 포워딩 방식을 바꾸는 것이 바로 SDN인가? 그렇지 않다. 이것은 SDN이라는 큰 구조를 구성하는 하나의 요소일 뿐이다. OpenFlow가 바로 패킷 포워딩 방식을 표준화된 방법으로 바꿀 수 있는 하나의 콤포넌트이다. SDN은 아키텍쳐 혹은 프레임을 제공하는 큰 개념이자 구조 혹은 패러다임으로, 하드웨어와 어플리케이션, 하드웨어 추상화 계층, 하드웨어와 분리된 제어 기능(control plane, controller 등으로 불리운다.), 하드웨어 추상화 계층과 통신하는 표준 기능 등을 모두 포함한다.\n\n \n\nSDN 구조에서, 하드웨어는 스위치나 라우터 등이며, 시스코, 쥬니퍼, HP, NEC 등이 개발하고 현재 인터넷 공급자들이 서비스를 제공하기 위하여 설치하고 운영하는 하드웨어 박스(box)를 의미한다. Nick이 말했듯이 오늘날의 인터넷이 거의 변화하지 못한 가장 큰 원인을 제공하는 주범들이다. SDN은 이 기계덩어리에 하드웨어 추상화를 위한 계층을 더해준다. 즉, OpenFlow와 같이 표준화된 인터페이스를 통하여 하드웨어에 접근하고, 소프트웨어에 기반하여 하드웨어를 “통제”할 수 있는 기반을 제공하는 것이다. 이 추상화 계층은 OpenFlow의 플로우 테이블과 같은 형태로 하드웨어에 구현되어야 한다. 따라서 하드웨어 벤더들의 지원과 협력이 필수적이다. (현재 약 16개의 주요 네트워크 벤더들이 구현했거나 구현중이다. SDN을 지향하는 OpenFlow가 가장 선두에서 탄력받는 기술로 주목받고 있는 배경이기도 하다.) 추상화 계층을 구현함에 있어서 가장 중요한 것은 표준화된 인터페이스를 지원하는 것이고, 두 번째는 각 개별 벤더가 원하는 “독립성”을 보장해 주는 것이다. 개별 벤더의 고유 기술이나 고유 기능은 자치적으로 보장되면서 SDN을 위한 표준화된 통로를 제공하는 것은 벤더 고유의 기술을 보호하면서도 호환성을 유지하는데 있어서 필수적이며, OpenFlow의 경우 이를 매우 잘 준수하고 있다.\n\n \n\n다음으로 무엇보다 중요한 요소는 바로 하드웨어와 분리된 제어 기능이다. Controller로 불리기도 하는 이 제어 기능은 스위치나 라우터가 아닌 별도의 머쉰 상에서 구현된다. 머쉰은 PC가 될 수도 있고 성능 좋은 서버가 될 수도 있다. 이 제어 기능은 두 가지 SDN의 다른 두 가지 콤포넌트와 상호작용한다. 한 가지가 어플리케이션이고 다른 한 가지는 하드웨어, 좀 더 정확히는 하드웨어에 구현된 추상화 계층이다. 따라서 제어 기능을 통해서 어플리케이션은 네트워크의 다양한 정보를 얻을 수 있고, 반대로 네트워크 역시 어플리케이션 요구 사항 등의 정보를 얻을 수 있다. 이러한 핵심적인 역할 때문에, 제어 기능은 종종 네트워크 운영 체제(Network OS)라고 불리우기도 한다.\n\n \n\n제어 기능은 주로 API와 같은 방법을 통해서 어플리케이션이 원하는 기능을 제공한다. 반대의 경우도 거의 같다. 어플리케이션과 제어 기능 간의 통로인 셈이다. 그렇다면 제어 기능과 하드웨어 추상화 계층의 통신은 어떻게 이루어질까? OpenFlow가 이 질문에 대한 해답이며, 이미 많은 연구가 진행되어 있다.\n\n \n\n \n\n2. SDN이 대두된 이유\n\n \n\n지금까지 위키피디어, ONF, 테크니컬 리뷰 등을 인용하고 몇 가지 살을 붙여 SDN이 무엇인지 정리해 보았다. SDN은 새로운 네트워크 구조이며 패러다임이다. 그럼 이제부터 SDN이 왜 대두되었는지 알아보기로 하자.\n\n \n\n가장 최근에 ONF가 개최한 컨퍼런스인 Open Networking Summit(2011년 10월)에서 ONF의 또 다른 창립자인 Scott Shenker가 발표한 내용을 보면, 인터넷이 이렇게 크게 성공한 가장 큰 이유가 바로 “계층화(layering)”에 있다는 것을 알 수 있다. 어플리케이션(WWW, e-mail 등)은 신뢰성/비신뢰성 트랜스포트 계층(TCP/UDP)위에서 돌아가고, 트랜스포트 계층은 최선형 글로벌 패킷 전달 계층(IP)위에서 동작되며, IP는 최선형 로컬 패킷 전달 계층(Ethernet, PPP 등)위에서, 다시 로컬 패킷 전달 계층은 물리 계층(copper, fiber, radio등) 위에서 돌아가는 것이 바로 계층화이다. 즉, 서로 다른 계층이 독립적으로 동작하되 상/하위 계층과 상호 호환되는 방식으로 일종의 혁신을 이룬 것이다. 덕분에 인터넷은 교육/연구망으로 시작되었으나, 상업적으로도 엄청난 성공을 거두었고 지금까지 가장 널리 사용되고 있다. 그러나, 초창기 개발된 인터넷 구조나 모델이 아직도 거의 그대로 사용되고 있는 형편으로, 상업적인 성공이 또 다른 인터넷의 혁신으로 이어지지 못했다. 왜 그럴까?\n\n \n\n이 질문에 답하기 전에 먼저 다른 분야를 한 번 살펴보자. 예를 들어 컴퓨터 운영체제(OS), 데이터베이스(DB), 분산 시스템 등은 소프트웨어의 연구 개발을 통해 발전해 왔다. 학교에서 배우는 기본적 원리들을 바탕으로 소프트웨어를 개발하고 진화시킨 것이다. 이러한 소프트웨어는 우리가 흔히 알고 있는 고급 프로그래밍 언어로 작성할 수 있으며, 새로운 기능이 필요한 경우 기존에 개발된 소프트웨어를 바탕으로 새로운 버젼으로 계속해서 발전할 수 있다. LINUX, Windows, Mac OS 등이 이런 방식으로 진화해온 대표적인 OS 이다. 데이터베이스나 분산 시스템도 마찬가지이다. 그리고 이러한 소프트웨어 기반의 시스템들은 대부분 편리하고 쉬운 유저 인터페이스를 통해 쉽게 관리 가능한 환경을 가지고 있다.\n\n \n\n그렇다면 네트워크는 어떤가? 일단 학교에서 OSI 7 계층 부터 시작하여 TCP, IP 등등의 여러 프로토콜에 대해서 배운다. 이들의 기본적 원리나 알고리즘에 대해서도 배우지만 대부분 동작 원리 등 실용적인 부분에 집중된다. 물론 이른바 단말 시스템(end-system)에서 돌아가는 TCP 등의 일부 프로토콜은 기능이 향상된 버젼이 개발되었거나 개발이 진행 중이다. 하지만, 단말과 단말 사이에서 데이터 전송과 전달을 담당하는 액세스/코어 네트워크의 프로토콜이나 기타 기능들은 이미 대부분 개발이 끝나서 적용 및 서비스 되고 있는 단계이기 때문에 새롭게 수정하거나 개발하기 어렵다. 물론 네트워크 벤더의 경우 추가적인 기술 개발과 적용이 가능하지만 DB나 운영체제 등과 비교할 때 많은 진보가 일어나지는 않았다. 네트워크 분야의 경우, 소프트웨어와 프로그래밍을 바탕으로 한 새로운 기술의 개발과 진화가 다른 컴퓨터 과학 분야와 비교할 때 그 발전이 더디게 진행되어 온 것은 분명한 사실이다. 네트워크 관리 환경은 어떠한가? 일부 운영자나 엔지니어에게 종속되어 있는데다가 그 마저도 사용하기가 쉽지 않다. 전문적인 지식이나 기술을 요구하는 경우도 많다. 최근 몇 몇 연구들이 이러한 관리 환경을 개선하는데 그 촛점을 맞추고 있긴 하지만 다른 분야에 비해서 부족한 상태로, 매우 불편한 사용자 인터페이스를 가지고 있다.\n\n \n\n“구조는 단순하지만 관리가 복잡하고 인터페이스는 어렵다.” 이것이 현재의 인터넷이 가지고 있는 가장 큰 문제로 귀결된다. SDN이 대두된 이유는 바로 이 화두를 타파하기 위해서이다.\n\n \n\n단순한 구조는 물론 장점이다. 덕분에 인터넷이 오늘날의 압도적 지위를 누리게 되었다. 하지만 단순성을 유지하기 위하여 새로운 기술을 적용하거나 소프트웨어를 개발하는 측면에서 다른 분야에 비해 큰 제약을 가져야만 했다. 이와 같은 단점을 보완하기 위하여 SDN은 제어 프레임워크, 혹은 제어 기능을 분리하여 소프트웨어의 개발을 촉진시키고자 한다. 즉, 소프트웨어를 기반으로 인터넷이 보다 빠른 속도로 진화할 수 있도록 하자는 것이다. 그리고, SDN은 관리의 복잡성을 해소하기 위하여 제어 기능을 기존 하드웨어에서 분리시키고, 사용자 인터페이스를 매우 단순하고 편리하게 만듦으로써 엔지니어, 운영자 뿐만 아니라 일반 사용자도 쉽게 네트워크를 관리하고, 가상 네트워크를 생성하여 이용할 수 있도록 한다.\n\n \n\n참고로, 미래인터넷 포럼(FIF)의 테스트베드 워킹그룹 이슈 분석서 #2에서 Nick의 발표 내용을 정리한 부분을 보면, SDN이 어떻게 인터넷이 가지고 있는 문제를 해결하고 혁신을 가속화할 수 있는지에 대하여 잘 알 수 있다.\n\n\nNick은 SDN을 기반으로 한 새로운 패러다임을 만들어 내어서 아래와 같은 혜택들을 누리는 네트워크 장치 생태계를 만드는 것이 필요하다고 역설한다.\n\n\n    1.     데이터 전달(Forwarding) 추상화에 따라서 OpenFlow 표준에 의해 검증된   하드웨어를 이용하고, 또한 이에 연동하여 소프트웨어 기반으로 제공되는 네트워킹 특성이 요구되는 모든 절차들에 대해서 각각의 절차마다 충분하게 증빙되어 있는 견실한 네트워킹을 실현할 수 있는 토대를 구축할 수 있다.\n    2.     장비 사용자들이 자신의 필요에 따라 네트워킹을 유연하게 구성(customize)하고 불필요한 구성요소들은 과감하게 제거하면서 자신만을 위해서 가상화된 네트워크를 생성하기 쉽도록 지원한다.\n    3.     하드웨어 추상화(abstraction)에 따라 확장성을 고려한 상태에서 공통화된(즉 commodity 형식으로) 하드웨어를 구입하고, 또한 소프트웨어도 분리해서 구입하도록 하여 기존의 폐쇄적인 네트워크 장비 공급자 체인을 벗어나서 자체 개발, 외주 개발, 오픈 소스 형식을 모두 포함하는 다변화된 공급자 체인으로 체질 개선을 유도할 수 있다.\n    4.     소프트웨어를 개발하는 속도로 혁신이 일어나도록 하고, 표준은 구현된 소프트웨어의 확산을 위해 뒤따라가는 방식을 취하고, 소프트웨어적인 개방성에 근간하여 기술의 공유 협력을 쉽도록 함으로써 혁신의 속도를 가속하도록 지원한다.\n\n \n\n다른 내용도 모두 중요하지만, 특히 4번에 주목하자. 소프트웨어를 개발하는 속도로 혁신이 일어나게 하고 표준은 이를 뒤따르게 하자는 것이야 말로 SDN이 왜 대두되었는지 설명하는 핵심 중의 핵심이라 할 수 있다.\n\n \n\n내용이 두서없이 길어진 듯 하다. 이제 정리해보자. SDN은 아직도 정의되고 있는 단계이다. 본문에서 언급한 여러 레퍼런스를 보면 분명히 주된 맥락은 있지만 모호하고 추상적인 부분도 있는 것이 사실이다. 따라서 앞으로 SDN의 모습은 계속해서 변화될 가능성이 틀림없이 존재한다. 그렇지만 주된 맥락을 고려할 때 SDN은 현재 시점에서 다음과 같이 정의될 수 있을 것이라 생각한다. \n\n\n\"SDN은 이른바 소프트웨어를 통해서 현재의 인터넷이 가지는 구조적 문제를 근본적으로 해결하고 혁신할 수 있도록 대두된 새로운 네트워크 구조 혹은 패러다임으로써, 어플리케이션, 네트워크 OS, 하드웨어 추상화, 표준화된 인터페이스 및 하드웨어를 모두 아우르는 개념\"이다.\n\n</pre>\n\n==== SDN 관련 소식 ( Open Daylight ) ====\n\n\n* [http://www.zdnet.co.kr/news/news_view.asp?artice_id=20130422135700 SDN 국면 전환을 꿈꾸다 \'오픈데이라이트\']\n\n <pre>\n\n[ Open Daylight 출범으로 본 SDN 물결의 변화 ] \n\n\n* 시스코, 주니퍼 등 벤더 중심의 SDN 연합체인 Open Daylight 출범(\'13년 4월 초)\n\n \n\n\n\n* 리눅스재단이 주도, 시스코, 주니퍼, IBM, MS, 레드햇,\n\n빅스위치, 브로케이드, 시트릭스 등등이 Platinum 스폰서로 참여.\n\n\n\n \n\n* 의미\n\n  : 학계, 구글, AT&T, NTT 등 서비스 업체에 의해 주도 되었던 기존 SDN 움직임과는 달리,\n\n    Open Daylight는 IT의 Big Vendor들이 주도하는 SDN 움직임이라는 차이점이 있음\n\n    (시스코는 오픈네트워크환경(ONE) 컨트롤러를 Open Daylight에 기증하는 등,\n\n     기존의 SDN에 대한 미온적인 입장을 버리고 적극적으로 SDN 물결에 동참)\n\n \n\n\n\n* Open Daylight의 프로젝트 구성\n\n   - Flexible 컨트롤러 프로젝트\n\n   - 가상 네트워크 프로젝트\n\n   - Java 기반 프로토콜 플러그인 프로젝트\n\n   - 프로그램 가능한 인터페이스 프로젝트\n\n   - SDN 응용 프로젝트 ++\n\n\n(++) SDN 응용의 중요성:\n\nSDN 컨트롤러 자체만으로는 의미가 크지 않음.\n\nSDN을 적극적으로 활용하는 응용들이 많아져야 SDN 생태계가 활성화될 수 있음\n\n\n\n \n\n* Open Daylight은 올해(\'13년) 3분기에 정식 코드 공개 예정\n\n\n\n \n\n* Open Daylight의 라이센스는 EPL(Eclipse Public License)로서,\n\n   코드를 수정해 사용하거나 별도 애플리케이션을 개발했더라도, 코드 자체를 공개하지 않아도 됨.\n\n\n\n* 관련 기사:\nSDN 국면전환을 꿈꾸다 \'오픈데이라이트\' [ZDnet 4/22]\nhttp://www.zdnet.co.kr/news/news_view.asp?artice_id=20130422135700\n\n\n\n \n\n \n\n--\n\n\nZDnet KR - All\n SDN 국면전환을 꿈꾸다 \'오픈데이라이트\' \n\n[지디넷코리아] 세계 IT거인들이 집결한 소프트웨어정의네트워킹(SDN) 연합체 ‘오픈데이라이트’가 출범했다. 시스코, IBM, 레드햇, 마이크로소프트(MS), 빅스위치 등 데이터센터 관련업체 대부분이 참여한 범 개방형 네트워크 연합체다. \n\n오픈데이라이트는 이달 7일 공식 출범을 선언하고 오픈소스 기반의 표준 SDN 프레임워크를 개발하겠다고 밝혔다. \n\n리눅스재단이 주도하며, 시스코, IBM, MS, 레드햇, 빅스위치, 브로케이드, 주니퍼네트웍스, 에릭스, 시트릭스 등이 플래티넘 스폰서로 참여했다. 이밖에 VM웨어, NEC 등이 골드 스폰서로, HP, 델, 아리스타, 인텔, 누아지네트웍스(알카텔루슨트), 플럼그리드 등이 실버 스폰서로 등록했다. \n\n오픈데이라이트는 오픈소스 SDN 컨트롤러와 가상 오버레이 네트워크, 프로토콜 플러그인, 애플리케이션, 아키텍처 및 프로그램 가능한 인터페이스 등의 개발프로젝트를 진행한다. 오는 3분기 정식 코드가 공개될 예정이다. \n\n그동안 SDN 분야는 학계와 구글, AT&T, NTT 등 서비스업체를 중심으로 발전했다. 벤더 종속없는 네트워크 환경을 구축해보자는 움직임에서 출발한 SDN은 오픈플로란 오픈소스 프로토콜을 탄생시키기에 이른다. SDN 바람 속에서 기존 벤더들은 끌려가는 듯한 인상을 줬다. \n\n오픈데이라이트는 그동안 주도권을 쥐지 못했던 벤더들이 뭉쳐 SDN 흐름을 주도하려는 노림수다. 때문에 기존 개방형 네트워크를 주도해온 진영으로부터 의심의 눈초리를 받는 것도 사실이다. \n \n \n■오픈데이라이트 주도 시스코 ‘태도변화 or 전략’ \n\n오픈데이라이트는 공식 출범 이전부터 화제였다. 당초 SDN 분야에 미온적인 이미지를 줬던 시스코가 주도적인 역할을 담당한다는 소문 때문이었다. 네트워크업계 독불장군의 대명사였던 시스코의 참여만으로 관심을 끌기 충분했다. \n\n시스코는 그동안 업계표준 작성을 위한 각종 연합체와 대립하면서, 독자 행보를 고집했었다. 그러던 시스코도 최근 2년 사이 아파치, 오픈스택, 리눅스 등 오픈소스 재단을 적극 지원하는 등 변화된 모습을 보이긴 했다. \n\n데이비드 옌 시스코 데이터센터그룹 수석부사장은 최근 텔레프레즌스를 통한 기자간담회에서 오픈데이라이트의 의의를 “업계리더들이 리눅스 재단 아래서 형성한 오픈소스 프로젝트로, 기업들이 SDN을 채택하고 혁신하도록 하는 걸 목표로 한다”라며 “기업의 SDN 도입을 실현하기 위해 벤더들이 지원 프레임워크를 구축하게 된다”라고 설명했다. \n\n그에 따르면, 오픈데이라이트는 업계 전문업체들이 머리를 맛대고 SDN 환경의 기업 도입을 앞당기기 위한 움직임이다. 그동안 중구난방으로 개발됐던 SDN관련 기술을 통합해 어디서나 활용가능한 개방형 표준을 만든다는 것이다. \n\n오픈소스의 정신과 이점을 살려 자유로운 참여를 보장함으로써, SDN 프레임워크 개발속도와 완성도를 빠르게 높이겠다는 의도도 있다. \n \n▲ 오픈데이라이트 프레임워크 1.0 버전 \n \n공식적으로 오픈데이라이트는 컨트롤러 프로젝트를 중심으로, 가상 네트워크, 자바 기반 프로토콜 플러그인, 애플리케이션, 아키텍처 및 프로그램 가능한 인터페이스 등의 프로젝트로 구성된다. \n\n현재 다운로드 가능한 오픈데이라이트 컨트롤러는 1.0 버전이다. 이를 기반으로 노스바운드API로 오픈스택, 클라우드스택 등과 연동되며, 사우스바운드 API로 오픈플로 네트워크 환경을 제어한다. 시스코가 자사의 오픈네트워크환경(ONE) 컨트롤러를 기증했다. \n\n데이비드 옌 부사장은 “현재 사우스바운드 API모듈을 통해 오픈플로 1.0 아키텍처에 추가함으로써 사용할 수 있다”라며 “자바 번들에 들어가는 HA 모듈을 시스코에서 기증했고, 다른 벤더와 고객사들이 여러 애플리케이션 모듈을 자유롭게 개발해 사용하고, 프로젝트에 기여하게 된다”라고 강조했다. \n\n오픈플로 컨트롤러 ‘플러드라이트’를 보유한 빅스위치도 컨트롤러 고도화에 기여한다. 시스코에서 기증한 컨트롤러 코드를 기초로 하지만, 자유롭게 코드를 수정할 수 있기 때문이다. 여기에 IBM, MS, 레드햇, HP, 델, 브로케이드, 주니퍼, 아리스타, 인텔 같은 회사의 소속 개발자가 다양한 애플리케이션과 운영사례를 개발해 기여한다. \n\n오픈데이라이트의 라이선스는 자바영역에서 주로 활용되는 EPL(Eclipse Public License)이다. 코드를 수정해 사용하거나 별도 애플리케이션을 개발했더라도, 코드 자체를 공개하지 않아도 된다. \n\n■오픈데이라이트는 ONF를 하위로 끌어내리려는 노림수? \n\n오픈네트워킹파운데이션(ONF) 주도의 오픈플로는 향후에도 별도로 존재한다. 오픈데이라이트는 사우스바운드API에 집중했던 ONF에 비해 노스바운드API와 전반적인 클라우드 매니지먼트 자동화란 큰 틀에서 접근한다. \n\n옌 부사장은 “오픈데이라이트 프로젝트의 목표는 두 가지로 첫 번째는 컨트롤러, 사우스바운드 및 노스바운드 API, 관련 툴과 서비스 기능을 포함하는 완벽한 SDN 컨트롤러 스택을 개발”이라며 “이 보다 더 광범위한 목표가 바로 애플리케이션, 툴, 서비스 전달은 물론 시장 지원도 가능한 컨트롤러 스택 전반을 구축할 수 있는 생태계를 마련하려는 것”이라고 강조했다. \n\n오픈플로진영은 그동안 컨트롤 플레인과 데이터 플레인을 구별하고, 하부 데이터 플레인의 관리분야에 집중했다. ONF가 클라우드 플랫폼 상의 네트워크 환경 관리를 위한 애플리케이션 개발에 눈을 돌리기 시작한 건 최근의 일이다. \n\n오픈데이라이트는 ONF의 오픈플로 버전 고도화를 반영하는 방향으로 접근할 것으로 예상된다. \n\n데이비드 옌 부사장은 “ONF는 오픈데이라이트 프로젝트의 공식 멤버는 아니지만, 프로젝트 태동 초기 단계부터 관여해왔다”라며 “오픈데이라이트와 ONF 모두 유사한 목표를 갖고 있기 때문인데, 오픈데이라이트 프로젝트의 초창기 목표 중 하나는 ‘사우스바운드’ 오픈플로우 플러그인이 될 것”이라고 설명했다. \n\n오픈데이라이트의 첫 코드 공개와 별도로, 시스코는 개발자 활용이 가능한 코드를 포스팅하고 있다. \n\n옌 부사장은 “오픈데이라이트 코드 자체의 가용성 측면에서만 본다면 시스코의 경우는 이미 개발자들이 활용 가능하도록 코드 포스팅을 시작했다”라며 “이에 첫 번째 공식 릴리즈, 즉 구현 가능한 완벽한 패키지는 올해 3분기 정도에는 나올 수 있을 것으로 기대하고 있다”고 전망했다. \n\n그는 “시스코는 올해 6월 시스코ONE 컨트롤러 SW 발표 후 추가적인 모듈을 공개할 계획”이라며 “트러블 슈팅, 인증, 슬라이싱 등을 위한 애플리케이션이 포함될 예정”이라고 밝혔다. \n\n6월말 개최될 ‘시스코라이브’ 행사에서 SDN 컨트롤러를 위한 모듈을 공개한다는 것이다. 그는 크게 3가지 정도의 모듈이 공개될 것으로 설명했다. \n\n구체적으로 우선, 네트워크 슬라이싱이다. 공유된 물리적 네트워크를 논리적 네트워크로 파티션하는 기술이다. 다음은 네트워크 태핑으로 모니터링, 분석, 디버깅 등을 네트워크 플로 상에서 할 수 있게 하는 기술이다. 세 번째는 세 번째는 커스텀 포워딩으로, 어떤 조건을 구체적으로 설정해놓고, 그 조건을 만족시킬 경우 네트워크 패킷이 엔지니어링된 경로 통해 바로 포워딩되는 기술이다. \n\n■벤더 중심의 SDN \'꼼수인가, 반성인가\'\n\n오픈데이라이트의 출범은 벤더 중심의 SDN이란 큰 틀로 읽힌다. 고객에 빼앗긴 시장 주도권을 되찾아야 한다는 위기감의 발로임을 부인할 수는 없다. \n\n실제로 오픈플로가 발전하면서 그 개발을 주도했던 서비스업체들이 각자 개발한 SDN 기술을 솔루션 및 서비스 형태로 사업화하려는 움직임을 보이고 있다. IT업체의 먹잇감을 고객이 취하겠다고 달려드는 형국이다.\n\n오픈데이라이트를 통해 벤더는 기술을 선도하는 건 자신들임을 증명할 수 있다. 경쟁적인 개발참여를 통해 기업에서 개발해낸 오픈데이라트 성과보다 항상 앞서가는 모습을 보이면 가능하다. \n \n▲ 오픈데이라이트 참여사 \n \n네트워크 장비업체들이 자신들의 이익을 빼앗기지 않으려 지연전략을 펴는 것이란 삐딱한 시각도 존재한다. \n\n기존 네트워크는 제공업체의 장비마다 제각각인 하드웨어에 기능이 좌우됐다. 멀티 벤더로 네트워크 환경을 구현하려는 기업은 장비업체에서 제공하는 성능과 기능이 저마다 달라 인프라 관리 자동화는커녕 통합적인 관리조차 불가능했다. \n\nSDN과 오픈플로는 네트워크 상의 하드웨어 종속에서 벗어나 SW로 모든 네트워크 환경을 구성함으로써 벤더 종속에서 탈피하자는 의도에서 발전하고 있다. 이는 기존 네트워크업체의 차별성을 무너뜨리고, 저가 장비 판매 중심으로 장비업체를 몰아넣는다. 그러므로 장비업체가 SDN과 오픈플로를 달가워할 이유는 별로 없다. \n\n그러나 벤더의 자존심을 건 밥그릇 지키기로 치부하기엔 무리가 있다. 오픈데이라이트를 통해 개발된 기술이 어느 참여업체에도 소유권이 없기 때문이다. \n\n데이비드 옌 부사장은 “어떤 기부가 프로젝트를 촉진시킬 수 있을지, 무엇을 씨드 코드로 삼을지, 누가 활동을 리드할지에 대한 궁극적인 결정은 TSC(The Technical Steering Committee)가 하게 된다”라며 “TSC가 오픈데이라이트 프로젝트를 위한 모든 기부 제안과 기술 방향에 대한 의사결정을 주관하므로, 특정 벤더 소유의 코드가 일단 기부가 되면 이는 커뮤니티의 범용 코드가 되고 커뮤니티는 어떤 방향을 취할지 또 결정하게 되는 과정을 거치게 된다”라고 설명했다. \n\n그는 “오픈소스다 보니 모든 사람들이 오리지널 코드 액세스 갖고 있으며, 어느 벤더도 이 소프트웨어의 기본을 독점할 수 없다”라며 “벤더들은 오픈소스 프레임워크에 기여하는 모듈을 제공하고, 고부가가치 앱을 만들어내면서 수익을 창출하게 될 것”이라고 덧붙였다. \n\n지연전략에 대해서도 오픈데이라이트 TSC의 결정에 따라 어느 누구도 주도하기 힘든 구조다. 한 회사가 코드를 늦게 개발하거나 일부러 헝클어 놓는 건 불가능하다는 설명이다. \n\n그는 “컨트롤러의 경우도 시스코가 기본 코드를 제공했지만, 어느 회사도 컨트롤러 코드를 개선해 기여할 수 있다”라며 “모든 컨트롤러는 서로 다른 다수의 컨트롤러로부터 얻게 되는 코드들의 종합 반영본으로 발전하게 될 것”이라고 답했다. \n\n이어 “다양한 소스에서 전문적인 기술과 경험을 뽑아내 궁극적으로는 최상의 결정과 더불어 지속적인 발전을 이뤄갈 수 있는 것이 오픈소스의 최대 이점”이라고 강조했다. \n\n현재까지 나온 발언들은 종합해보면 오픈데이라이트의 방향성 자체는 나쁘지 않다. 구체적인 개선과 성과가 이어질 경우 최종사용할 고객사 입장에선 고도화된 오픈소스 네트워크를 쉽게 구축하고, 어느 벤더에서건 지원을 받을 수 있게 되는 탓이다. \n\n또한, 오픈데이라이트에 대해 벤더가 주도권을 되찾으려 한다는 시각도 비관적인 입장을 일단 배제하는 게 옳아 보인다. \n\n확실히 현재의 SDN과 오픈플로는 점차 파편화될 기미를 보이고 있으며, 중구난방식의 개발이 이뤄지고 있다. 오랜 네트워크 기술 개발경험을 통해 능숙한 역량을 보유한 벤더가 공통의 표준을 형성한다는 건 주도권 되찾기보다 솔루션 완성도 높이기를 위한 선순환 구조 형성 측면으로 봐야할 것으로 보인다. \n\n \n\n-- 이상 --\n\n \n\n \n\n------- Original Message -------\n\nSender : 심은수<eunsoo.shim@samsung.com> 상무/Lab장/Intelligent Computing Lab(기술원)/삼성전자\n\nDate : 2013-04-25 12:41 (GMT+09:00)\n\nTitle : EMC·IBM 업은 레노버, 기업시장 강자 부상?\n\n \n\n스토리지 서버에 애플리케이션을 실행시킨다는 대목이 눈에 띕니다.\n\n이 접근의 의미, 장/단점에 대해서 생각해 볼 필요가 있습니다.\n\n \n\nhttp://media.daum.net/digital/newsview?newsid=20130425090106567\n\n \n\n감사합니다.\n\n \n\n심은수 드림\n\n</pre>\n\n\n\n\n==== References ====\n\n* [http://ettrends.etri.re.kr/PDFData/27-2_129-136.pdf ETRI - SDN 미래 네트워킹 기술]\n\n=== Disk Performance / Speed Specifications ===\n\n\n\n==== Samsung SSD spec (128GB 2.5-inch SSD 830 Series) ====\n\n* Interface\n:- SATA 6Gb/s\n\n* Capacity\n: 128GB\n\n* Performance Benchmark (Random Read, SSD, echo 3 > /proc/sys/vm/drop_caches, hdparm -W0 /dev/sdf)\n\n <pre>\n$ time iozone -i 2 -+R -w -s 400000 -r 4 -f /x/iozone/test/ssd_test/400m.1\n        Iozone: Performance Test of File I/O\n                Version $Revision: 3.398 $ -- blusjune 20130502_023327\n                Compiled for 6          Build: linux-AMD64\n\n                Build: linux-AMD64\n\n        Contributors:William Norcott, Don Capps, Isom Crawford, Kirby Collins\n                     Al Slater, Scott Rhine, Mike Wisner, Ken Goss\n                     Steve Landherr, Brad Smith, Mark Kelly, Dr. Alain CYR,\n                     Randy Dunlap, Mark Montague, Dan Million, Gavin Brebner,\n                     Jean-Marc Zucconi, Jeff Blomberg, Benny Halevy, Dave Boone,\n                     Erik Habbinga, Kris Strecker, Walter Wong, Joshua Root,\n                     Fabrice Bacchella, Zhenghua Xue, Qin Li, Darren Sawyer.\n                     Ben England.\n\n        Run began: Thu May  2 09:20:02 2013\n\n        Setting no_unlink\n        File size set to 400000 KB\n        Record Size 4 KB\n        Command line used: iozone -i 2 -+R -w -s 400000 -r 4 -f /x/iozone/test/ssd_test/400m.1\n        Output is in Kbytes/sec\n        Time Resolution = 0.000001 seconds.\n        Processor cache size set to 1024 Kbytes.\n        Processor cache line size set to 32 bytes.\n        File stride size set to 17 * record size.\n                                                            random  random    bkwd   record   stride                   \n              KB  reclen   write rewrite    read    reread    read   write    read  rewrite     read   fwrite frewrite   fread  freread\n          400000       4                                     45213       0                                             \n\niozone test complete.\n\n==================================================\nblusjune performance metrics (random-read)\n==================================================\nblusjune__file_size (bytes)      : 409600000\nblusjune__record_size (bytes)    : 4096\nblusjune__time_elapsed (secs)    : 8.84687\nblusjune__num_of_ios             : 100000\n--------------------------------------------------\nblusjune__latency_per_io (usecs) : 88.4687\nblusjune__iops                   : 11303\nblusjune__throughput (bytes/sec) : 46298866\n==================================================\nreal    0m8.912s\nuser    0m0.072s\nsys     0m2.072s\n</pre>\n\n <pre>\n$ iozone -i 2 -+R -w -s 400000 -r 4 -f /x/bmt_iozone/test/ssd_test/400m.1 \n	Iozone: Performance Test of File I/O\n	        Version $Revision: 3.398 $ -- blusjune 20130502_110038\n		Compiled for 6		Build: linux-AMD64 \n\n		Build: linux-AMD64 \n\n	Contributors:William Norcott, Don Capps, Isom Crawford, Kirby Collins\n	             Al Slater, Scott Rhine, Mike Wisner, Ken Goss\n	             Steve Landherr, Brad Smith, Mark Kelly, Dr. Alain CYR,\n	             Randy Dunlap, Mark Montague, Dan Million, Gavin Brebner,\n	             Jean-Marc Zucconi, Jeff Blomberg, Benny Halevy, Dave Boone,\n	             Erik Habbinga, Kris Strecker, Walter Wong, Joshua Root,\n	             Fabrice Bacchella, Zhenghua Xue, Qin Li, Darren Sawyer.\n	             Ben England.\n\n	Run began: Thu May  2 11:31:38 2013\n\n	Setting no_unlink\n	File size set to 400000 KB\n	Record Size 4 KB\n	Command line used: iozone -i 2 -+R -w -s 400000 -r 4 -f /x/bmt_iozone/test/ssd_test/400m.1\n	Output is in Kbytes/sec\n	Time Resolution = 0.000001 seconds.\n	Processor cache size set to 1024 Kbytes.\n	Processor cache line size set to 32 bytes.\n	File stride size set to 17 * record size.\n                                                            random  random    bkwd   record   stride                                   \n              KB  reclen   write rewrite    read    reread    read   write    read  rewrite     read   fwrite frewrite   fread  freread\n          400000       4                                     41462       0                                                                                              \n\niozone test complete.\n\n==================================================\nblusjune performance metrics (random-read)\n==================================================\nblusjune__file_size (bytes)      : 409600000\nblusjune__record_size (bytes)    : 4096\nblusjune__time_elapsed (secs)    : 9.64722\nblusjune__num_of_ios             : 100000\n--------------------------------------------------\nblusjune__latency_per_io (usecs) : 96.4722\nblusjune__iops                   : 10365\nblusjune__throughput (bytes/sec) : 42457810\n==================================================\n</pre>\n\n\n\n\n* Performance Benchmark (Random Write, SSD, echo 3 > /proc/sys/vm/drop_caches, hdparm -W0 /dev/sdf)\n\n <pre>\n$ iozone -i 2 -+W -w -s 400000 -r 4 -f /x/bmt_iozone/test/ssd_test/400m.3\n	Iozone: Performance Test of File I/O\n	        Version $Revision: 3.398 $ -- blusjune 20130502_110038\n		Compiled for 6		Build: linux-AMD64 \n\n		Build: linux-AMD64 \n\n	Contributors:William Norcott, Don Capps, Isom Crawford, Kirby Collins\n	             Al Slater, Scott Rhine, Mike Wisner, Ken Goss\n	             Steve Landherr, Brad Smith, Mark Kelly, Dr. Alain CYR,\n	             Randy Dunlap, Mark Montague, Dan Million, Gavin Brebner,\n	             Jean-Marc Zucconi, Jeff Blomberg, Benny Halevy, Dave Boone,\n	             Erik Habbinga, Kris Strecker, Walter Wong, Joshua Root,\n	             Fabrice Bacchella, Zhenghua Xue, Qin Li, Darren Sawyer.\n	             Ben England.\n\n	Run began: Thu May  2 11:35:44 2013\n\n	Setting no_unlink\n	File size set to 400000 KB\n	Record Size 4 KB\n	Command line used: iozone -i 2 -+W -w -s 400000 -r 4 -f /x/bmt_iozone/test/ssd_test/400m.3\n	Output is in Kbytes/sec\n	Time Resolution = 0.000001 seconds.\n	Processor cache size set to 1024 Kbytes.\n	Processor cache line size set to 32 bytes.\n	File stride size set to 17 * record size.\n                                                            random  random    bkwd   record   stride                                   \n              KB  reclen   write rewrite    read    reread    read   write    read  rewrite     read   fwrite frewrite   fread  freread\n          400000       4                                         0  960635                                                                                              \n\niozone test complete.\n\n==================================================\nblusjune performance metrics (random-write)\n==================================================\nblusjune__file_size (bytes)      : 409600000\nblusjune__record_size (bytes)    : 4096\nblusjune__time_elapsed (secs)    : 0.416391\nblusjune__num_of_ios             : 100000\n--------------------------------------------------\nblusjune__latency_per_io (usecs) : 4.16391\nblusjune__iops                   : 240158\nblusjune__throughput (bytes/sec) : 983690602\n==================================================\n</pre>\n\n==== Samsung SSD spec (128GB 2.5-inch SSD 840 Pro Series) ====\n\n* Interface\n:- SATA 6Gb/s\n\n* Capacity\n: 128GB\n\n* Performance\n:- Sequential Read Speed: Up to 530 MB/s\n:- Sequential Write Speed: Up to 390 MB/s\n:- Random Read Speed: Up to 97,000 IOPS (4KB/IO) // Read Latency: 10.31 usec (80~200 usec, NCQ, Non-blocking) // python -c \"print (float(1 * (10 ** 6)) / float(97000))\"\n:- Random Write Speed: Up to 90,000 IOPS (4KB/IO) // Write Latency: 11.11 usec (80~200 usec, NCQ, Non-blocking) // python -c \"print (float(1 * (10 ** 6)) / float(90000))\"\n\n* Power Consumption (W)\n: 0.15W\n\n==== Intel SSD spec (320 Series) ====\n\n* Interface\n:- SATA 6Gb/s\n\n* Capacity\n: 120GB\n\n* Performance\n:- Sequential Read Speed: up to 270 MB/s \n:- Sequential Write Speed: up to 130 MB/s\n:- Random Read Speed: 38,000 IOPS (4KB/IO) // Read Latency: 26.32 usec // python -c \"print (float(1 * (10 ** 6)) / float(38000))\"\n:- Random Write Speed: 14,000 IOPS (4KB/IO) // Write Latency: 71.43 usec // python -c \"print (float(1 * (10 ** 6)) / float(14000))\"\n\n==== Seagate HDD spec (Barracuda 2TB) ====\n\n* Model Number\n:- ST2000DM001\n\n* Interface\n:- SATA 6Gb/s NCQ\n\n* Performance\n:- Spindle Speed (RPM): 7200\n:- Cache, Multisegmented (MB): 64\n:- SATA Transfer Rates Supported (Gb/s): 6.0/3.0/1.5\n:- Seek Average, Read (ms): <8.5 // 117 IOPS\n:- Seek Average, Write (ms): <9.5 // 105 IOPS\n:- Average Data Rate, Read/Write (MB/s): 156\n:- Max Sustained Data Rate, OD Read (MB/s): 210\n\n* Configuration/Organization\n:- Heads/Disks: 6/3\n:- Bytes per Sector: 4096\n\n* Reliability/Data Integrity\n:- Annualized Failure Rate (AFR): <1%\n:- Power-On Hours: 2400\n\n* Power Management\n:- Startup Power (A): 2.0\n:- Operating Mode, Typical (W): 8.0\n:- Idle2 Average (W): 5.40\n:- Idle Average (W): N/A\n:- Standby Mode (W): 0.75\n:- Sleep Mode (W): 0.75\n\n* Performance Benchmark (Random Read, HDD, echo 3 > /proc/sys/vm/drop_caches, hdparm -W0 /dev/sdd)\n <pre>\n$ time iozone -i 2 -+R -w -s 400000 -r 4 -f /x/iozone/test/hdd_test/400m\n        Iozone: Performance Test of File I/O\n                Version $Revision: 3.398 $ -- blusjune 20130502_023327\n                Compiled for 6          Build: linux-AMD64\n\n                Build: linux-AMD64\n\n        Contributors:William Norcott, Don Capps, Isom Crawford, Kirby Collins\n                     Al Slater, Scott Rhine, Mike Wisner, Ken Goss\n                     Steve Landherr, Brad Smith, Mark Kelly, Dr. Alain CYR,\n                     Randy Dunlap, Mark Montague, Dan Million, Gavin Brebner,\n                     Jean-Marc Zucconi, Jeff Blomberg, Benny Halevy, Dave Boone,\n                     Erik Habbinga, Kris Strecker, Walter Wong, Joshua Root,\n                     Fabrice Bacchella, Zhenghua Xue, Qin Li, Darren Sawyer.\n                     Ben England.\n\n        Run began: Thu May  2 08:24:47 2013\n\n        Setting no_unlink\n        File size set to 400000 KB\n        Record Size 4 KB\n        Command line used: iozone -i 2 -+R -w -s 400000 -r 4 -f /x/iozone/test/hdd_test/400m\n        Output is in Kbytes/sec\n        Time Resolution = 0.000001 seconds.\n        Processor cache size set to 1024 Kbytes.\n        Processor cache line size set to 32 bytes.\n        File stride size set to 17 * record size.\n                                                            random  random    bkwd   record   stride                   \n              KB  reclen   write rewrite    read    reread    read   write    read  rewrite     read   fwrite frewrite   fread  freread\n          400000       4                                      2116       0                                             \n\niozone test complete.\n\n==================================================\nblusjune performance metrics (random-read)\n==================================================\nblusjune__file_size (bytes)      : 409600000\nblusjune__record_size (bytes)    : 4096\nblusjune__time_elapsed (secs)    : 189.019\nblusjune__num_of_ios             : 100000\n--------------------------------------------------\nblusjune__latency_per_io (usecs) : 1890.19\nblusjune__iops                   : 529\nblusjune__throughput (bytes/sec) : 2166974\n==================================================\nreal    3m9.229s\nuser    0m0.120s\nsys     0m3.208s\n</pre>\n\n <pre>\n $ iozone -i 2 -+R -w -s 400000 -r 4 -f /x/bmt_iozone/test/hdd_test/400m.1\n	Iozone: Performance Test of File I/O\n	        Version $Revision: 3.398 $ -- blusjune 20130502_110038\n		Compiled for 6		Build: linux-AMD64 \n\n		Build: linux-AMD64 \n\n	Contributors:William Norcott, Don Capps, Isom Crawford, Kirby Collins\n	             Al Slater, Scott Rhine, Mike Wisner, Ken Goss\n	             Steve Landherr, Brad Smith, Mark Kelly, Dr. Alain CYR,\n	             Randy Dunlap, Mark Montague, Dan Million, Gavin Brebner,\n	             Jean-Marc Zucconi, Jeff Blomberg, Benny Halevy, Dave Boone,\n	             Erik Habbinga, Kris Strecker, Walter Wong, Joshua Root,\n	             Fabrice Bacchella, Zhenghua Xue, Qin Li, Darren Sawyer.\n	             Ben England.\n\n	Run began: Thu May  2 11:25:26 2013\n\n	Setting no_unlink\n	File size set to 400000 KB\n	Record Size 4 KB\n	Command line used: iozone -i 2 -+R -w -s 400000 -r 4 -f /x/bmt_iozone/test/hdd_test/400m.1\n	Output is in Kbytes/sec\n	Time Resolution = 0.000001 seconds.\n	Processor cache size set to 1024 Kbytes.\n	Processor cache line size set to 32 bytes.\n	File stride size set to 17 * record size.\n                                                            random  random    bkwd   record   stride                                   \n              KB  reclen   write rewrite    read    reread    read   write    read  rewrite     read   fwrite frewrite   fread  freread\n          400000       4                                      2123       0                                                                                              \n\niozone test complete.\n\n==================================================\nblusjune performance metrics (random-read)\n==================================================\nblusjune__file_size (bytes)      : 409600000\nblusjune__record_size (bytes)    : 4096\nblusjune__time_elapsed (secs)    : 188.403\nblusjune__num_of_ios             : 100000\n--------------------------------------------------\nblusjune__latency_per_io (usecs) : 1884.03\nblusjune__iops                   : 530\nblusjune__throughput (bytes/sec) : 2174068\n==================================================\n</pre>\n\n== ## bNote-2013-04-25 ==\n\n=== News / Articles ===\n\n* [http://www.zdnet.co.kr/news/news_view.asp?artice_id=20130422135700 SDN 국면 전환을 꿈꾸다 \'오픈데이라이트 (Open Daylight)\' // 2013-04-22]\n* [http://www.bloter.net/archives/150410?utm_source=feedly Facebook, Datacenter 운영 현황 공개 // 2013-04-19]\n\n\n=== Bayesian Inference with R (r_stat) ===\n\n* [http://books.google.co.kr/books?hl=en&lr=&id=AALhk_mt7SYC&oi=fnd&pg=PP5&dq=famous+R+package+bayesian+inference&ots=XvK-GIW-Ji&sig=TK-Xz5xQ1urvG6vge2YV9BAnlPo&redir_esc=y#v=onepage&q&f=false Book - \"Bayesian Computation with R\", 2nd Ed., Jim Albert]\n* [http://books.google.co.kr/books?hl=en&lr=&id=GTJUt8fcFx8C&oi=fnd&pg=PP1&dq=famous+R+package+bayesian+inference+example&ots=Iy1FKoP4ux&sig=y1_93HNmPIMDaRCCdzyzvSlReAU&redir_esc=y#v=onepage&q=famous%20R%20package%20bayesian%20inference%20example&f=false \"Bayesian Methods for Data Analysis\", 3rd Ed., Bradley P. Carlin, Thomas A. Louis]\n* [http://www.bayesian-inference.com/index Bayesian-Inference // LaplacesDemon - a complete environment for Bayesian inference in R // Statisticat, LLC.]\n* [http://cran.r-project.org/web/packages/LaplacesDemon/vignettes/BayesianInference.pdf \"Bayesian Inference\", Statisticat, LLC]\n* [http://cran.r-project.org/web/packages/LaplacesDemon/LaplacesDemon.pdf Package \'LaplacesDemon\']\n\n== ## bNote-2013-04-24 ==\n\n\n\n=== Patents ===\n\n* [http://www.google.com/patents/US7792882 Method and system for block allocation for hybrid drives // Oracle // US 7792882 B2]\n* [http://www.google.com/patents/US8285758 Tiering storage between multiple classes of storage on the same container file system // EMC // US 8285758 B1]\n\n=== Paper References ===\n\n\n* [http://research.microsoft.com/pubs/63596/usenix-08-ssd.pdf Design Tradeoffs for SSD Performance // USENIX 2008 // Microsoft Research, Silicon Valley; University of Wisconsin-Madison]\n* [http://research.microsoft.com/en-us/um/people/srikanth/data/imc09_dcTraffic.pdf The Nature of Datacenter Traffic: Measurements & Analysis // IMC 2009 // Microsoft Research]\n* [http://www.cs.ucsb.edu/~arijitkhan/Papers/multiple_timeseries_prediction.pdf Workload Characterization and Prediction in the Cloud: A Multiple Time Series Approach // UCSB, IBM Watson Research]\n* [http://infolab.stanford.edu/~ragho/hive-icde2010.pdf Hive - A Petabyte Scale Data Warehouse Using Hadoop // Facebook]\n* [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36632.pdf Dremel: Interactive Analysis of Web-scale Datasets // VLDB 2010 // Google]\n* [http://www.cs.berkeley.edu/~istoica/classes/cs294/11/papers/pacman-draft.pdf PACMan: Coordinated Memory Caching for Parallel Jobs]\n* [http://www.cs.berkeley.edu/~matei/talks/2012/hadoop_summit_spark.pdf Spark and Shark // AMPLab UC Berkeley]\n* [http://grid.hust.edu.cn/xhshi/paper/MR-Scope.pdf MR-Scope: A Real-time Tracing Tool for MapReduce]\n* [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGRID 2010]\n* [http://bnrg.eecs.berkeley.edu/~randy/Courses/CS268.F08/papers/42_osdi_08.pdf Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008 // UC Berkeley]\n* [http://webhdd.ru/library/files/RebalanceDesign6.pdf Rebalance an HDFS Cluster]\n* [http://www.ibm.com/developerworks/web/library/wa-introhdfs/ An introduction to the Hadoop Distributed File System -- HDFS Data Block Rebalancing]\n* [http://www.stanford.edu/~cdel/epic.talk.workloads.pdf Data Center Workload Characterization]\n* [http://seelab.ucsd.edu/virtualefficiency/related_papers/42_caecw05.pdf Data Center Workload Monitoring, Analysis, and Emulation]\n* [https://amplab.cs.berkeley.edu/projects/real-life-workloads/ AMPLab UC Berkeley - Real-life Workloads]\n* [http://davidmeisner.org/wp-content/uploads/2011/04/meisner-exert10.pdf Stochastic Queuing Simulation for Data Center Workloads]\n* [http://research.microsoft.com/en-us/um/people/srikanth/data/imc09_dcTraffic.pdf The Nature of Datacenter Traffic: Measurements & Analysis // MSR]\n* [http://nowlab.cse.ohio-state.edu/publications/tech-reports/2005/vaidyana-caecw05-tr.pdf Workload-driven Analysis of File Systems in Shared Multi-tier Data Centers over InfiniBand // Ohio State University]\n* [http://research.microsoft.com/pubs/81782/2009-06-19%20Data%20Center%20Tutorial%20-%20SIGMETRICS.pdf What Goes Into a Data Center? // MSR]\n* [http://delivery.acm.org/10.1145/1780000/1773400/p34-mishra.pdf?ip=202.20.193.254&acc=ACTIVE%20SERVICE&key=986B26D8D17D60C855C5A4E2351BBB67&CFID=209231705&CFTOKEN=89669612&__acm__=1366870790_f6ca461a6119bb2a2d6f017ca1bc9ce7 Towards Characterizing Cloud Backend Workloads: Insights from Google Compute Clusters // ACM SIGMETRICS Performance Evaluation Review Vol. 37, Issue 4, 2010-03 // Google]\n\n=== White Paper from EMC (Benchmark) ===\n\n* [http://www.emc.com/collateral/hardware/white-papers/h8131-storage-tiering-oracle-vmax-wp.pdf Implementing FAST VP and Storage Tiering for Oracle Database 11g and EMC Symmetrix VMAX // 2011-04 // ((B.GOOD))]\n* [http://www.emc.com/collateral/hardware/white-papers/h11210-p690-emc-vnx7500-scaling-performance-oracle-11g-vsphere-wp.pdf EMC VNX7500 Scaling Performance for Oracle 11gR2 RAC on VMware vSphere 5.1 // 2012-12 ((B.GOOD))]\n* [http://www.compucom.com/sites/default/files/Whitepaper-Deploying-Oracle-Database-Apps-on-EMC-VNX.pdf Deploying Oracle Database Applications on EMC VNX Unified Storage // 2011-04]\n* [http://www.emc.com/collateral/hardware/white-papers/h8850-oracle-performance-vnx-fastcache-wp.pdf EMC Performance for Oracle (EMC VNX, Enterprise Flash Drives, FAST Cache, VMware vSphere // 2011-12]\n* [http://www.emc.com/collateral/white-papers/h11209-p773-osc-integration-vmax-mgt-virtualized-oracle-ebs-wp.pdf EMC Oracle VM Storage Connect Integration Module - Efficient Infrastructure Management for a Virtualized Oracle E-Business Suite // 2013-03]\n* [http://www.emc.com/collateral/analyst-reports/esg-20091208-fast.pdf ESG Whitepaper - Automate and Optimize a Tiered Storage Environment - FAST // 2009-12]\n* [http://www.emc.com/collateral/hardware/white-papers/h8091-leveraging-fast-sql-wp.pdf White Paper - Leveraging EMC Fully Automated Storage Tiering (FAST) and FAST Cache for SQL Server Enterprise Deployments // 2010-11]\n* [http://www.emc.com/collateral/software/white-papers/h10938-vnx-best-practices-wp.pdf EMC VNX Unified Best Practices for Performance // 2012-08]\n* [http://www.emc.com/collateral/white-papers/h11122-vmax10k-fastvp-tiering-oracledb-wp.pdf Implementing FAST VP and Storage Tiering for Oracle Database 11g and EMC Symmetrix VMAX 10K // 2012-09]\n* [http://www.theregister.co.uk/2013/03/05/emc_xtremsf/ EMC declares war on all-flash array, server flash card rivals - Rolls out XtremIO array, renamed VFCache - XtremSF Server Flash, XtremSW Cache Software, XtremIO all-flash array // 2013-03-05]\n\n== ## bNote-2013-04-23 ==\n\n\n=== Real Trace: MS Exchange ===\n\n <pre>\na1mjjung@secm:[iowa] $ grep \'__valu__sig__\' f030.infile_A.iowa.anal_s0010 > summary\na1mjjung@secm:[iowa] $ cat summary \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  88\n__valu__sig__ _n_o_sigaddrs :  9989\n__valu__sig__ _sigioc_acc :  7266636\n__valu__sig__ _sigaddrs_efficiency :  727.463810191\n__valu__sig__ _n_o_addr_total :  8322343\n__valu__sig__ _ioc_total :  28866005\n</pre>\n\n=== Real Trace: MSN FileServer ===\n\n <pre>\na1mjjung@secm:[iowa] $ grep \'__valu__sig__\' iowa.anal_s0010.A.out > summary\na1mjjung@secm:[iowa] $ cat summary  \n__valu__sig__ _ioc_percent :  25\n__valu__sig__ _sighitcnt :  18\n__valu__sig__ _n_o_sigaddrs :  239328\n__valu__sig__ _sigioc_acc :  7571590\n__valu__sig__ _sigaddrs_efficiency :  31.6368749164\n__valu__sig__ _n_o_addr_total :  7425792\n__valu__sig__ _ioc_total :  29345085\n</pre>\n\n=== I/O Intensiveness Analysis (3D plot) ===\n\n* pre-processing for 3D-plot (access count per iomw)\n <pre>\n\na1mjjung@secm:[iowa.R] $ cat .bdx.0100.y.iowa_anal.sh \n#!/bin/sh\n\n_iowa_anal_cmd=\"bsc.iowa.lsp.anal_s0010\";\n\n_file_010=\"f010.T021.msnfs.R.out\";\n_file_020=\"f020.infile_R\";\n_file_030=\"f030.infile_R.iowa.anal_s0010\";\n_file_040=\"f040.iomw_acs_cnt.sorted_by_acs_cnt\";\n\ncat $_file_010 | awk \'{ print $5, \",\", $14}\' > $_file_020\ncat $_file_020 | $_iowa_anal_cmd -c 25 -w 600000000 -p 60000000 > $_file_030\ncat $_file_030 | grep __list__iomw_acs_cnt | awk \'{ print $9, $3, $6 }\' | sort -n > $_file_040\n\n</pre>\n\n* gnuplot command\n\n:* iowa.anal - Reads\n <pre>\n\na1mjjung@secm:[iowa.R] $ cat .plot_acs_cnt.gp \nclear\nunset key\nset title \"I/O intensiveness - Reads (MSN FileServer 6H)\"\nset xlabel \"time (10-minute)\"\nset ylabel \"address (LBA)\"\nset zlabel \"hit count per addr\"\nshow view\nset view 45, 60\nsplot \'./f040.iomw_acs_cnt.sorted_by_acs_cnt\' u 3:2:1 with points palette pointsize 1 pointtype 7\n\n</pre>\n\n:* iowa.anal - Writes\n <pre>\n\na1mjjung@secm:[iowa.W] $ cat .plot_acs_cnt.gp \nclear\nunset key\nset title \"I/O intensiveness - Writes (MSN FileServer 6H)\"\nset xlabel \"time (10-minute)\"\nset ylabel \"address (LBA)\"\nset zlabel \"hit count per addr\"\nshow view\nset view 45, 60\nsplot \'./f040.iomw_acs_cnt.sorted_by_acs_cnt\' u 3:2:1 with points palette pointsize 1 pointtype 7\n\n</pre>\n\n\n* References\n\n:* [http://psy.swansea.ac.uk/staff/carter/gnuplot/gnuplot_3d.htm]\n:* [http://lowrank.net/gnuplot/plot3d-e.html]\n\n== ## bNote-2013-04-18 ==\n\n=== EMC FAST (VP, Cache) Technology Study ===\n\n* [http://www.emc.com/collateral/hardware/white-papers/h8131-storage-tiering-oracle-vmax-wp.pdf Implementing FAST VP and Storage Tiering for Oracle Database 11g and EMC Symmetrix VMAX]\n: Shows performance improvement by FAST VP\n\n== ## bNote-2013-04-17 ==\n\n=== Proactive Data Placement Research Planning ===\n\n== ## bNote-2013-04-16 ==\n\n=== Git on the Server - Setting Up the Server ===\n\nPlease check this wiki page: [[Git server on my local machine]]\n\n== ## bNote-2013-04-15 ==\n\n=== Sorting Algorithms ===\n\n* [[Sorting algorithms]]\n* [http://corte.si/posts/code/visualisingsorting/index.html Visualising Sorting Algorithms]\n\n== ## bNote-2013-04-11 ==\n\n=== Failure trend in a data center ===\n\n==== 꽤 충실한 내용의 블로그 ((B.GOOD)) ====\n\n* URL: [http://bart7449.tistory.com/m/post/view/id/258 현실에서의 메모리와 디스크의 오류 // 2010-10-04]\n\n\n\n=== 수퍼컴 협력 ===\n\n* TAT 단축\n\n:- 40시간 걸리던 작업이 32분만에 완료 (75배 TAT 단축)\n (32 * 75) / 60 = 40 시간 \n\n:- 18시간 걸리던 작업이 27분만에 완료 (40배 TAT 단축)\n (27 * 40) / 60 = 18시간\n\n* 업무 협조 요청 (to 김혁호 책임)\n\n <pre>\n\nI/O Workload 분석 & 시뮬레이션 환경 병렬화\n\n ㆍ배경 및 목적\n① I/O Workload 분석 & 시뮬레이션 환경 병렬화\n     - 다양한 종류의 Trace Log 데이터 Set에 대한 분석 필요\n     - 하나의 데이터 Set에 대해서도 다양한 Parameter 설정 별 분석 수행\n   ② 데이터 전송 자동화\n     - 분석 케이스 별로 산출되는 대규모 Data Output들 중, 특정 위치의\n       데이터에 대한 자동적 백업 (to 별도 파일서버)\n\n ㆍ추진 방향\n   ① Parameter 설정 별 동시 병렬 분석 구조 구현 (Parameter Sweeping)\n   ② 슈퍼컴-파일서버 간 데이터 자동 백업 구현 (특정 디렉토리 대상)\n      예) 특정 디렉토리 내 데이터를 파일서버로 정기적 백업 (by cronjob)\n\n ㆍ추진 일정\n   ① 단계별 소요시간 (Workload Analysis 기반 Data Placement)\n      [Step 1] Trace Data 수집                         : 5주\n      [Step 2] 분석 위한 전처리(Data Parsing/Transform)  : 2주\n      [Step 3] I/O Workload Outlook 분석               : 4주 (2+2)\n      [Step 4] Dominant I/O 패턴 추출                   : 8주 (3+5)\n      [Step 5] I/O 패턴 예측 모델링/학습/시뮬레이션 검증    : 10주 (3+7)\n      [Step 6] Data Placement 알고리즘 구현 및 검증       : 12주\n      ※ 상기 소요 시간은 수퍼컴에서의 작업 시간임\n        (이미 수퍼컴으로 PC 서버 대비 40~75배의 TAT 단축됨)\n      ※ Workload Analysis 작업 Set (24주): Step 2 ~ Step 5\n   ② 협업 업무 및 소요시간\n      - 상기 3, 4, 5 단계의 Parameter Sweeping 부분 병렬화 협력\n        : 22주 → 8주로 단축예상 (수퍼컴 이전 대비 110~200배 TAT 단축)\n\n ㆍ추진 효과\n   ① 당사 자체적인 I/O Workload 분석 및 성능 시뮬레이션 시스템 확보\n   ② Workload에 최적화된 Data Management 알고리즘 개발로\n      데이터센터 및 분산 스토리지 시스템의 성능 향상 기대\n   ③ I/O Workload 분석 및 시뮬레이션 작업 TAT 개선\n\n [참고1]\n ㆍ전체 Simulation 단계 중, Preprocessing단계에서 슈퍼컴 파일시스템의\n   Real I/O Trace Log Data가 필요하며, Analysis(Outlook, Modeling)\n   단계에서 Algorithm 병렬화를 통한 TAT 단축 필요\n \n [참고2]\n ㆍ과제 Workflow : 첨부파일 참조\n \n\n\n\n</pre>\n\n== ## bNote-2013-04-10 ==\n\n=== IOWA:: MSN FileServer I/O Trace ===\n\n==== T034 ====\n\n* # of reads/writes (total I/Os): 982166\n* # of reads: 811784\n* # of writes: 170382\n\n\n <pre>\na1mjjung@secm:[T033.discovery.full_path] $ pwd\n/home/X0101/a1mjjung/x/iowa.sidewinder/sidewinder/microsoft_msn_filesrvr_6h/MSNStorageFileServer/tdir/T033.discovery.full_path\n\na1mjjung@secm:[T033.discovery.full_path] $ cat T033.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.out | grep \'_iorw_ DiskRead\' > _t034/T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.R.out\n\na1mjjung@secm:[T033.discovery.full_path] $ cat T033.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.out | grep \'_iorw_ DiskWrite\' > _t034/T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.W.out\n</pre>\n\n\n <pre>\na1mjjung@secm:[_t034] $ pwd\n/home/X0101/a1mjjung/x/iowa.sidewinder/sidewinder/microsoft_msn_filesrvr_6h/MSNStorageFileServer/tdir/T033.discovery.full_path/_t034\n\n\na1mjjung@secm:[_t034] $ tail -10 ../../T031.msnfs.fld_dstr.full_path.out \n 382331  _path_ _Disk8__s4_s5_s6_\n 403592  _path_ _Disk9__s0_s1_s13_\n 413900  _path_ _Disk8__s0_s1_s7_\n 418544  _path_ _Disk9__s3_s4_s6_\n 428969  _path_ _Disk8__s0_s1_s9_\n 450260  _path_ _Disk9__s0_s1_s7_\n 466085  _path_ _Disk9__s3_s4_s5_\n 600647  _path_ _Disk9__s0_s1_s3_\n 811630  _path_ _Disk8__s0_s1_s2_\n 982166  _path_ _Disk9__s0_s1_s2_\n\n\na1mjjung@secm:[_t034] $ wc -l T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.R.out\n811784 T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.R.out\na1mjjung@secm:[_t034] $ wc -l T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.W.out\n170382 T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.W.out\n</pre>\n\n\n <pre>\na1mjjung@secm:[_t034] $ head -10 T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.R.out \n_iorw_ DiskRead   , _time_ 248425 , _prid_ p0_2004 , _thid_           540   , _addr_  100276969472  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 263287 , _prid_ p0_2004 , _thid_           540   , _addr_  97312505856  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 281149 , _prid_ p0_2004 , _thid_           540   , _addr_  92908216320  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 305383 , _prid_ p0_2004 , _thid_           540   , _addr_  98594537472  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 320338 , _prid_ p0_2004 , _thid_           540   , _addr_  100699373568  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 325746 , _prid_ p0_2004 , _thid_           540   , _addr_  100944601088  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 330001 , _prid_ p0_2004 , _thid_           540   , _addr_  94236082176  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 339520 , _prid_ p0_2004 , _thid_           540   , _addr_  92404563968  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 344061 , _prid_ p0_2004 , _thid_           540   , _addr_  94653767680  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n_iorw_ DiskRead   , _time_ 399820 , _prid_ p0_2004 , _thid_           540   , _addr_  157423190016  , _iosz_  8192  , _fobj_    0xfffffadf3ad4ec10   , _path_ _Disk9__s0_s1_s2_\n</pre>\n\n\n* _thid_ based T034 analysis\n\n <pre>\na1mjjung@secm:[_t034] $ cat T033.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.out | awk \'{print $11}\' | sort | uniq -c | sort | wc -l\n249\n\na1mjjung@secm:[_t034] $ cat T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.R.out | awk \'{print $11}\' | sort | uniq -c | sort | wc -l\n245\n\na1mjjung@secm:[_t034] $ cat T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.W.out | awk \'{print $11}\' | sort | uniq -c | sort | wc -l\n240\n</pre>\n\n <pre>\na1mjjung@secm:[_t034] $ cat T034.msnfs.discovery.full_path._path___Disk9__s0_s1_s2_.R.out | awk \'{ print $11 }\' | sort | uniq -c | sort | tail -10 \n   8602 540\n   9006 8716\n   9254 7792\n   9603 9440\n   9692 8788\n  10393 10948\n  10842 10000\n  11079 8172\n  12332 11240\n  51747 4060\n</pre>\n\n=== R plot ===\n\n* [http://www.cyclismo.org/tutorial/R/plotting.html plotting in R // R tutorial]\n\n=== Beautiful graphs in Gnuplot ===\n\n* [http://labs.guidolin.net/2010/03/how-to-create-beautiful-gnuplot-graphs.html How to create beautiful Gnuplot graphs (tips and tricks) // GuidoLabs]\n:- [[exemplary .gnuplot configuration file for beautiful gnuplot graphs]]\n:- [http://www.dafont.com/sv-basic-manual.font?text=fdgdfd SV Basic Manual Font]\n\n=== Summary of the ways to call external programs (and pros/cons) ===\n\n* os.system(\"some_command with args\") [http://docs.python.org/lib/os-process.html os.system() // Python tutorial]\n: passes the command and arguments to your system\'s shell. This is nice because you can actually run multiple commands at once in this manner and set up pipes and input/output redirection. For example,\n os.system(\"some_command < input_file | another_command > output_file\")\n: However, while this is convenient, you have to manually handle the escaping of shell characters such as spaces, etc. On the other hand, this also lets you run commands which are simply shell commands and not actually external programs.\n\n* stream = os.popen(\"some_command with args\") [http://docs.python.org/lib/os-newstreams.html os.popen() // Python Tutorial]\n: will do the same thing as os.system except that it gives you a file-like object that you can use to access standard input/output for that process. There are 3 other variants of popen that all handle the i/o slightly differently. If you pass everything as a string, then your command is passed to the shell; if you pass them as a list then you don\'t need to worry about escaping anything.\n\n* The Popen class of the subprocess module. [http://docs.python.org/lib/node528.html Popen class // Python Tutorial]\n: This is intended as a replacement for os.popen but has the downside of being slightly more complicated by virtue of being so comprehensive. For example, you\'d say\n print Popen(\"echo Hello World\", stdout=PIPE, shell=True).stdout.read()\n: instead of\n print os.popen(\"echo Hello World\").read()\n: but it is nice to have all of the options there in one unified class instead of 4 different popen functions.\n\n* The call function from the subprocess module. [http://docs.python.org/lib/node529.html call() // Python Toturial]\n: This is basically just like the Popen class and takes all of the same arguments, but it simply wait until the command completes and gives you the return code. For example: \n return_code = call(\"echo Hello World\", shell=True)\n\n* The last resort (not-recommended)\n: The os module also has all of the fork/exec/spawn functions that you\'d have in a C program, but I don\'t recommend using them directly.\n\n* <span style=\"color:blue\">\'\'\'The subprocess module should probably be what you use.\'\'\'</span>\n\n----\n=== Calling an external command in Python ===\n\n* How can I call an external command in Python?\n:->\n: Look at the subprocess module in the stdlib:\n\n from subprocess import call\n call([\"ls\", \"-l\"])\n\n: The advantage of subprocess vs system is that it is more flexible (you can get the stdout, stderr, the \"real\" status code, better error handling, etc...). I think os.system is deprecated, too, or will be ... [http://docs.python.org/library/subprocess.html#replacing-older-functions-with-the-subprocess-module related part of the python tutorial]\n: But for quick/dirty/one time scripts, os.system is enough, though.\n\n----\n=== String edit in python ===\n\n* sed to python [http://objectmix.com/python/387782-sed-python-replace-q.html]\n\nFor some reason I\'m unable to grok Python\'s string.replace() function.\n:-> replace() does not work with regular expressions. Try the following.\n import re\n p = re.compile(\"^.*\\[\")\n q = re.compile(\"].*$\")\n q.sub(\'\',p.sub(\'\', line))\n\nJust trying to parse a simple IP address, wrapped in square brackets, from Postfix logs.\n\nIn sed this is straightforward given:\n line = \"date process text [ip] more text\"\n sed -e \'s/^.*\\[//\' -e \'s/].*$//\'\n\nyet the following Python code does \'\'\'nothing\'\'\':\n line = line.replace(\'^.*\\[\', \'\', 1)\n line = line.replace(\'].*$\', \'\')\n\nIs there a decent description of string.replace() somewhere?\n:-> use re.sub()\n\n----\n\n=== Python string translate() method ===\n\n* [http://www.tutorialspoint.com/python/string_translate.htm Python string translate() method]\n: the method translate() returns a copy of the string in which all characters have been translated using table (constructed with the maketrans() function in the string module), optionally deleting all characters found in the string deletechars.\n\n* The following example shows the usage of translate() method. Under this every vowel in a string is replaced by its vowel position:\n\n <pre>\n#!/usr/bin/python\n\nfrom string import maketrans   # Required to call maketrans function.\n\nintab = \"aeiou\"\nouttab = \"12345\"\ntrantab = maketrans(intab, outtab)\n\nstr = \"this is string example....wow!!!\";\nprint str.translate(trantab);\n</pre>\n\n <pre>\nth3s 3s str3ng 2x1mpl2....w4w!!!\n</pre>\n\n* Following is the example to delete \'x\' and \'m\' characters from the string:\n\n <pre>\n#!/usr/bin/python\n\nfrom string import maketrans   # Required to call maketrans function.\n\nintab = \"aeiou\"\nouttab = \"12345\"\ntrantab = maketrans(intab, outtab)\n\nstr = \"this is string example....wow!!!\";\nprint str.translate(trantab, \'xm\');\n</pre>\n\n <pre>\nth3s 3s str3ng 21pl2....w4w!!!\n</pre>\n\n== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n=== 과제 목표 평가 방법 객관화/구체화 ===\n\n* 데이터 Access 성능 목표 달성 기준\n: Realistic enterprise workload 인가 시, (*)\n: Data placement 및 RACS-1 기술이 적용되지 않은 vanilla SW-RAID 대비\n: 80% 속도 향상 (in IOPS) (**)\n:: (*) Realistic enterprise workload라고 했을 때, 아래 workload 인가 방안의 Case A,B,C가 해당될 수 있겠다고 보고 있으며, \'13년 9월 쯤을 고려했을 때, 현실적으로는 Case C로 가고, workload type은 DB 혹은 fileserver 쪽으로 가는 것을 생각하고 있습니다. (\'14년 정도에는 Case B나 Case A도 생각해볼 수 있을 것 같습니다)\n:: (**) metric에 대한 부분인 \"SW-RAID 대비 속도 향상 비율: 80%\"은 MBO에 이대로 명시가 되어 수정이 어렵다고 생각하여 그대로 두었습니다.  다만, 속도를 향상시키는 요소 기술로서 동작하는 것은 RACS-1 뿐만 아니라 Data Placement도 가능하겠으므로, 이를 염두에 두고 아래 내용을 작성하였습니다.\n\n\n* Workload type\n: 대표적인 enterprise workload type인 DB, FileServer, Web 중 택일하거나\n: Scientific Computing (R, Matlab, Hadoop 등)을 선택할 수 있겠음\n: VDI workload를 고려할지에 대해서는 추가 고민 필요 (아직까지는 고려하지 않고 있음)\n\n\n* Workload 인가 방안\n:- 대표적인 enterprise workload로 성능 benchmark 필요\n::- Case A) [최선] real enterprise workload 사용\n::- Case B) [차선] enterprise workload에 대한 real trace에 기반한 real workload replay 활용 (workload replayer 구현 필요)\n::- Case C) [차차선] TPC-C, filebench 등과 같이 real workload를 모방한 realistic synthesized workload 사용\n::- Case D) [차차차선] (비추천), iozone 등과 같은 random/sequential read/write I/O pattern 생성기 사용\n\n\n\n* Workload 인가 방식에 따른 환경 구성\n: Case A, B, C 모두 storage system과 network 구성은 동일하며, client 구성 방식에서 차이 존재\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n:- Case A: real workload 인가 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case B: real workload replay 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case C: TPC-C 혹은 filebench를 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n\n* Storage system interface exposed\n: SAN, NAS 방식 중 결정 필요\n\n\n* 가상화 이슈\n: 기본적으로 Hypervisor (Xen, VMware 등)를 사용 하지 않는 환경으로 구성\n\n\n* 성능 측정 방법\n:- 1-day (case A), 1-week (case B) 동안 workload를 인가하여 계산된 IOPS로 판별\n:- 가급적 경쟁사 제품의 성능측정 tool에서 제공하는 metric과 동일하게 가는 것이 좋을 것으로 보임\n::- Real enterprise workload 인가 시, iostat, sar 등 별도의 성능 측정 모듈 구동 필요\n::- Real workload replayer의 경우, total IOPS metric를 계산할 수 있도록 구현 필요\n::- TPC-C, filebench의 경우 (물론 iozone 역시) total IOPS metric을 계산하여 benchmark 결과로서 제공\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n# [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=8&ved=0CG4QFjAH&url=http%3A%2F%2Fwww.cse.buffalo.edu%2F~okennedy%2Fcourses%2Fcse704fa2012%2F2.2-HDFS.pptx&ei=WRtkUfz4NOTwiAeq1oGoCA&usg=AFQjCNGLoDZhdDfLkPO1RKcLINvTG7iL9Q&sig2=pvXlwiUJr302_D-BnHrISg&cad=rjt The Hadoop Distributed File System // Konstantin Shvachko, Hairong Kuang, Sanjay Radia, Robert Chansler, Yahoo! Sunnyvale, California USA]\n# [http://storageconference.org/2010/Papers/MSST/Shvachko.pdf \"The Hadoop Distributed File System,\" MSST 2010]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== Convolution, Cross-Correlation, Autocorrelation ===\n\n* [http://en.wikipedia.org/wiki/Convolution Convolution ((B.GOOD))]\n \n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n\n\n=== SR-IOV, MR-IOV ===\n\n* [http://searchstorage.techtarget.com/video/I-O-virtualization-video-SR-IOV-MR-IOV-NICs-and-more I/O virtualization video: SR-IOV, MR-IOV, NICs and more ((B.GOOD)) // 2012-09-17]\n* [http://blog.scottlowe.org/2009/12/02/what-is-sr-iov/ What is SR-IOV? ((B.GOOD)) // 2009-12-02]\n\n\n\n=== Linux VM and SBC NAS ===\n\n* NAS (ETRI GloryFS 기반) 사용 방법\n\n <pre>\nFrom: Jaewook Oh [mailto:jaew00k.oh@samsung.com] \nSent: Tuesday, April 02, 2013 5:06 PM\nTo: 정명준\nSubject: Re: Re: [결재 통보][SBC 자원신청]실험 데이터 보관 및 전처리 작업 수행을 위한 Linux VM 신청\n\n----\n\n안녕하세요 전문님,\n \n우분투 12.xx 지원이 여의치 않아 일단 11.10으로 할당 진행하였습니다.\n \n첨부된 패키지 및 fuse 패키지(배포된 이미지에 설치되어 있을것으로 예상됩니다...)  설치 후,\n \nmount.ifs 75.2.251.181:/FSM2/fit_icl 마운트될곳 -obigwrite\n \n명령어로 마운트하시면 되며, 패스워드를 물어보는데 secicl123 입력하시면 됩니다.\n\n</pre>\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>\n\n=== Trace Data 저장용 Linux VM 및 대용량 스토리지 (10TB) ===\n\n\n\n\n\n==== Linux VM 및 대용량 스토리지 사용 정보 ====\n\n\n* original hostname of \'gaesung-gongdan\': SECSAITU006-021\n\n\n\n <pre>\n------- Original Message -------\n\nSender : Jaewook Oh<jaew00k.oh@samsung.com> S5/Senior Engineer/R&D Infrastructure Group/Samsung Electronics\n\nDate : 2013-04-02 17:05 (GMT+09:00)\n\nTitle : Re: Re: [결재 통보][SBC 자원신청]실험 데이터 보관 및 전처리 작업 수행을 위한 Linux VM 신청\n\n\n안녕하세요 전문님,\n\n우분투 12.xx 지원이 여의치 않아 일단 11.10으로 할당 진행하였습니다.\n\n첨부된 패키지 및 fuse 패키지(배포된 이미지에 설치되어 있을것으로 예상됩니다...)  설치 후,\n\nmount.ifs 75.2.251.181:/FSM2/fit_icl 마운트될곳 -obigwrite\n\n명령어로 마운트하시면 되며, 패스워드를 물어보는데 secicl123 입력하시면 됩니다.\n\n\n------- Original Message -------\n\nSender : Chae Hwan Lim<ch2036.lim@partner.samsung.com> Partner/Cloud Platform Operation Group/SAMSUNG SDS\n\nDate : 2013-04-02 16:57 (GMT+09:00)\n\nTitle : Re: [결재 통보][SBC 자원신청]실험 데이터 보관 및 전처리 작업 수행을 위한 Linux VM 신청\n\n안녕하세요 클라우드플랫폼운영그룹 임채환대리입니다.\n\n신청하신 VM을 할당 완료 하였습니다.\n\n리눅스의 경우 escort가 적용이 되지 않기 때문에 \n\nescort예외 신청을 하셔야 네트웍을 사용할 수 있습니다...\n\nIP및 ID, password는 아래와 같습니다...\n\n좋은하루되십시오...\n\n==============================\n\nIP : 75.2.252.71\n\nID : root\n\nPW: 개발100%#&\n\nVNC접속(5900) PW : qwe123\n\n접속 PORT : 22(SSH)\n                  3389(RDP)\n\n- 신청 메뉴 : IT4U - ESCORT예외신청 - 설치예외(삭제) - ESCORT 삭제 불필요\n\n - 사업장 : 종합기술원 - 종합기술원\n\n - 제조번호, 모델명 : VMware\n\n - 대상구분 : PC\n\n - 신청항목 : 인터넷 사용\n\n - OS 구분 : Linux 계열\n\n - 신청 항목중 IP / MAC 주소 입력하는 부분에서 확인은 \n\n - 터미널 모드에서 ifconfig  입력 -> \n   \n   MAC 주소는 HWaddr : 00:xx~~ 시작되는 값주소\n   \n   IP 주소는 inet addr : 75.2.252.X 주소값 입니다.\n\n감사합니다.\n\n</pre>','utf-8'),(133,'== Wikini (radiohead) Hot Pages ==\n\n* [http://kandinsky/wikini/index.php?title=Special%3ASearch&search=bnote&go=Go Bnote Search]\n* [[Bnote_2013]] | [[Bnote_patidea_2013]] | [[Bnote - References ( Papers / Patents / Articles )]]\n* [http://kandinsky/wikini/index.php/Special:AllPages Special:AllPages] | [http://kandinsky/wikini/index.php/Special:ListFiles Special:ListFiles]\n\n== Links ==\n* [https://mail.google.com/mail Gmail] [https://www.google.com/calendar/ Gcal] | [http://www.naver.com/ Naver] | [http://github.com/ GitHub] | [http://mobile.olleh.com/index.asp?code=A000000 Olleh_SMS]\n* [http://klimt/ Klimt (Rolling-Stones)]','utf-8'),(134,'== Wikini (radiohead) Hot Pages ==\n\n* [http://kandinsky/wikini/index.php?title=Special%3ASearch&search=bnote&go=Go Bnote Search]\n* [[Bnote_2013]] | [[Bnote_patidea_2013]] | [[Bnote - References ( Papers / Patents / Articles )]]\n* [http://kandinsky/wikini/index.php/Special:AllPages Special:AllPages] | [http://kandinsky/wikini/index.php/Special:ListFiles Special:ListFiles]\n\n== Links ==\n* [https://mail.google.com/mail Gmail] [https://www.google.com/calendar/ Gcal] | [http://www.naver.com/ Naver] | [http://github.com/ GitHub] | [http://mobile.olleh.com/index.asp?code=A000000 Olleh_SMS]\n* [http://klimt/ Klimt (Rolling-Stones)]\n\n== Tasks ==\n\n# patent submit\n# HMM script','utf-8'),(135,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R Package (HMM) ==\n\n* http://cran.r-project.org/web/packages/HMM/HMM.pdf\n* February 15, 2013\n* Maintainer: Lin Himmelmann <hmm@linhi.com>\n* Depends: R (>= 2.0.0)\n* Description: easy to use library to setup, apply and make inference with discrete time and discrete space Hidden Markov Models\n\n=== backward ===\n: computes the backward probabilities.\n\n* The backward probability for state X and observation at time k is defined as the probability of observing the sequence of observations e_k+1, ..., e_n under the condition that the state at time k is X.\n: b[X,k] := Prob(E_k+1 = e_k+1, ..., E_n = e_n | X_k = X)\n: , where E_k+1 ... E_n = e_k+1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.\n\n* Usage\n: backward(hmm, observation)\n::- hmm\n::: A Hidden Markov Model\n::- observation\n::: A sequence of observations\n::- return value\n::: A matrix containing the backward probabilities. The probabilities are given on a logarithmic scale (natural logarithm). The first dimension refers to the state and the second dimension to time.\n\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate backward probablities\nlogBackwardProbabilities = backward(hmm,observations)\nprint(exp(logBackwardProbabilities))\n</pre>\n\n=== baumWelch ===\n\n\n <pre>\n# Initial HMM\nhmm = initHMM(c(\"A\",\"B\"),c(\"L\",\"R\"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))\nprint(hmm)\n\n# Sequence of observation\na = sample(c(rep(\"L\",100),rep(\"R\",300)))\nb = sample(c(rep(\"L\",300),rep(\"R\",100)))\nobservation = c(a,b)\n\n# Baum-Welch\nbw = baumWelch(hmm,observation,10)\nprint(bw$hmm)\n</pre>\n\n=== dishonestCasino ===\n\n <pre>\n\n</pre>\n\n=== forward ===\n: computes the forward probabilities.\n\n* The forward probability for state X up to observation at time k is defined as the probability of observing the sequence of observations e_1, e_2, ..., e_k and that the state at time k is X.\n: f[X,k] : = Prob(E_1 = e_1, ..., E_k = e_k, X_k = X)\n: , where E_1 ... E_n = e_1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.\n\n* Usage\n: forward(hmm, observation)\n::- hmm\n::: A Hidden Markov Model\n::- observation\n::: A sequence of observations\n::- return value\n::: A matrix containing the forward probabilities. The probabilities are given on a logarithmic scale (natural logarithm). The first dimension refers to the state and the second dimension to time.\n\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate forward probablities\nlogForwardProbabilities = forward(hmm,observations)\nprint(exp(logForwardProbabilities))\n</pre>\n\n=== HMM ===\n\n <pre>\n\n</pre>\n\n=== initHMM ===\n\n <pre>\n\n</pre>\n\n=== posterior ===\n: computes the posterior probabilities of being in state X at time k for a given sequence of observations and a given Hidden Markov Model.\n\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate posterior probablities of the states\nposterior = posterior(hmm,observations)\nprint(posterior)\n</pre>\n\n=== simHMM ===\n\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"X\",\"Y\"),c(\"a\",\"b\",\"c\"))\n\n# Simulate from the HMM\nsimHMM(hmm, 100)\n</pre>\n\n=== viterbi ===\n\n* The Viterbi-algorithm computes the most probable path of states for a sequence of observations for a given Hidden Markov Model.\n\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.6,.4,.4,.6),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate Viterbi path\nviterbi = viterbi(hmm,observations)\nprint(viterbi)\n</pre>\n\n=== viterbiTraining ===\n\n: For an initial Hidden Markov Model (HMM) and a given sequence of observations, the Viterbitraining algorithm infers optimal parameters to the HMM. Viterbi-training usually converges much faster than the Baum-Welch algorithm, but the underlying algorithm is theoretically less justified.  Be careful: The algorithm converges to a local solution which might not be the optimum.\n\n <pre>\n# Initial HMM\nhmm = initHMM(c(\"A\",\"B\"),c(\"L\",\"R\"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))\nprint(hmm)\n\n# Sequence of observation\na = sample(c(rep(\"L\",100),rep(\"R\",300)))\nb = sample(c(rep(\"L\",300),rep(\"R\",100)))\nobservation = c(a,b)\n\n# Viterbi-training\nvt = viterbiTraining(hmm,observation,10)\nprint(vt$hmm)\n</pre>\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n\n* http://www.jstatsoft.org/v36/i07/paper\n: Journal of Statistical Software, August 2010, Vol. 36, Issue 7\n: depmixS4: An R Package for Hidden Markov Models\n\n* http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf\n: depmixS4: An R Package for Hidden Markov Models\n\n\n* http://www.cs.columbia.edu/4761/notes07/chapter4.3-HMM.pdf\n: Chapter 4: Hidden Markov Models (Columbia University)\n\n\n* http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n: Chapter 11 Markov Chains (Dartmouth College)\n\n\n* http://www.princeton.edu/~rvan/orf557/hmm080728.pdf\n: Hidden Markov Models - Lecture Notes 2008-07-28 (Princeton University)\n\n\n\n* http://www.stat.ucla.edu/~twng/storage/Hidden%20Markov%20Models%20for%20Time%20Series%20An%20Introduction%20Using%20R.pdf\n: Hidden Markov Models for Time Series - An Introduction Using R (UCLA)\n\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n* http://www.tjdb.org/123456789/7390/1/IJRSP%2039(1)%2039-44.pdf\n: Higher order Markov chain models for monsoon rainfall over West Bengal, India','utf-8'),(136,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R Package (HMM) ==\n\n* http://cran.r-project.org/web/packages/HMM/HMM.pdf\n* February 15, 2013\n* Maintainer: Lin Himmelmann <hmm@linhi.com>\n* Depends: R (>= 2.0.0)\n* Description: easy to use library to setup, apply and make inference with discrete time and discrete space Hidden Markov Models\n\n=== backward ===\n: computes the backward probabilities.\n\n* The backward probability for state X and observation at time k is defined as the probability of observing the sequence of observations e_k+1, ..., e_n under the condition that the state at time k is X.\n: b[X,k] := Prob(E_k+1 = e_k+1, ..., E_n = e_n | X_k = X)\n: , where E_k+1 ... E_n = e_k+1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.\n\n* Usage\n: backward(hmm, observation)\n::- hmm\n::: A Hidden Markov Model\n::- observation\n::: A sequence of observations\n::- return value\n::: A matrix containing the backward probabilities. The probabilities are given on a logarithmic scale (natural logarithm). The first dimension refers to the state and the second dimension to time.\n\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate backward probablities\nlogBackwardProbabilities = backward(hmm,observations)\nprint(exp(logBackwardProbabilities))\n</pre>\n\n=== baumWelch ===\n\n\n <pre>\n# Initial HMM\nhmm = initHMM(c(\"A\",\"B\"),c(\"L\",\"R\"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))\nprint(hmm)\n\n# Sequence of observation\na = sample(c(rep(\"L\",100),rep(\"R\",300)))\nb = sample(c(rep(\"L\",300),rep(\"R\",100)))\nobservation = c(a,b)\n\n# Baum-Welch\nbw = baumWelch(hmm,observation,10)\nprint(bw$hmm)\n</pre>\n\n=== dishonestCasino ===\n\n <pre>\ndishonestCasino()\n</pre>\n\n=== forward ===\n: computes the forward probabilities.\n\n* The forward probability for state X up to observation at time k is defined as the probability of observing the sequence of observations e_1, e_2, ..., e_k and that the state at time k is X.\n: f[X,k] : = Prob(E_1 = e_1, ..., E_k = e_k, X_k = X)\n: , where E_1 ... E_n = e_1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.\n\n* Usage\n: forward(hmm, observation)\n::- hmm\n::: A Hidden Markov Model\n::- observation\n::: A sequence of observations\n::- return value\n::: A matrix containing the forward probabilities. The probabilities are given on a logarithmic scale (natural logarithm). The first dimension refers to the state and the second dimension to time.\n\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate forward probablities\nlogForwardProbabilities = forward(hmm,observations)\nprint(exp(logForwardProbabilities))\n</pre>\n\n=== HMM ===\n\n <pre>\n\n</pre>\n\n=== initHMM ===\n\n <pre>\n\n</pre>\n\n=== posterior ===\n: computes the posterior probabilities of being in state X at time k for a given sequence of observations and a given Hidden Markov Model.\n\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate posterior probablities of the states\nposterior = posterior(hmm,observations)\nprint(posterior)\n</pre>\n\n=== simHMM ===\n\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"X\",\"Y\"),c(\"a\",\"b\",\"c\"))\n\n# Simulate from the HMM\nsimHMM(hmm, 100)\n</pre>\n\n=== viterbi ===\n\n* The Viterbi-algorithm computes the most probable path of states for a sequence of observations for a given Hidden Markov Model.\n\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.6,.4,.4,.6),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate Viterbi path\nviterbi = viterbi(hmm,observations)\nprint(viterbi)\n</pre>\n\n=== viterbiTraining ===\n\n: For an initial Hidden Markov Model (HMM) and a given sequence of observations, the Viterbitraining algorithm infers optimal parameters to the HMM. Viterbi-training usually converges much faster than the Baum-Welch algorithm, but the underlying algorithm is theoretically less justified.  Be careful: The algorithm converges to a local solution which might not be the optimum.\n\n <pre>\n# Initial HMM\nhmm = initHMM(c(\"A\",\"B\"),c(\"L\",\"R\"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))\nprint(hmm)\n\n# Sequence of observation\na = sample(c(rep(\"L\",100),rep(\"R\",300)))\nb = sample(c(rep(\"L\",300),rep(\"R\",100)))\nobservation = c(a,b)\n\n# Viterbi-training\nvt = viterbiTraining(hmm,observation,10)\nprint(vt$hmm)\n</pre>\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n\n* http://www.jstatsoft.org/v36/i07/paper\n: Journal of Statistical Software, August 2010, Vol. 36, Issue 7\n: depmixS4: An R Package for Hidden Markov Models\n\n* http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf\n: depmixS4: An R Package for Hidden Markov Models\n\n\n* http://www.cs.columbia.edu/4761/notes07/chapter4.3-HMM.pdf\n: Chapter 4: Hidden Markov Models (Columbia University)\n\n\n* http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n: Chapter 11 Markov Chains (Dartmouth College)\n\n\n* http://www.princeton.edu/~rvan/orf557/hmm080728.pdf\n: Hidden Markov Models - Lecture Notes 2008-07-28 (Princeton University)\n\n\n\n* http://www.stat.ucla.edu/~twng/storage/Hidden%20Markov%20Models%20for%20Time%20Series%20An%20Introduction%20Using%20R.pdf\n: Hidden Markov Models for Time Series - An Introduction Using R (UCLA)\n\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n* http://www.tjdb.org/123456789/7390/1/IJRSP%2039(1)%2039-44.pdf\n: Higher order Markov chain models for monsoon rainfall over West Bengal, India','utf-8'),(137,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R Package (HMM) ==\n\n* _ver=20130603_014942\n* http://cran.r-project.org/web/packages/HMM/HMM.pdf\n* February 15, 2013\n* Maintainer: Lin Himmelmann <hmm@linhi.com>\n* Depends: R (>= 2.0.0)\n* Description: easy to use library to setup, apply and make inference with discrete time and discrete space Hidden Markov Models\n\n=== backward ===\n: computes the backward probabilities.\n\n* The backward probability for state X and observation at time k is defined as the probability of observing the sequence of observations e_k+1, ..., e_n under the condition that the state at time k is X.\n: b[X,k] := Prob(E_k+1 = e_k+1, ..., E_n = e_n | X_k = X)\n: , where E_k+1 ... E_n = e_k+1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.\n\n* Usage\n: backward(hmm, observation)\n::- hmm\n::: A Hidden Markov Model\n::- observation\n::: A sequence of observations\n::- return value\n::: A matrix containing the backward probabilities. The probabilities are given on a logarithmic scale (natural logarithm). The first dimension refers to the state and the second dimension to time.\n\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate backward probablities\nlogBackwardProbabilities = backward(hmm,observations)\nprint(exp(logBackwardProbabilities))\n</pre>\n\n=== baumWelch ===\n\n\n <pre>\n# Initial HMM\nhmm = initHMM(c(\"A\",\"B\"),c(\"L\",\"R\"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))\nprint(hmm)\n\n# Sequence of observation\na = sample(c(rep(\"L\",100),rep(\"R\",300)))\nb = sample(c(rep(\"L\",300),rep(\"R\",100)))\nobservation = c(a,b)\n\n# Baum-Welch\nbw = baumWelch(hmm,observation,10)\nprint(bw$hmm)\n</pre>\n\n=== dishonestCasino ===\n\n <pre>\ndishonestCasino()\n</pre>\n\n=== forward ===\n: computes the forward probabilities.\n\n* The forward probability for state X up to observation at time k is defined as the probability of observing the sequence of observations e_1, e_2, ..., e_k and that the state at time k is X.\n: f[X,k] : = Prob(E_1 = e_1, ..., E_k = e_k, X_k = X)\n: , where E_1 ... E_n = e_1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.\n\n* Usage\n: forward(hmm, observation)\n::- hmm\n::: A Hidden Markov Model\n::- observation\n::: A sequence of observations\n::- return value\n::: A matrix containing the forward probabilities. The probabilities are given on a logarithmic scale (natural logarithm). The first dimension refers to the state and the second dimension to time.\n\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate forward probablities\nlogForwardProbabilities = forward(hmm,observations)\nprint(exp(logForwardProbabilities))\n</pre>\n\n=== HMM ===\n\n <pre>\n\n</pre>\n\n=== initHMM ===\n\n <pre>\n\n</pre>\n\n=== posterior ===\n: computes the posterior probabilities of being in state X at time k for a given sequence of observations and a given Hidden Markov Model.\n\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate posterior probablities of the states\nposterior = posterior(hmm,observations)\nprint(posterior)\n</pre>\n\n=== simHMM ===\n\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"X\",\"Y\"),c(\"a\",\"b\",\"c\"))\n\n# Simulate from the HMM\nsimHMM(hmm, 100)\n</pre>\n\n=== viterbi ===\n\n* The Viterbi-algorithm computes the most probable path of states for a sequence of observations for a given Hidden Markov Model.\n\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.6,.4,.4,.6),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate Viterbi path\nviterbi = viterbi(hmm,observations)\nprint(viterbi)\n</pre>\n\n=== viterbiTraining ===\n\n: For an initial Hidden Markov Model (HMM) and a given sequence of observations, the Viterbitraining algorithm infers optimal parameters to the HMM. Viterbi-training usually converges much faster than the Baum-Welch algorithm, but the underlying algorithm is theoretically less justified.  Be careful: The algorithm converges to a local solution which might not be the optimum.\n\n <pre>\n# Initial HMM\nhmm = initHMM(c(\"A\",\"B\"),c(\"L\",\"R\"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))\nprint(hmm)\n\n# Sequence of observation\na = sample(c(rep(\"L\",100),rep(\"R\",300)))\nb = sample(c(rep(\"L\",300),rep(\"R\",100)))\nobservation = c(a,b)\n\n# Viterbi-training\nvt = viterbiTraining(hmm,observation,10)\nprint(vt$hmm)\n</pre>\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n\n* http://www.jstatsoft.org/v36/i07/paper\n: Journal of Statistical Software, August 2010, Vol. 36, Issue 7\n: depmixS4: An R Package for Hidden Markov Models\n\n* http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf\n: depmixS4: An R Package for Hidden Markov Models\n\n\n* http://www.cs.columbia.edu/4761/notes07/chapter4.3-HMM.pdf\n: Chapter 4: Hidden Markov Models (Columbia University)\n\n\n* http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n: Chapter 11 Markov Chains (Dartmouth College)\n\n\n* http://www.princeton.edu/~rvan/orf557/hmm080728.pdf\n: Hidden Markov Models - Lecture Notes 2008-07-28 (Princeton University)\n\n\n\n* http://www.stat.ucla.edu/~twng/storage/Hidden%20Markov%20Models%20for%20Time%20Series%20An%20Introduction%20Using%20R.pdf\n: Hidden Markov Models for Time Series - An Introduction Using R (UCLA)\n\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n* http://www.tjdb.org/123456789/7390/1/IJRSP%2039(1)%2039-44.pdf\n: Higher order Markov chain models for monsoon rainfall over West Bengal, India','utf-8'),(138,'== Table of Contents ==\n\n# [[Bnote 2013-01]]\n# [[Bnote 2013-02]]\n# [[Bnote 2013-03]]\n# [[Bnote 2013-04]]\n# [[Bnote 2013-05]]\n# [[Bnote 2013-06]]','utf-8'),(139,'== ## bNote-2013-06-03 ==\n\n=== R HMM ===','utf-8'),(140,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R Package (HMM) ==\n\n: _ver=20130603_014942\n: _ver=20130604_002222\n\n* http://cran.r-project.org/web/packages/HMM/HMM.pdf\n* February 15, 2013\n* Maintainer: Lin Himmelmann <hmm@linhi.com>\n* Depends: R (>= 2.0.0)\n* Description: easy to use library to setup, apply and make inference with discrete time and discrete space Hidden Markov Models\n\n\n\n\n=== backward ===\n: Computes the backward probabilities.\n\n* The backward probability for state X and observation at time k is defined as the probability of observing the sequence of observations e_k+1, ..., e_n under the condition that the state at time k is X.\n: b[X,k] := Prob(E_k+1 = e_k+1, ..., E_n = e_n | X_k = X)\n: , where E_k+1 ... E_n = e_k+1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate backward probablities\nlogBackwardProbabilities = backward(hmm,observations)\nprint(exp(logBackwardProbabilities))\n</pre>\n\n\n\n\n=== baumWelch ===\n: Inferring the parameters of a HMM via the Baum-Welch algorithm\n\n* For an initial HMM and a given sequence of observations, the Baum-Welch algorithm infers optimal parameters to the HMM. Since the Baum-Welch algorithm is a variant of the Expectation-Maximisation algorithm, the algorithm converges to a local solution which might not be the global optimum.\n\n* Arguments\n:- hmm: A Hidden Markov Model\n:- observation: A sequence of observations\n:- maxIterations: The maximum number of iterations in the Baum-Welch algorithm\n:- delta: Additional termination condition, if the transition and emission matrices converge, before reaching the maximum number of iterations (maxIterations). The difference of transition and emission parameters in consecutive iterations must be smaller than delta to terminate the algorithm\n:- pseudoCount: Adding this amount of pseudo counts in the estimation-step of the Baum-Welch algorithm\n\n* Return Values\n:- hmm: The inferred HMM, of which representation is equivalent to the representation in initHMM()\n:- difference: Vector of differences calculated from consequtive transition and emission matrices in each iteration of the Baum-Welch procedure. The difference is the sum of the distance between consecutive transition and emission matrices in the L2-Norm\n\n* Usage\n <pre>\n# Initial HMM\nhmm = initHMM(c(\"A\",\"B\"),c(\"L\",\"R\"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))\nprint(hmm)\n\n# Sequence of observation\na = sample(c(rep(\"L\",100),rep(\"R\",300)))\nb = sample(c(rep(\"L\",300),rep(\"R\",100)))\nobservation = c(a,b)\n\n# Baum-Welch\nbw = baumWelch(hmm,observation,10) # bw <- baumWelch(hmm, observation, maxIterations=100, delta=1E-9, pseudoCount=0)\nprint(bw$hmm) # the inferred HMM // representation is equivalent to the representation in initHMM()\nprint(bw$difference)\n</pre>\n\n\n\n\n=== dishonestCasino ===\n: Example application for HMM\n\n* The dishonest casino gives an example for the application of Hidden Markov Models. This example is taken from Durbin et. al. 1999: A dishonest casino uses two dice, one of them is fair the other is loaded. The probabilities of the fair die are (1/6,...,1/6) for throwing (\"1\",...,\"6\"). The probabilities of the loaded die are (1/10,...,1/10,1/2) for throwing (\"1\",...\"5\",\"6\"). The observer doesn’t know which die is actually taken (the state is hidden), but the sequence of throws (observations) can be used to infer which die (state) was used.\n\n* Usage\n <pre>\ndishonestCasino()\n</pre>\n\n* Source Code\n <pre>\ndishonestCasino <- function () {\n    nSim = 2000\n    States = c(\"Fair\", \"Unfair\")\n    Symbols = 1:6\n    transProbs = matrix(c(0.99, 0.01, 0.02, 0.98), c(length(States), \n        length(States)), byrow = TRUE)\n    emissionProbs = matrix(c(rep(1/6, 6), c(rep(0.1, 5), 0.5)), \n        c(length(States), length(Symbols)), byrow = TRUE)\n    hmm = initHMM(States, Symbols, transProbs = transProbs, emissionProbs = emissionProbs)\n    sim = simHMM(hmm, nSim)\n    vit = viterbi(hmm, sim$observation)\n    f = forward(hmm, sim$observation)\n    b = backward(hmm, sim$observation)\n    i <- f[1, nSim]\n    j <- f[2, nSim]\n    probObservations = (i + log(1 + exp(j - i)))\n    posterior = exp((f + b) - probObservations)\n    x = list(hmm = hmm, sim = sim, vit = vit, posterior = posterior)\n    readline(\"Plot simulated throws:\\n\")\n    mn = \"Fair and unfair die\"\n    xlb = \"Throw nr.\"\n    ylb = \"\"\n    plot(x$sim$observation, ylim = c(-7.5, 6), pch = 3, main = mn, \n        xlab = xlb, ylab = ylb, bty = \"n\", yaxt = \"n\")\n    axis(2, at = 1:6)\n    readline(\"Simulated, which die was used:\\n\")\n    text(0, -1.2, adj = 0, cex = 0.8, col = \"black\", \"True: green = fair die\")\n    for (i in 1:nSim) {\n        if (x$sim$states[i] == \"Fair\") \n            rect(i, -1, i + 1, 0, col = \"green\", border = NA)\n        else rect(i, -1, i + 1, 0, col = \"red\", border = NA)\n    }\n    readline(\"Most probable path (viterbi):\\n\")\n    text(0, -3.2, adj = 0, cex = 0.8, col = \"black\", \"Most probable path\")\n    for (i in 1:nSim) {\n        if (x$vit[i] == \"Fair\") \n            rect(i, -3, i + 1, -2, col = \"green\", border = NA)\n        else rect(i, -3, i + 1, -2, col = \"red\", border = NA)\n    }\n    readline(\"Differences:\\n\")\n    text(0, -5.2, adj = 0, cex = 0.8, col = \"black\", \"Difference\")\n    differing = !(x$sim$states == x$vit)\n    for (i in 1:nSim) {\n        if (differing[i]) \n            rect(i, -5, i + 1, -4, col = rgb(0.3, 0.3, 0.3), \n                border = NA)\n        else rect(i, -5, i + 1, -4, col = rgb(0.9, 0.9, 0.9), \n            border = NA)\n    }\n    readline(\"Posterior-probability:\\n\")\n    points(x$posterior[2, ] - 3, type = \"l\")\n    readline(\"Difference with classification by posterior-probability:\\n\")\n    text(0, -7.2, adj = 0, cex = 0.8, col = \"black\", \"Difference by posterior-probability\")\n    differing = !(x$sim$states == x$vit)\n    for (i in 1:nSim) {\n        if (posterior[1, i] > 0.5) {\n            if (x$sim$states[i] == \"Fair\") \n                rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                  border = NA)\n            else rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3), \n                border = NA)\n        }\n        else {\n            if (x$sim$states[i] == \"Unfair\") \n                rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                  border = NA)\n            else rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3), \n                border = NA)\n        }\n    }\n    readline(\"Difference with classification by posterior-probability > .95:\\n\")\n    text(0, -7.2, adj = 0, cex = 0.8, col = \"black\", \"Difference by posterior-probability > .95\")\n    differing = !(x$sim$states == x$vit)\n    for (i in 1:nSim) {\n        if (posterior[2, i] > 0.95 || posterior[2, i] < 0.05) {\n            if (differing[i]) \n                rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3), \n                  border = NA)\n            else rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                border = NA)\n        }\n        else {\n            rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                border = NA)\n        }\n    }\n    invisible(x)\n}\n<environment: namespace:HMM>\n</pre>\n\n\n\n\n=== forward ===\n: Computes the forward probabilities.\n\n* The forward probability for state X up to observation at time k is defined as the probability of observing the sequence of observations e_1, e_2, ..., e_k and that the state at time k is X.\n: f[X,k] : = Prob(E_1 = e_1, ..., E_k = e_k, X_k = X)\n: , where E_1 ... E_n = e_1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate forward probablities\nlogForwardProbabilities = forward(hmm,observations)\nprint(exp(logForwardProbabilities))\n</pre>\n\n\n\n\n=== HMM ===\n: HMM - Hidden Markov Models\n\n* Modelling, analysis and inference with discrete time and discrete space Hidden Markov Models.\n\n\n\n\n=== initHMM ===\n: Initialization of HMM\'s\n\n* This function initialises a general discrete time and discrete space Hidden Markov Model (HMM).  A HMM consists of an alphabet of states and emission symbols. A HMM assumes that the states are hidden from the observer, while only the emissions of the states are observable. The HMM is de- signed to make inference on the states through the observation of emissions. The stochastics of the HMM is fully described by the initial starting probabilities of the states, the transition probabilities between states and the emission probabilities of the states.\n\n* The function initHMM returns a HMM that consists of a list of 5 elements:\n:- States: Vector with the names of the states.\n:- Symbols: Vector with the names of the symbols.\n:- startProbs: Annotated vector with the starting probabilities of the states.\n:- transProbs: Annotated matrix containing the transition probabilities between the states.\n:- emissionProbs: Annotated matrix containing the emission probabilities of the states.\n\n* Usage\n <pre>\n# function interface\ninitHMM(States, Symbols, startProbs=NULL, transProbs=NULL, emissionProbs=NULL)\n\n# Initialise HMM (example 1)\nhmm_1 <- initHMM(c(\"X\",\"Y\"), c(\"a\",\"b\",\"c\"))\n\n# Initialise HMM (example 2)\nhmm_2 <- initHMM(c(\"X\",\"Y\"), c(\"a\",\"b\"), c(.3,.7), matrix(c(.9,.1,.1,.9),2), matrix(c(.3,.7,.7,.3),2))\n</pre>\n\n\n\n\n=== posterior ===\n: Computes the posterior probabilities for the states\n\n* This function computes the posterior probabilities of being in state X at time k for a given sequence of observations and a given HMM.\n\n* The posterior probability of being in a state X at time k can be computed from the forward and backward probabilities:\n: Ws(X_k = X | E_1 = e_1, ..., E_n = e_n) = f[X,k] * b[X,k] / Prob(E_1 = e_1, ..., E_n = e_n)\n: , where E_1 ... E_n = e_1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate posterior probablities of the states\nposterior = posterior(hmm,observations)\nprint(posterior)\n</pre>\n\n\n\n\n=== simHMM ===\n: Simulates states and observations for a HMM\n\n* Simulates a path of states and observations for a given HMM\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"X\",\"Y\"),c(\"a\",\"b\",\"c\")) # does this provide enough initial conditions for simulation?\n\n# Simulate from the HMM\nsimHMM(hmm, 100)\n</pre>\n\n\n\n\n=== viterbi ===\n: Computes the most probable path of states\n\n* The Viterbi-algorithm computes the most probable path of states for a sequence of observations for a given HMM.\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.6,.4,.4,.6),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate Viterbi path\nviterbiPath = viterbi(hmm,observations)\nprint(viterbiPath)\n</pre>\n\n\n\n\n=== viterbiTraining ===\n: Inferring the parameters of a HMM via Viterbi-training\n\n* For an initial Hidden Markov Model (HMM) and a given sequence of observations, the Viterbitraining algorithm infers optimal parameters to the HMM. Viterbi-training usually converges much faster than the Baum-Welch algorithm, but the underlying algorithm is theoretically less justified.  Be careful: The algorithm converges to a local solution which might not be the optimum.\n\n* Arguments\n:- hmm: A Hidden Markov Model\n:- observation: A sequence of observations\n:- maxIterations: The maximum number of iterations in the Viterbi-training algorithm\n:- delta: Additional termination condition, if the transition and emission matrices converge, before reaching the maximum number of iterations (maxIterations). The difference of transition and emission parameters in consecutive iterations must be smaller than delta to terminate the algorithm\n:- pseudoCount: Adding this amount of pseudo counts in the estimation-step of the Viterbi-training algorithm\n\n* Return Values\n:- hmm: The inferred HMM, of which representation is equivalent to the representation in initHMM()\n:- difference: Vector of differences calculated from consequtive transition and emission matrices in each iteration of the Viterbi-training. The difference is the sum of the distance between consecutive transition and emission matrices in the L2-Norm\n\n* Usage\n <pre>\n# Initial HMM\nhmm = initHMM(c(\"A\",\"B\"),c(\"L\",\"R\"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))\nprint(hmm)\n\n# Sequence of observation\na = sample(c(rep(\"L\",100),rep(\"R\",300)))\nb = sample(c(rep(\"L\",300),rep(\"R\",100)))\nobservation = c(a,b)\n\n# Viterbi-training\nvt = viterbiTraining(hmm,observation,10)\nprint(vt$hmm)\n</pre>\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n\n* http://www.jstatsoft.org/v36/i07/paper\n: Journal of Statistical Software, August 2010, Vol. 36, Issue 7\n: depmixS4: An R Package for Hidden Markov Models\n\n* http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf\n: depmixS4: An R Package for Hidden Markov Models\n\n\n* http://www.cs.columbia.edu/4761/notes07/chapter4.3-HMM.pdf\n: Chapter 4: Hidden Markov Models (Columbia University)\n\n\n* http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n: Chapter 11 Markov Chains (Dartmouth College)\n\n\n* http://www.princeton.edu/~rvan/orf557/hmm080728.pdf\n: Hidden Markov Models - Lecture Notes 2008-07-28 (Princeton University)\n\n\n\n* http://www.stat.ucla.edu/~twng/storage/Hidden%20Markov%20Models%20for%20Time%20Series%20An%20Introduction%20Using%20R.pdf\n: Hidden Markov Models for Time Series - An Introduction Using R (UCLA)\n\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n* http://www.tjdb.org/123456789/7390/1/IJRSP%2039(1)%2039-44.pdf\n: Higher order Markov chain models for monsoon rainfall over West Bengal, India','utf-8'),(141,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R Package (HMM) ==\n\n: _ver=20130603_014942\n: _ver=20130604_002222\n\n\n* http://cran.r-project.org/web/packages/HMM/HMM.pdf\n* February 15, 2013\n* Maintainer: Lin Himmelmann <hmm@linhi.com>\n* Depends: R (>= 2.0.0)\n* Description: easy to use library to setup, apply and make inference with discrete time and discrete space Hidden Markov Models\n\n\n\n\n=== backward ===\n: Computes the backward probabilities.\n\n* The backward probability for state X and observation at time k is defined as the probability of observing the sequence of observations e_k+1, ..., e_n under the condition that the state at time k is X.\n: b[X,k] := Prob(E_k+1 = e_k+1, ..., E_n = e_n | X_k = X)\n: , where E_k+1 ... E_n = e_k+1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate backward probablities\nlogBackwardProbabilities = backward(hmm,observations)\nprint(exp(logBackwardProbabilities))\n</pre>\n\n\n\n\n=== baumWelch ===\n: Inferring the parameters of a HMM via the Baum-Welch algorithm\n\n* For an initial HMM and a given sequence of observations, the Baum-Welch algorithm infers optimal parameters to the HMM. Since the Baum-Welch algorithm is a variant of the Expectation-Maximisation algorithm, the algorithm converges to a local solution which might not be the global optimum.\n\n* Arguments\n:- hmm: A Hidden Markov Model\n:- observation: A sequence of observations\n:- maxIterations: The maximum number of iterations in the Baum-Welch algorithm\n:- delta: Additional termination condition, if the transition and emission matrices converge, before reaching the maximum number of iterations (maxIterations). The difference of transition and emission parameters in consecutive iterations must be smaller than delta to terminate the algorithm\n:- pseudoCount: Adding this amount of pseudo counts in the estimation-step of the Baum-Welch algorithm\n\n* Return Values\n:- hmm: The inferred HMM, of which representation is equivalent to the representation in initHMM()\n:- difference: Vector of differences calculated from consequtive transition and emission matrices in each iteration of the Baum-Welch procedure. The difference is the sum of the distance between consecutive transition and emission matrices in the L2-Norm\n\n* Usage\n <pre>\n# Initial HMM\nhmm = initHMM(c(\"A\",\"B\"),c(\"L\",\"R\"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))\nprint(hmm)\n\n# Sequence of observation\na = sample(c(rep(\"L\",100),rep(\"R\",300)))\nb = sample(c(rep(\"L\",300),rep(\"R\",100)))\nobservation = c(a,b)\n\n# Baum-Welch\nbw = baumWelch(hmm,observation,10) # bw <- baumWelch(hmm, observation, maxIterations=100, delta=1E-9, pseudoCount=0)\nprint(bw$hmm) # the inferred HMM // representation is equivalent to the representation in initHMM()\nprint(bw$difference)\n</pre>\n\n\n\n\n=== dishonestCasino ===\n: Example application for HMM\n\n* The dishonest casino gives an example for the application of Hidden Markov Models. This example is taken from Durbin et. al. 1999: A dishonest casino uses two dice, one of them is fair the other is loaded. The probabilities of the fair die are (1/6,...,1/6) for throwing (\"1\",...,\"6\"). The probabilities of the loaded die are (1/10,...,1/10,1/2) for throwing (\"1\",...\"5\",\"6\"). The observer doesn’t know which die is actually taken (the state is hidden), but the sequence of throws (observations) can be used to infer which die (state) was used.\n\n* Usage\n <pre>\ndishonestCasino()\n</pre>\n\n* Source Code\n <pre>\ndishonestCasino <- function () {\n    nSim = 2000\n    States = c(\"Fair\", \"Unfair\")\n    Symbols = 1:6\n    transProbs = matrix(c(0.99, 0.01, 0.02, 0.98), c(length(States), \n        length(States)), byrow = TRUE)\n    emissionProbs = matrix(c(rep(1/6, 6), c(rep(0.1, 5), 0.5)), \n        c(length(States), length(Symbols)), byrow = TRUE)\n    hmm = initHMM(States, Symbols, transProbs = transProbs, emissionProbs = emissionProbs)\n    sim = simHMM(hmm, nSim)\n    vit = viterbi(hmm, sim$observation)\n    f = forward(hmm, sim$observation)\n    b = backward(hmm, sim$observation)\n    i <- f[1, nSim]\n    j <- f[2, nSim]\n    probObservations = (i + log(1 + exp(j - i)))\n    posterior = exp((f + b) - probObservations)\n    x = list(hmm = hmm, sim = sim, vit = vit, posterior = posterior)\n    readline(\"Plot simulated throws:\\n\")\n    mn = \"Fair and unfair die\"\n    xlb = \"Throw nr.\"\n    ylb = \"\"\n    plot(x$sim$observation, ylim = c(-7.5, 6), pch = 3, main = mn, \n        xlab = xlb, ylab = ylb, bty = \"n\", yaxt = \"n\")\n    axis(2, at = 1:6)\n    readline(\"Simulated, which die was used:\\n\")\n    text(0, -1.2, adj = 0, cex = 0.8, col = \"black\", \"True: green = fair die\")\n    for (i in 1:nSim) {\n        if (x$sim$states[i] == \"Fair\") \n            rect(i, -1, i + 1, 0, col = \"green\", border = NA)\n        else rect(i, -1, i + 1, 0, col = \"red\", border = NA)\n    }\n    readline(\"Most probable path (viterbi):\\n\")\n    text(0, -3.2, adj = 0, cex = 0.8, col = \"black\", \"Most probable path\")\n    for (i in 1:nSim) {\n        if (x$vit[i] == \"Fair\") \n            rect(i, -3, i + 1, -2, col = \"green\", border = NA)\n        else rect(i, -3, i + 1, -2, col = \"red\", border = NA)\n    }\n    readline(\"Differences:\\n\")\n    text(0, -5.2, adj = 0, cex = 0.8, col = \"black\", \"Difference\")\n    differing = !(x$sim$states == x$vit)\n    for (i in 1:nSim) {\n        if (differing[i]) \n            rect(i, -5, i + 1, -4, col = rgb(0.3, 0.3, 0.3), \n                border = NA)\n        else rect(i, -5, i + 1, -4, col = rgb(0.9, 0.9, 0.9), \n            border = NA)\n    }\n    readline(\"Posterior-probability:\\n\")\n    points(x$posterior[2, ] - 3, type = \"l\")\n    readline(\"Difference with classification by posterior-probability:\\n\")\n    text(0, -7.2, adj = 0, cex = 0.8, col = \"black\", \"Difference by posterior-probability\")\n    differing = !(x$sim$states == x$vit)\n    for (i in 1:nSim) {\n        if (posterior[1, i] > 0.5) {\n            if (x$sim$states[i] == \"Fair\") \n                rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                  border = NA)\n            else rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3), \n                border = NA)\n        }\n        else {\n            if (x$sim$states[i] == \"Unfair\") \n                rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                  border = NA)\n            else rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3), \n                border = NA)\n        }\n    }\n    readline(\"Difference with classification by posterior-probability > .95:\\n\")\n    text(0, -7.2, adj = 0, cex = 0.8, col = \"black\", \"Difference by posterior-probability > .95\")\n    differing = !(x$sim$states == x$vit)\n    for (i in 1:nSim) {\n        if (posterior[2, i] > 0.95 || posterior[2, i] < 0.05) {\n            if (differing[i]) \n                rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3), \n                  border = NA)\n            else rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                border = NA)\n        }\n        else {\n            rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                border = NA)\n        }\n    }\n    invisible(x)\n}\n<environment: namespace:HMM>\n</pre>\n\n\n\n\n=== forward ===\n: Computes the forward probabilities.\n\n* The forward probability for state X up to observation at time k is defined as the probability of observing the sequence of observations e_1, e_2, ..., e_k and that the state at time k is X.\n: f[X,k] : = Prob(E_1 = e_1, ..., E_k = e_k, X_k = X)\n: , where E_1 ... E_n = e_1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate forward probablities\nlogForwardProbabilities = forward(hmm,observations)\nprint(exp(logForwardProbabilities))\n</pre>\n\n\n\n\n=== HMM ===\n: HMM - Hidden Markov Models\n\n* Modelling, analysis and inference with discrete time and discrete space Hidden Markov Models.\n\n\n\n\n=== initHMM ===\n: Initialization of HMM\'s\n\n* This function initialises a general discrete time and discrete space Hidden Markov Model (HMM).  A HMM consists of an alphabet of states and emission symbols. A HMM assumes that the states are hidden from the observer, while only the emissions of the states are observable. The HMM is de- signed to make inference on the states through the observation of emissions. The stochastics of the HMM is fully described by the initial starting probabilities of the states, the transition probabilities between states and the emission probabilities of the states.\n\n* The function initHMM returns a HMM that consists of a list of 5 elements:\n:- States: Vector with the names of the states.\n:- Symbols: Vector with the names of the symbols.\n:- startProbs: Annotated vector with the starting probabilities of the states.\n:- transProbs: Annotated matrix containing the transition probabilities between the states.\n:- emissionProbs: Annotated matrix containing the emission probabilities of the states.\n\n* Usage\n <pre>\n# function interface\ninitHMM(States, Symbols, startProbs=NULL, transProbs=NULL, emissionProbs=NULL)\n\n# Initialise HMM (example 1)\nhmm_1 <- initHMM(c(\"X\",\"Y\"), c(\"a\",\"b\",\"c\"))\n\n# Initialise HMM (example 2)\nhmm_2 <- initHMM(c(\"X\",\"Y\"), c(\"a\",\"b\"), c(.3,.7), matrix(c(.9,.1,.1,.9),2), matrix(c(.3,.7,.7,.3),2))\n</pre>\n\n\n\n\n=== posterior ===\n: Computes the posterior probabilities for the states\n\n* This function computes the posterior probabilities of being in state X at time k for a given sequence of observations and a given HMM.\n\n* The posterior probability of being in a state X at time k can be computed from the forward and backward probabilities:\n: Ws(X_k = X | E_1 = e_1, ..., E_n = e_n) = f[X,k] * b[X,k] / Prob(E_1 = e_1, ..., E_n = e_n)\n: , where E_1 ... E_n = e_1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate posterior probablities of the states\nposterior = posterior(hmm,observations)\nprint(posterior)\n</pre>\n\n\n\n\n=== simHMM ===\n: Simulates states and observations for a HMM\n\n* Simulates a path of states and observations for a given HMM\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"X\",\"Y\"),c(\"a\",\"b\",\"c\")) # does this provide enough initial conditions for simulation?\n\n# Simulate from the HMM\nsimHMM(hmm, 100)\n</pre>\n\n\n\n\n=== viterbi ===\n: Computes the most probable path of states\n\n* The Viterbi-algorithm computes the most probable path of states for a sequence of observations for a given HMM.\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.6,.4,.4,.6),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate Viterbi path\nviterbiPath = viterbi(hmm,observations)\nprint(viterbiPath)\n</pre>\n\n\n\n\n=== viterbiTraining ===\n: Inferring the parameters of a HMM via Viterbi-training\n\n* For an initial Hidden Markov Model (HMM) and a given sequence of observations, the Viterbitraining algorithm infers optimal parameters to the HMM. Viterbi-training usually converges much faster than the Baum-Welch algorithm, but the underlying algorithm is theoretically less justified.  Be careful: The algorithm converges to a local solution which might not be the optimum.\n\n* Arguments\n:- hmm: A Hidden Markov Model\n:- observation: A sequence of observations\n:- maxIterations: The maximum number of iterations in the Viterbi-training algorithm\n:- delta: Additional termination condition, if the transition and emission matrices converge, before reaching the maximum number of iterations (maxIterations). The difference of transition and emission parameters in consecutive iterations must be smaller than delta to terminate the algorithm\n:- pseudoCount: Adding this amount of pseudo counts in the estimation-step of the Viterbi-training algorithm\n\n* Return Values\n:- hmm: The inferred HMM, of which representation is equivalent to the representation in initHMM()\n:- difference: Vector of differences calculated from consequtive transition and emission matrices in each iteration of the Viterbi-training. The difference is the sum of the distance between consecutive transition and emission matrices in the L2-Norm\n\n* Usage\n <pre>\n# Initial HMM\nhmm = initHMM(c(\"A\",\"B\"),c(\"L\",\"R\"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))\nprint(hmm)\n\n# Sequence of observation\na = sample(c(rep(\"L\",100),rep(\"R\",300)))\nb = sample(c(rep(\"L\",300),rep(\"R\",100)))\nobservation = c(a,b)\n\n# Viterbi-training\nvt = viterbiTraining(hmm,observation,10)\nprint(vt$hmm)\n</pre>\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n\n* http://www.jstatsoft.org/v36/i07/paper\n: Journal of Statistical Software, August 2010, Vol. 36, Issue 7\n: depmixS4: An R Package for Hidden Markov Models\n\n* http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf\n: depmixS4: An R Package for Hidden Markov Models\n\n\n* http://www.cs.columbia.edu/4761/notes07/chapter4.3-HMM.pdf\n: Chapter 4: Hidden Markov Models (Columbia University)\n\n\n* http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n: Chapter 11 Markov Chains (Dartmouth College)\n\n\n* http://www.princeton.edu/~rvan/orf557/hmm080728.pdf\n: Hidden Markov Models - Lecture Notes 2008-07-28 (Princeton University)\n\n\n\n* http://www.stat.ucla.edu/~twng/storage/Hidden%20Markov%20Models%20for%20Time%20Series%20An%20Introduction%20Using%20R.pdf\n: Hidden Markov Models for Time Series - An Introduction Using R (UCLA)\n\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n* http://www.tjdb.org/123456789/7390/1/IJRSP%2039(1)%2039-44.pdf\n: Higher order Markov chain models for monsoon rainfall over West Bengal, India','utf-8'),(142,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R Package (HMM) ==\n <pre>\n_ver=20130603_014942\n_ver=20130604_002222\n</pre>\n\n* http://cran.r-project.org/web/packages/HMM/HMM.pdf\n* February 15, 2013\n* Maintainer: Lin Himmelmann <hmm@linhi.com>\n* Depends: R (>= 2.0.0)\n* Description: easy to use library to setup, apply and make inference with discrete time and discrete space Hidden Markov Models\n\n\n\n\n=== backward ===\n: Computes the backward probabilities.\n\n* The backward probability for state X and observation at time k is defined as the probability of observing the sequence of observations e_k+1, ..., e_n under the condition that the state at time k is X.\n: b[X,k] := Prob(E_k+1 = e_k+1, ..., E_n = e_n | X_k = X)\n: , where E_k+1 ... E_n = e_k+1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate backward probablities\nlogBackwardProbabilities = backward(hmm,observations)\nprint(exp(logBackwardProbabilities))\n</pre>\n\n\n\n\n=== baumWelch ===\n: Inferring the parameters of a HMM via the Baum-Welch algorithm\n\n* For an initial HMM and a given sequence of observations, the Baum-Welch algorithm infers optimal parameters to the HMM. Since the Baum-Welch algorithm is a variant of the Expectation-Maximisation algorithm, the algorithm converges to a local solution which might not be the global optimum.\n\n* Arguments\n:- hmm: A Hidden Markov Model\n:- observation: A sequence of observations\n:- maxIterations: The maximum number of iterations in the Baum-Welch algorithm\n:- delta: Additional termination condition, if the transition and emission matrices converge, before reaching the maximum number of iterations (maxIterations). The difference of transition and emission parameters in consecutive iterations must be smaller than delta to terminate the algorithm\n:- pseudoCount: Adding this amount of pseudo counts in the estimation-step of the Baum-Welch algorithm\n\n* Return Values\n:- hmm: The inferred HMM, of which representation is equivalent to the representation in initHMM()\n:- difference: Vector of differences calculated from consequtive transition and emission matrices in each iteration of the Baum-Welch procedure. The difference is the sum of the distance between consecutive transition and emission matrices in the L2-Norm\n\n* Usage\n <pre>\n# Initial HMM\nhmm = initHMM(c(\"A\",\"B\"),c(\"L\",\"R\"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))\nprint(hmm)\n\n# Sequence of observation\na = sample(c(rep(\"L\",100),rep(\"R\",300)))\nb = sample(c(rep(\"L\",300),rep(\"R\",100)))\nobservation = c(a,b)\n\n# Baum-Welch\nbw = baumWelch(hmm,observation,10) # bw <- baumWelch(hmm, observation, maxIterations=100, delta=1E-9, pseudoCount=0)\nprint(bw$hmm) # the inferred HMM // representation is equivalent to the representation in initHMM()\nprint(bw$difference)\n</pre>\n\n\n\n\n=== dishonestCasino ===\n: Example application for HMM\n\n* The dishonest casino gives an example for the application of Hidden Markov Models. This example is taken from Durbin et. al. 1999: A dishonest casino uses two dice, one of them is fair the other is loaded. The probabilities of the fair die are (1/6,...,1/6) for throwing (\"1\",...,\"6\"). The probabilities of the loaded die are (1/10,...,1/10,1/2) for throwing (\"1\",...\"5\",\"6\"). The observer doesn’t know which die is actually taken (the state is hidden), but the sequence of throws (observations) can be used to infer which die (state) was used.\n\n* Usage\n <pre>\ndishonestCasino()\n</pre>\n\n* Source Code\n <pre>\ndishonestCasino <- function () {\n    nSim = 2000\n    States = c(\"Fair\", \"Unfair\")\n    Symbols = 1:6\n    transProbs = matrix(c(0.99, 0.01, 0.02, 0.98), c(length(States), \n        length(States)), byrow = TRUE)\n    emissionProbs = matrix(c(rep(1/6, 6), c(rep(0.1, 5), 0.5)), \n        c(length(States), length(Symbols)), byrow = TRUE)\n    hmm = initHMM(States, Symbols, transProbs = transProbs, emissionProbs = emissionProbs)\n    sim = simHMM(hmm, nSim)\n    vit = viterbi(hmm, sim$observation)\n    f = forward(hmm, sim$observation)\n    b = backward(hmm, sim$observation)\n    i <- f[1, nSim]\n    j <- f[2, nSim]\n    probObservations = (i + log(1 + exp(j - i)))\n    posterior = exp((f + b) - probObservations)\n    x = list(hmm = hmm, sim = sim, vit = vit, posterior = posterior)\n    readline(\"Plot simulated throws:\\n\")\n    mn = \"Fair and unfair die\"\n    xlb = \"Throw nr.\"\n    ylb = \"\"\n    plot(x$sim$observation, ylim = c(-7.5, 6), pch = 3, main = mn, \n        xlab = xlb, ylab = ylb, bty = \"n\", yaxt = \"n\")\n    axis(2, at = 1:6)\n    readline(\"Simulated, which die was used:\\n\")\n    text(0, -1.2, adj = 0, cex = 0.8, col = \"black\", \"True: green = fair die\")\n    for (i in 1:nSim) {\n        if (x$sim$states[i] == \"Fair\") \n            rect(i, -1, i + 1, 0, col = \"green\", border = NA)\n        else rect(i, -1, i + 1, 0, col = \"red\", border = NA)\n    }\n    readline(\"Most probable path (viterbi):\\n\")\n    text(0, -3.2, adj = 0, cex = 0.8, col = \"black\", \"Most probable path\")\n    for (i in 1:nSim) {\n        if (x$vit[i] == \"Fair\") \n            rect(i, -3, i + 1, -2, col = \"green\", border = NA)\n        else rect(i, -3, i + 1, -2, col = \"red\", border = NA)\n    }\n    readline(\"Differences:\\n\")\n    text(0, -5.2, adj = 0, cex = 0.8, col = \"black\", \"Difference\")\n    differing = !(x$sim$states == x$vit)\n    for (i in 1:nSim) {\n        if (differing[i]) \n            rect(i, -5, i + 1, -4, col = rgb(0.3, 0.3, 0.3), \n                border = NA)\n        else rect(i, -5, i + 1, -4, col = rgb(0.9, 0.9, 0.9), \n            border = NA)\n    }\n    readline(\"Posterior-probability:\\n\")\n    points(x$posterior[2, ] - 3, type = \"l\")\n    readline(\"Difference with classification by posterior-probability:\\n\")\n    text(0, -7.2, adj = 0, cex = 0.8, col = \"black\", \"Difference by posterior-probability\")\n    differing = !(x$sim$states == x$vit)\n    for (i in 1:nSim) {\n        if (posterior[1, i] > 0.5) {\n            if (x$sim$states[i] == \"Fair\") \n                rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                  border = NA)\n            else rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3), \n                border = NA)\n        }\n        else {\n            if (x$sim$states[i] == \"Unfair\") \n                rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                  border = NA)\n            else rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3), \n                border = NA)\n        }\n    }\n    readline(\"Difference with classification by posterior-probability > .95:\\n\")\n    text(0, -7.2, adj = 0, cex = 0.8, col = \"black\", \"Difference by posterior-probability > .95\")\n    differing = !(x$sim$states == x$vit)\n    for (i in 1:nSim) {\n        if (posterior[2, i] > 0.95 || posterior[2, i] < 0.05) {\n            if (differing[i]) \n                rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3), \n                  border = NA)\n            else rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                border = NA)\n        }\n        else {\n            rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                border = NA)\n        }\n    }\n    invisible(x)\n}\n<environment: namespace:HMM>\n</pre>\n\n\n\n\n=== forward ===\n: Computes the forward probabilities.\n\n* The forward probability for state X up to observation at time k is defined as the probability of observing the sequence of observations e_1, e_2, ..., e_k and that the state at time k is X.\n: f[X,k] : = Prob(E_1 = e_1, ..., E_k = e_k, X_k = X)\n: , where E_1 ... E_n = e_1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate forward probablities\nlogForwardProbabilities = forward(hmm,observations)\nprint(exp(logForwardProbabilities))\n</pre>\n\n\n\n\n=== HMM ===\n: HMM - Hidden Markov Models\n\n* Modelling, analysis and inference with discrete time and discrete space Hidden Markov Models.\n\n\n\n\n=== initHMM ===\n: Initialization of HMM\'s\n\n* This function initialises a general discrete time and discrete space Hidden Markov Model (HMM).  A HMM consists of an alphabet of states and emission symbols. A HMM assumes that the states are hidden from the observer, while only the emissions of the states are observable. The HMM is de- signed to make inference on the states through the observation of emissions. The stochastics of the HMM is fully described by the initial starting probabilities of the states, the transition probabilities between states and the emission probabilities of the states.\n\n* The function initHMM returns a HMM that consists of a list of 5 elements:\n:- States: Vector with the names of the states.\n:- Symbols: Vector with the names of the symbols.\n:- startProbs: Annotated vector with the starting probabilities of the states.\n:- transProbs: Annotated matrix containing the transition probabilities between the states.\n:- emissionProbs: Annotated matrix containing the emission probabilities of the states.\n\n* Usage\n <pre>\n# function interface\ninitHMM(States, Symbols, startProbs=NULL, transProbs=NULL, emissionProbs=NULL)\n\n# Initialise HMM (example 1)\nhmm_1 <- initHMM(c(\"X\",\"Y\"), c(\"a\",\"b\",\"c\"))\n\n# Initialise HMM (example 2)\nhmm_2 <- initHMM(c(\"X\",\"Y\"), c(\"a\",\"b\"), c(.3,.7), matrix(c(.9,.1,.1,.9),2), matrix(c(.3,.7,.7,.3),2))\n</pre>\n\n\n\n\n=== posterior ===\n: Computes the posterior probabilities for the states\n\n* This function computes the posterior probabilities of being in state X at time k for a given sequence of observations and a given HMM.\n\n* The posterior probability of being in a state X at time k can be computed from the forward and backward probabilities:\n: Ws(X_k = X | E_1 = e_1, ..., E_n = e_n) = f[X,k] * b[X,k] / Prob(E_1 = e_1, ..., E_n = e_n)\n: , where E_1 ... E_n = e_1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate posterior probablities of the states\nposterior = posterior(hmm,observations)\nprint(posterior)\n</pre>\n\n\n\n\n=== simHMM ===\n: Simulates states and observations for a HMM\n\n* Simulates a path of states and observations for a given HMM\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"X\",\"Y\"),c(\"a\",\"b\",\"c\")) # does this provide enough initial conditions for simulation?\n\n# Simulate from the HMM\nsimHMM(hmm, 100)\n</pre>\n\n\n\n\n=== viterbi ===\n: Computes the most probable path of states\n\n* The Viterbi-algorithm computes the most probable path of states for a sequence of observations for a given HMM.\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.6,.4,.4,.6),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate Viterbi path\nviterbiPath = viterbi(hmm,observations)\nprint(viterbiPath)\n</pre>\n\n\n\n\n=== viterbiTraining ===\n: Inferring the parameters of a HMM via Viterbi-training\n\n* For an initial Hidden Markov Model (HMM) and a given sequence of observations, the Viterbitraining algorithm infers optimal parameters to the HMM. Viterbi-training usually converges much faster than the Baum-Welch algorithm, but the underlying algorithm is theoretically less justified.  Be careful: The algorithm converges to a local solution which might not be the optimum.\n\n* Arguments\n:- hmm: A Hidden Markov Model\n:- observation: A sequence of observations\n:- maxIterations: The maximum number of iterations in the Viterbi-training algorithm\n:- delta: Additional termination condition, if the transition and emission matrices converge, before reaching the maximum number of iterations (maxIterations). The difference of transition and emission parameters in consecutive iterations must be smaller than delta to terminate the algorithm\n:- pseudoCount: Adding this amount of pseudo counts in the estimation-step of the Viterbi-training algorithm\n\n* Return Values\n:- hmm: The inferred HMM, of which representation is equivalent to the representation in initHMM()\n:- difference: Vector of differences calculated from consequtive transition and emission matrices in each iteration of the Viterbi-training. The difference is the sum of the distance between consecutive transition and emission matrices in the L2-Norm\n\n* Usage\n <pre>\n# Initial HMM\nhmm = initHMM(c(\"A\",\"B\"),c(\"L\",\"R\"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))\nprint(hmm)\n\n# Sequence of observation\na = sample(c(rep(\"L\",100),rep(\"R\",300)))\nb = sample(c(rep(\"L\",300),rep(\"R\",100)))\nobservation = c(a,b)\n\n# Viterbi-training\nvt = viterbiTraining(hmm,observation,10)\nprint(vt$hmm)\n</pre>\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n\n* http://www.jstatsoft.org/v36/i07/paper\n: Journal of Statistical Software, August 2010, Vol. 36, Issue 7\n: depmixS4: An R Package for Hidden Markov Models\n\n* http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf\n: depmixS4: An R Package for Hidden Markov Models\n\n\n* http://www.cs.columbia.edu/4761/notes07/chapter4.3-HMM.pdf\n: Chapter 4: Hidden Markov Models (Columbia University)\n\n\n* http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n: Chapter 11 Markov Chains (Dartmouth College)\n\n\n* http://www.princeton.edu/~rvan/orf557/hmm080728.pdf\n: Hidden Markov Models - Lecture Notes 2008-07-28 (Princeton University)\n\n\n\n* http://www.stat.ucla.edu/~twng/storage/Hidden%20Markov%20Models%20for%20Time%20Series%20An%20Introduction%20Using%20R.pdf\n: Hidden Markov Models for Time Series - An Introduction Using R (UCLA)\n\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n* http://www.tjdb.org/123456789/7390/1/IJRSP%2039(1)%2039-44.pdf\n: Higher order Markov chain models for monsoon rainfall over West Bengal, India','utf-8'),(143,'== HMM: Hidden Markov Model ==\n\n* hidden Markov model (HMM)은 modeling되는 system이 unobserved (hidden) states을 갖는 [[Markov process]]인 것을 가정하는 statistical Markov model임.\n\n* HMM은 가장 간단한 dynamic [[Bayesian network]]으로 생각될 수 있음.\n\n* Markov chain과 같은 \'\'\'simpler Markov model\'\'\'에서는, state은 관측자에게 directly visible하다. 따라서 state transition 확률이 유일한 parameter이다. 그러나 \'\'\'hidden Markov model\'\'\'에서는 state은 directly visible하지 않지만 state에 dependent한 output은 visible하다는 특성을 가진다.\n\n* 각 state은 possible output token들에 대한 probability distribution을 가진다. 따라서 HMM에 의해 생성된 token들의 시퀀스는 state들의 시퀀스에 대한 정보를 갖는다. 여기서, \"hidden\"이란 말은, model이 pass하는 state sequence를 의미하는 것이며, model의 parameter들에 대한 것이 아니다. model parameter들이 정확히 알려진다 하더라도, 그 model은 여전히 \'hidden\'이다.\n\n* Hidden Markov model은 다음과 같은 temporal pattern recognition에서의 응용으로 잘 알려져 있다.\n:- speech\n:- handwriting\n:- gesture recognition\n:- part-of-speech tagging\n:- musical score following\n:- partial discharges\n:- bioinformatics\n\n* Description in terms of urns (항아리를 이용한 설명)\n: http://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg\n\n== R Package (HMM) ==\n <pre>\n_ver=20130603_014942\n_ver=20130604_002222\n</pre>\n\n\n* http://cran.r-project.org/web/packages/HMM/HMM.pdf\n* February 15, 2013\n* Maintainer: Lin Himmelmann <hmm@linhi.com>\n* Depends: R (>= 2.0.0)\n* Description: easy to use library to setup, apply and make inference with discrete time and discrete space Hidden Markov Models\n\n\n\n\n=== backward ===\n: Computes the backward probabilities.\n\n* The backward probability for state X and observation at time k is defined as the probability of observing the sequence of observations e_k+1, ..., e_n under the condition that the state at time k is X.\n: b[X,k] := Prob(E_k+1 = e_k+1, ..., E_n = e_n | X_k = X)\n: , where E_k+1 ... E_n = e_k+1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate backward probablities\nlogBackwardProbabilities = backward(hmm,observations)\nprint(exp(logBackwardProbabilities))\n</pre>\n\n\n\n\n=== baumWelch ===\n: Inferring the parameters of a HMM via the Baum-Welch algorithm\n\n* For an initial HMM and a given sequence of observations, the Baum-Welch algorithm infers optimal parameters to the HMM. Since the Baum-Welch algorithm is a variant of the Expectation-Maximisation algorithm, the algorithm converges to a local solution which might not be the global optimum.\n\n* Arguments\n:- hmm: A Hidden Markov Model\n:- observation: A sequence of observations\n:- maxIterations: The maximum number of iterations in the Baum-Welch algorithm\n:- delta: Additional termination condition, if the transition and emission matrices converge, before reaching the maximum number of iterations (maxIterations). The difference of transition and emission parameters in consecutive iterations must be smaller than delta to terminate the algorithm\n:- pseudoCount: Adding this amount of pseudo counts in the estimation-step of the Baum-Welch algorithm\n\n* Return Values\n:- hmm: The inferred HMM, of which representation is equivalent to the representation in initHMM()\n:- difference: Vector of differences calculated from consequtive transition and emission matrices in each iteration of the Baum-Welch procedure. The difference is the sum of the distance between consecutive transition and emission matrices in the L2-Norm\n\n* Usage\n <pre>\n# Initial HMM\nhmm = initHMM(c(\"A\",\"B\"),c(\"L\",\"R\"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))\nprint(hmm)\n\n# Sequence of observation\na = sample(c(rep(\"L\",100),rep(\"R\",300)))\nb = sample(c(rep(\"L\",300),rep(\"R\",100)))\nobservation = c(a,b)\n\n# Baum-Welch\nbw = baumWelch(hmm,observation,10) # bw <- baumWelch(hmm, observation, maxIterations=100, delta=1E-9, pseudoCount=0)\nprint(bw$hmm) # the inferred HMM // representation is equivalent to the representation in initHMM()\nprint(bw$difference)\n</pre>\n\n\n\n\n=== dishonestCasino ===\n: Example application for HMM\n\n* The dishonest casino gives an example for the application of Hidden Markov Models. This example is taken from Durbin et. al. 1999: A dishonest casino uses two dice, one of them is fair the other is loaded. The probabilities of the fair die are (1/6,...,1/6) for throwing (\"1\",...,\"6\"). The probabilities of the loaded die are (1/10,...,1/10,1/2) for throwing (\"1\",...\"5\",\"6\"). The observer doesn’t know which die is actually taken (the state is hidden), but the sequence of throws (observations) can be used to infer which die (state) was used.\n\n* Usage\n <pre>\ndishonestCasino()\n</pre>\n\n* Source Code\n <pre>\ndishonestCasino <- function () {\n    nSim = 2000\n    States = c(\"Fair\", \"Unfair\")\n    Symbols = 1:6\n    transProbs = matrix(c(0.99, 0.01, 0.02, 0.98), c(length(States), \n        length(States)), byrow = TRUE)\n    emissionProbs = matrix(c(rep(1/6, 6), c(rep(0.1, 5), 0.5)), \n        c(length(States), length(Symbols)), byrow = TRUE)\n    hmm = initHMM(States, Symbols, transProbs = transProbs, emissionProbs = emissionProbs)\n    sim = simHMM(hmm, nSim)\n    vit = viterbi(hmm, sim$observation)\n    f = forward(hmm, sim$observation)\n    b = backward(hmm, sim$observation)\n    i <- f[1, nSim]\n    j <- f[2, nSim]\n    probObservations = (i + log(1 + exp(j - i)))\n    posterior = exp((f + b) - probObservations)\n    x = list(hmm = hmm, sim = sim, vit = vit, posterior = posterior)\n    readline(\"Plot simulated throws:\\n\")\n    mn = \"Fair and unfair die\"\n    xlb = \"Throw nr.\"\n    ylb = \"\"\n    plot(x$sim$observation, ylim = c(-7.5, 6), pch = 3, main = mn, \n        xlab = xlb, ylab = ylb, bty = \"n\", yaxt = \"n\")\n    axis(2, at = 1:6)\n    readline(\"Simulated, which die was used:\\n\")\n    text(0, -1.2, adj = 0, cex = 0.8, col = \"black\", \"True: green = fair die\")\n    for (i in 1:nSim) {\n        if (x$sim$states[i] == \"Fair\") \n            rect(i, -1, i + 1, 0, col = \"green\", border = NA)\n        else rect(i, -1, i + 1, 0, col = \"red\", border = NA)\n    }\n    readline(\"Most probable path (viterbi):\\n\")\n    text(0, -3.2, adj = 0, cex = 0.8, col = \"black\", \"Most probable path\")\n    for (i in 1:nSim) {\n        if (x$vit[i] == \"Fair\") \n            rect(i, -3, i + 1, -2, col = \"green\", border = NA)\n        else rect(i, -3, i + 1, -2, col = \"red\", border = NA)\n    }\n    readline(\"Differences:\\n\")\n    text(0, -5.2, adj = 0, cex = 0.8, col = \"black\", \"Difference\")\n    differing = !(x$sim$states == x$vit)\n    for (i in 1:nSim) {\n        if (differing[i]) \n            rect(i, -5, i + 1, -4, col = rgb(0.3, 0.3, 0.3), \n                border = NA)\n        else rect(i, -5, i + 1, -4, col = rgb(0.9, 0.9, 0.9), \n            border = NA)\n    }\n    readline(\"Posterior-probability:\\n\")\n    points(x$posterior[2, ] - 3, type = \"l\")\n    readline(\"Difference with classification by posterior-probability:\\n\")\n    text(0, -7.2, adj = 0, cex = 0.8, col = \"black\", \"Difference by posterior-probability\")\n    differing = !(x$sim$states == x$vit)\n    for (i in 1:nSim) {\n        if (posterior[1, i] > 0.5) {\n            if (x$sim$states[i] == \"Fair\") \n                rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                  border = NA)\n            else rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3), \n                border = NA)\n        }\n        else {\n            if (x$sim$states[i] == \"Unfair\") \n                rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                  border = NA)\n            else rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3), \n                border = NA)\n        }\n    }\n    readline(\"Difference with classification by posterior-probability > .95:\\n\")\n    text(0, -7.2, adj = 0, cex = 0.8, col = \"black\", \"Difference by posterior-probability > .95\")\n    differing = !(x$sim$states == x$vit)\n    for (i in 1:nSim) {\n        if (posterior[2, i] > 0.95 || posterior[2, i] < 0.05) {\n            if (differing[i]) \n                rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3), \n                  border = NA)\n            else rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                border = NA)\n        }\n        else {\n            rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), \n                border = NA)\n        }\n    }\n    invisible(x)\n}\n<environment: namespace:HMM>\n</pre>\n\n\n\n\n=== forward ===\n: Computes the forward probabilities.\n\n* The forward probability for state X up to observation at time k is defined as the probability of observing the sequence of observations e_1, e_2, ..., e_k and that the state at time k is X.\n: f[X,k] : = Prob(E_1 = e_1, ..., E_k = e_k, X_k = X)\n: , where E_1 ... E_n = e_1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate forward probablities\nlogForwardProbabilities = forward(hmm,observations)\nprint(exp(logForwardProbabilities))\n</pre>\n\n\n\n\n=== HMM ===\n: HMM - Hidden Markov Models\n\n* Modelling, analysis and inference with discrete time and discrete space Hidden Markov Models.\n\n\n\n\n=== initHMM ===\n: Initialization of HMM\'s\n\n* This function initialises a general discrete time and discrete space Hidden Markov Model (HMM).  A HMM consists of an alphabet of states and emission symbols. A HMM assumes that the states are hidden from the observer, while only the emissions of the states are observable. The HMM is de- signed to make inference on the states through the observation of emissions. The stochastics of the HMM is fully described by the initial starting probabilities of the states, the transition probabilities between states and the emission probabilities of the states.\n\n* The function initHMM returns a HMM that consists of a list of 5 elements:\n:- States: Vector with the names of the states.\n:- Symbols: Vector with the names of the symbols.\n:- startProbs: Annotated vector with the starting probabilities of the states.\n:- transProbs: Annotated matrix containing the transition probabilities between the states.\n:- emissionProbs: Annotated matrix containing the emission probabilities of the states.\n\n* Usage\n <pre>\n# function interface\ninitHMM(States, Symbols, startProbs=NULL, transProbs=NULL, emissionProbs=NULL)\n\n# Initialise HMM (example 1)\nhmm_1 <- initHMM(c(\"X\",\"Y\"), c(\"a\",\"b\",\"c\"))\n\n# Initialise HMM (example 2)\nhmm_2 <- initHMM(c(\"X\",\"Y\"), c(\"a\",\"b\"), c(.3,.7), matrix(c(.9,.1,.1,.9),2), matrix(c(.3,.7,.7,.3),2))\n</pre>\n\n\n\n\n=== posterior ===\n: Computes the posterior probabilities for the states\n\n* This function computes the posterior probabilities of being in state X at time k for a given sequence of observations and a given HMM.\n\n* The posterior probability of being in a state X at time k can be computed from the forward and backward probabilities:\n: Ws(X_k = X | E_1 = e_1, ..., E_n = e_n) = f[X,k] * b[X,k] / Prob(E_1 = e_1, ..., E_n = e_n)\n: , where E_1 ... E_n = e_1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate posterior probablities of the states\nposterior = posterior(hmm,observations)\nprint(posterior)\n</pre>\n\n\n\n\n=== simHMM ===\n: Simulates states and observations for a HMM\n\n* Simulates a path of states and observations for a given HMM\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"X\",\"Y\"),c(\"a\",\"b\",\"c\")) # does this provide enough initial conditions for simulation?\n\n# Simulate from the HMM\nsimHMM(hmm, 100)\n</pre>\n\n\n\n\n=== viterbi ===\n: Computes the most probable path of states\n\n* The Viterbi-algorithm computes the most probable path of states for a sequence of observations for a given HMM.\n\n* Usage\n <pre>\n# Initialise HMM\nhmm = initHMM(c(\"A\",\"B\"), c(\"L\",\"R\"), transProbs=matrix(c(.6,.4,.4,.6),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))\nprint(hmm)\n\n# Sequence of observations\nobservations = c(\"L\",\"L\",\"R\",\"R\")\n\n# Calculate Viterbi path\nviterbiPath = viterbi(hmm,observations)\nprint(viterbiPath)\n</pre>\n\n\n\n\n=== viterbiTraining ===\n: Inferring the parameters of a HMM via Viterbi-training\n\n* For an initial Hidden Markov Model (HMM) and a given sequence of observations, the Viterbitraining algorithm infers optimal parameters to the HMM. Viterbi-training usually converges much faster than the Baum-Welch algorithm, but the underlying algorithm is theoretically less justified.  Be careful: The algorithm converges to a local solution which might not be the optimum.\n\n* Arguments\n:- hmm: A Hidden Markov Model\n:- observation: A sequence of observations\n:- maxIterations: The maximum number of iterations in the Viterbi-training algorithm\n:- delta: Additional termination condition, if the transition and emission matrices converge, before reaching the maximum number of iterations (maxIterations). The difference of transition and emission parameters in consecutive iterations must be smaller than delta to terminate the algorithm\n:- pseudoCount: Adding this amount of pseudo counts in the estimation-step of the Viterbi-training algorithm\n\n* Return Values\n:- hmm: The inferred HMM, of which representation is equivalent to the representation in initHMM()\n:- difference: Vector of differences calculated from consequtive transition and emission matrices in each iteration of the Viterbi-training. The difference is the sum of the distance between consecutive transition and emission matrices in the L2-Norm\n\n* Usage\n <pre>\n# Initial HMM\nhmm = initHMM(c(\"A\",\"B\"),c(\"L\",\"R\"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))\nprint(hmm)\n\n# Sequence of observation\na = sample(c(rep(\"L\",100),rep(\"R\",300)))\nb = sample(c(rep(\"L\",300),rep(\"R\",100)))\nobservation = c(a,b)\n\n# Viterbi-training\nvt = viterbiTraining(hmm,observation,10)\nprint(vt$hmm)\n</pre>\n\n== References ==\n\n* http://en.wikipedia.org/wiki/Hidden_Markov_model\n\n\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/src/chapter10.html\n: Very good explanation of HMM using R ((B.GOOD))\n\n* http://a-little-book-of-r-for-bioinformatics.readthedocs.org/en/latest/index.html\n: A little book of R for bioinformatics ((B.GOOD))\n:- How to install R and a brief introduction to R\n:- DNA sequence statistics\n:- Sequence databases\n:- Pairwise sequence alignment\n:- Multiple alignment and phylogenetic trees\n:- Computational gene-finding\n:- Comparative genomics\n:- Hidden Markov models\n\n\n\n* [http://cran.r-project.org/web/packages/HMM/HMM.pdf CRAN HMM Tutorial]\n\n* [http://cran.r-project.org/web/packages/HMM/index.html CRAN HMM]\n\n\n* [http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", Lawrence R. Rabiner, Fellow, IEEE ((B.GOOD))]\n\n* [http://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf Slides for \"A Tutorial on Hidden Markov Models by Lawrence R. Rabiner\", Marcin Marszalek, Feb 16, 2009 ((B.GOOD))]\n\n\n\n* [http://www.niedermayer.ca/papers/bayesian/bayes.html An Introduction to Bayesian Networks and their Contemporary Applications]\n\n\n* http://www.jstatsoft.org/v36/i07/paper\n: Journal of Statistical Software, August 2010, Vol. 36, Issue 7\n: depmixS4: An R Package for Hidden Markov Models\n\n* http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf\n: depmixS4: An R Package for Hidden Markov Models\n\n\n* http://www.cs.columbia.edu/4761/notes07/chapter4.3-HMM.pdf\n: Chapter 4: Hidden Markov Models (Columbia University)\n\n\n* http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n: Chapter 11 Markov Chains (Dartmouth College)\n\n\n* http://www.princeton.edu/~rvan/orf557/hmm080728.pdf\n: Hidden Markov Models - Lecture Notes 2008-07-28 (Princeton University)\n\n\n\n* http://www.stat.ucla.edu/~twng/storage/Hidden%20Markov%20Models%20for%20Time%20Series%20An%20Introduction%20Using%20R.pdf\n: Hidden Markov Models for Time Series - An Introduction Using R (UCLA)\n\n\n* [http://www.eecs.qmul.ac.uk/~norman/BBNs/BBNs.htm Making Sense of Probability: Fallacies, Myths and Puzzles]\n\n* [http://research.microsoft.com/apps/pubs/default.aspx?id=69588 \"A Tutorial on Learning With Bayesian Networks\", David Heckerman, March 1995, MSR-TR-95-06]\n\n* HTK [http://htk.eng.cam.ac.uk/register.php]\n\n* GHMM Library (General Hidden Markov Model) Library - a freely available C library [http://ghmm.sourceforge.net/index.html]\n\n* http://www.tjdb.org/123456789/7390/1/IJRSP%2039(1)%2039-44.pdf\n: Higher order Markov chain models for monsoon rainfall over West Bengal, India','utf-8');
/*!40000 ALTER TABLE `radiohead_text` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `radiohead_transcache`
--

DROP TABLE IF EXISTS `radiohead_transcache`;
