INSERT INTO `mw_text` VALUES (1748,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013 PATENT-BRIAN-2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013 PATENT-BRIAN-2013-002]]\n\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n==== Memo ====\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n=== # 요약 ===\n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n\n\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n* System and Method for Analyzing Data Records\n: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n* Scalable user clustering based on set similarity\n: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n* Google Search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google Search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010] [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1749,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013 PATENT-BRIAN-2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013 PATENT-BRIAN-2013-002]]\n\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n==== Memo ====\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n=== # 요약 ===\n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n\n\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n* System and Method for Analyzing Data Records\n: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n* Scalable user clustering based on set similarity\n: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n* Google Search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google Search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1750,'== Lessons learned ==\n\n* \"An Analysis of Traces from a Production MapReduce Cluster,\" CCGRID 2010\n\n* CCGRID 2010에 소개되었던 CMU의 연구인 [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5493490 An Analysis of Traces from a Production MapReduce Cluster // Kavulya, S. CMU // CCGRID 2010]에는 Yahoo!의 M45 Supercomputing Cluster에서의 10달 간의 MapReduce tracelog에 대한 분석 결과가 있다. 이를 보면, task type과 failure 와의 연관성이 있음을 알 수 있다.\n\n* Failure와 straggler 간의 관계는 다음과 같다. straggler는 아직 명확한 exception이 일어나서 task가 명시적인 failure로 mark가 된 상태는 아니지만, 통상적인 경우보다 꽤 느리게 task가 수행되고 있는 상태를 의미한다.\n\n\n== Excerpts from paper ==\n\n{| class=\"wikitable\" border=\"1\" \n|+ Summary M45 dataset\n! Key\n! Value\n|-\n| Log Period\n|\n: Apr 25 - Nov 12, 2008\n: Jan 19 - Apr 24, 2009\n|-\n| Hadoop versions\n|\n: 0.16: Apr 2008 -\n: 0.17: Jun 2008 -\n: 0.18: Jan 2009 -\n|-\n| Number of active users\n| 31\n|-\n| Number of jobs\n| 171079\n|-\n| Successful jobs\n| 165948 (97%)\n|-\n| Failed jobs\n| 4100 (2.4%)\n|-\n| Cancelled jobs\n| 1031 (0.6%)\n|-\n| Average maps per job\n| 154 +/- 558 sigma\n|-\n| Average reduces per job\n| 19 +/- 145 sigma\n|-\n| Average nodes per job\n| 27 +/- 22 sigma\n|-\n| Maximum nodes per job\n| 299\n|-\n| Average job duration\n| 1214 +/- 13875 seconds\n|-\n| Maximum job duration\n| 6.84 days\n|-\n| Node days used\n| 132624\n|}\n\n\n* There were 4100 failed jobs and 1031 incomplete jobs in our dataset. These failures accounted for 3% of the total jobs run.\n\n\n* We observed that:\n# <span style=\"color:red\">\'\'\'Most jobs fail within 150 seconds after the first aborted task\'\'\'</span>. Figure 3(c) shows that 90% of jobs exhibited an error latency of less than 150 seconds from the first aborted task to the last retry of that task (the default number of retries for aborted tasks was 4). We observed a maximum error latency of 4.3 days due to a copy failure in a single reduce task. Better diagnosis and recovery approaches are needed to reduce error latency in long-running tasks.\n# Most failures occurred during the map phase. Failures due to task exceptions in the <span style=\"color:red\">\'\'\'map phase\'\'\'</span> were the most prevalent - 36% of these failures were due to <span style=\"color:red\">\'\'\'array indexing errors\'\'\'</span> (see Figure 4). <span style=\"color:red\">\'\'\'IO exceptions\'\'\'</span> were common in the <span style=\"color:red\">\'\'\'reduce phase\'\'\'</span> accounting for 23% of failures. Configuration problems, such as missing files, led to failures during job initialization.\n# Application hangs and node failures were more prevalent in cancelled jobs. Task timeouts, which occur when a task hangs and fails to report progress for more than 10 minutes, and lost TaskTracker daemons due to node or process failures were more prevalent in cancelled jobs than in failed jobs.\n# Spurious exceptions exist in the logs. The logs contained spurious exceptions due to debugging statements that were turned on by default in certain versions of Hadoop. For example, a debug exception to troubleshoot a problem in the DFS client attributed to data buffering, and an exception due to a disabled feature that ignored the nonzero exit codes in Hadoop streaming, accounted for 90% of exceptions from January 21 to February 18, 2009. This motivates the need for error-log analysis tools that highlight important exceptions to users','utf-8'),(1751,'== Lessons learned ==','utf-8'),(1752,'#REDIRECT [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]','utf-8'),(1753,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013 PATENT-BRIAN-2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013 PATENT-BRIAN-2013-002]]\n\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n==== Memo ====\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n=== # 요약 ===\n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n\n\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n* System and Method for Analyzing Data Records\n: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n* Scalable user clustering based on set similarity\n: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n* Google patent search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google patent search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8');
/*!40000 ALTER TABLE `mw_text` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mw_trackbacks`
--

DROP TABLE IF EXISTS `mw_trackbacks`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
