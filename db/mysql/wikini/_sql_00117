INSERT INTO `mw_text` VALUES (1748,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013 PATENT-BRIAN-2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013 PATENT-BRIAN-2013-002]]\n\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n==== Memo ====\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n=== # 요약 ===\n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n\n\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n* System and Method for Analyzing Data Records\n: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n* Scalable user clustering based on set similarity\n: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n* Google Search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google Search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010] [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1749,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013 PATENT-BRIAN-2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013 PATENT-BRIAN-2013-002]]\n\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n==== Memo ====\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n=== # 요약 ===\n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n\n\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n* System and Method for Analyzing Data Records\n: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n* Scalable user clustering based on set similarity\n: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n* Google Search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google Search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1750,'== Lessons learned ==\n\n* \"An Analysis of Traces from a Production MapReduce Cluster,\" CCGRID 2010\n\n* CCGRID 2010에 소개되었던 CMU의 연구인 [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5493490 An Analysis of Traces from a Production MapReduce Cluster // Kavulya, S. CMU // CCGRID 2010]에는 Yahoo!의 M45 Supercomputing Cluster에서의 10달 간의 MapReduce tracelog에 대한 분석 결과가 있다. 이를 보면, task type과 failure 와의 연관성이 있음을 알 수 있다.\n\n* Failure와 straggler 간의 관계는 다음과 같다. straggler는 아직 명확한 exception이 일어나서 task가 명시적인 failure로 mark가 된 상태는 아니지만, 통상적인 경우보다 꽤 느리게 task가 수행되고 있는 상태를 의미한다.\n\n\n== Excerpts from paper ==\n\n{| class=\"wikitable\" border=\"1\" \n|+ Summary M45 dataset\n! Key\n! Value\n|-\n| Log Period\n|\n: Apr 25 - Nov 12, 2008\n: Jan 19 - Apr 24, 2009\n|-\n| Hadoop versions\n|\n: 0.16: Apr 2008 -\n: 0.17: Jun 2008 -\n: 0.18: Jan 2009 -\n|-\n| Number of active users\n| 31\n|-\n| Number of jobs\n| 171079\n|-\n| Successful jobs\n| 165948 (97%)\n|-\n| Failed jobs\n| 4100 (2.4%)\n|-\n| Cancelled jobs\n| 1031 (0.6%)\n|-\n| Average maps per job\n| 154 +/- 558 sigma\n|-\n| Average reduces per job\n| 19 +/- 145 sigma\n|-\n| Average nodes per job\n| 27 +/- 22 sigma\n|-\n| Maximum nodes per job\n| 299\n|-\n| Average job duration\n| 1214 +/- 13875 seconds\n|-\n| Maximum job duration\n| 6.84 days\n|-\n| Node days used\n| 132624\n|}\n\n\n* There were 4100 failed jobs and 1031 incomplete jobs in our dataset. These failures accounted for 3% of the total jobs run.\n\n\n* We observed that:\n# <span style=\"color:red\">\'\'\'Most jobs fail within 150 seconds after the first aborted task\'\'\'</span>. Figure 3(c) shows that 90% of jobs exhibited an error latency of less than 150 seconds from the first aborted task to the last retry of that task (the default number of retries for aborted tasks was 4). We observed a maximum error latency of 4.3 days due to a copy failure in a single reduce task. Better diagnosis and recovery approaches are needed to reduce error latency in long-running tasks.\n# Most failures occurred during the map phase. Failures due to task exceptions in the <span style=\"color:red\">\'\'\'map phase\'\'\'</span> were the most prevalent - 36% of these failures were due to <span style=\"color:red\">\'\'\'array indexing errors\'\'\'</span> (see Figure 4). <span style=\"color:red\">\'\'\'IO exceptions\'\'\'</span> were common in the <span style=\"color:red\">\'\'\'reduce phase\'\'\'</span> accounting for 23% of failures. Configuration problems, such as missing files, led to failures during job initialization.\n# Application hangs and node failures were more prevalent in cancelled jobs. Task timeouts, which occur when a task hangs and fails to report progress for more than 10 minutes, and lost TaskTracker daemons due to node or process failures were more prevalent in cancelled jobs than in failed jobs.\n# Spurious exceptions exist in the logs. The logs contained spurious exceptions due to debugging statements that were turned on by default in certain versions of Hadoop. For example, a debug exception to troubleshoot a problem in the DFS client attributed to data buffering, and an exception due to a disabled feature that ignored the nonzero exit codes in Hadoop streaming, accounted for 90% of exceptions from January 21 to February 18, 2009. This motivates the need for error-log analysis tools that highlight important exceptions to users','utf-8'),(1751,'== Lessons learned ==','utf-8'),(1752,'#REDIRECT [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]','utf-8'),(1753,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013 PATENT-BRIAN-2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013 PATENT-BRIAN-2013-002]]\n\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n==== Memo ====\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n=== # 요약 ===\n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n\n\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n* System and Method for Analyzing Data Records\n: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n* Scalable user clustering based on set similarity\n: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n* Google patent search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google patent search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1754,'== ## bNote-2013-04-08 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then \'\'\'one replica\'\'\' of that block is allocated on the same machine on which the client is running. \'\'\'The second replica\'\'\' is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. \'\'\'The third replica\'\'\' is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1755,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then \'\'\'one replica\'\'\' of that block is allocated on the same machine on which the client is running. \'\'\'The second replica\'\'\' is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. \'\'\'The third replica\'\'\' is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1756,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013 PATENT-BRIAN-2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013 PATENT-BRIAN-2013-002]]\n\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n==== Memo ====\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n=== # 요약 ===\n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n\n\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n* Google patent search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google patent search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1757,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013 PATENT-BRIAN-2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013 PATENT-BRIAN-2013-002]]\n\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n==== Memo ====\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n=== # 요약 ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n\n\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n* Google patent search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google patent search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1758,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013 PATENT-BRIAN-2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013 PATENT-BRIAN-2013-002]]\n\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n\n\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n* Google patent search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google patent search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1759,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:orange\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:orange\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:orange\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1760,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:violet\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:violet\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:violet\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1761,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1762,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013 PATENT-BRIAN-2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013 PATENT-BRIAN-2013-002]]\n\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. 첫 번째, speculative execution으로 인한 additional data copy는 근본적으로 피할 수 없는 overhead임. 두 번째, additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 찾는 것이 쉽지 않을 수도 있음. 게다가 적절한 node가 찾아질 때 까지 기다리는 것 자체가 job completion time을 늘이게 되는 것이므로 또한 부담임.\n\n\n* straggler가 발생한 후에 대응하기 보다는, HDFS에 처음 data가 놓여질 때부터 straggler-immune 될 수 있도록 {node, task-type -> failure rate} tuple을 고려한 replica data 배치를 통해, speculative data copy overhead를 없애거나 최소화할 수 있게 하자는 것이 본 발명의 목표임. (기존 HDFS replica management는 이러한 straggler-immune data placement는 고려되어 있지 않음)\n\n\n* 본 발명의 구현 용이성. Apache Hadoop Jira HDFS-385에서 제공하는 framework을 통해 replica-management 알고리즘을 쉽게 HDFS에 추가할 수 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n* Google patent search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google patent search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1763,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013 PATENT-BRIAN-2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013 PATENT-BRIAN-2013-002]]\n\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. 첫 번째, speculative execution으로 인한 additional data copy는 근본적으로 피할 수 없는 overhead임. 두 번째, additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 찾는 것이 쉽지 않을 수도 있음. 게다가 적절한 node가 찾아질 때 까지 기다리는 것 자체가 job completion time을 늘이게 되는 것이므로 또한 부담임.\n\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n* Google patent search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google patent search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1764,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013 PATENT-BRIAN-2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013 PATENT-BRIAN-2013-002]]\n\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. 첫 번째, speculative execution으로 인한 additional data copy는 근본적으로 피할 수 없는 overhead임. 두 번째, additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 찾는 것이 쉽지 않을 수도 있음. 게다가 적절한 node가 찾아질 때 까지 기다리는 것 자체가 job completion time을 늘이게 되는 것이므로 또한 부담임.\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* straggler가 발생한 후에 대응하기 보다는, HDFS에 처음 data가 놓여질 때부터 straggler-immune 될 수 있도록 {node, task-type -> failure rate} tuple을 고려한 replica data 배치를 통해, speculative data copy overhead를 없애거나 최소화할 수 있게 하자는 것이 본 발명의 목표임. (기존 HDFS replica management는 이러한 straggler-immune data placement는 고려되어 있지 않음)\n\n\n* 본 발명의 구현 용이성. Apache Hadoop Jira HDFS-385에서 제공하는 framework을 통해 replica-management 알고리즘을 쉽게 HDFS에 추가할 수 있음.\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n* Google patent search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google patent search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1765,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013 PATENT-BRIAN-2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013 PATENT-BRIAN-2013-002]]\n\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. 첫 번째, speculative execution으로 인한 additional data copy는 근본적으로 피할 수 없는 overhead임. 두 번째, additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 찾는 것이 쉽지 않을 수도 있음. 게다가 적절한 node가 찾아질 때 까지 기다리는 것 자체가 job completion time을 늘이게 되는 것이므로 또한 부담임.\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* straggler가 발생한 후에 대응하기 보다는, HDFS에 처음 data가 놓여질 때부터 straggler-immune 될 수 있도록 {node, task-type -> failure rate} tuple을 고려한 replica data 배치를 통해, speculative data copy overhead를 없애거나 최소화할 수 있게 하자는 것이 본 발명의 목표임. (기존 HDFS replica management는 이러한 straggler-immune data placement는 고려되어 있지 않음)\n\n\n* 본 발명의 구현 용이성. Apache Hadoop Jira HDFS-385에서 제공하는 framework을 통해 replica-management 알고리즘을 쉽게 HDFS에 추가할 수 있음.\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1766,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013 PATENT-BRIAN-2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013 PATENT-BRIAN-2013-002]]\n\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. 첫 번째, speculative execution으로 인한 additional data copy는 근본적으로 피할 수 없는 overhead임. 두 번째, additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* straggler가 발생한 후에 대응하기 보다는, HDFS에 처음 data가 놓여질 때부터 straggler-immune 될 수 있도록 {node, task-type -> failure rate} tuple을 고려한 replica data 배치를 통해, speculative data copy overhead를 없애거나 최소화할 수 있게 하자는 것이 본 발명의 목표임. (기존 HDFS replica management는 이러한 straggler-immune data placement는 고려되어 있지 않음)\n\n\n* 본 발명의 구현 용이성. Apache Hadoop Jira HDFS-385에서 제공하는 framework을 통해 replica-management 알고리즘을 쉽게 HDFS에 추가할 수 있음.\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1767,'#REDIRECT [[Bnote patidea 2013-001]]','utf-8'),(1768,'#REDIRECT [[Bnote patidea 2013-002]]','utf-8'),(1769,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013 PATENT-BRIAN-2013-002]]\n\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. 첫 번째, speculative execution으로 인한 additional data copy는 근본적으로 피할 수 없는 overhead임. 두 번째, additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* straggler가 발생한 후에 대응하기 보다는, HDFS에 처음 data가 놓여질 때부터 straggler-immune 될 수 있도록 {node, task-type -> failure rate} tuple을 고려한 replica data 배치를 통해, speculative data copy overhead를 없애거나 최소화할 수 있게 하자는 것이 본 발명의 목표임. (기존 HDFS replica management는 이러한 straggler-immune data placement는 고려되어 있지 않음)\n\n\n* 본 발명의 구현 용이성. Apache Hadoop Jira HDFS-385에서 제공하는 framework을 통해 replica-management 알고리즘을 쉽게 HDFS에 추가할 수 있음.\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1770,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. 첫 번째, speculative execution으로 인한 additional data copy는 근본적으로 피할 수 없는 overhead임. 두 번째, additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* straggler가 발생한 후에 대응하기 보다는, HDFS에 처음 data가 놓여질 때부터 straggler-immune 될 수 있도록 {node, task-type -> failure rate} tuple을 고려한 replica data 배치를 통해, speculative data copy overhead를 없애거나 최소화할 수 있게 하자는 것이 본 발명의 목표임. (기존 HDFS replica management는 이러한 straggler-immune data placement는 고려되어 있지 않음)\n\n\n* 본 발명의 구현 용이성. Apache Hadoop Jira HDFS-385에서 제공하는 framework을 통해 replica-management 알고리즘을 쉽게 HDFS에 추가할 수 있음.\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1771,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n=== 과제 목표 평가 방법 객관화/구체화 ===\n\n* 데이터 Access 성능 목표 달성 기준\n: Realistic enterprise workload (fileserver) 인가 시,\n: Data placement 기술이 적용되지 않은 vanilla storage system 대비\n: 80% (IOPS) 성능 향상\n\n\n\n* Workload 인가 방안\n:- 대표적인 enterprise workload로 성능 benchmark 필요\n::- Case A) [최선] real enterprise workload 사용\n::- Case B) [차선] enterprise workload에 대한 real trace에 기반한 real workload replay 활용\n::- Case C) [차차선] filebench 등과 같이 real workload를 모방한 realistic synthesized workload 사용\n::- Case D) [차차차선] (비추천), iozone 등과 같은 random/sequential read/write I/O pattern 생성기 사용\n\n\n\n* Workload 인가 방식에 따른 환경 구성\n:- Case A: TPC-C 혹은 filebench를 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n:- Case B: real workload replay 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n\n* Storage system interface exposed\n: SAN, NAS 방식 중 결정 필요\n\n\n* 성능 측정 방법\n:- 1-day (case A), 1-week (case B) 동안 workload를 인가하여 계산된 IOPS로 판별\n::- TPC-C, filebench의 경우 (물론 iozone 역시) total IOPS metric을 계산하여 benchmark 결과로서 제공\n::- Realistic workload generator의 경우, total IOPS metric를 계산할 수 있도록 구현 필요\n::- Real enterprise workload 인가 시,\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1772,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n=== 과제 목표 평가 방법 객관화/구체화 ===\n\n* 데이터 Access 성능 목표 달성 기준\n: Realistic enterprise workload (fileserver) 인가 시,\n: Data placement 기술이 적용되지 않은 vanilla storage system 대비\n: 80% (IOPS) 성능 향상\n\n\n\n* Workload 인가 방안\n:- 대표적인 enterprise workload로 성능 benchmark 필요\n::- Case A) [최선] real enterprise workload 사용\n::- Case B) [차선] enterprise workload에 대한 real trace에 기반한 real workload replay 활용\n::- Case C) [차차선] TPC-C, filebench 등과 같이 real workload를 모방한 realistic synthesized workload 사용\n::- Case D) [차차차선] (비추천), iozone 등과 같은 random/sequential read/write I/O pattern 생성기 사용\n\n\n\n* Workload 인가 방식에 따른 환경 구성\n\n:- Case A: real workload 인가 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n:- Case B: real workload replay 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n:- Case C: TPC-C 혹은 filebench를 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n\n\n* Storage system interface exposed\n: SAN, NAS 방식 중 결정 필요\n\n\n* 성능 측정 방법\n:- 1-day (case A), 1-week (case B) 동안 workload를 인가하여 계산된 IOPS로 판별\n::- TPC-C, filebench의 경우 (물론 iozone 역시) total IOPS metric을 계산하여 benchmark 결과로서 제공\n::- Realistic workload generator의 경우, total IOPS metric를 계산할 수 있도록 구현 필요\n::- Real enterprise workload 인가 시,\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1773,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n=== 과제 목표 평가 방법 객관화/구체화 ===\n\n* 데이터 Access 성능 목표 달성 기준\n: Realistic enterprise workload (fileserver) 인가 시,\n: Data placement 기술이 적용되지 않은 vanilla storage system 대비\n: 80% (IOPS) 성능 향상\n\n\n\n* Workload 인가 방안\n:- 대표적인 enterprise workload로 성능 benchmark 필요\n::- Case A) [최선] real enterprise workload 사용\n::- Case B) [차선] enterprise workload에 대한 real trace에 기반한 real workload replay 활용\n::- Case C) [차차선] TPC-C, filebench 등과 같이 real workload를 모방한 realistic synthesized workload 사용\n::- Case D) [차차차선] (비추천), iozone 등과 같은 random/sequential read/write I/O pattern 생성기 사용\n\n\n\n* Workload 인가 방식에 따른 환경 구성\n: Case A, B, C 모두 storage system과 network 구성은 동일하며, client 구성 방식에서 차이 존재\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n:- Case A: real workload 인가 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case B: real workload replay 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case C: TPC-C 혹은 filebench를 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n\n\n* Storage system interface exposed\n: SAN, NAS 방식 중 결정 필요\n\n\n\n* 성능 측정 방법\n:- 1-day (case A), 1-week (case B) 동안 workload를 인가하여 계산된 IOPS로 판별\n::- TPC-C, filebench의 경우 (물론 iozone 역시) total IOPS metric을 계산하여 benchmark 결과로서 제공\n::- Realistic workload generator의 경우, total IOPS metric를 계산할 수 있도록 구현 필요\n::- Real enterprise workload 인가 시,\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1774,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n=== 과제 목표 평가 방법 객관화/구체화 ===\n\n* 데이터 Access 성능 목표 달성 기준\n: Realistic enterprise workload (fileserver) 인가 시,\n: Data placement 기술이 적용되지 않은 vanilla storage system 대비\n: 80% (IOPS) 성능 향상\n\n\n* Workload type\n: 대표적인 enterprise workload type인 DB, FileServer, Web 중 택일하거나\n: Scientific Computing (R, Matlab, Hadoop 등)을 선택할 수 있겠음\n: VDI workload를 고려할지에 대해서는 추가 고민 필요 (아직까지는 고려하지 않고 있음)\n\n\n* Workload 인가 방안\n:- 대표적인 enterprise workload로 성능 benchmark 필요\n::- Case A) [최선] real enterprise workload 사용\n::- Case B) [차선] enterprise workload에 대한 real trace에 기반한 real workload replay 활용\n::- Case C) [차차선] TPC-C, filebench 등과 같이 real workload를 모방한 realistic synthesized workload 사용\n::- Case D) [차차차선] (비추천), iozone 등과 같은 random/sequential read/write I/O pattern 생성기 사용\n\n\n\n* Workload 인가 방식에 따른 환경 구성\n: Case A, B, C 모두 storage system과 network 구성은 동일하며, client 구성 방식에서 차이 존재\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n:- Case A: real workload 인가 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case B: real workload replay 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case C: TPC-C 혹은 filebench를 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n\n* Storage system interface exposed\n: SAN, NAS 방식 중 결정 필요\n\n\n* 가상화 이슈\n: 기본적으로 Hypervisor (Xen, VMware 등)를 사용 하지 않는 환경으로 구성\n\n\n* 성능 측정 방법\n:- 1-day (case A), 1-week (case B) 동안 workload를 인가하여 계산된 IOPS로 판별\n:- 가급적 경쟁사 제품의 성능측정 tool에서 제공하는 metric과 동일하게 가는 것이 좋을 것으로 보임\n::- TPC-C, filebench의 경우 (물론 iozone 역시) total IOPS metric을 계산하여 benchmark 결과로서 제공\n::- Realistic workload generator의 경우, total IOPS metric를 계산할 수 있도록 구현 필요\n::- Real enterprise workload 인가 시, iostat, sar 등 별도의 성능 측정 모듈 구동 필요\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1775,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n=== 과제 목표 평가 방법 객관화/구체화 ===\n\n* 데이터 Access 성능 목표 달성 기준\n: Realistic enterprise workload (fileserver) 인가 시,\n: Data placement 기술이 적용되지 않은 vanilla storage system 대비\n: 80% (IOPS) 성능 향상\n\n\n* Workload type\n: 대표적인 enterprise workload type인 DB, FileServer, Web 중 택일하거나\n: Scientific Computing (R, Matlab, Hadoop 등)을 선택할 수 있겠음\n: VDI workload를 고려할지에 대해서는 추가 고민 필요 (아직까지는 고려하지 않고 있음)\n\n\n* Workload 인가 방안\n:- 대표적인 enterprise workload로 성능 benchmark 필요\n::- Case A) [최선] real enterprise workload 사용\n::- Case B) [차선] enterprise workload에 대한 real trace에 기반한 real workload replay 활용 (workload replayer 구현 필요)\n::- Case C) [차차선] TPC-C, filebench 등과 같이 real workload를 모방한 realistic synthesized workload 사용\n::- Case D) [차차차선] (비추천), iozone 등과 같은 random/sequential read/write I/O pattern 생성기 사용\n\n\n\n* Workload 인가 방식에 따른 환경 구성\n: Case A, B, C 모두 storage system과 network 구성은 동일하며, client 구성 방식에서 차이 존재\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n:- Case A: real workload 인가 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case B: real workload replay 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case C: TPC-C 혹은 filebench를 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n\n* Storage system interface exposed\n: SAN, NAS 방식 중 결정 필요\n\n\n* 가상화 이슈\n: 기본적으로 Hypervisor (Xen, VMware 등)를 사용 하지 않는 환경으로 구성\n\n\n* 성능 측정 방법\n:- 1-day (case A), 1-week (case B) 동안 workload를 인가하여 계산된 IOPS로 판별\n:- 가급적 경쟁사 제품의 성능측정 tool에서 제공하는 metric과 동일하게 가는 것이 좋을 것으로 보임\n::- Real enterprise workload 인가 시, iostat, sar 등 별도의 성능 측정 모듈 구동 필요\n::- Real workload replayer의 경우, total IOPS metric를 계산할 수 있도록 구현 필요\n::- TPC-C, filebench의 경우 (물론 iozone 역시) total IOPS metric을 계산하여 benchmark 결과로서 제공\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1776,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n=== 과제 목표 평가 방법 객관화/구체화 ===\n\n* 데이터 Access 성능 목표 달성 기준\n: Realistic enterprise workload (아래 workload들 중 택일) 인가 시,\n: Data placement 및 RACS-1 기술이 적용되지 않은 vanilla SW-RAID 대비\n: 80% 속도 향상 (in IOPS)\n:: (주1) metric에 대한 부분인 \"SW-RAID 대비 속도 향상 비율: 80%\"은 이대로 MBO에 명시가 되어있어서 수정이 어렵다고 생각하여 그대로 두었습니다.\n:: (주2) 다만, 속도를 향상시키는 요소 기술은 RACS-1 뿐만 아니라 Data Placement에 가능하겠으므로, 이를 염두에 두고 아래 내용을 작성하였습니다.\n\n\n* Workload type\n: 대표적인 enterprise workload type인 DB, FileServer, Web 중 택일하거나\n: Scientific Computing (R, Matlab, Hadoop 등)을 선택할 수 있겠음\n: VDI workload를 고려할지에 대해서는 추가 고민 필요 (아직까지는 고려하지 않고 있음)\n\n\n* Workload 인가 방안\n:- 대표적인 enterprise workload로 성능 benchmark 필요\n::- Case A) [최선] real enterprise workload 사용\n::- Case B) [차선] enterprise workload에 대한 real trace에 기반한 real workload replay 활용 (workload replayer 구현 필요)\n::- Case C) [차차선] TPC-C, filebench 등과 같이 real workload를 모방한 realistic synthesized workload 사용\n::- Case D) [차차차선] (비추천), iozone 등과 같은 random/sequential read/write I/O pattern 생성기 사용\n\n\n\n* Workload 인가 방식에 따른 환경 구성\n: Case A, B, C 모두 storage system과 network 구성은 동일하며, client 구성 방식에서 차이 존재\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n:- Case A: real workload 인가 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case B: real workload replay 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case C: TPC-C 혹은 filebench를 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n\n* Storage system interface exposed\n: SAN, NAS 방식 중 결정 필요\n\n\n* 가상화 이슈\n: 기본적으로 Hypervisor (Xen, VMware 등)를 사용 하지 않는 환경으로 구성\n\n\n* 성능 측정 방법\n:- 1-day (case A), 1-week (case B) 동안 workload를 인가하여 계산된 IOPS로 판별\n:- 가급적 경쟁사 제품의 성능측정 tool에서 제공하는 metric과 동일하게 가는 것이 좋을 것으로 보임\n::- Real enterprise workload 인가 시, iostat, sar 등 별도의 성능 측정 모듈 구동 필요\n::- Real workload replayer의 경우, total IOPS metric를 계산할 수 있도록 구현 필요\n::- TPC-C, filebench의 경우 (물론 iozone 역시) total IOPS metric을 계산하여 benchmark 결과로서 제공\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1777,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n=== 과제 목표 평가 방법 객관화/구체화 ===\n\n* 데이터 Access 성능 목표 달성 기준\n: Realistic enterprise workload 인가 시, (*)\n: Data placement 및 RACS-1 기술이 적용되지 않은 vanilla SW-RAID 대비\n: 80% 속도 향상 (in IOPS) (**)\n:: (*) Realistic enterprise workload라고 했을 때, 아래 workload 인가 방안의 Case A,B,C가 해당될 수 있겠다고 보고 있으며, \'13년 9월 쯤을 고려했을 때, 현실적으로는 Case C로 가고, workload type은 DB 혹은 fileserver 쪽으로 가는 것을 생각하고 있습니다. (\'14년 정도에는 Case B나 Case A도 생각해볼 수 있을 것 같습니다)\n:: (**) metric에 대한 부분인 \"SW-RAID 대비 속도 향상 비율: 80%\"은 이대로 MBO에 명시가 되어있어서 수정이 어렵다고 생각하여 그대로 두었습니다.  다만, 속도를 향상시키는 요소 기술은 RACS-1 뿐만 아니라 Data Placement에 가능하겠으므로, 이를 염두에 두고 아래 내용을 작성하였습니다.\n\n\n* Workload type\n: 대표적인 enterprise workload type인 DB, FileServer, Web 중 택일하거나\n: Scientific Computing (R, Matlab, Hadoop 등)을 선택할 수 있겠음\n: VDI workload를 고려할지에 대해서는 추가 고민 필요 (아직까지는 고려하지 않고 있음)\n\n\n* Workload 인가 방안\n:- 대표적인 enterprise workload로 성능 benchmark 필요\n::- Case A) [최선] real enterprise workload 사용\n::- Case B) [차선] enterprise workload에 대한 real trace에 기반한 real workload replay 활용 (workload replayer 구현 필요)\n::- Case C) [차차선] TPC-C, filebench 등과 같이 real workload를 모방한 realistic synthesized workload 사용\n::- Case D) [차차차선] (비추천), iozone 등과 같은 random/sequential read/write I/O pattern 생성기 사용\n\n\n\n* Workload 인가 방식에 따른 환경 구성\n: Case A, B, C 모두 storage system과 network 구성은 동일하며, client 구성 방식에서 차이 존재\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n:- Case A: real workload 인가 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case B: real workload replay 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case C: TPC-C 혹은 filebench를 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n\n* Storage system interface exposed\n: SAN, NAS 방식 중 결정 필요\n\n\n* 가상화 이슈\n: 기본적으로 Hypervisor (Xen, VMware 등)를 사용 하지 않는 환경으로 구성\n\n\n* 성능 측정 방법\n:- 1-day (case A), 1-week (case B) 동안 workload를 인가하여 계산된 IOPS로 판별\n:- 가급적 경쟁사 제품의 성능측정 tool에서 제공하는 metric과 동일하게 가는 것이 좋을 것으로 보임\n::- Real enterprise workload 인가 시, iostat, sar 등 별도의 성능 측정 모듈 구동 필요\n::- Real workload replayer의 경우, total IOPS metric를 계산할 수 있도록 구현 필요\n::- TPC-C, filebench의 경우 (물론 iozone 역시) total IOPS metric을 계산하여 benchmark 결과로서 제공\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1778,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n=== 과제 목표 평가 방법 객관화/구체화 ===\n\n* 데이터 Access 성능 목표 달성 기준\n: Realistic enterprise workload 인가 시, (*)\n: Data placement 및 RACS-1 기술이 적용되지 않은 vanilla SW-RAID 대비\n: 80% 속도 향상 (in IOPS) (**)\n:: (*) Realistic enterprise workload라고 했을 때, 아래 workload 인가 방안의 Case A,B,C가 해당될 수 있겠다고 보고 있으며, \'13년 9월 쯤을 고려했을 때, 현실적으로는 Case C로 가고, workload type은 DB 혹은 fileserver 쪽으로 가는 것을 생각하고 있습니다. (\'14년 정도에는 Case B나 Case A도 생각해볼 수 있을 것 같습니다)\n:: (**) metric에 대한 부분인 \"SW-RAID 대비 속도 향상 비율: 80%\"은 MBO에 이대로 명시가 되어 수정이 어렵다고 생각하여 그대로 두었습니다.  다만, 속도를 향상시키는 요소 기술로서 동작하는 것은 RACS-1 뿐만 아니라 Data Placement도 가능하겠으므로, 이를 염두에 두고 아래 내용을 작성하였습니다.\n\n\n* Workload type\n: 대표적인 enterprise workload type인 DB, FileServer, Web 중 택일하거나\n: Scientific Computing (R, Matlab, Hadoop 등)을 선택할 수 있겠음\n: VDI workload를 고려할지에 대해서는 추가 고민 필요 (아직까지는 고려하지 않고 있음)\n\n\n* Workload 인가 방안\n:- 대표적인 enterprise workload로 성능 benchmark 필요\n::- Case A) [최선] real enterprise workload 사용\n::- Case B) [차선] enterprise workload에 대한 real trace에 기반한 real workload replay 활용 (workload replayer 구현 필요)\n::- Case C) [차차선] TPC-C, filebench 등과 같이 real workload를 모방한 realistic synthesized workload 사용\n::- Case D) [차차차선] (비추천), iozone 등과 같은 random/sequential read/write I/O pattern 생성기 사용\n\n\n\n* Workload 인가 방식에 따른 환경 구성\n: Case A, B, C 모두 storage system과 network 구성은 동일하며, client 구성 방식에서 차이 존재\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n:- Case A: real workload 인가 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case B: real workload replay 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case C: TPC-C 혹은 filebench를 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n\n* Storage system interface exposed\n: SAN, NAS 방식 중 결정 필요\n\n\n* 가상화 이슈\n: 기본적으로 Hypervisor (Xen, VMware 등)를 사용 하지 않는 환경으로 구성\n\n\n* 성능 측정 방법\n:- 1-day (case A), 1-week (case B) 동안 workload를 인가하여 계산된 IOPS로 판별\n:- 가급적 경쟁사 제품의 성능측정 tool에서 제공하는 metric과 동일하게 가는 것이 좋을 것으로 보임\n::- Real enterprise workload 인가 시, iostat, sar 등 별도의 성능 측정 모듈 구동 필요\n::- Real workload replayer의 경우, total IOPS metric를 계산할 수 있도록 구현 필요\n::- TPC-C, filebench의 경우 (물론 iozone 역시) total IOPS metric을 계산하여 benchmark 결과로서 제공\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1779,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n-\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* straggler가 발생한 후에 대응하기 보다는, HDFS에 처음 data가 놓여질 때부터 straggler-immune 될 수 있도록 {node, task-type -> failure rate} tuple을 고려한 replica data 배치를 통해, speculative data copy overhead를 없애거나 최소화할 수 있게 하자는 것이 본 발명의 목표임. (기존 HDFS replica management는 이러한 straggler-immune data placement는 고려되어 있지 않음)\n\n\n* 본 발명의 구현 용이성. Apache Hadoop Jira HDFS-385에서 제공하는 framework을 통해 replica-management 알고리즘을 쉽게 HDFS에 추가할 수 있음.\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1780,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* straggler가 발생한 후에 대응하기 보다는, HDFS에 처음 data가 놓여질 때부터 straggler-immune 될 수 있도록 {node, task-type -> failure rate} tuple을 고려한 replica data 배치를 통해, speculative data copy overhead를 없애거나 최소화할 수 있게 하자는 것이 본 발명의 목표임. (기존 HDFS replica management는 이러한 straggler-immune data placement는 고려되어 있지 않음)\n\n\n* 본 발명의 구현 용이성. Apache Hadoop Jira HDFS-385에서 제공하는 framework을 통해 replica-management 알고리즘을 쉽게 HDFS에 추가할 수 있음.\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1781,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* straggler가 발생한 후에 대응하기 보다는, HDFS에 처음 data가 놓여질 때부터 straggler-immune 될 수 있도록 {node, task-type -> failure rate} tuple을 고려한 replica data 배치를 통해, speculative data copy overhead를 없애거나 최소화할 수 있게 하자는 것이 본 발명의 목표임. (기존 HDFS replica management는 이러한 straggler-immune data placement는 고려되어 있지 않음)\n\n\n* 본 발명의 구현 용이성. Apache Hadoop Jira HDFS-385에서 제공하는 framework을 통해 replica-management 알고리즘을 쉽게 HDFS에 추가할 수 있음.\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1782,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* straggler가 발생한 후에 대응하기 보다는, HDFS에 처음 data가 놓여질 때부터 straggler-immune 될 수 있도록 {node, task-type -> failure rate} tuple을 고려한 replica data 배치를 통해, speculative data copy overhead를 없애거나 최소화할 수 있게 하자는 것이 본 발명의 목표임. (기존 HDFS replica management는 이러한 straggler-immune data placement는 고려되어 있지 않음)\n\n\n* 본 발명의 구현 용이성. Apache Hadoop Jira HDFS-385에서 제공하는 framework을 통해 replica-management 알고리즘을 쉽게 HDFS에 추가할 수 있음.\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8');
INSERT INTO `mw_text` VALUES (1783,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 사전 대응 방식\n: 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임 (기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음).\n\n* 기대 효과\n: speculative execution을 위한 data copy overhead 해소/최소화.\n\n* 구현의 용이성\n: Apache Hadoop Jira HDFS-385에서 제공하는 framework을 통해 replica-management 알고리즘을 쉽게 HDFS에 추가할 수 있음.\n\n* 침해 적발의 용이성\n:\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1784,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n: 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n: {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n: 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n: speculative execution을 위한 data copy overhead 감소\n: 제2, 제3의 straggler / failure 발생 확률 감소\n: 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n: Apache Hadoop Jira HDFS-385에서 제공하는 framework을 통해 replica-management 알고리즘을 쉽게 HDFS에 추가할 수 있음.\n\n\n* 침해 적발의 용이성\n: HDFS-385\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1785,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n: 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n: {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n: 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n: speculative execution을 위한 data copy overhead 감소\n: 제2, 제3의 straggler / failure 발생 확률 감소\n: 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n: Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n: namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1786,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n: Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:@ namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1787,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1788,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1789,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생으로 인한 추가 처리 비용을 감소시킬 수 있음\n\n:- Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨)\n::- 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커진다.\n\n:- Failed task의 경우, 두 가지 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법. 두 번째 옵션은 다른 node에서 task를 재시작하는 방법. task failure의 원인이 H/W 결함이나 시스템 S/W에 문제가 있는 것이 아니었거나 (물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알아낼 수 있어야 함), 이 task를 받아서 실행할 수 있는 여력을 가지고 있는 다른 node가 존재하지 않는 경우에 첫 번째 옵션을 선택할 수 있다. 그러나 해당 node에 H/W, S/W적인 결함이 있는 경우에는 반드시 다른 node를 찾아서 해당 task를 재시작할 수 있어야 한다. 그러나 만약 다른 node들도 execution slot의 여유가 없거나, H/W 혹은 S/W적으로 문제가 있는 상황이어서 해당 task의 재시작을 못하고 기다려야 하는 상황이라면, 그만큼 job 처리 속도가 저하되는 결과로 이어진다. 두 번째 옵션의 경우, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1790,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생으로 인한 추가 처리 비용을 감소시킬 수 있음\n\n:- Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 됨.\n\n:- Failed task의 경우, 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법. 두 번째 옵션은 다른 node에서 task를 재시작하는 방법. task failure의 원인이 H/W 결함이나 시스템 S/W에 문제가 있는 것이 아니었거나 (물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알아낼 수 있어야 함), 이 task를 받아서 실행할 수 있는 여력을 가지고 있는 다른 node가 존재하지 않는 경우에 첫 번째 옵션을 선택할 수 있다. 그러나 해당 node에 H/W, S/W적인 결함이 있는 경우에는 반드시 다른 node를 찾아서 해당 task를 재시작할 수 있어야 한다. 그러나 만약 다른 node들도 execution slot의 여유가 없거나, H/W 혹은 S/W적으로 문제가 있는 상황이어서 해당 task의 재시작을 못하고 기다려야 하는 상황이라면, 그만큼 job 처리 속도가 저하되는 결과로 이어진다. 두 번째 옵션의 경우, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n:- Anomaly-\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1791,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1792,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1793,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1794,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* Google patent search (5 results)\n (HDFS OR \"distributed file system\")((block replica) OR data ) placement\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1795,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1796,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보 update는 JobTracker에 추가되는 Bottleneck Risk Record \n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1797,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1798,'== PATENT-BRIAN-2013-001 ==\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n\n[[Bnote patidea 2013-001]]\n\n== PATENT-BRIAN-2013-002 ==\n=== # Low-computation-overhead and Memory-efficient Periodicity Detection Method ===\n[[Bnote patidea 2013-002]]\n\n== PATENT-BRIAN-2013-003 ==\n\n=== 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술 (A monitoring framework for detecting and analyzing execution bottlenecks of MapReduce jobs) ===\n\n=== # 요약 ===\n\n본 발명은 Hadoop MapReduce 기반의 분산 병렬 시스템의 성능 향상을 위해 이용할 수 있는 맵리듀스 작업의 병목 탐지 및 원인 분석을 위한 모니터링 프레임워크 기술에 대한 것임.\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce 기반의 분산 병렬 시스템 (Enterprise, Cloud 등 Data Center)\n\n\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* 본 발명이 속하는 기술 분야는 맵리듀스 작업을 위한 성능 모니터링 기술이다. 대용량 데이터 처리를 위한 맵리듀스 작업에서는 대량의 컴퓨팅 리소스를 이용하여 신속하게 작업을 처리하기 위해 하나의 작업을 다수의 태스크로 나누어 실행하게 된다. 이 때, 하나의 태스크라도 느려지게 되면 그를 제외한 태스크들이 해당 태스크의 수행 완료 시까지 대기해야 하므로 지연 태스크를 탐지해내는 일은 맵리듀스 작업의 효율성을 위해 매우 중요하다. \n\n* 기존의 모니터링 기술은 CPU나 메모리와 같은 리소스 사용량을 모니터링하여 병목 노드를 탐지해냈다. 그러나 이는 일반적이고 개략적인 정보로서, 이 정보만으로 맵리듀스 작업에서의 병목을 일으키는 태스크를 탐지해내고 원인을 분석해내기에는 정확도가 매우 떨어진다. 따라서 본 발명은 기존 기술과의 차별화를 위해 모니터링 대상을 다각적으로 분석하여 확장함으로써 병목 원인 분석 및 병목 태스크 탐지의 정확도를 향상시켰다.\n\n=== # 본 발명의 특징 / 효과 ===\n\n* 발명 기술에서는 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링한다. 지연되는 태스크가 발생하면 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계 분석하여 병목 원인 정보를 사용자에게 제공한다.\n\n* 모든 단계의 모니터링 및 분석 정보를 자동화하여 사용자에게 제공하기 위해서는 계층적 정보들 중 성능 저하와 깊은 연관을 보이는 특정 리소스를 탐지하는 것이 중요하다. 즉, 병목이 발생한 노드의 리소스 사용률, 병목을 일으킨 작업의 하둡 환경, 그리고 병목을 일으킨 태스크의 I/O 특성 정보는기계 학습 기법을 통해 병목이 발생한 것으로 예측되는 태스크를 탐지할 수 있으며, 기계에 의해 자동적으로 사용자에게 제공되는 메커니즘을 구성할 수 있다. 이 때의 메커니즘은 I/O intensive한 곳을 알아내기 위해 전체 모니터링 parameter들 중에서 비정상적인 결과를 나타내는 리소스 특성을 효과적으로 선정할 수 있어야 한다. 사용자들은 이러한 분석 결과를 통해 하둡 설정을 변경하는 것이 효율적일지 혹은 리소스가 충분한 다른 노드에서 실행시키는 것이 효율적일지 결정할 수 있게 된다.\n\n=== # 대표 청구항 ===\n\n\n* 맵리듀스 작업에서 병목 탐지 및 자동적인 원인 분석을 위한 모니터링 프레임워크 기술로서,\n: 맵리듀스 작업을 실행중인 각 노드에 대해 리소스 사용량, 하둡 설정 값, 그리고 I/O 특성의 계층적 정보를 모니터링하여, 지연되는 태스크가 발생하면 자동적으로 해당 태스크의 I/O 특성을 추출한 후, 리소스 사용량 및 하둡 설정 값과 같은 상위 수준의 정보와 연계하여 분석하여 성능 저하와 깊은 연관을 보이는 요소를 탐지할 수 있게 하는 방법 및 시스템 구조\n\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* 검색식\n: Hadoop MapReduce I/O bottleneck\n: 44 results @ Google Patent Search\n\n=== # Memo / Questions ===\n\n\n==== References ====\n\n* [https://www.usenix.org/conference/hotcloud12/predicting-execution-bottlenecks-map-reduce-clusters Predicting Execution Bottlenecks in Map-Reduce Clusters, Yahoo! Labs, USENIX HotCloud 2012]\n\n* [http://www.nagios.org/ Nagios - Industry Standard in IT Infrastructure Monitoring]\n\n* [http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems Comparison of network monitoring systems]\n\n* [http://blogs.msdn.com/b/ntdebugging/archive/2008/04/03/windows-performance-toolkit-xperf.aspx Windows Performance Toolkit - Xperf]\n\n* [http://sqlvelocity.typepad.com/blog/2011/06/windows-io-tracing.html Windows I/O Tracing (2011-06-22)]\n\n==== 발명자 인적사항 ====\n\n <pre>\n성명: 엄현상 \n주민번호: 690721-1051717\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 016-232-4667\n유선전화번호: 02-880-6755\n이메일: hseom@cse.snu.ac.kr\n주소: 서울시 관악구 관악로 1, (봉천동 서울대교수아파트) 122H동 103호\n\n성명: 조인순 (Jo, Insoon)\n주민번호: 750608-2029511\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-5131-7886\n유선전화번호: x\n이메일: insoonjo@gmail.com\n주소: 서울시 관악구 낙성대동 162-12 206호\n\n성명: 성민영 (Sung, Minyoung)\n주민번호: 871114-2079919\n소속기관: 서울대학교\n소속부서: 컴퓨터공학부\n국적: 대한민국\n핸드폰번호: 010-4724-5304\n유선전화번호: 02-876-2159\n이메일: eunice.sung87@gmail.com\n주소: 서울시 송파구 방이1동 대림아파트 6동 901호\n\n</pre>\n\n\n\n== PATENT-BRIAN-2013-004 ==\n\n<!-- === Straggler-immune Data/Task Placement in the Distributed Environment === -->\n\n=== Straggler-immune Data Placement in the Distributed Environment (HDFS?) ===\n\n\n\n\n=== Memo ===\n\n* title candidates\n: outlier(straggler, failure, ...)-immune data placement for distributed file system\n: straggler-immune data placement in the distributed file system (DFS)\n\n\n* Assumptions // 환경\n: replica management가 있는 DFS 환경\n: replica management algorithm 변경 가능\n: MapReduce/HDFS처럼 data가 있는 곳에 computation을 보내는 구조\n: DFS 위에서 Hadoop MapReduce처럼 분산 병렬 처리를 수행하는 환경\n\n* Assumptions // Failure Characteristics \n: straggler 발생과 task의 type 간의 연관성이 있음\n: {task type, node} tuple이 \n\nHDFS 및 MapReduce 환경에서,\n\n\n특정 node와 특정 task type의 조합과 straggler 발생 빈도 간에 연관 관계가 있을 수 있을까?\n\n\n즉 straggler가 발생했던 case를 관찰했을 때,\nnode 정보만으로는 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않고,\ntask type 정보만으로도 패턴 (혹은 straggler 발생과의 연관성)이 보이지 않지만,\nnode 정보와 task type 정보를 같이 고려했을 때 straggler 발생 빈도/확률과 연관성을 볼 수 있었다면?\n\n\n\n\n* Bottleneck History Record (BHR) based approach\n: do not throw away the bottleneck experience (history). KEEP IT to use the knowledge for later use.\n\n* Replica management\n: at the very first time to place the data\nDistributed File Systems\n\n\n\n=== # 요약 ===\n\n\n=== # 발명의 이용분야 ===\n\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Enterprise 데이터 분석 클러스터.\n* Hadoop MapReduce/HDFS 혹은 이와 유사한 분산 처리 시스템이 구동되는 Public Cloud 데이터센터\n\n\n----\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* Hadoop의 기본 DFS인 HDFS (Hadoop Distributed File System)은 replica management를 수행하고 있음.\n\n\n* HDFS에서의 replica management는 3-copy replica 구성 시, 다음 몇 가지 원칙에 의해 data를 분배하고 있음.\n:- 하나의 node에 동일 block이 두 개 이상 존재하지 않는다\n:- 하나의 rack에 동일 block이 세 개 이상 존재하지 않는다\n:- 첫 번째 replica는 client가 구동하고 있는 node에 배치\n:- 두 번째 replica는 첫 번째 replica가 있던 node가 속하지 않은 다른 rack의 임의의 node에 배치\n:- 세 번째 replica는 두 번째 replica가 배치된 rack 내의 다른 node에 배치\n:- 1st-2nd-3rd replica 배치는 순차적으로 실행\n\n\n* Yahoo!의 Hadoop Cluster에서 10개월간 구동된 17만개의 MapReduce job들을 분석한 연구에 따르면 약 3% 정도의 Job들이 fail되었으며, 각 task의 특성과 failure 발생과 연관성이 있음을 확인할 수 있다. (예를 들어, MapReduce 전체 failure의 80% 이상이 map task에서 발생하였으며, map task failure의 36%는 array indexing error에 의한 것이었음. 그리고 reduce task 의 23%는 I/O exception에 의한 것이었음)  이러한 failure 및 straggler로 인해 task restart가 많이 발생하고 있으며, 이는 MapReduce/HDFS 성능을 저하시키는 주요 요인임.\n\n\n* 현존 Hadoop MapReduce/HDFS 시스템에서는 straggler가 발생했다고 판단되면, 해당 task를 drop 시키고 새로운 node에서 task가 실행될 수 있도록 하고 있음. 즉, 이미 발생한 straggler에 대해서는 speculative execution을 수행하고 있지만 여전히 한계점은 존재함. (1) speculative execution을 위해 필요한 additional data copy overhead 존재. (2) 새로운 execution node를 찾는다 하더라도 그 node가 straggler-free한 node인지 보장할 수 없기 때문에, 제2, 제3의 straggler가 발생할 가능성 존재. (3) additional copy를 하려고 하더라도 이미 task들이 tight하게 많이 구동되고 있는 상황에서, 여유 execution slot을 가지고 있는 적절한 node를 바로 찾지 못할 수 있으며, 이렇게 기다리는 것 자체가 job completion time을 증가시키는 부정적인 효과가 있음.\n\n\n----\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n* 특징\n:- 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n:- {node, task-type, bottleneck-risk-score} tuple로 구성되는 Bottleneck History Record (BHR) 정보에 기반하여 해당 task-type에 대해 bottleneck-risk-score 값이 가장 작은 node에 replica data를 배치하는 방식임\n:- {node, task-type}에 대한 bottleneck-risk-score 정보는 JobTracker에 추가되는 Bottleneck History Record Manager 모듈에 의해 update됨\n:- 참고로, 기존 HDFS에서의 data placement (replica management 시)에서는 이러한 straggler-immune data placement는 고려되어 있지 않음.\n\n\n* 기대 효과\n:- speculative execution을 위한 data copy overhead 감소\n:- 제2, 제3의 straggler / failure 발생 확률 감소\n:- 결과적으로, Hadoop MapReduce Job의 성능 향상 효과\n\n\n* 구현의 용이성\n:- Apache Jira HDFS-385에서 언급된 pluggable interface를 이용 시, 본 발명에서 제안하는 data placement 알고리즘을 HDFS의 block placement algorithm으로 추가하기가 용이함.\n\n\n* 침해 적발의 용이성\n:- namenode에서 수집하는 데이터들을 관찰하거나, HDFS-385 interface를 통해 오가는 데이터들을 관찰하였을 때, node, task-type, failure rate 정보를 namenode가 (혹은 pluggable data placement module이) 읽어들이고, 그 중 가장 failure rate 값이 낮은 하나의 node가 replica data를 저장하기 위한 datanode로 선택된다면, 본 특허를 침해한 것으로 판단 가능.\n\n\n----\n\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n\n\n* 기존처럼 straggler가 발생한 후에 대응하기 보다는 (사후 대응), HDFS에 처음 data가 놓여질 때부터 straggler-immune node에 배치될 수 있도록 하는 사전 대응 방식을 특징으로 하고 있음.\n\n\n* {node, task-type, failure rate} tuple 정보에 기반하여 해당 task-type에 대해 failure rate이 최소화 될 수 있는 node에 replica data를 배치하는 방식임\n\n\n==== 시스템 구성 요소 ====\n\n\n==== 처리 절차 ====\n\n\n==== 예상 효과 ====\n\n* Anomaly (straggler, failed task) 발생 자체를 줄임으로써, Straggler 혹은 Failed Task로 인해 야기되는 추가 처리 비용을 감소시킬 수 있음.\n\n* Straggler의 경우, speculative execution으로 야기되는 additional data copy overhead 감소 (사라지거나 감소됨) 효과를 기대할 수 있음. 특히, HDFS block size가 큰 경우 (처리 해야 할 data는 크지만, node의 수가 적은 경우, 64MB 이상으로 tuning하여 사용하는 경우가 많이 있음 - 그런데, 실제로 이렇게 tuning하면 어떤 장점/효과가 얼만큼 생기나?) additional data copy로 인한 overhead가 그만큼 커지게 되므로, 이 경우 straggler 발생 확률을 낮춤으로써 얻는 이득 역시 그만큼 커지게 됨.\n\n* Failed task의 경우, 상황에 따라서 두 가지 처리 옵션이 있다. 첫 번째 옵션은 failure가 발생했던 해당 node에서 task를 재시작하는 방법이며, 두 번째 옵션은 다른 node에서 task를 재시작하는 방법임.\n\n* Task failure의 원인이 해당 node의 H/W 혹은 S/W에 문제가 있는 것이 아니었다면 첫 번째 옵션을 택할 수 있다. 물론 H/W 혹은 시스템 S/W 결함이 아니었다는 것을 알수 있었어야 한다. 그러나 해당 node의 H/W 혹은 S/W에 문제가 있다는 것을 알고 있는데, 당장 다른 node의 execution slot도 여유가 없는 상황이라면, execution slot이 생길 때까지 좀 더 기다렸다가 재시작을 해야 한다. 이때, 기다려야만 하는 경우라면, 그만큼 job 처리 속도의 저하로 이어진다. 다른 노드에서 재시작을 하게 되더라도, 역시 additional data copy가 필요하며, 이로 인한 overhead는 피할 수 없다.\n\n\n----\n\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) (straggler OR fail*)\n\n* Google patent search (2 results, no relevant result)\n (HDFS OR \"hadoop distributed file system\") (((block replica) OR data) placement) bottleneck\n\n* Google patent search (5 results)\n HDFS block replica placement\n:- [http://www.google.com/patents/EP2288998A2?cl=en Directed placement of data in a redundant data storage system]\n:: Filed 9 Apr 2009 - Published 2 Mar 2011, John Howe - Omneon, Inc.\n\n\n\n* 검색식\n: mapreduce straggler (\"historical record\" OR \"history\")\n: 2건\n\n:* System and Method for Analyzing Data Records\n:: [http://www.google.com/patents/US20120215787 www.google.com/patents/US20120215787]\n:: App. - Filed 28 Feb 2012 - Published 23 Aug 2012 - Jeffrey Dean - Dean Jeffrey, Dorward Sean M, Ghemawat Sanjay, Pike Robert C, Quinlan Sean\n\n:* Scalable user clustering based on set similarity\n:: [http://www.google.com/patents/US7962529 www.google.com/patents/US7962529]\n:: Grant - Filed 6 May 2010 - Issued 14 Jun 2011 - Mayur Datar - Google Inc.\n\n=== # Memo / Questions ===\n\n==== References ====\n\n\n\n* Google search\n map reduce straggler study\n\n:- [http://static.usenix.org/event/osdi08/tech/full_papers/zaharia/zaharia_html/ Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]\n:: Matei Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz, Ion Stoica // University of California, Berkeley\n\n:- [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDAQFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2FUM%2Fpeople%2Fsrikanth%2Fdata%2FCombating%2520Outliers%2520in%2520Map-Reduce.web.pptx&ei=pq1iUer9NK6eiAfN2IHYBQ&usg=AFQjCNEOOEtTE2_nVb6f2qQN09OpoLcS5A&sig2=HakGM53pMqVB1y2PqhQZJQ Combating Outliers in Map-Reduce - Microsoft Research]\n:: Srikanth Kandula, Ganesh Ananthanarayanan, Albert Greenberg, Ion Stoica, Yi Lu, Bikas Saha, Ed Harris\n\n\n* Google search\n map reduce straggler study once relationship\n\n:- [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5493490&tag=1 An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]\n:: Soila Kavulya, Jiaqi Tan, Rajeev Gandhi and Priya Narasimhan. Carnegie Mellon Univ., Pittsburgh, PA, USA\n:: [http://www.pdl.cs.cmu.edu/PDL-FTP/associated/CMU-PDL-09-107.pdf Another Version, CMU-PDL-09-107 December 2009]\n:: [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster, CCGrid 2010]]\n\n:- [http://cseweb.ucsd.edu/~vahdat/papers/themis_socc12.pdf Themis: An I/O-Efﬁcient MapReduce // SOCC 2012]\n\n\n\n\n<br/>\n\n== PATENT-BRIAN-2013-007 ==\n\n=== data와 IO insight을 packaging 하는 기술 ===\n\n: 해당 [[data에 대한 IO pattern/insight 정보]]를 data와 함께 packaging하여 (마치 object-oriented manner) 같이 이동되게 함으로써, replication, migration, (node 간 tiering?) 등 분산 환경에서 data가 여러 노드로 이동하는 경우에도 해당 data에 대한 처리가 최적으로 이루어질 수 있도록 하는 기술\n: data에 대한 IO insight 정보로서 다음 정보가 포함될 수 있다\n\n:* data access patterns:\n::- 어떤 application이 얼마나 자주 이 data를 access하는지? (이 data를 access하는 application에 대한 정보가 없으면 insight가 잘못 적용될 수도 있을지도 모른다 - Hadoop 같은 경우는 어떻게 MapReduce Application 정보를 알 수 있을까? 혹시 MapReduce application에 대한 정보가 Hadoop layer에 가려지는 것은 아닐까? JobTracker/TaskTracker를 고려해야 할까?)\n::- 이 데이터(파일?)는 Rd(Read)-intensive 인가? Wr(write)-intensive인가?\n::- 이 데이터는 RRd(random read) / RWr(random write) / SRd(sequential read) / SWr(sequential write) 중 어느 것이 dominant한가? 혹은 Mix 되어 있다면 그 비율은 어떻게 되는가?\n\n:* data간 access pattern 연관성\n::- 이 data가 access되고 나면 어느 정도 확률로 어떤 다른 data가 access되는지?\n::- 어떤 data가 access되고 나면 어느 정도 확률로 이 data가 access되는지?\n\n:* data hot/cold history\n::- 이 data가 hot한 적이 얼마나 자주 있었나?\n::- 이 data가 hot한 시기가 어떤 패턴을 가지고 나타나는가?\n::- 한 번 hot하고 나면 이후에도 다시 hot할 가능성이 높은가?\n\n== PATENT-BRIAN-2013-005 ==\n\n=== IOWA based Proactive Data Placement 자체 특허 ===\n\n: 다양한 정보/Insight을 기반으로 Proactive하게 Data를 Placement하는 기술\n:: (Caching/Tiering in Local Case, Data Replication/Migration in Distributed Case)\n\n\n: 여기에, ML (혹은 HML까지도?)을 적용한다는 아이디어를 추가하자\n:: ML 기반의 IO Prediction\n::: ML을 통해서 예측을 한다면, 어떤어떤 정보들로부터, 어떤 예측을 해야하는 걸까?\n::: Fine-grained prediction이 말이 되는 소리인가?\n::: Coarse-grained prediction을 한다면 어느 스케일까지 fine/coarse-grained 해져야할까?\n\n\n:: ML 기반의 IO Insight (Data Placement를 위한 Macroscopic Guideline)\n::: ML을 통해서 최적의 배치를 할 수도 있는 것일까?\n:::: 가능함. 예를 들어, \"이러이러한 sign/indication을 보이는 data는 언제쯤 어떤 형태의 IO 양상을 보일 확률이 __%임\" 같은 형태의 insight이 있다면, \"이런 data는 현재 local system 뿐만 아니라 networked system의 상태도 같이 고려하여 어디에 위치시켜두는 것이 적당\" 하다는 식의 data placement를 \"proactive 하게\" 할 수 있겠음.\n::: IO Insight의 요건:\n::::# indication, as simple as possible\n::::# indication, as specific as possible\n::::# indication, efficiently traversable - corresponding indication case node들을 쉽게, 효과적으로 traversing하면서 최종 insight leaf에 도달 (Huffman code? Radix tree?)\n::::# indication, easily extensible - indication 추가 시에 변경되는 부분이 최소화될 수 있어야 함\n::: UCB study같은 형태로 나오는 것이 최선일까? 그런 형태/내용 외의 다른 것도 얻어낼 수 있을까?\n\n\n: HML이 도움이 되는 이유는 무엇일까?\n:: 굳이 HML이 아니더라도 ML 만으로도 잘 할 수 있는 범위는 어디까지일까?\n\n\n* Y1 = Proactive Data Placement\n* Y2 = Data-system-optimal Placement\n\n: Y1.x1 = 어느 address의 데이터(들)이\n:: Y1.x1.1 = 그 데이터들은 sequential access가 가능한 형태로 배열되어 있는가? (만약 그렇다면 굳이 cache시킬 필요가 있을까? 혹시 있는 건 아닐까? 정말 없을까? Sequential read하는 경우 SSD case와 HDD case를 비교해볼 필요 있음)\n:: Y1.x1.2 = 그 데이터들이 access되고 나면, ___%의 확률로 따라서 access되는 데이터들도 있지 않을까? (그렇다면, 그 놈들도 연달아서 미리 loading?)\n: Y1.x2 = 앞으로 얼마 후에\n: Y1.x3 = Access될 것인가?\n:: Y1.x3.1 = Read일까? Write일까?\n:: Y1.x3.2 = 그 얼마 후 Access되고 나서 몇 번을 더 Access될까? (이것을 알 수 있을까?)\n\n\n* X1 = IO Access Pattern\n* X2 = IO 유발자 정보\n\n\n\n\n\n\n\nproactive data placement (caching/tiering)를 위해서, layer abstraction 혹은 virtualization이 필요하지는 않을까?\n- 예를 들어, IBM의 GPFS에서 AFM (Active File Management)을 구현할 때, data의 lifecycle 및 next use에 의거한 automatic data transfer를 위해서, 기존에는 없었던 새로운 component 혹은 새로운 layer가 필요하지는 않았을까? [1][2]\n- proactive data placement 입장에서는, next IO use를 예측하거나, user/process context를 이해한다는 측면에서, 오히려 block layer보다는 file system layer에서 바라보는 것이 더욱 적절한 것은 아닐까?\nRead cache, write cache colocation을 하면 어떨까?\nadvanced tiering: access pattern-aware optimal placement (APOP)\n\n== PATENT-BRIAN-2013-006 ==\n\n=== SSD Retention Time Controlling for Caching/Tiering 특허 ===\n\n* Caching-optimal SSD Retention Time Control\n* SSD Retention Time Controlling for Caching\n\nCache-I/O의 특성에 맞도록 SSD Retention Time을 Control하는 기술.\n\n== PATENT-BRIAN-2013-XXX ==\n\n=== State Machine based Macro IO Prediction ===\n\n== PATENT-BRIAN-2013-00X ==\n\n=== Coarse-grained Spatial Locality Based SSD Cache I/O ===\n\n=== # 배경 / 기존 기술의 문제점 ===\n\n* DRAM Cache의 부족한 용량을 극복하기 위한 솔루션으로 SSD Cache가 도입되고 있음. 기본 원리는 자주 액세스되는 디스크 블록을 SSD에 Caching함으로써 HDD의 느린 I/O 속도를 극복하도록 하는 것임.\n* 그러나 블록 레이어에 구현되는 SSD Cache의 경우, Kernel에서 관리하는 Page Cache에 의해 이미 Hot data가 Serve되고 있기 때문에, Hit Ratio를 높이기에 근본적인 한계점을 가지고 있음.\n* 한편, kernel에서 관리되는 기존 Page Cache는 다음과 같은 한계점을 가지고 있음. (1) SSD 혹은 HDD에 비해 비싼 스토리지인 RAM 기반이기 때문에 다른 스토리지에 비해 작은 용량을 가지고 있는 경우가 일반적이므로 cache로 사용할 수 있는 공간의 제약이 큼. (2) 시스템에서 사용되지 않고 있는 idle RAM 영역을 이용하기 때문에, 시스템에서 구동되는 프로세스들이 요구하는 RAM 요구량이 많아지게 되면, 그만큼 page cache가 버려지게 되며 그만큼 시스템 I/O 성능의 저하가 발생하게 됨. 또한 이로 인해 성능의 편차가 불규칙하다는 단점 또한 근본적으로 가지고 있음.\n* 한편, SSD를 이용하는 page cache가 기존에 시도된 적이 있으나 매 page access 시마다 SSD에 page를 저장하는 메커니즘으로서 (synchronous I/O) NAND flash와 RAM I/O 특성 차이로 인해 기인하는 근본적인 성능적 한계점을 가지고 있음.\n\n<br/>\n\n\n=== # 본 발명의 특징 / 효과 ===\n\n\n=== # 대표 청구항 ===\n\n* (page tiering 기반의 hybrid cache에서) spatial locality에 기반한 효율적인 page cache I/O 방법\n** 고속이면서도 자원 효율적으로 spatial locality 패턴을 파악하는 방법\n***(range size를 동적으로, 혹은 서로 다르게 할 수 있는 방법은 필요 없을까?)\n\n* page chunk handling 방법\n** [host side] spatial-locality가 있는 page들이 가급적 page chunk 단위로 묶이도록 하는 방법\n** [host side] SSD 내부의 parallelism을 극대화할 수 있도록 page chunk 크기를 정하는 방법 (i.e., channel_# x erase_block_size)\n** [host side / ssd side] tier-2 page cache에 저장된 특정 page가 access될 때, 그 page가 속한 page chunk의 데이터를 같이 pre-loading 하는 방법\n*** 해당 page chunk에 대한 status update하는 것이 필요할까?\n\n* page chunk eviction 방법\n** [ssd side] tier-2 page cache에서 page chunk를 eviction 시키기 전에, page chunk 중에서 popular 한 page는 SSD에서 evict하기 전에 RAM에 올리는 것\n\n\n=== # 기술 상세 ===\n\n=== # 선행 기술 ===\n\n다음 검색식\n (Spatial locality based I/O SSD cache)\n으로 1건의 미국 공개/등록 문건을 검색할 수 있었으나,\n본 발명과 유사한 spatial locality based I/O for SSD cache 관련 선행 기술은 발견하지 못하였음.\n\n유사한 주제를 다루는 논문으로\n __\n이 있었음.\n\n그러나 __ 측면에서 본 발명과 상이함.\n\n<br/>\n\n\n=== # 침해 적발 ===\n\n== PATENT-BRIAN-2013-00X ==\nContent Repeatability-Aware SSD Cache Management\n\n=== 대표 청구항 ===\n\n== PATENT-BRIAN-2013-00X ==\n=== Likely Zone Based Page Pre-placement ===\n\n\n* periodicity의 특성을 이용해 pre-placement하는 개념도 추가할 수 있을까?\n\n* associated-likely-zone 기반의 page pre-placement (to RAM)\n\n* page cache tiering의 방향이 RAM으로부터 SSD로 가는 경우는 page eviction 시 class-2에 해당하는 page들을 SSD로 저장하는 경우임. page cache tiering의 방향이 SSD로부터 RAM으로 가는 경우는 class-1의 “currently-hot”한 page들과 매칭되는 associated-likely-zone이 존재하고, 동시에 현재 시스템에 page cache로 사용할 수 있는 RAM의 여유 공간이 존재할 때, SSD에 저장되어 있었던 해당 associated-likely-zone을 RAM에 미리 올리는 경우임. page cache로 사용할 수 있는 RAM의 여유 공간보다 associated-likely-zone의 크기가 큰 경우에는 RAM의 여유 공간 만큼만 우선적으로 RAM에 올려짐.\n\n* 이때, associated-likely-zone 중에서 우선적으로 RAM에 올려져야 할 영역을 결정하는 방법으로써, “currently-hot”한 현재 data와 지리적으로 가까운 영역의 data를 우선적으로 선택하는 방법, associated-likely-zone의 data들 중에서 access frequency가 높았던 data들을 우선적으로 선택하는 방법 등을 사용할 수 있다. 이에 대한 구체적인 방식은 본 특허와 개별적으로 구현될 수 있으므로, 별도의 특허에서 다루어진다.\n\n* associated-likely-zone은 과거의 access 패턴에 기반하여 서로 비슷한 시간대에 access되었던 data들의 set으로 구성한다. 여기서 ‘서로 비슷한 시간대’라는 개념은 window 2의 window size에 의해 결정된다.\n\n* 특정 address arange가 LZ인지, ULZ인지 구분하는 방법?\nLZ (Likely Zone)와 ULZ (Unlikely Zone)을 선정하는 방법:\naccess되는 page를 보면, 해당되는 inode정보와 이를 access한 process 정보를 알 수 있다. inode 정보를 보면, 이 page가 어떤 파일에 연결되어있는지를 알 수 있다. 이렇게 되면 그 파일이 걸쳐있는 address space 정보 (LBA range)를 알 수 있으며, 이는 likely zone의 일부로 마킹될 수 있다. 여기서 해당 파일 전체를 likely zone으로 할 것인지 해당 파일의 일부를 likely zone으로 할 것인지는 별도의 likely zone determinition rule에 의해 결정된다. 그리고 likely zone으로 마킹된 영역을 언제 class-2 t2 page cache media (e.g., SSD)에 loading할 것인지, loading한다면 likely zone 영역의 데이터들을 어떤 순서로 읽어들일지는 likely-zond loading rule에 의거하여 결정된다. LZ과 ULZ를 선정하는 빈도/시기는\n\n== PATENT-BRIAN-2013-00X ==\n\n* 하나 이상의 SSD를 Page Cache Media로 사용하는 경우, (1) SSD array manager와 연계하여 보다 효율적인 caching I/O를 달성하는 방법 (RACS 기반의 기존 SAVL을 그대로 이용하되 interfacing에 관련된 내용), 혹은 (2) 복수개의 SSD를 caching I/O에 맞도록 coordination하는 방법 (RACS 기반의 기존 SAVL에 추가적으로 caching I/O를 잘 handling할 수 있도록 하는 최적화된 I/O management 방법)\n\n\n* 복수 개의 SSD를 tier-2 page cache media로 사용하는 경우, SSD 1에 free space가 부족한데, 그 SSD 내에 cache되어 있는 page chunk들이 계속 caching해둘만한 가치가 있는 경우, page chunk migration (between SSDs)을 수행한다. 이때, tier-1 page cache 내의 page node 내의 page data location field 값은 update한다. (이때, 누가 migration을 initiation하는 것이 적절할까? host의 hybrid page cache manager? 혹은 activie SSD? 아무래도 SATA 기반의 SSD를 사용하는 경우에는 전자가 좀 더 현실적일 것으로 보임)\n\n\n* tier-2 page cache의 eviction threshold period를 두어서 RAM page cache 경우보다는 길겠지만 tier-2 page cache의 총량이 허용할 수 있는 page 용량을 감안하여 계산된 time period 동안 access 실적이 없으면 tier-2 page cache 중에서도 evictable flag를 set하게 되는데, 이때 복수개의 SSD를 tier-2 page cache media로 사용하는 경우, 전체 SSD 가용 용량을 합산해야 한다.\n\n== PATENT-BRIAN-2013-00X ==\nMultiple I/O Queue Handling for SSD Cache\n\n\n[청구항]\n\n== PATENT-BRIAN-2013-00X ==\npage chunk I/O handling mechanism for multi-tenancy SSD page cache\n\n\n[요약]\n물리적인 SSD 하나가 통째로 page cache로 사용되는 경우에는 SSD 전체를 위와 같이 나누어 사용하면 되겠으나, 하나의 SSD를 page cache media와 다른 용도로 같이 사용해야 하는 경우를 위한 구조 및 방법은 별도의 특허에서 기술하는 것으로 함. page cache media로 사용되는 공간과 다른 목적으로 사용될 공간을 별도의 partition으로 잡고, page cache media로 동작하기 위한 partition을 다시 meta 정보 영역과 page data 영역으로 slice하여 사용함. 이때, SSD는 page cache media로 partitioning된 영역에 해당하는 I/O에 대해서는 병렬성과 page chunk lookup table의 단순성을 극대화 할 수 있게 설계된 page chunk I/O 방식으로 처리할 수 있도록 하는 것이 필요함.\n\n[청구항]\n\n\n== PATENT-BRIAN-2013-00X ==\n\n=== # I/O Pattern-optimal Data Placement for Tiering ===\n\n* Automatic tiering 시, 단순히 hot data들을 fast media에 가져다 놓고 마는 것이 아니라, IO bottleneck이 미연에 방지될 수 있도록 data의 access pattern을 aware해서 차별적으로 배치하는 방법\n예) (a) random-read-intensive 한 data들, (b) sequential-write-intensive한 data들을 다른 방식으로 배치 (tiering)\n\n\n* 이에 필요한 data access pattern 모니터링/분석 방법\n데이터 수집 및 분석 시 PCIe 카드 엔진 활용 가능?\nNIC 이나 DMA를 통해서 data move가 일어나는 경우, PCIe 카드 등을 통해서 IO stream 분석\n\n\n* 이를 위해 필요한 system architecture 구조\n기본적으로 하나 이상의 SSD와 하나 이상의 HDD, 그리고 PCIe 카드, DRAM 일부 사용 방식, Tiering Mapping Table 구조, ...\n\n\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===\n\n\n<br/>\n\n\n\n== Patent pool ==\n\n=== (Tiering-based?) Proactive Data Placement ===\n* 핵심 아이디어\n: \"proactive\" 하게 data를 배치시킨다는 것 자체!\n: Local node 내에서의 vertical tiering 뿐만 아니라 분산 node들 간의 horizontal tiering도 염두에 둘 것\n\n* 경쟁 기술과의 차별화\n: EMC의 FAST 기술이 안고 있는 근본적인 한계점은 무엇일까?\n: data access 패턴의 hot/cold 만 분류해서 hot은 fast tier에, cold는 slow tier에 두는 것이 전부인가? 아니면 그것 외에 뭔가가 더 있는가?\n: 어떻게 보면 기존의 automated storage tiering은 naive proactive placement로 볼 수도 있다. (과거에 자주 access되었던 data가 앞으로도 자주 access될 것이라고 믿고 data를 이동시키는 것이므로. 그러나 근거가 부족함)\n:: 이러한 naive proactive placement 방식으로 야기되는 단점이 있다. 만약 과거에는 자주 access되었는데 마침 tiering되고 난 후에 자주 access되지 않게 되면, 괜히 SSD의 wear-out만 유발시킨 결과가 된다. 다음 tiering 주기가 돌아오기 전 까지는 계속 느린 media에서 I/O가 serve되기 때문에 그만큼 pain이 된다.\n:: 물론 read에 대해서는 cache를 적극 활용함으로써 첫 access 시에만 slow media access로 인한 penalty를 얻고 이후에는 cache가 제공하는 수준의 성능을 얻을 수 있게 된다.\n:: 그러나 write에 대해서는 문제가 다르다. HDD의 경우 sequential write 경우에는 큰 문제가 되지 않는다. 그러나 random write 특성을 가지고 있으며, 게다가 write-intensive한 I/O라면? 그런데 random write이면서 write-intensive한 경우, write되는 address range가 생각보다 넓지 않다면 어떻게 될까? 즉 특정 구간 특정 데이터에 대한 update가 빈번한 것을 의미한다.\n:: write address range가 N개의 4KB 블럭으로 이루어져 있고, Storage System 내에 물리적인 Disk 갯수가 M개 존재한다고 가정하자. 만약 M이 N보다 적절하게 커서 RAID 10을 하건, RAID 5 등으로 이루어져 있건 간데, N개의 4KB 블럭을 각각 별도의 HDD로 분산 시킴으로써 random write을 random write이 아닌 것처럼 보이게 할 수 있다면? 예를 들어 VNX 5300 처럼 SAN 박스 하나 내에 HDD가 125개가 들어있고, random하고 intensive한 write pattern이 오고 있고, write access range가 125개 이하의 address 내에서 반복되고 있다면, 매번 도달하는 random write request를 마치 RAID 0로 striping 하듯이 계속 다른 HDD로 보냄으로써 I/O de-randomization을 할 수 있을 것이다. 여기서 좀 더 나아가서 하나의 request에 대해서 HDD가 완벽하게 처리하는 데에 걸리는 시간이 10ms 이고, 매 random I/O가 도달하는 평균 시간은 (평균 가지고 되려나? 아무튼 이것은 조금 뒤에 다시 생각해보자) 1ms 라고 한다면, 이론적으로 10번의 random I/O가 지난 다음, 11번째의 random I/O가 올 때에는 처음에 write했던 HDD에다가 다시 write을 해도 된다. 즉, 그 HDD에서 직전의 I/O가 끝나기를 기다리고 있지 않아도 된다는 것이다. 그러나 이러한 방식은 나중에 scatter했던 I/O들을 다시 불러모으려고 할 때 contribution한 HDD가 다른 I/O를 serving하고 있지 않을 수 있어야 최고의 성능을 낼 수 있게 된다. 즉, HDD cluster 구성을 상당히 dynamic하게 가져갈 수 있다면 어떨까? 이러한 dynamic clustering이 정말 최고의 효과를 낼 수 있는 workload case로는 어떤 것이 있을까? data placement를 함에 있어서 결국은 어딘가에 써야 할 것이고, (slow tier로 마크된 HDD들 array에다가 data를 쓴다고 그냥 푸대접하면서 써버릴 것이 아니다) slow tier에다가 쓸 때에도 나중에 어떻게 read되고 다시 write 될 지를 고려한다면, workload 특성에 따라서 concurrent I/O의 갯수 및 address range 크기가 다를 수 있는데, 그것을 aware해서 write을 해둔다면, 비록 slow-tier에다가 write했지만, 그리 slow하지만은 않은 성능을 내게 할 수 도 있을 것이다. 이것은 proactive data placement에 대한 이야기가 아니다. proactive와는 별개로, automated storage tiering을 함에 있어서 workload characteristics를 고려한 data placement에 대한 이야기이다.\n\n\n\n\n서로 독립적인 I/O stream의 갯수가 100개 라면 (즉 100개의 process가 I/O를 쏟아내고 있는 경우), 그리고 I/O queue의 크기가 N_Q라면, 어떻게 이러한 문제를 해결할 수 있을까?  \n\n  \n:: write에 대해서는 \n:: write-intensive하다는 것의 정의는? 이러한 경우에는 in-memory write caching (OS가 허용하는 범위의 delayed write을 최대한 활용)을 이용하되 transaction 기반으 atomicity를 보장함으로써 문제를 부분적으로 해결할 수 있다. transaction으로 묶여질 수 있는 여러 번의 I/O request (write)가 끝나기 전까지는 commit되지 않은 것으로 치는 것임. Fusion IO의 atomic write은 이를 어떻게 해결하는지? \n\n만약 write commit이 매우 중요한 민감한 데이터에 대해서는 어떻게 해야할까? 그리고 SSD-internal memory buffer를 이용해서\n: SAN 장비 내에서의 HDD와 SSD 간의 tiering만 되는 것일까? (vertical tiering within local node)\n: SAN 장비 간의 tiering이 되기 위해서는 어떤 것이 더 필요할까? EMC에서 이미 그러한 기술/솔루션을 가지고 있지는 않을까? (horizontal tiering between boxes)\n\n* 다양한 사례\n: 기존에 이미 생성되어 있던 data를 미래 access 예측에 따라 위치 변동을 시키는 경우\n: Tiering을 한다고 했을 때, access 빈도에 따라서 fast tier / slow tier 간 이동시키는 것이 전부일까? 그런 것 말고 다른 차원의 tiering은 없을까?\n: 새롭게 write되려는 data를 처음부터 어디에 배치시키는 것이 좋을까?\n\n* Placement 접근 방식\n: Tiering으로 한정?\n: Tiering과 Caching과의 결합 방식? (Caching engine에게 hint를 주는 방식)\n\n* I/O Prediction 결과를 받아서 proactive하게 data를 tiering 시키는 방법 및 아키텍쳐\n* Key questions\n:- 기존엔 proactive tiering이 없었나?\n:- 만약 없었다면 어떤 어떤 부분들을 청구항으로 넣어야 할까?\n:- 만약 있었다면 어떤 차별화가 필요할까?\n\n=== I/O Workload Analyzer Engine ===\n* I/O Trace 및 다양한 시스템 정보를 기반으로 I/O를 prediction하는 방법 및 구조\n* \n\n\n== Disclosure of Invention :: Template ==\n\n<!--\n== PATENT-BRIAN-2013-00X ==\n-->\n\n=== # RAM-SSD Hybrid Page Cache Architecture ===\n=== # 발명의 이용분야 ===\n=== # 배경 / 기존 기술의 문제점 ===\n=== # 본 발명의 특징 / 효과 ===\n=== # 대표 청구항 ===\n=== # 대표 도면 ===\n=== # 도면 목록 ===\n=== # 기술 상세 ===\n=== # 침해 적발 ===\n=== # 선행 기술 ===\n=== # Memo / Questions ===','utf-8'),(1799,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n=== 과제 목표 평가 방법 객관화/구체화 ===\n\n* 데이터 Access 성능 목표 달성 기준\n: Realistic enterprise workload 인가 시, (*)\n: Data placement 및 RACS-1 기술이 적용되지 않은 vanilla SW-RAID 대비\n: 80% 속도 향상 (in IOPS) (**)\n:: (*) Realistic enterprise workload라고 했을 때, 아래 workload 인가 방안의 Case A,B,C가 해당될 수 있겠다고 보고 있으며, \'13년 9월 쯤을 고려했을 때, 현실적으로는 Case C로 가고, workload type은 DB 혹은 fileserver 쪽으로 가는 것을 생각하고 있습니다. (\'14년 정도에는 Case B나 Case A도 생각해볼 수 있을 것 같습니다)\n:: (**) metric에 대한 부분인 \"SW-RAID 대비 속도 향상 비율: 80%\"은 MBO에 이대로 명시가 되어 수정이 어렵다고 생각하여 그대로 두었습니다.  다만, 속도를 향상시키는 요소 기술로서 동작하는 것은 RACS-1 뿐만 아니라 Data Placement도 가능하겠으므로, 이를 염두에 두고 아래 내용을 작성하였습니다.\n\n\n* Workload type\n: 대표적인 enterprise workload type인 DB, FileServer, Web 중 택일하거나\n: Scientific Computing (R, Matlab, Hadoop 등)을 선택할 수 있겠음\n: VDI workload를 고려할지에 대해서는 추가 고민 필요 (아직까지는 고려하지 않고 있음)\n\n\n* Workload 인가 방안\n:- 대표적인 enterprise workload로 성능 benchmark 필요\n::- Case A) [최선] real enterprise workload 사용\n::- Case B) [차선] enterprise workload에 대한 real trace에 기반한 real workload replay 활용 (workload replayer 구현 필요)\n::- Case C) [차차선] TPC-C, filebench 등과 같이 real workload를 모방한 realistic synthesized workload 사용\n::- Case D) [차차차선] (비추천), iozone 등과 같은 random/sequential read/write I/O pattern 생성기 사용\n\n\n\n* Workload 인가 방식에 따른 환경 구성\n: Case A, B, C 모두 storage system과 network 구성은 동일하며, client 구성 방식에서 차이 존재\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n:- Case A: real workload 인가 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case B: real workload replay 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case C: TPC-C 혹은 filebench를 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n\n* Storage system interface exposed\n: SAN, NAS 방식 중 결정 필요\n\n\n* 가상화 이슈\n: 기본적으로 Hypervisor (Xen, VMware 등)를 사용 하지 않는 환경으로 구성\n\n\n* 성능 측정 방법\n:- 1-day (case A), 1-week (case B) 동안 workload를 인가하여 계산된 IOPS로 판별\n:- 가급적 경쟁사 제품의 성능측정 tool에서 제공하는 metric과 동일하게 가는 것이 좋을 것으로 보임\n::- Real enterprise workload 인가 시, iostat, sar 등 별도의 성능 측정 모듈 구동 필요\n::- Real workload replayer의 경우, total IOPS metric를 계산할 수 있도록 구현 필요\n::- TPC-C, filebench의 경우 (물론 iozone 역시) total IOPS metric을 계산하여 benchmark 결과로서 제공\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n# [http://www.cse.buffalo.edu/~okennedy/courses/.../2.2-HDFS.pptx The Hadoop Distributed File System // Konstantin Shvachko, Hairong Kuang, Sanjay Radia, Robert Chansler, Yahoo! Sunnyvale, California USA]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1800,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n=== 과제 목표 평가 방법 객관화/구체화 ===\n\n* 데이터 Access 성능 목표 달성 기준\n: Realistic enterprise workload 인가 시, (*)\n: Data placement 및 RACS-1 기술이 적용되지 않은 vanilla SW-RAID 대비\n: 80% 속도 향상 (in IOPS) (**)\n:: (*) Realistic enterprise workload라고 했을 때, 아래 workload 인가 방안의 Case A,B,C가 해당될 수 있겠다고 보고 있으며, \'13년 9월 쯤을 고려했을 때, 현실적으로는 Case C로 가고, workload type은 DB 혹은 fileserver 쪽으로 가는 것을 생각하고 있습니다. (\'14년 정도에는 Case B나 Case A도 생각해볼 수 있을 것 같습니다)\n:: (**) metric에 대한 부분인 \"SW-RAID 대비 속도 향상 비율: 80%\"은 MBO에 이대로 명시가 되어 수정이 어렵다고 생각하여 그대로 두었습니다.  다만, 속도를 향상시키는 요소 기술로서 동작하는 것은 RACS-1 뿐만 아니라 Data Placement도 가능하겠으므로, 이를 염두에 두고 아래 내용을 작성하였습니다.\n\n\n* Workload type\n: 대표적인 enterprise workload type인 DB, FileServer, Web 중 택일하거나\n: Scientific Computing (R, Matlab, Hadoop 등)을 선택할 수 있겠음\n: VDI workload를 고려할지에 대해서는 추가 고민 필요 (아직까지는 고려하지 않고 있음)\n\n\n* Workload 인가 방안\n:- 대표적인 enterprise workload로 성능 benchmark 필요\n::- Case A) [최선] real enterprise workload 사용\n::- Case B) [차선] enterprise workload에 대한 real trace에 기반한 real workload replay 활용 (workload replayer 구현 필요)\n::- Case C) [차차선] TPC-C, filebench 등과 같이 real workload를 모방한 realistic synthesized workload 사용\n::- Case D) [차차차선] (비추천), iozone 등과 같은 random/sequential read/write I/O pattern 생성기 사용\n\n\n\n* Workload 인가 방식에 따른 환경 구성\n: Case A, B, C 모두 storage system과 network 구성은 동일하며, client 구성 방식에서 차이 존재\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n:- Case A: real workload 인가 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case B: real workload replay 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case C: TPC-C 혹은 filebench를 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n\n* Storage system interface exposed\n: SAN, NAS 방식 중 결정 필요\n\n\n* 가상화 이슈\n: 기본적으로 Hypervisor (Xen, VMware 등)를 사용 하지 않는 환경으로 구성\n\n\n* 성능 측정 방법\n:- 1-day (case A), 1-week (case B) 동안 workload를 인가하여 계산된 IOPS로 판별\n:- 가급적 경쟁사 제품의 성능측정 tool에서 제공하는 metric과 동일하게 가는 것이 좋을 것으로 보임\n::- Real enterprise workload 인가 시, iostat, sar 등 별도의 성능 측정 모듈 구동 필요\n::- Real workload replayer의 경우, total IOPS metric를 계산할 수 있도록 구현 필요\n::- TPC-C, filebench의 경우 (물론 iozone 역시) total IOPS metric을 계산하여 benchmark 결과로서 제공\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n# [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=8&ved=0CG4QFjAH&url=http%3A%2F%2Fwww.cse.buffalo.edu%2F~okennedy%2Fcourses%2Fcse704fa2012%2F2.2-HDFS.pptx&ei=WRtkUfz4NOTwiAeq1oGoCA&usg=AFQjCNGLoDZhdDfLkPO1RKcLINvTG7iL9Q&sig2=pvXlwiUJr302_D-BnHrISg&cad=rjt The Hadoop Distributed File System // Konstantin Shvachko, Hairong Kuang, Sanjay Radia, Robert Chansler, Yahoo! Sunnyvale, California USA]\n# [http://storageconference.org/2010/Papers/MSST/Shvachko.pdf \"The Hadoop Distributed File System,\" MSST 2010]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1801,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n=== 과제 목표 평가 방법 객관화/구체화 ===\n\n* 데이터 Access 성능 목표 달성 기준\n: Realistic enterprise workload 인가 시, (*)\n: Data placement 및 RACS-1 기술이 적용되지 않은 vanilla SW-RAID 대비\n: 80% 속도 향상 (in IOPS) (**)\n:: (*) Realistic enterprise workload라고 했을 때, 아래 workload 인가 방안의 Case A,B,C가 해당될 수 있겠다고 보고 있으며, \'13년 9월 쯤을 고려했을 때, 현실적으로는 Case C로 가고, workload type은 DB 혹은 fileserver 쪽으로 가는 것을 생각하고 있습니다. (\'14년 정도에는 Case B나 Case A도 생각해볼 수 있을 것 같습니다)\n:: (**) metric에 대한 부분인 \"SW-RAID 대비 속도 향상 비율: 80%\"은 MBO에 이대로 명시가 되어 수정이 어렵다고 생각하여 그대로 두었습니다.  다만, 속도를 향상시키는 요소 기술로서 동작하는 것은 RACS-1 뿐만 아니라 Data Placement도 가능하겠으므로, 이를 염두에 두고 아래 내용을 작성하였습니다.\n\n\n* Workload type\n: 대표적인 enterprise workload type인 DB, FileServer, Web 중 택일하거나\n: Scientific Computing (R, Matlab, Hadoop 등)을 선택할 수 있겠음\n: VDI workload를 고려할지에 대해서는 추가 고민 필요 (아직까지는 고려하지 않고 있음)\n\n\n* Workload 인가 방안\n:- 대표적인 enterprise workload로 성능 benchmark 필요\n::- Case A) [최선] real enterprise workload 사용\n::- Case B) [차선] enterprise workload에 대한 real trace에 기반한 real workload replay 활용 (workload replayer 구현 필요)\n::- Case C) [차차선] TPC-C, filebench 등과 같이 real workload를 모방한 realistic synthesized workload 사용\n::- Case D) [차차차선] (비추천), iozone 등과 같은 random/sequential read/write I/O pattern 생성기 사용\n\n\n\n* Workload 인가 방식에 따른 환경 구성\n: Case A, B, C 모두 storage system과 network 구성은 동일하며, client 구성 방식에서 차이 존재\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n:- Case A: real workload 인가 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case B: real workload replay 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case C: TPC-C 혹은 filebench를 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n\n* Storage system interface exposed\n: SAN, NAS 방식 중 결정 필요\n\n\n* 가상화 이슈\n: 기본적으로 Hypervisor (Xen, VMware 등)를 사용 하지 않는 환경으로 구성\n\n\n* 성능 측정 방법\n:- 1-day (case A), 1-week (case B) 동안 workload를 인가하여 계산된 IOPS로 판별\n:- 가급적 경쟁사 제품의 성능측정 tool에서 제공하는 metric과 동일하게 가는 것이 좋을 것으로 보임\n::- Real enterprise workload 인가 시, iostat, sar 등 별도의 성능 측정 모듈 구동 필요\n::- Real workload replayer의 경우, total IOPS metric를 계산할 수 있도록 구현 필요\n::- TPC-C, filebench의 경우 (물론 iozone 역시) total IOPS metric을 계산하여 benchmark 결과로서 제공\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n# [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=8&ved=0CG4QFjAH&url=http%3A%2F%2Fwww.cse.buffalo.edu%2F~okennedy%2Fcourses%2Fcse704fa2012%2F2.2-HDFS.pptx&ei=WRtkUfz4NOTwiAeq1oGoCA&usg=AFQjCNGLoDZhdDfLkPO1RKcLINvTG7iL9Q&sig2=pvXlwiUJr302_D-BnHrISg&cad=rjt The Hadoop Distributed File System // Konstantin Shvachko, Hairong Kuang, Sanjay Radia, Robert Chansler, Yahoo! Sunnyvale, California USA]\n# [http://storageconference.org/2010/Papers/MSST/Shvachko.pdf \"The Hadoop Distributed File System,\" MSST 2010]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n\n\n=== SR-IOV, MR-IOV ===\n\n* [http://searchstorage.techtarget.com/video/I-O-virtualization-video-SR-IOV-MR-IOV-NICs-and-more I/O virtualization video: SR-IOV, MR-IOV, NICs and more // 2012-09-17]\n* [http://blog.scottlowe.org/2009/12/02/what-is-sr-iov/ What is SR-IOV? // 2009-12-02]\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1802,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n=== 과제 목표 평가 방법 객관화/구체화 ===\n\n* 데이터 Access 성능 목표 달성 기준\n: Realistic enterprise workload 인가 시, (*)\n: Data placement 및 RACS-1 기술이 적용되지 않은 vanilla SW-RAID 대비\n: 80% 속도 향상 (in IOPS) (**)\n:: (*) Realistic enterprise workload라고 했을 때, 아래 workload 인가 방안의 Case A,B,C가 해당될 수 있겠다고 보고 있으며, \'13년 9월 쯤을 고려했을 때, 현실적으로는 Case C로 가고, workload type은 DB 혹은 fileserver 쪽으로 가는 것을 생각하고 있습니다. (\'14년 정도에는 Case B나 Case A도 생각해볼 수 있을 것 같습니다)\n:: (**) metric에 대한 부분인 \"SW-RAID 대비 속도 향상 비율: 80%\"은 MBO에 이대로 명시가 되어 수정이 어렵다고 생각하여 그대로 두었습니다.  다만, 속도를 향상시키는 요소 기술로서 동작하는 것은 RACS-1 뿐만 아니라 Data Placement도 가능하겠으므로, 이를 염두에 두고 아래 내용을 작성하였습니다.\n\n\n* Workload type\n: 대표적인 enterprise workload type인 DB, FileServer, Web 중 택일하거나\n: Scientific Computing (R, Matlab, Hadoop 등)을 선택할 수 있겠음\n: VDI workload를 고려할지에 대해서는 추가 고민 필요 (아직까지는 고려하지 않고 있음)\n\n\n* Workload 인가 방안\n:- 대표적인 enterprise workload로 성능 benchmark 필요\n::- Case A) [최선] real enterprise workload 사용\n::- Case B) [차선] enterprise workload에 대한 real trace에 기반한 real workload replay 활용 (workload replayer 구현 필요)\n::- Case C) [차차선] TPC-C, filebench 등과 같이 real workload를 모방한 realistic synthesized workload 사용\n::- Case D) [차차차선] (비추천), iozone 등과 같은 random/sequential read/write I/O pattern 생성기 사용\n\n\n\n* Workload 인가 방식에 따른 환경 구성\n: Case A, B, C 모두 storage system과 network 구성은 동일하며, client 구성 방식에서 차이 존재\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n:- Case A: real workload 인가 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case B: real workload replay 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case C: TPC-C 혹은 filebench를 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n\n* Storage system interface exposed\n: SAN, NAS 방식 중 결정 필요\n\n\n* 가상화 이슈\n: 기본적으로 Hypervisor (Xen, VMware 등)를 사용 하지 않는 환경으로 구성\n\n\n* 성능 측정 방법\n:- 1-day (case A), 1-week (case B) 동안 workload를 인가하여 계산된 IOPS로 판별\n:- 가급적 경쟁사 제품의 성능측정 tool에서 제공하는 metric과 동일하게 가는 것이 좋을 것으로 보임\n::- Real enterprise workload 인가 시, iostat, sar 등 별도의 성능 측정 모듈 구동 필요\n::- Real workload replayer의 경우, total IOPS metric를 계산할 수 있도록 구현 필요\n::- TPC-C, filebench의 경우 (물론 iozone 역시) total IOPS metric을 계산하여 benchmark 결과로서 제공\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n# [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=8&ved=0CG4QFjAH&url=http%3A%2F%2Fwww.cse.buffalo.edu%2F~okennedy%2Fcourses%2Fcse704fa2012%2F2.2-HDFS.pptx&ei=WRtkUfz4NOTwiAeq1oGoCA&usg=AFQjCNGLoDZhdDfLkPO1RKcLINvTG7iL9Q&sig2=pvXlwiUJr302_D-BnHrISg&cad=rjt The Hadoop Distributed File System // Konstantin Shvachko, Hairong Kuang, Sanjay Radia, Robert Chansler, Yahoo! Sunnyvale, California USA]\n# [http://storageconference.org/2010/Papers/MSST/Shvachko.pdf \"The Hadoop Distributed File System,\" MSST 2010]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n\n\n=== SR-IOV, MR-IOV ===\n\n* [http://searchstorage.techtarget.com/video/I-O-virtualization-video-SR-IOV-MR-IOV-NICs-and-more I/O virtualization video: SR-IOV, MR-IOV, NICs and more ((B.GOOD)) // 2012-09-17]\n* [http://blog.scottlowe.org/2009/12/02/what-is-sr-iov/ What is SR-IOV? ((B.GOOD)) // 2009-12-02]\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8'),(1803,'== ## bNote-2013-04-09 ==\n\n=== Straggler-immune Data Placement for Distributed File System ===\n* [[Bnote patidea 2013]]\n\n\n=== 과제 목표 평가 방법 객관화/구체화 ===\n\n* 데이터 Access 성능 목표 달성 기준\n: Realistic enterprise workload 인가 시, (*)\n: Data placement 및 RACS-1 기술이 적용되지 않은 vanilla SW-RAID 대비\n: 80% 속도 향상 (in IOPS) (**)\n:: (*) Realistic enterprise workload라고 했을 때, 아래 workload 인가 방안의 Case A,B,C가 해당될 수 있겠다고 보고 있으며, \'13년 9월 쯤을 고려했을 때, 현실적으로는 Case C로 가고, workload type은 DB 혹은 fileserver 쪽으로 가는 것을 생각하고 있습니다. (\'14년 정도에는 Case B나 Case A도 생각해볼 수 있을 것 같습니다)\n:: (**) metric에 대한 부분인 \"SW-RAID 대비 속도 향상 비율: 80%\"은 MBO에 이대로 명시가 되어 수정이 어렵다고 생각하여 그대로 두었습니다.  다만, 속도를 향상시키는 요소 기술로서 동작하는 것은 RACS-1 뿐만 아니라 Data Placement도 가능하겠으므로, 이를 염두에 두고 아래 내용을 작성하였습니다.\n\n\n* Workload type\n: 대표적인 enterprise workload type인 DB, FileServer, Web 중 택일하거나\n: Scientific Computing (R, Matlab, Hadoop 등)을 선택할 수 있겠음\n: VDI workload를 고려할지에 대해서는 추가 고민 필요 (아직까지는 고려하지 않고 있음)\n\n\n* Workload 인가 방안\n:- 대표적인 enterprise workload로 성능 benchmark 필요\n::- Case A) [최선] real enterprise workload 사용\n::- Case B) [차선] enterprise workload에 대한 real trace에 기반한 real workload replay 활용 (workload replayer 구현 필요)\n::- Case C) [차차선] TPC-C, filebench 등과 같이 real workload를 모방한 realistic synthesized workload 사용\n::- Case D) [차차차선] (비추천), iozone 등과 같은 random/sequential read/write I/O pattern 생성기 사용\n\n\n\n* Workload 인가 방식에 따른 환경 구성\n: Case A, B, C 모두 storage system과 network 구성은 동일하며, client 구성 방식에서 차이 존재\n::- storage system: target storage system 1 box (comprised of controller + disk arrays)\n::- network: switching hub를 통해 1G Ethernet으로 연결 (혹은 상황에 따라 10G Ethernet으로 연결)\n\n:- Case A: real workload 인가 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case B: real workload replay 방식을 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC N 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n:- Case C: TPC-C 혹은 filebench를 이용하는 경우\n::- client: benchmark S/W가 구동되는 PC 1 대 (최소 사양: CPU 4-core, RAM 8GB)\n\n\n* Storage system interface exposed\n: SAN, NAS 방식 중 결정 필요\n\n\n* 가상화 이슈\n: 기본적으로 Hypervisor (Xen, VMware 등)를 사용 하지 않는 환경으로 구성\n\n\n* 성능 측정 방법\n:- 1-day (case A), 1-week (case B) 동안 workload를 인가하여 계산된 IOPS로 판별\n:- 가급적 경쟁사 제품의 성능측정 tool에서 제공하는 metric과 동일하게 가는 것이 좋을 것으로 보임\n::- Real enterprise workload 인가 시, iostat, sar 등 별도의 성능 측정 모듈 구동 필요\n::- Real workload replayer의 경우, total IOPS metric를 계산할 수 있도록 구현 필요\n::- TPC-C, filebench의 경우 (물론 iozone 역시) total IOPS metric을 계산하여 benchmark 결과로서 제공\n\n== ## bNote-2013-04-08 ==\n\n=== 서울대 산학 특허 처리 완료 ===\n* [[Bnote patidea 2013]]\n\n=== MapReduce Workload Analysis Study ===\n* [[Bnote PaperStudy // An Analysis of Traces from a Production MapReduce Cluster // CCGrid 2010]]\n* [[Bnote PaperStudy // Improving MapReduce Performance in Heterogeneous Environments // OSDI 2008]]\n\n\n\n== ## bNote-2013-04-05 ==\n\n=== Hadoop HDFS Data Placement ===\n\n==== How can I be sure that data is distributed evenly across the hadoop nodes? (2011-02-21) ====\n\n* Article\'s URL\n: [http://stackoverflow.com/questions/5065394/how-an-i-be-sure-that-data-is-distributed-evenly-across-the-hadoop-nodes Question (2011-02-21)]\n\n* Answer 1 (2011-02-21)\n: If your replication is set to 3, it will be put on 3 separate nodes. The number of nodes it\'s placed on is controlled by your replication factor. If you want greater distribution then you can increase the replication number by editing the $HADOOP_HOME/conf/hadoop-site.xml and changing the dfs.replication value.\n\n: \'\'\'I believe new blocks are placed almost randomly.\'\'\' There is some consideration for distribution across different racks (when \'\'\'hadoop is made aware of racks\'\'\'). There is an example (can\'t find link) that if you have replication at 3 and 2 racks, 2 blocks will be in one rack and the third block will be placed in the other rack. I would guess that there is no preference shown for what node gets the blocks in the rack.\n\n: I haven\'t seen anything indicating or stating a preference to store blocks of the same file on the same nodes.\n\n: If you are looking for ways to force balancing data across nodes (with replication at whatever value) a simple option is <span style=\"color:red\">\'\'\'$HADOOP_HOME/bin/start-balancer.sh\'\'\'</span> which will run a balancing process to move blocks around the cluster automatically. This and a few other balancing options can be found in at the Hadoop FAQs\n\n: Hope that helps.\n\n* Answer 2 (2011-02-21)\n: You can open \'\'\'HDFS Web UI on port 50070\'\'\' of Your namenode. It will show you the information about data nodes. One thing you will see there - used space per node.  If you do not have UI - you can look on the space used in the HDFS directories of the data nodes. If you have a data skew, you can run rebalancer which will solve it gradually.\n\n* Answer 3 (2013-03-01)\n: Now with \'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)] patch\'\'\', we can choose the block placement policy, so as to place all blocks of a file in the same node (and similarly for replicated nodes). Read this blog about this topic - look at the comments section.\n\n==== HDFS block replica placement in your hands now (2009-09-14) ====\n\n* Article\'s URL [http://hadoopblog.blogspot.kr/2009/09/hdfs-block-replica-placement-in-your.html HDFS block replica placement in your hands now]\n\n\n: Most Hadoop administrators set the default replication factor for their files to be three. The main assumption here is that if you keep three copies of the data, your data is safe. I have observed this to be true in the big clusters that we manage and operate. In actuality, administrators are managing two failure aspects: data corruption and data availability.\n\n\n: If all the datanodes on which the replicas of a block exist catch fire at the same time, then that data is lost and cannot be recovered. Or if an administrative error causes all the existing replicas of a block to be deleted, then it is a catastrophic failure. This is data corruption. On the other hand, if a rack switch goes down for sometime, the datanodes on that rack are in-accessible during that time. When that faulty rack switch is fixed, the data on the rack rejoins the HDFS cluster and life goes on as usual. This is a data avilability issue; in this case data was not corrupted or lost, it was just unavailable for some time. HDFS keeps three copies of a block on three different datanodes to protect against true data corruption. HDFS also tries to distribute these three replicas on more than one rack to protect against data availability issues. The fact that HDFS actively monitors any failed datanode(s) and upon failure detection immediately schedules re-replication of blocks (if needed) implies that three copies of data on three different nodes is sufficient to avoid corrupted files.\n\n\n: HDFS uses a <span style=\"color:red\">\'\'\'simple but highly effective policy\'\'\'</span> to allocate replicas for a block. If a process that is running on any of the HDFS cluster nodes open a file for writing a block, then <span style=\"color:blue\">\'\'\'one replica\'\'\'</span> of that block is allocated on the same machine on which the client is running. <span style=\"color:blue\">\'\'\'The second replica\'\'\'</span> is allocated on a randomly chosen rack that is different from the rack on which the first replica was allocated. <span style=\"color:blue\">\'\'\'The third replica\'\'\'</span> is allocated on a randomly chosen machine on the same remote rack that was chosen in the earlier step. This means that a block is present on two unique racks. One point to note is that there is <span style=\"color:red\">\'\'\'no relationship between replicas of different blocks of the same file\'\'\'</span> as far as their location is concerned. Each block is allocated independently.\n\n\n: The above algorithm is great for availability and scalability. However, there are <span style=\"color:red\">\'\'\'scenarios where co-locating many block of the same file on the same set of datanode(s) or rack(s) is beneficial\'\'\'</span> for performance reasons. For example, if many blocks of the same file are present on the same datanode(s), a single mapper instance could process all these blocks using the CombineFileInputFormat. Similarly, if a dataset contains many small files that are co-located on the same datanode(s) or rack(s), one can use CombineFileInputFormat to process all these file together by using fewer mapper instances via CombineFileInputFormat. If an application always uses one dataset with another dataset (think Hive or Pig join), then co-locating these two datasets on the same set of datanodes is beneficial.\n\n\n: Another reason when one might want to allocate replicas using a different policy is to ensure that replicas and their parity blocks truly <span style=\"color:red\">\'\'\'reside in different failure domains\'\'\'</span>. The erasure code work in HDFS could effectively bring down the physical replication factor of a file to about 1.5 (while keeping the logical replication factor at 3) if it can place replicas of all blocks in a stripe more intelligently.\n\n\n: Yet another reason, however exotic, is <span style=\"color:red\">\'\'\'to allow HDFS to place replicas based on the HeatMap\'\'\'</span> of your cluster. If one of of the node in the cluster is at a higher temperature than that of another, then it might be better to prefer the cooler node while allocating a new replica. If you want to experiment with HDFS across two data centers, you might want to try out new policies for replica placement.\n\n\n: Well, now you can finally get your hands wet! <span style=\"color:red\">\'\'\'[https://issues.apache.org/jira/browse/HDFS-385 HDFS-385 (Design a pluggable interface to place replicas of blocks in HDFS)]\'\'\'</span> is part of the Hadoop trunk and will be part of the next major HDFS 0.21 release. This feature provides a way for the adventurous developer to write Java code that specifies how HDFS should allocate replicas of blocks of a file. The API is experimental in nature, and could change in the near future if we discover any in-efficiencies in it. Please let the Hadoop community know if you need any changes in this API or if you come across novel uses of this API. \n\n: Posted by Dhruba Borthakur at 10:41 PM\n\n==== References ====\n\n# [http://developer.yahoo.com/hadoop/tutorial/module1.html#data Apache Hadoop - Data Distribution - Yahoo! Developer Network]\n# [http://developer.yahoo.com/hadoop/tutorial/module2.html Apache Hadoop - The Hadoop Distributed File System]\n# [http://www-01.ibm.com/software/data/infosphere/hadoop/hdfs/ What is the HDFS? - IBM]\n# [https://www.ibm.com/services/forms/signup.do?source=sw-infomgt&S_PKG=500016891&S_CPM=is_bdebook1_hdfs Download the entire eBook: Understanding Big Data]\n# [http://www.aosabook.org/en/hdfs.html The Hadoop Distributed File System - The Architecture of Open Source Applications]\n# [https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=8&ved=0CG4QFjAH&url=http%3A%2F%2Fwww.cse.buffalo.edu%2F~okennedy%2Fcourses%2Fcse704fa2012%2F2.2-HDFS.pptx&ei=WRtkUfz4NOTwiAeq1oGoCA&usg=AFQjCNGLoDZhdDfLkPO1RKcLINvTG7iL9Q&sig2=pvXlwiUJr302_D-BnHrISg&cad=rjt The Hadoop Distributed File System // Konstantin Shvachko, Hairong Kuang, Sanjay Radia, Robert Chansler, Yahoo! Sunnyvale, California USA]\n# [http://storageconference.org/2010/Papers/MSST/Shvachko.pdf \"The Hadoop Distributed File System,\" MSST 2010]\n\n== ## bNote-2013-04-04 ==\n\n=== Data Placement 특허 검색 ===\n\n* [http://bit.ly/YWEiwX 34 results @ Google Patent Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n==== automated storage tiering ====\n\n:* Techniques for automated storage management [http://www.google.com/patents/US8239584 EMC, Filing in Dec 2010]\n:: 전형적 automated storage tiering 기술, I/O operation 분석을 수행하고, 그 결과에 따라 다양한 configuration option들 중 하나를 결정하여 적용.\n:: I/O operation으로부터 hint를 어떻게 추출하는가?\n:: Examples\n:::- I/O 패턴이 Ave I/O size = small, 최소 30% 이상의 random writes가 있는 경우 --> RAID 0 or RAID 1 configuration 적용\n:::- I/O 패턴이 30% 이하의 Writes와 최소 30% 이상의 random I/O가 존재하는 경우 --> RAID 5 configuration 적용\n:::- I/O 패턴이 I/O rate > threshold --> SSD tier configuration 적용\n\n:* System and method for automatic storage load balancing in virtual server environments  [http://www.google.com/patents/EP2248003A1?cl=en NetApp, Filing in Dec 2008]\n:: VM들이 포함된 storage network 환경에서 주기적으로 storage load imbalances를 분석하고 조정하는 기술\n\n==== Information Management in a Networked Environment ====\n:* Environment classification and service analysis [http://www.google.com/patents/US8346748 EMC]\n:: 다양한 서비스가 구동되는 Networked 환경에서, 다양한 정보 수집을 통해 여러 service들을 orchestrate하는 기술\n\n==== Storage Capacity Management ====\n:* Storage capacity management system in dynamic area provisioning storage [http://www.google.com/patents/US8296544, Hitachi, Filing in Apr 2010]\n\n==== Power-saving ====\n:* Methods and apparatus to provision power-saving storage system [http://www.google.com/patents/US8155766 Hitachi]\n\n\n----\n\n* [http://bit.ly/YWE9K2 541 results @ Google Patents Search]\n (inassignee:\"EMC\" OR inassignee:\"NetApp\" OR inassignee:\"Hitachi\" OR inassignee:\"International Business Machines Corporation\") (automated storage (tiering OR tiered)) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDiJb 6 results @ Google Patents Search]\n inassignee:\"Emc Corporation\" (storage tier) (frequent access) (analyze OR analysis OR analyzing)\n\n* [http://bit.ly/YWDkRj 29 results @Google Patents Search]\n inassignee:\"Emc Corporation\" (automated storage (tiering OR tiered)) performance\n\n* [http://bit.ly/YWDp7B 28 results @ Google Patents Search] \n inassignee:\"Emc Corporation\" (storage (tiering OR tiered)) (frequent access)\n\n*\n\n== ## bNote-2013-04-03 ==\n\n=== Convolution, Cross-Correlation, Autocorrelation ===\n\n* [http://en.wikipedia.org/wiki/Convolution Convolution ((B.GOOD))]\n \n\n=== 서울대 산학 특허 리뷰 ===\n\n* Naive Bayesian Classifier\n:- [https://en.wikipedia.org/wiki/Naive_Bayes_classifier Naive Bayes classifier]\n:: Examples - sex classification with features of height, weight, foot size\n\n* Mails\n <pre>\n참고로 아래 메일에서,\n\n본 연구에서 사용된 ML의 input/output data model 및\n\n그것의 구체적인 예를 요청드렸었습니다만,\n\n다음과 같은 것을 생각하고 말씀 드렸었습니다.\n\n \n\n\n(여기에 있는 것은 Wikipedia에 나오는 Naive Bayes Classifier의 예제입니다)\n\n(https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\nsex height (feet) weight (lbs) foot size(inches) \nmale 6 180 12 \nmale 5.92 (5\'11\") 190 11 \nmale 5.58 (5\'7\") 170 12 \nmale 5.92 (5\'11\") 165 10 \nfemale 5 100 6 \nfemale 5.5 (5\'6\") 150 8 \nfemale 5.42 (5\'5\") 130 7 \nfemale 5.75 (5\'9\") 150 9 \n\n\n \n\nTesting\nBelow is a sample to be classified as a male or female.\n\nsex height (feet) weight (lbs) foot size(inches) \nsample 6 130 8 \n\n\n \n\nWe wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n    ........... (1)\nFor the classification as female the posterior is given by\n\n ............... (2)\n\n \n\n \n\n키, 몸무게, 발 크기 등의 feature set을 가지고 성별을 판별하는 위 예제의 경우,\n\n위 테이블과 같은 식으로 데이터의 값을 구성하여 training을 시켰을텐데,\n\n우리 연구에서는 어떤 형태의 input data가 구성되어 들어갔을지?\n\n사용되었던 실제 값들이 Table로 보여지면 더욱 좋겠습니다.\n\n정상 진행/비정상 진행 판단에 대한 equation은 어떻게 표현 가능한지? (위의 식 (1), (2) 처럼)\n\n등이 기술되면 좋겠습니다.\n\n \n\n그리고 최종적으로 std_slowdown_per_proc 및 std_total_io_per_proc의 2가지 feature가\n\n중요한 영향력을 가진 것으로 판정되었는데,\n\n그 판정 과정도 같이 기술되어야 할 것 같습니다.\n\n \n\n \n\n \n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 21:46 (GMT+09:00)\n\nTitle : Re: Re: 특허 개정안입니다.\n\n \n\n민영씨, 고생이 많으십니다.\n\n \n\n조금 전에 전화로 이야기 나누었던 것처럼,\n\n좀 더 보강할 필요가 있는 부분들을 아래에 정리해보았습니다.\n\n \n\n한 번 보시고, 궁금하신 사항이 있다면 연락 부탁드립니다.\n\n\nMapReduce 성능에 결정적인 영향을 미치는 병목 원인을\n\n자동적으로 파악할 수 있게 하는 기술로서,\n\n \n\n- 수집된 다양한 정보들을 결합하여\n\n    Straggler를 자동으로 찾아내는 분석 과정이 \n\n    한 단계 더 구체적으로 표현될 수 있으면 좋겠습니다.\n\n    (그리고 이를 위한 시스템 구조 그림 역시 필요)\n\n\n  > 시스템 리소스 상황, 하둡 Conf., Block I/O 정보 등,\n\n      수집된 각각의 정보들을 따로따로 분석하는 것이 아니라, 결합하여 분석하게 될텐데,\n\n      이 결합하여 분석하는 방법을 구체적으로 서술 및 도식화 필요.\n\n      만약 사람이 눈으로 보고 뇌로 판단하는 형태가 아니라,\n\n      기계가 자동적으로 처리하도록 SW로 구현을 한다면\n\n      그 처리 흐름은 구체적으로 어떻게 될 것인지?\n\n      (특히 Table 2에 있는 5개의 Node 정보와, 8개의 Hadoop parameter들,\n\n       9개의 I/O 정보들이 구체적으로 어떻게 결합되어\n\n       분석에 사용되는지가 그림으로 표현되면 좋겠습니다.)\n\n\n  > 이 내용을 SW로 구현한다고 했을 때, 그 SW의 모듈 구성은 어떻게 될 것인지?\n\n     그리고, 그 SW는 시스템의 어느 layer에 위치하며,\n\n     시스템에 있는 어떤 interface와 엮이게 될 것인지 등의\n\n     측면에서도 그림이 나와야 할 것 같습니다.\n\n     예를 들어보면, Ganglia의 어떤어떤 interface를 통해서 어떤 정보를 수집하여,\n\n     그것들을 Hadoop conf. 정보 중 어떤 parameter와 결합하여 분석을 하게 될 텐데,\n\n     이를 수행하는 모듈들이 그림으로 표현될 수 있을 것 같습니다.\n  \n\n\n- 그리고 추가적으로 기술되어야 할 항목들이 있습니다.\n\n \n\n(1) 병목 현상 파악을 위한 Machine Learning 알고리즘으로서 어떤 것을 사용했는지?\n\n      이번 연구에서 사용된 Naive Bayesian은 어떤 의미를 갖는 알고리즘인지?\n\n      어떤 입력을 받아서 어떤 결과값을 내어주는 알고리즘이며,\n\n      그 결과값의 해석은 어떻게 할 수 있는지?\n\n      (Supervised Learning이었다면, 어떤 식으로 Supervise를 해주었는지?) \n\n \n\n(2) ML 알고리즘의 input으로 사용하기 위해,\n\n     어떠한 Raw 데이터에 어떤 처리가 이루어졌는지?\n\n     학습 모델을 그림으로 표현 필요.\n\n     input/output 데이터 모델/타입을 그림 혹은 Table로 표현 필요.\n\n     (예를 들어, Raw Data가 Non-numeric 타입이었다고 하더라도,\n\n       SVM이나 K-means등을 사용하기 위해서는, numerical data로\n\n       encoding/representation을 해줄 필요가 있는데,\n\n       이번 ML에서는 어떤 식으로 데이터를 처리했는지?)\n\n\n------- Original Message -------\n\nSender : 정명준<brian.m.jung@samsung.com> 전문연구원/Intelligence그룹(기술원)/삼성전자\n\nDate : 2013-04-03 20:28 (GMT+09:00)\n\nTitle : Re: 특허 개정안입니다.\n\n \n\n성민영씨, 안녕하세요?\n\n보내주신 문서 잘 보았습니다.\n\n \n\n혹시 지금 잠시 통화 가능하신지요?\n\n아니면 통화 가능한 시간을 알려주셔도 좋습니다.\n\n \n\n감사합니다.\n\n정명준 드림.\n\n \n\n \n\n------- Original Message -------\n\nSender : Minyoung Sung<eunice.sung87@gmail.com>\n\nDate : 2013-04-03 16:20 (GMT+09:00)\n\nTitle : 특허 개정안입니다.\n\n \n\n안녕하세요 정전문님, 성민영입니다. \n\n특허 개정안을 첨부하였으니 검토해주시고 피드백 부탁드립니다.\n\n감사합니다.\n\n\n\n-- \n\n성민영 드림\nmysung@dcslab.snu.ac.kr\n \n</pre>\n\n== ## bNote-2013-04-02 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Per \"process_id\" plot\n\n\n\n=== SR-IOV, MR-IOV ===\n\n* [http://searchstorage.techtarget.com/video/I-O-virtualization-video-SR-IOV-MR-IOV-NICs-and-more I/O virtualization video: SR-IOV, MR-IOV, NICs and more ((B.GOOD)) // 2012-09-17]\n* [http://blog.scottlowe.org/2009/12/02/what-is-sr-iov/ What is SR-IOV? ((B.GOOD)) // 2009-12-02]\n\n== ## bNote-2013-04-01 ==\n\n=== Dominant I/O Pattern Mining ===\n\n* Objectives\n:- To find any interesting relationship between X\'s and Y\'s\n:- To find any cause-and-effect relationship between two or more types of information/data which reside in the different moment of time\n\n==== Processing T031 ====\n:- Analyze the distribution of each field item\n <pre>\na1mjjung@secm:[tdir] $ cat proc_T031.sh \n#!/bin/sh\n##proc_T031.sh\n##_ver=20130401_194105\n\n\n_infile=\"T011.msn_filesrvr.out\";\n\n\n_fld_dstr()\n{\n	if [ \"X$_field_num\" = \"X\" ]; then\n		echo \"_field_num is empty -- EXIT\";\n		exit 1;\n	fi\n	if [ \"X$_field_name\" = \"X\" ]; then\n		echo \"_field_name is empty -- EXIT\";\n		exit 1;\n	fi\n	\n	_logfile=\"T031.fld_dstr.$_field_name.log\";\n	echo -n \">>> Creating \'$_logfile\' ... $(tstamp) -> \";\n	cat $_infile | cut -d \',\' -f $_field_num | sort | uniq -c | sort -n > $_logfile;\n	tstamp;\n}\n\n\n_field_num=3; _field_name=\"process_id\"; _fld_dstr;\n_field_num=4; _field_name=\"thread_id\"; _fld_dstr;\n_field_num=5; _field_name=\"address\"; _fld_dstr;\n_field_num=7; _field_name=\"file_obj\"; _fld_dstr;\n_field_num=8; _field_name=\"full_path\"; _fld_dstr;\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ ./proc_T031.sh \n>>> Creating \'T031.fld_dstr.process_id.log\' ... 20130401_192754 -> 20130401_192825\n>>> Creating \'T031.fld_dstr.thread_id.log\' ... 20130401_192825 -> 20130401_192902\n>>> Creating \'T031.fld_dstr.address.log\' ... 20130401_192902 -> 20130401_192946\n>>> Creating \'T031.fld_dstr.file_obj.log\' ... 20130401_192946 -> 20130401_193029\n>>> Creating \'T031.fld_dstr.full_path.log\' ... 20130401_193029 -> 20130401_193110\n</pre>\n\n <pre>\na1mjjung@secm:[tdir] $ wc -l T031.fld_dstr.*\n  7425792 T031.fld_dstr.address.log\n      670 T031.fld_dstr.file_obj.log\n     2988 T031.fld_dstr.full_path.log\n      119 T031.fld_dstr.process_id.log\n      819 T031.fld_dstr.thread_id.log\n  7430388 total\n</pre>\n\n==== Comparison between T02x and T031 ====\n <pre>\na1mjjung@secm:[tdir] $ wc -l T011.msn_filesrvr.out T02*\n  29345085 T011.msn_filesrvr.out\n  29345085 T020.msn_filesrvr.A.out\n  19729611 T021.msn_filesrvr.R.out\n   9615474 T022.msn_filesrvr.W.out\n  88035255 total\n</pre>\n\n==== Discovery based on T031 (distribution characteristics for each field) ====\n\n* fld_dstr.process_id\n\n <pre>\na1mjjung@secm:[tdir] $ cat T031.fld_dstr.process_id.log \n      1     p10 ( 620)   \n      1     p5 (4292)   \n      1     p6 ( 976)   \n      1     p6 (4748)   \n      1     p7 ( 580)   \n      1     p7 ( 664)   \n      1     p7 (4640)   \n      1     p7 (4912)   \n      1     p8 (4460)   \n      1     p9 (7768)   \n      2     p10 ( 664)   \n      2     p4 (7768)   \n      2     p5 ( 976)   \n      2     p5 (7768)   \n      2     p6 (9748)   \n      2     p7 (3424)   \n      2     p7 (9748)   \n      2     p8 (3424)   \n      2     p8 (5648)   \n      2     p9 ( 676)   \n      3     p6 (3424)   \n      3     p9 (11100)   \n      4     p10 (3908)   \n      4     p9 (3424)   \n      6     p10 (1944)   \n      7     p6 ( 664)   \n      7     p8 (1944)   \n      8     p2 ( 676)   \n      8     p7 (4748)   \n     10     p2 ( 664)   \n     11     p5 ( 664)   \n     11     p8 ( 620)   \n     12     p9 (1944)   \n     13     p11 (1292)   \n     14     p6 ( 380)   \n     14     p6 (10368)   \n     17     p10 (1292)   \n     17     p6 ( 676)   \n     21     p8 (6432)   \n     23     p3 ( 676)   \n     33     p8 ( 676)   \n     34     p8 (7160)   \n     34     p8 (9752)   \n     34     p8 (9924)   \n     34     p9 (1292)   \n     35     p9 (6576)   \n     38     p5 ( 676)   \n     41     p10 (5464)   \n     41     p8 ( 712)   \n     41     p8 (10452)   \n     41     p8 (10908)   \n     41     p8 (3564)   \n     41     p8 (7808)   \n     41     p8 (7904)   \n     41     p8 (8484)   \n     41     p8 (9376)   \n     41     p8 (9956)   \n     41     p9 (10260)   \n     41     p9 (9980)   \n     42     p8 (2164)   \n     42     p8 (6412)   \n     42     p8 (7444)   \n     42     p8 (9408)   \n     42     p8 (9480)   \n     42     p9 (10968)   \n     42     p9 (3612)   \n     42     p9 (5856)   \n     42     p9 (8408)   \n     43     p9 (10592)   \n     43     p9 (8772)   \n     48     p10 (7056)   \n     48     p10 (7780)   \n     48     p8 (11020)   \n     48     p8 (5644)   \n     49     p10 (8588)   \n     49     p10 (9684)   \n     49     p8 (1012)   \n     49     p8 (8812)   \n     49     p9 (4324)   \n     49     p9 (8224)   \n     50     p8 (10088)   \n     55     p8 ( 392)   \n     55     p8 (7144)   \n     55     p9 (3920)   \n     56     p10 (8840)   \n     56     p8 (6680)   \n     56     p9 (10532)   \n     57     p4 ( 676)   \n     58     p3 ( 664)   \n     63     p7 ( 676)   \n     63     p9 (9420)   \n     64     p10 (9300)   \n     65     p10 (10376)   \n     71     p6 (1944)   \n     71     p9 (6436)   \n     72     p4 (1944)   \n     72     p9 (8132)   \n     76     p6 (8052)   \n     78     p9 ( 392)   \n     90     p4 (1216)   \n    118     p7 (1292)   \n    119     p5 (1944)   \n    126     p4 ( 664)   \n    184     p7 (1944)   \n    260     p7 ( 392)   \n    263     p5 (1216)   \n    322     p5 ( 392)   \n    325     p6 ( 392)   \n    477     p3 (10292)   \n    584     p1 (1500)   \n    646     p1 (1216)   \n    826     p2 (1216)   \n    972     p4 (1500)   \n   1428     p3 (1216)   \n   2512     p3 (1500)   \n   2900     p2 (1500)   \n  37866     p2 ( 4)   \n  78975     p1 ( 4)   \n29212972     p0 (2004) \n</pre>\n\n=== EMC Price List ===\n\n* [http://www.kernelsoftware.com/products/catalog/emc.html EMC Price List as of 18 Mar 2013]\n* [http://www.emc.com/collateral/emcwsca/master-price-list.pdf EMC WSCA Contract B27161 Price List // Effective December 1, 2012]\n* [http://storagemojo.com/storagemojos-pricing-guide/emc-price-list/ EMC Price List // StorageMojo]\n\n=== 서울대 산학 특허 리뷰 ===\n\n* Mail Sent\n\n조인순 박사님, 성민영씨, 안녕하세요?\n주말에 정말 고민/고생 많으셨습니다. ^^;;\n이제 \'GOAL\' 라인이 얼마 남지 않았네요~\n조금만 더 같이 노력하면 좋은 결과 얻을 수 있을 거라고 \'강하게\' 믿습니다!!! ^____^\n\n저의 작은 의견을 아래와 같이 보내드리오니\n연구/업무에 참고하시기 바라겠습니다.\n\n전반적으로,\n제안하는 본 발명이, 기존 기술과는 결정적으로 어느 부분이 \'차별화\'되며,\n그 차별점으로 인해 (어떤 원리에 의해서), 어느 정도 수준의 \'문제점 혁신/개선\'이 이루어질 수 있는지가\n보강되면 좋겠습니다.\n\n이렇게 되면 본 발명의 존재 가치에 대한 설득력이 크게 강화될 것 같습니다.\n(설득 대상: 기술원 내부 특허 심사, 그리고 Patent Office)\n\n다음은 좀 더 상세한 의견/가이드라인입니다.\n\n\n1. 비교대상이 되는 기존 기술 비교\n: 현재 버전의 문서에는 아직 미기재되어 있습니다만, 다음 내용이 추가되어야 합니다.\n\n:* 검색 식 및 검색 결과\n::- 검색 식도 나름 자세하게 기술되어야 합니다. (case-by-case 이겠습니다만, 통상적으로 검색 결과가 대략 수십건 이내로 떨어지도록 작성된 식이면 될 것 같습니다. 검색 결과 뿐만 아니라 검색 식도 같이 기재하는 이유는 정말 관련 특허가 검색되지 않는 경우도 있을 수 있기 때문에, 이를 뒷받침하기 위함입니다)\n::- 검색을 수행한 patent search site도 명시 부탁합니다 - e.g., Google Search, US PTO Search, ...\n\n:* 기술 비교\n::- 검색 결과 들 중, 특히 관련이 있는 것을 선정 후, 제안하는 발명과 비교.\n:::- 이 때, 가급적 apparatus (장치구조) 측면과 method (방법/순서도) 측면으로 구분하여 비교해주시면 좋습니다. 일반적으로 \'방법\'이 위주가 되는 알고리즘에 대한 특허라 하더라도 가급적이면 \'장치\'처럼 모듈 다이어그램이 있으면 특허로 \'등록\' 되기에 좀 더 수월합니다. 즉, 본 알고리즘/방법이 어떤 식으로 \'모듈\'화 되어, 시스템 내에서 어느 부분에 적용될 수 있고, 기존 대비 이러이러한 측면에서 구조적 차이가 있다는 식으로 설명될 수 있다면 좋습니다.\n:::- 당연히 \'방법\' 측면의, 차이가 있다면 그 부분도 다이어그램, 혹은 순서도등을 활용하여 명확히 차이를 짚어줄 필요가 있습니다. \n:::- 결국, 본 발명의 존재 가치를 설명하게 되는 것인데, 상세한 내용들은 뒤의 Section 2에서 자세하게 설명이 나올 것이므로, 이 Section 1에서는 핵심만을 요약해준다는 느낌으로 작성이 되면 좋겠습니다.\n\n\n2. 제안 기술 (본 발명)의 차별적 특징 및 그로인한 문제 개선 효과\n: 이 부분에 대한 도면과 기술(description)이 특히 강화가 되면 좋겠습니다.\n\n:* Section 1에서 요약되어 언급이 되긴 합니다만, 본격적으로 본 발명에 대한 존재 가치를 설명해주는 부분입니다. 가급적 도면을 많이 사용하여 메커니즘을 설명하고, 기존 기술 대비 차이점을 부각하는 것이 중요합니다. 그리고 그 차이로 인해서 어떤 장점/가치를 얻게 되는지가 명확하게 설명이 되면 좋겠습니다. 이를 통해 구조적인 특징/차별점과 방법을 심사위원들에게 설득하게 되므로 이 부분의 작성이 매우 중요합니다. (현재 그림-1,2,3 으로는 이러한 힘을 갖는 메시지가 잘 보이지 않으므로, 본 발명만의 차별적인 apparatus 및 method 측면을 잘 설명하는 그림들이 잘 나오는 것이 매우 중요합니다)\n \n:* 그리고 이 발명의 가치를 설명해주는 핵심 그림들이 몇 개가 존재하겠지만, 그 중에서도 대표 도면으로 세울 수 있는, 이 발명만의 차별적 특징을 가장 잘 담는 그림을 하나 선정해서 표기 부탁드립니다.\n\n\n3. 실험 결과\n\n:* 위에서 설명되고 주장된 내용들을 뒷받침할 수 있는 근거들이 실험 결과로서 제시되면 좋겠습니다.\n:* 그러나 현재 남은 일정 상, 추가로 실험을 더 하는 것은 무리일 수 있으므로, 기존의 실험 결과들이 이 특허의 존재가치로 잘 이어질 수 있도록, description을 매끄럽게 만들어주시면 좋겠습니다.\n\n\n4. 침해 적발\n\n:* 어떻게 하면 본 발명에 대한 침해 사실을 적발할 수 있는지에 대한 description이 상세히 기술되어야 합니다. 예전보다 이 침해적발 측면의 질문도 많이 까다로와졌는데요, 그냥 \"SW debugging으로 확인 가능합니다\", \"network traffic sniffing해서 protocol analyzer로 확인 가능합니다\" 수준으로 이야기하면 안되고, 그보다 한 단계 깊이 이야기를 해줄 수 있어야 합니다. 이 부분도 잘 정리가 되어야 합니다.\n\n \n우선 이 정도만 잘 되어도 크게 좋아질 것 같습니다.\n목요일 최종 제출 전에 수요일 정도라도 미리 \'중간\'본을 한 번 보내주셔도 좋습니다.\n그러면 \'도면\' 및 \'순서도\' 등에 대해서 추가 의견을 드릴 수 있을 것 같습니다.\n\n끝까지 성심으로 노력해주셔서 감사합니다.\n- 정명준 드림 -\n\n=== Automated Storage System Tiering // (c)2011 Evaluator Group, Inc. ===\n\n* Article [http://www.evaluatorgroup.com/wp-content/uploads/2011/03/EGI_Tiering_Comparison_Preso.pdf]\n\n* Agenda\n:- Tiering Technology for Selected Vendors\n::- Tiering vs. Cache\n:- Tiering Implementation & Messaging for Selected Vendors\n::- Compellent\n::- Dell EqualLogic\n::- EMC VNX\n::- EMC VMAX\n::- HP 3PAR\n::- HDS VSP\n::- IBM (DS8000 & V7000)\n::- NetApp\n::- Xiotech\n\n* Tiering vs. Caching\n:- Storage Tiers\n::- Implies a particular price and performance metric (?)\n::- Provides actual capacity\n::- All content resides on media\n::- Performance is limited only to media speed\n::- May be improved with Caching\n:- Caching\n::- Not considered as actual storage\n::- All capacity must be backed by non volatile media\n::- Performance limited to size of cache\n::- Limited use for random workloads\n\n* Flash as Cache\n:- Pros\n::- Relatively low cost way to add performance\n::- Automated, very little tuning required\n::- Highly effective for sequential read workloads\n::- Somewhat effective for random write workloads\n:- Cons\n::- Not as effective for random workloads, particularly reads\n::- Adds cost, without adding capacity\n::- More workloads are becoming more random\n:::- VMware drives seemingly random workloads to storage\n:::- VDI workloads are moderately random\n\n\n=== 만우절 뉴스 ===\n\n* [http://www.storagenewsletter.com/news/people/emc-and-netapp-exchange-ceo EMC and NetApp Exchange CEO: Tucci Replaces Georgens and Vice Versa]\n* [http://www.google.com/landing/nose/ Google Nose]\n\n=== 과제 정보 업데이트 ===\n\n과제명: Intelligent Large-scale Data Management\n: (구과제명) Real-Time Big Data Platform\n\n <pre>\n□ 부회장님 지시사항\n응용 Target을 명확화하고, 경쟁 우위 확보 가능한 목표를 설정할 것\n\n□ 지시사항에 따른 변경내용\n응용 Target을 스토리지 시스템 신사업으로 집중하여 경쟁사 대비 차별화가 가능한 \n핵심 Seed 기술에 집중하기로 함. \n\n□ 과제의 목적\n   - 데이터의 특성 발견 및 학습에 기반한 지능적 분산 데이터 관리기술 확보\n : 스토리지 시스템의 데이터 Access 성능 혁신을 위한 데이터 I/O 패턴 예측에 기반 \n   Proactive Data Placement 기술 \n : 스토리지 시스템의 저장효율 극대화를 위한 데이터의 유사성 학습 기반 데이터 \n   Deduplication 기술\n\n□ 기대되는 Business Impact\n   - 스토리지 시스템은 \'16년 1,000억불 이상이 예측되는 대규모이고 안정적인 시장임\n   - 당사는 기존 NAND/SSD 부품사업에서 고부가 시스템사업으로 Value Chain을 확장 계획 \n (신사업추진단 TF에서 스토리지 시스템 신사업 기획 중)\n\n□ Goals\n   - 데이터 I/O 속도 향상 : HW RAID대비 80% 이상(\'13), 400% 이상(\'15)\n   ※ 경쟁사 : 15%(RedHat사)\n- 데이터 저장 효율 향상 : 중복제거효율 3배@Coverage 4node(\'13), 10배@100node(\'15)\n※ 경쟁사 : 3배@4node(SolidFire사) (올해는 경쟁사 동등수준을 목표로 함. )\n\n□ Challenges\n- 복잡하고 시간에 따라 변화하는 데이터 Access Log로부터 숨겨진 패턴의 발견 및 학습.\n- 대규모 스토리지 시스템에서 분산되어 있는 데이터의 효율적인 Clustering.  \n</pre>','utf-8');
/*!40000 ALTER TABLE `mw_text` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mw_trackbacks`
--

DROP TABLE IF EXISTS `mw_trackbacks`;
